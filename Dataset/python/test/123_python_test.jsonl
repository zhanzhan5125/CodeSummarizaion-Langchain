{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/bfgs_utils.py", "func_name": "_restrict_along_direction", "original_string": "def _restrict_along_direction(value_and_gradients_function,\n                              position,\n                              direction):\n  \"\"\"Restricts a function in n-dimensions to a given direction.\n\n  Suppose f: R^n -> R. Then given a point x0 and a vector p0 in R^n, the\n  restriction of the function along that direction is defined by:\n\n  ```None\n  g(t) = f(x0 + t * p0)\n  ```\n\n  This function performs this restriction on the given function. In addition, it\n  also computes the gradient of the restricted function along the restriction\n  direction. This is equivalent to computing `dg/dt` in the definition above.\n\n  Args:\n    value_and_gradients_function: Callable accepting a single real `Tensor`\n      argument of shape `[..., n]` and returning a tuple of a real `Tensor` of\n      shape `[...]` and a real `Tensor` of shape `[..., n]`. The multivariate\n      function whose restriction is to be computed. The output values of the\n      callable are the function value and the gradients at the input argument.\n    position: `Tensor` of real dtype and shape consumable by\n      `value_and_gradients_function`. Corresponds to `x0` in the definition\n      above.\n    direction: `Tensor` of the same dtype and shape as `position`. The direction\n      along which to restrict the function. Note that the direction need not\n      be a unit vector.\n\n  Returns:\n    restricted_value_and_gradients_func: A callable accepting a tensor of shape\n      broadcastable to `[...]` and same dtype as `position` and returning a\n      namedtuple of `Tensors`. The input tensor is the parameter along the\n      direction labelled `t` above. The return value contains fields:\n        x: A real `Tensor` of shape `[...]`. The input value `t` where the line\n          function was evaluated, after any necessary broadcasting.\n        f: A real `Tensor` of shape `[...]` containing the value of the\n          function at the point `position + t * direction`.\n        df: A real `Tensor` of shape `[...]` containing the derivative at\n          `position + t * direction`.\n        full_gradient: A real `Tensor` of shape `[..., n]`, the full gradient\n          of the original `value_and_gradients_function`.\n  \"\"\"\n  def _restricted_func(t):\n    t = _broadcast(t, position)\n    pt = position + tf.expand_dims(t, axis=-1) * direction\n    objective_value, gradient = value_and_gradients_function(pt)\n    return ValueAndGradient(\n        x=t,\n        f=objective_value,\n        df=tf.reduce_sum(input_tensor=gradient * direction, axis=-1),\n        full_gradient=gradient)\n\n  return _restricted_func", "language": "python", "code": "def _restrict_along_direction(value_and_gradients_function,\n                              position,\n                              direction):\n  \"\"\"Restricts a function in n-dimensions to a given direction.\n\n  Suppose f: R^n -> R. Then given a point x0 and a vector p0 in R^n, the\n  restriction of the function along that direction is defined by:\n\n  ```None\n  g(t) = f(x0 + t * p0)\n  ```\n\n  This function performs this restriction on the given function. In addition, it\n  also computes the gradient of the restricted function along the restriction\n  direction. This is equivalent to computing `dg/dt` in the definition above.\n\n  Args:\n    value_and_gradients_function: Callable accepting a single real `Tensor`\n      argument of shape `[..., n]` and returning a tuple of a real `Tensor` of\n      shape `[...]` and a real `Tensor` of shape `[..., n]`. The multivariate\n      function whose restriction is to be computed. The output values of the\n      callable are the function value and the gradients at the input argument.\n    position: `Tensor` of real dtype and shape consumable by\n      `value_and_gradients_function`. Corresponds to `x0` in the definition\n      above.\n    direction: `Tensor` of the same dtype and shape as `position`. The direction\n      along which to restrict the function. Note that the direction need not\n      be a unit vector.\n\n  Returns:\n    restricted_value_and_gradients_func: A callable accepting a tensor of shape\n      broadcastable to `[...]` and same dtype as `position` and returning a\n      namedtuple of `Tensors`. The input tensor is the parameter along the\n      direction labelled `t` above. The return value contains fields:\n        x: A real `Tensor` of shape `[...]`. The input value `t` where the line\n          function was evaluated, after any necessary broadcasting.\n        f: A real `Tensor` of shape `[...]` containing the value of the\n          function at the point `position + t * direction`.\n        df: A real `Tensor` of shape `[...]` containing the derivative at\n          `position + t * direction`.\n        full_gradient: A real `Tensor` of shape `[..., n]`, the full gradient\n          of the original `value_and_gradients_function`.\n  \"\"\"\n  def _restricted_func(t):\n    t = _broadcast(t, position)\n    pt = position + tf.expand_dims(t, axis=-1) * direction\n    objective_value, gradient = value_and_gradients_function(pt)\n    return ValueAndGradient(\n        x=t,\n        f=objective_value,\n        df=tf.reduce_sum(input_tensor=gradient * direction, axis=-1),\n        full_gradient=gradient)\n\n  return _restricted_func", "code_tokens": ["def", "_restrict_along_direction", "(", "value_and_gradients_function", ",", "position", ",", "direction", ")", ":", "def", "_restricted_func", "(", "t", ")", ":", "t", "=", "_broadcast", "(", "t", ",", "position", ")", "pt", "=", "position", "+", "tf", ".", "expand_dims", "(", "t", ",", "axis", "=", "-", "1", ")", "*", "direction", "objective_value", ",", "gradient", "=", "value_and_gradients_function", "(", "pt", ")", "return", "ValueAndGradient", "(", "x", "=", "t", ",", "f", "=", "objective_value", ",", "df", "=", "tf", ".", "reduce_sum", "(", "input_tensor", "=", "gradient", "*", "direction", ",", "axis", "=", "-", "1", ")", ",", "full_gradient", "=", "gradient", ")", "return", "_restricted_func"], "docstring": "Restricts a function in n-dimensions to a given direction.\n\n  Suppose f: R^n -> R. Then given a point x0 and a vector p0 in R^n, the\n  restriction of the function along that direction is defined by:\n\n  ```None\n  g(t) = f(x0 + t * p0)\n  ```\n\n  This function performs this restriction on the given function. In addition, it\n  also computes the gradient of the restricted function along the restriction\n  direction. This is equivalent to computing `dg/dt` in the definition above.\n\n  Args:\n    value_and_gradients_function: Callable accepting a single real `Tensor`\n      argument of shape `[..., n]` and returning a tuple of a real `Tensor` of\n      shape `[...]` and a real `Tensor` of shape `[..., n]`. The multivariate\n      function whose restriction is to be computed. The output values of the\n      callable are the function value and the gradients at the input argument.\n    position: `Tensor` of real dtype and shape consumable by\n      `value_and_gradients_function`. Corresponds to `x0` in the definition\n      above.\n    direction: `Tensor` of the same dtype and shape as `position`. The direction\n      along which to restrict the function. Note that the direction need not\n      be a unit vector.\n\n  Returns:\n    restricted_value_and_gradients_func: A callable accepting a tensor of shape\n      broadcastable to `[...]` and same dtype as `position` and returning a\n      namedtuple of `Tensors`. The input tensor is the parameter along the\n      direction labelled `t` above. The return value contains fields:\n        x: A real `Tensor` of shape `[...]`. The input value `t` where the line\n          function was evaluated, after any necessary broadcasting.\n        f: A real `Tensor` of shape `[...]` containing the value of the\n          function at the point `position + t * direction`.\n        df: A real `Tensor` of shape `[...]` containing the derivative at\n          `position + t * direction`.\n        full_gradient: A real `Tensor` of shape `[..., n]`, the full gradient\n          of the original `value_and_gradients_function`.", "docstring_tokens": ["Restricts", "a", "function", "in", "n", "-", "dimensions", "to", "a", "given", "direction", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/bfgs_utils.py#L202-L255", "partition": "test"}
{"repo": "Azure/azure-sdk-for-python", "path": "azure-servicemanagement-legacy/azure/servicemanagement/_serialization.py", "func_name": "_MinidomXmlToObject._find_namespaces_from_child", "original_string": "def _find_namespaces_from_child(parent, child, namespaces):\n        \"\"\"Recursively searches from the parent to the child,\n        gathering all the applicable namespaces along the way\"\"\"\n        for cur_child in parent.childNodes:\n            if cur_child is child:\n                return True\n            if _MinidomXmlToObject._find_namespaces_from_child(cur_child, child, namespaces):\n                # we are the parent node\n                for key in cur_child.attributes.keys():\n                    if key.startswith('xmlns:') or key == 'xmlns':\n                        namespaces[key] = cur_child.attributes[key]\n                break\n        return False", "language": "python", "code": "def _find_namespaces_from_child(parent, child, namespaces):\n        \"\"\"Recursively searches from the parent to the child,\n        gathering all the applicable namespaces along the way\"\"\"\n        for cur_child in parent.childNodes:\n            if cur_child is child:\n                return True\n            if _MinidomXmlToObject._find_namespaces_from_child(cur_child, child, namespaces):\n                # we are the parent node\n                for key in cur_child.attributes.keys():\n                    if key.startswith('xmlns:') or key == 'xmlns':\n                        namespaces[key] = cur_child.attributes[key]\n                break\n        return False", "code_tokens": ["def", "_find_namespaces_from_child", "(", "parent", ",", "child", ",", "namespaces", ")", ":", "for", "cur_child", "in", "parent", ".", "childNodes", ":", "if", "cur_child", "is", "child", ":", "return", "True", "if", "_MinidomXmlToObject", ".", "_find_namespaces_from_child", "(", "cur_child", ",", "child", ",", "namespaces", ")", ":", "# we are the parent node", "for", "key", "in", "cur_child", ".", "attributes", ".", "keys", "(", ")", ":", "if", "key", ".", "startswith", "(", "'xmlns:'", ")", "or", "key", "==", "'xmlns'", ":", "namespaces", "[", "key", "]", "=", "cur_child", ".", "attributes", "[", "key", "]", "break", "return", "False"], "docstring": "Recursively searches from the parent to the child,\n        gathering all the applicable namespaces along the way", "docstring_tokens": ["Recursively", "searches", "from", "the", "parent", "to", "the", "child", "gathering", "all", "the", "applicable", "namespaces", "along", "the", "way"], "sha": "d7306fde32f60a293a7567678692bdad31e4b667", "url": "https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicemanagement-legacy/azure/servicemanagement/_serialization.py#L420-L432", "partition": "test"}
{"repo": "neherlab/treetime", "path": "treetime/treeregression.py", "func_name": "TreeRegression.CovInv", "original_string": "def CovInv(self):\n        \"\"\"\n        Inverse of the covariance matrix\n\n        Returns\n        -------\n\n         H : (np.array)\n            inverse of the covariance matrix.\n        \"\"\"\n        self.recurse(full_matrix=True)\n        return self.tree.root.cinv", "language": "python", "code": "def CovInv(self):\n        \"\"\"\n        Inverse of the covariance matrix\n\n        Returns\n        -------\n\n         H : (np.array)\n            inverse of the covariance matrix.\n        \"\"\"\n        self.recurse(full_matrix=True)\n        return self.tree.root.cinv", "code_tokens": ["def", "CovInv", "(", "self", ")", ":", "self", ".", "recurse", "(", "full_matrix", "=", "True", ")", "return", "self", ".", "tree", ".", "root", ".", "cinv"], "docstring": "Inverse of the covariance matrix\n\n        Returns\n        -------\n\n         H : (np.array)\n            inverse of the covariance matrix.", "docstring_tokens": ["Inverse", "of", "the", "covariance", "matrix"], "sha": "f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0", "url": "https://github.com/neherlab/treetime/blob/f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0/treetime/treeregression.py#L133-L144", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/optimizer/linesearch/internal/hager_zhang_lib.py", "func_name": "_satisfies_wolfe", "original_string": "def _satisfies_wolfe(val_0,\n                     val_c,\n                     f_lim,\n                     sufficient_decrease_param,\n                     curvature_param):\n  \"\"\"Checks whether the Wolfe or approx Wolfe conditions are satisfied.\n\n  The Wolfe conditions are a set of stopping criteria for an inexact line search\n  algorithm. Let f(a) be the function value along the search direction and\n  df(a) the derivative along the search direction evaluated a distance 'a'.\n  Here 'a' is the distance along the search direction. The Wolfe conditions are:\n\n    ```None\n      f(a) <= f(0) + delta * a * df(0)   (Armijo/Sufficient decrease condition)\n      df(a) >= sigma * df(0)             (Weak curvature condition)\n    ```\n  `delta` and `sigma` are two user supplied parameters satisfying:\n   `0 < delta < sigma <= 1.`. In the following, delta is called\n   `sufficient_decrease_param` and sigma is called `curvature_param`.\n\n  On a finite precision machine, the Wolfe conditions are difficult to satisfy\n  when one is close to the minimum. Hence, Hager-Zhang propose replacing\n  the sufficient decrease condition with the following condition on the\n  derivative in the vicinity of a minimum.\n\n    ```None\n      df(a) <= (2 * delta - 1) * df(0)  (Approx Wolfe sufficient decrease)\n    ```\n  This condition is only used if one is near the minimum. This is tested using\n\n    ```None\n      f(a) <= f(0) + epsilon * |f(0)|\n    ```\n  The following function checks both the Wolfe and approx Wolfe conditions.\n  Here, `epsilon` is a small positive constant. In the following, the argument\n  `f_lim` corresponds to the product: epsilon * |f(0)|.\n\n  Args:\n    val_0: A namedtuple, as returned by value_and_gradients_function\n      evaluated at 0.\n    val_c: A namedtuple, as returned by value_and_gradients_function\n      evaluated at the point to be tested.\n    f_lim: Scalar `Tensor` of real dtype. The function value threshold for\n      the approximate Wolfe conditions to be checked.\n    sufficient_decrease_param: Positive scalar `Tensor` of real dtype.\n      Bounded above by the curvature param. Corresponds to 'delta' in the\n      terminology of [Hager and Zhang (2006)][2].\n    curvature_param: Positive scalar `Tensor` of real dtype. Bounded above\n      by `1.`. Corresponds to 'sigma' in the terminology of\n      [Hager Zhang (2005)][1].\n\n  Returns:\n    is_satisfied: A scalar boolean `Tensor` which is True if either the\n      Wolfe or approximate Wolfe conditions are satisfied.\n  \"\"\"\n  exact_wolfe_suff_dec = (sufficient_decrease_param * val_0.df >=\n                          (val_c.f - val_0.f) / val_c.x)\n  wolfe_curvature = val_c.df >= curvature_param * val_0.df\n  exact_wolfe = exact_wolfe_suff_dec & wolfe_curvature\n  approx_wolfe_applies = val_c.f <= f_lim\n  approx_wolfe_suff_dec = ((2 * sufficient_decrease_param - 1) * val_0.df\n                           >= val_c.df)\n  approx_wolfe = approx_wolfe_applies & approx_wolfe_suff_dec & wolfe_curvature\n  is_satisfied = exact_wolfe | approx_wolfe\n  return is_satisfied", "language": "python", "code": "def _satisfies_wolfe(val_0,\n                     val_c,\n                     f_lim,\n                     sufficient_decrease_param,\n                     curvature_param):\n  \"\"\"Checks whether the Wolfe or approx Wolfe conditions are satisfied.\n\n  The Wolfe conditions are a set of stopping criteria for an inexact line search\n  algorithm. Let f(a) be the function value along the search direction and\n  df(a) the derivative along the search direction evaluated a distance 'a'.\n  Here 'a' is the distance along the search direction. The Wolfe conditions are:\n\n    ```None\n      f(a) <= f(0) + delta * a * df(0)   (Armijo/Sufficient decrease condition)\n      df(a) >= sigma * df(0)             (Weak curvature condition)\n    ```\n  `delta` and `sigma` are two user supplied parameters satisfying:\n   `0 < delta < sigma <= 1.`. In the following, delta is called\n   `sufficient_decrease_param` and sigma is called `curvature_param`.\n\n  On a finite precision machine, the Wolfe conditions are difficult to satisfy\n  when one is close to the minimum. Hence, Hager-Zhang propose replacing\n  the sufficient decrease condition with the following condition on the\n  derivative in the vicinity of a minimum.\n\n    ```None\n      df(a) <= (2 * delta - 1) * df(0)  (Approx Wolfe sufficient decrease)\n    ```\n  This condition is only used if one is near the minimum. This is tested using\n\n    ```None\n      f(a) <= f(0) + epsilon * |f(0)|\n    ```\n  The following function checks both the Wolfe and approx Wolfe conditions.\n  Here, `epsilon` is a small positive constant. In the following, the argument\n  `f_lim` corresponds to the product: epsilon * |f(0)|.\n\n  Args:\n    val_0: A namedtuple, as returned by value_and_gradients_function\n      evaluated at 0.\n    val_c: A namedtuple, as returned by value_and_gradients_function\n      evaluated at the point to be tested.\n    f_lim: Scalar `Tensor` of real dtype. The function value threshold for\n      the approximate Wolfe conditions to be checked.\n    sufficient_decrease_param: Positive scalar `Tensor` of real dtype.\n      Bounded above by the curvature param. Corresponds to 'delta' in the\n      terminology of [Hager and Zhang (2006)][2].\n    curvature_param: Positive scalar `Tensor` of real dtype. Bounded above\n      by `1.`. Corresponds to 'sigma' in the terminology of\n      [Hager Zhang (2005)][1].\n\n  Returns:\n    is_satisfied: A scalar boolean `Tensor` which is True if either the\n      Wolfe or approximate Wolfe conditions are satisfied.\n  \"\"\"\n  exact_wolfe_suff_dec = (sufficient_decrease_param * val_0.df >=\n                          (val_c.f - val_0.f) / val_c.x)\n  wolfe_curvature = val_c.df >= curvature_param * val_0.df\n  exact_wolfe = exact_wolfe_suff_dec & wolfe_curvature\n  approx_wolfe_applies = val_c.f <= f_lim\n  approx_wolfe_suff_dec = ((2 * sufficient_decrease_param - 1) * val_0.df\n                           >= val_c.df)\n  approx_wolfe = approx_wolfe_applies & approx_wolfe_suff_dec & wolfe_curvature\n  is_satisfied = exact_wolfe | approx_wolfe\n  return is_satisfied", "code_tokens": ["def", "_satisfies_wolfe", "(", "val_0", ",", "val_c", ",", "f_lim", ",", "sufficient_decrease_param", ",", "curvature_param", ")", ":", "exact_wolfe_suff_dec", "=", "(", "sufficient_decrease_param", "*", "val_0", ".", "df", ">=", "(", "val_c", ".", "f", "-", "val_0", ".", "f", ")", "/", "val_c", ".", "x", ")", "wolfe_curvature", "=", "val_c", ".", "df", ">=", "curvature_param", "*", "val_0", ".", "df", "exact_wolfe", "=", "exact_wolfe_suff_dec", "&", "wolfe_curvature", "approx_wolfe_applies", "=", "val_c", ".", "f", "<=", "f_lim", "approx_wolfe_suff_dec", "=", "(", "(", "2", "*", "sufficient_decrease_param", "-", "1", ")", "*", "val_0", ".", "df", ">=", "val_c", ".", "df", ")", "approx_wolfe", "=", "approx_wolfe_applies", "&", "approx_wolfe_suff_dec", "&", "wolfe_curvature", "is_satisfied", "=", "exact_wolfe", "|", "approx_wolfe", "return", "is_satisfied"], "docstring": "Checks whether the Wolfe or approx Wolfe conditions are satisfied.\n\n  The Wolfe conditions are a set of stopping criteria for an inexact line search\n  algorithm. Let f(a) be the function value along the search direction and\n  df(a) the derivative along the search direction evaluated a distance 'a'.\n  Here 'a' is the distance along the search direction. The Wolfe conditions are:\n\n    ```None\n      f(a) <= f(0) + delta * a * df(0)   (Armijo/Sufficient decrease condition)\n      df(a) >= sigma * df(0)             (Weak curvature condition)\n    ```\n  `delta` and `sigma` are two user supplied parameters satisfying:\n   `0 < delta < sigma <= 1.`. In the following, delta is called\n   `sufficient_decrease_param` and sigma is called `curvature_param`.\n\n  On a finite precision machine, the Wolfe conditions are difficult to satisfy\n  when one is close to the minimum. Hence, Hager-Zhang propose replacing\n  the sufficient decrease condition with the following condition on the\n  derivative in the vicinity of a minimum.\n\n    ```None\n      df(a) <= (2 * delta - 1) * df(0)  (Approx Wolfe sufficient decrease)\n    ```\n  This condition is only used if one is near the minimum. This is tested using\n\n    ```None\n      f(a) <= f(0) + epsilon * |f(0)|\n    ```\n  The following function checks both the Wolfe and approx Wolfe conditions.\n  Here, `epsilon` is a small positive constant. In the following, the argument\n  `f_lim` corresponds to the product: epsilon * |f(0)|.\n\n  Args:\n    val_0: A namedtuple, as returned by value_and_gradients_function\n      evaluated at 0.\n    val_c: A namedtuple, as returned by value_and_gradients_function\n      evaluated at the point to be tested.\n    f_lim: Scalar `Tensor` of real dtype. The function value threshold for\n      the approximate Wolfe conditions to be checked.\n    sufficient_decrease_param: Positive scalar `Tensor` of real dtype.\n      Bounded above by the curvature param. Corresponds to 'delta' in the\n      terminology of [Hager and Zhang (2006)][2].\n    curvature_param: Positive scalar `Tensor` of real dtype. Bounded above\n      by `1.`. Corresponds to 'sigma' in the terminology of\n      [Hager Zhang (2005)][1].\n\n  Returns:\n    is_satisfied: A scalar boolean `Tensor` which is True if either the\n      Wolfe or approximate Wolfe conditions are satisfied.", "docstring_tokens": ["Checks", "whether", "the", "Wolfe", "or", "approx", "Wolfe", "conditions", "are", "satisfied", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/optimizer/linesearch/internal/hager_zhang_lib.py#L666-L730", "partition": "test"}
{"repo": "RRZE-HPC/kerncraft", "path": "kerncraft/cacheprediction.py", "func_name": "CacheSimulationPredictor.get_misses", "original_string": "def get_misses(self):\n        \"\"\"Return a list with number of missed cache lines per memory hierarchy level.\"\"\"\n        return [self.stats[cache_level]['MISS_count']/self.first_dim_factor\n                for cache_level in range(len(self.machine['memory hierarchy']))]", "language": "python", "code": "def get_misses(self):\n        \"\"\"Return a list with number of missed cache lines per memory hierarchy level.\"\"\"\n        return [self.stats[cache_level]['MISS_count']/self.first_dim_factor\n                for cache_level in range(len(self.machine['memory hierarchy']))]", "code_tokens": ["def", "get_misses", "(", "self", ")", ":", "return", "[", "self", ".", "stats", "[", "cache_level", "]", "[", "'MISS_count'", "]", "/", "self", ".", "first_dim_factor", "for", "cache_level", "in", "range", "(", "len", "(", "self", ".", "machine", "[", "'memory hierarchy'", "]", ")", ")", "]"], "docstring": "Return a list with number of missed cache lines per memory hierarchy level.", "docstring_tokens": ["Return", "a", "list", "with", "number", "of", "missed", "cache", "lines", "per", "memory", "hierarchy", "level", "."], "sha": "c60baf8043e4da8d8d66da7575021c2f4c6c78af", "url": "https://github.com/RRZE-HPC/kerncraft/blob/c60baf8043e4da8d8d66da7575021c2f4c6c78af/kerncraft/cacheprediction.py#L496-L499", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/grammar_vae.py", "func_name": "ProbabilisticGrammarVariational.call", "original_string": "def call(self, inputs):\n    \"\"\"Runs the model forward to return a stochastic encoding.\n\n    Args:\n      inputs: Tensor of shape [1, num_productions, num_production_rules]. It is\n        a sequence of productions of length `num_productions`. Each production\n        is a one-hot vector of length `num_production_rules`: it determines\n        which production rule the production corresponds to.\n\n    Returns:\n      latent_code_posterior: A random variable capturing a sample from the\n        variational distribution, of shape [1, self.latent_size].\n    \"\"\"\n    net = self.encoder_net(tf.cast(inputs, tf.float32))\n    return ed.MultivariateNormalDiag(\n        loc=net[..., :self.latent_size],\n        scale_diag=tf.nn.softplus(net[..., self.latent_size:]),\n        name=\"latent_code_posterior\")", "language": "python", "code": "def call(self, inputs):\n    \"\"\"Runs the model forward to return a stochastic encoding.\n\n    Args:\n      inputs: Tensor of shape [1, num_productions, num_production_rules]. It is\n        a sequence of productions of length `num_productions`. Each production\n        is a one-hot vector of length `num_production_rules`: it determines\n        which production rule the production corresponds to.\n\n    Returns:\n      latent_code_posterior: A random variable capturing a sample from the\n        variational distribution, of shape [1, self.latent_size].\n    \"\"\"\n    net = self.encoder_net(tf.cast(inputs, tf.float32))\n    return ed.MultivariateNormalDiag(\n        loc=net[..., :self.latent_size],\n        scale_diag=tf.nn.softplus(net[..., self.latent_size:]),\n        name=\"latent_code_posterior\")", "code_tokens": ["def", "call", "(", "self", ",", "inputs", ")", ":", "net", "=", "self", ".", "encoder_net", "(", "tf", ".", "cast", "(", "inputs", ",", "tf", ".", "float32", ")", ")", "return", "ed", ".", "MultivariateNormalDiag", "(", "loc", "=", "net", "[", "...", ",", ":", "self", ".", "latent_size", "]", ",", "scale_diag", "=", "tf", ".", "nn", ".", "softplus", "(", "net", "[", "...", ",", "self", ".", "latent_size", ":", "]", ")", ",", "name", "=", "\"latent_code_posterior\"", ")"], "docstring": "Runs the model forward to return a stochastic encoding.\n\n    Args:\n      inputs: Tensor of shape [1, num_productions, num_production_rules]. It is\n        a sequence of productions of length `num_productions`. Each production\n        is a one-hot vector of length `num_production_rules`: it determines\n        which production rule the production corresponds to.\n\n    Returns:\n      latent_code_posterior: A random variable capturing a sample from the\n        variational distribution, of shape [1, self.latent_size].", "docstring_tokens": ["Runs", "the", "model", "forward", "to", "return", "a", "stochastic", "encoding", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/grammar_vae.py#L267-L284", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/nose/suite.py", "func_name": "ContextSuiteFactory.ancestry", "original_string": "def ancestry(self, context):\n        \"\"\"Return the ancestry of the context (that is, all of the\n        packages and modules containing the context), in order of\n        descent with the outermost ancestor last.\n        This method is a generator.\n        \"\"\"\n        log.debug(\"get ancestry %s\", context)\n        if context is None:\n            return\n        # Methods include reference to module they are defined in, we\n        # don't want that, instead want the module the class is in now\n        # (classes are re-ancestored elsewhere).\n        if hasattr(context, 'im_class'):\n            context = context.im_class\n        elif hasattr(context, '__self__'):\n            context = context.__self__.__class__\n        if hasattr(context, '__module__'):\n            ancestors = context.__module__.split('.')\n        elif hasattr(context, '__name__'):\n            ancestors = context.__name__.split('.')[:-1]\n        else:\n            raise TypeError(\"%s has no ancestors?\" % context)\n        while ancestors:\n            log.debug(\" %s ancestors %s\", context, ancestors)\n            yield resolve_name('.'.join(ancestors))\n            ancestors.pop()", "language": "python", "code": "def ancestry(self, context):\n        \"\"\"Return the ancestry of the context (that is, all of the\n        packages and modules containing the context), in order of\n        descent with the outermost ancestor last.\n        This method is a generator.\n        \"\"\"\n        log.debug(\"get ancestry %s\", context)\n        if context is None:\n            return\n        # Methods include reference to module they are defined in, we\n        # don't want that, instead want the module the class is in now\n        # (classes are re-ancestored elsewhere).\n        if hasattr(context, 'im_class'):\n            context = context.im_class\n        elif hasattr(context, '__self__'):\n            context = context.__self__.__class__\n        if hasattr(context, '__module__'):\n            ancestors = context.__module__.split('.')\n        elif hasattr(context, '__name__'):\n            ancestors = context.__name__.split('.')[:-1]\n        else:\n            raise TypeError(\"%s has no ancestors?\" % context)\n        while ancestors:\n            log.debug(\" %s ancestors %s\", context, ancestors)\n            yield resolve_name('.'.join(ancestors))\n            ancestors.pop()", "code_tokens": ["def", "ancestry", "(", "self", ",", "context", ")", ":", "log", ".", "debug", "(", "\"get ancestry %s\"", ",", "context", ")", "if", "context", "is", "None", ":", "return", "# Methods include reference to module they are defined in, we", "# don't want that, instead want the module the class is in now", "# (classes are re-ancestored elsewhere).", "if", "hasattr", "(", "context", ",", "'im_class'", ")", ":", "context", "=", "context", ".", "im_class", "elif", "hasattr", "(", "context", ",", "'__self__'", ")", ":", "context", "=", "context", ".", "__self__", ".", "__class__", "if", "hasattr", "(", "context", ",", "'__module__'", ")", ":", "ancestors", "=", "context", ".", "__module__", ".", "split", "(", "'.'", ")", "elif", "hasattr", "(", "context", ",", "'__name__'", ")", ":", "ancestors", "=", "context", ".", "__name__", ".", "split", "(", "'.'", ")", "[", ":", "-", "1", "]", "else", ":", "raise", "TypeError", "(", "\"%s has no ancestors?\"", "%", "context", ")", "while", "ancestors", ":", "log", ".", "debug", "(", "\" %s ancestors %s\"", ",", "context", ",", "ancestors", ")", "yield", "resolve_name", "(", "'.'", ".", "join", "(", "ancestors", ")", ")", "ancestors", ".", "pop", "(", ")"], "docstring": "Return the ancestry of the context (that is, all of the\n        packages and modules containing the context), in order of\n        descent with the outermost ancestor last.\n        This method is a generator.", "docstring_tokens": ["Return", "the", "ancestry", "of", "the", "context", "(", "that", "is", "all", "of", "the", "packages", "and", "modules", "containing", "the", "context", ")", "in", "order", "of", "descent", "with", "the", "outermost", "ancestor", "last", ".", "This", "method", "is", "a", "generator", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/nose/suite.py#L428-L453", "partition": "test"}
{"repo": "jic-dtool/dtool-s3", "path": "dtool_s3/storagebroker.py", "func_name": "S3StorageBroker.get_item_abspath", "original_string": "def get_item_abspath(self, identifier):\n        \"\"\"Return absolute path at which item content can be accessed.\n\n        :param identifier: item identifier\n        :returns: absolute path from which the item content can be accessed\n        \"\"\"\n        admin_metadata = self.get_admin_metadata()\n        uuid = admin_metadata[\"uuid\"]\n        # Create directory for the specific dataset.\n        dataset_cache_abspath = os.path.join(self._s3_cache_abspath, uuid)\n        mkdir_parents(dataset_cache_abspath)\n\n        bucket_fpath = self.data_key_prefix + identifier\n        obj = self.s3resource.Object(self.bucket, bucket_fpath)\n        relpath = obj.get()['Metadata']['handle']\n        _, ext = os.path.splitext(relpath)\n\n        local_item_abspath = os.path.join(\n            dataset_cache_abspath,\n            identifier + ext\n        )\n        if not os.path.isfile(local_item_abspath):\n\n            tmp_local_item_abspath = local_item_abspath + \".tmp\"\n            self.s3resource.Bucket(self.bucket).download_file(\n                bucket_fpath,\n                tmp_local_item_abspath\n            )\n            os.rename(tmp_local_item_abspath, local_item_abspath)\n\n        return local_item_abspath", "language": "python", "code": "def get_item_abspath(self, identifier):\n        \"\"\"Return absolute path at which item content can be accessed.\n\n        :param identifier: item identifier\n        :returns: absolute path from which the item content can be accessed\n        \"\"\"\n        admin_metadata = self.get_admin_metadata()\n        uuid = admin_metadata[\"uuid\"]\n        # Create directory for the specific dataset.\n        dataset_cache_abspath = os.path.join(self._s3_cache_abspath, uuid)\n        mkdir_parents(dataset_cache_abspath)\n\n        bucket_fpath = self.data_key_prefix + identifier\n        obj = self.s3resource.Object(self.bucket, bucket_fpath)\n        relpath = obj.get()['Metadata']['handle']\n        _, ext = os.path.splitext(relpath)\n\n        local_item_abspath = os.path.join(\n            dataset_cache_abspath,\n            identifier + ext\n        )\n        if not os.path.isfile(local_item_abspath):\n\n            tmp_local_item_abspath = local_item_abspath + \".tmp\"\n            self.s3resource.Bucket(self.bucket).download_file(\n                bucket_fpath,\n                tmp_local_item_abspath\n            )\n            os.rename(tmp_local_item_abspath, local_item_abspath)\n\n        return local_item_abspath", "code_tokens": ["def", "get_item_abspath", "(", "self", ",", "identifier", ")", ":", "admin_metadata", "=", "self", ".", "get_admin_metadata", "(", ")", "uuid", "=", "admin_metadata", "[", "\"uuid\"", "]", "# Create directory for the specific dataset.", "dataset_cache_abspath", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_s3_cache_abspath", ",", "uuid", ")", "mkdir_parents", "(", "dataset_cache_abspath", ")", "bucket_fpath", "=", "self", ".", "data_key_prefix", "+", "identifier", "obj", "=", "self", ".", "s3resource", ".", "Object", "(", "self", ".", "bucket", ",", "bucket_fpath", ")", "relpath", "=", "obj", ".", "get", "(", ")", "[", "'Metadata'", "]", "[", "'handle'", "]", "_", ",", "ext", "=", "os", ".", "path", ".", "splitext", "(", "relpath", ")", "local_item_abspath", "=", "os", ".", "path", ".", "join", "(", "dataset_cache_abspath", ",", "identifier", "+", "ext", ")", "if", "not", "os", ".", "path", ".", "isfile", "(", "local_item_abspath", ")", ":", "tmp_local_item_abspath", "=", "local_item_abspath", "+", "\".tmp\"", "self", ".", "s3resource", ".", "Bucket", "(", "self", ".", "bucket", ")", ".", "download_file", "(", "bucket_fpath", ",", "tmp_local_item_abspath", ")", "os", ".", "rename", "(", "tmp_local_item_abspath", ",", "local_item_abspath", ")", "return", "local_item_abspath"], "docstring": "Return absolute path at which item content can be accessed.\n\n        :param identifier: item identifier\n        :returns: absolute path from which the item content can be accessed", "docstring_tokens": ["Return", "absolute", "path", "at", "which", "item", "content", "can", "be", "accessed", "."], "sha": "b92aca9b557797bb99390463982521a557a2e704", "url": "https://github.com/jic-dtool/dtool-s3/blob/b92aca9b557797bb99390463982521a557a2e704/dtool_s3/storagebroker.py#L298-L328", "partition": "test"}
{"repo": "streamlink/streamlink", "path": "src/streamlink/session.py", "func_name": "Streamlink.set_option", "original_string": "def set_option(self, key, value):\n        \"\"\"Sets general options used by plugins and streams originating\n        from this session object.\n\n        :param key: key of the option\n        :param value: value to set the option to\n\n\n        **Available options**:\n\n        ======================== =========================================\n        hds-live-edge            ( float) Specify the time live HDS\n                                 streams will start from the edge of\n                                 stream, default: ``10.0``\n\n        hds-segment-attempts     (int) How many attempts should be done\n                                 to download each HDS segment, default: ``3``\n\n        hds-segment-threads      (int) The size of the thread pool used\n                                 to download segments, default: ``1``\n\n        hds-segment-timeout      (float) HDS segment connect and read\n                                 timeout, default: ``10.0``\n\n        hds-timeout              (float) Timeout for reading data from\n                                 HDS streams, default: ``60.0``\n\n        hls-live-edge            (int) How many segments from the end\n                                 to start live streams on, default: ``3``\n\n        hls-segment-attempts     (int) How many attempts should be done\n                                 to download each HLS segment, default: ``3``\n\n        hls-segment-threads      (int) The size of the thread pool used\n                                 to download segments, default: ``1``\n\n        hls-segment-timeout      (float) HLS segment connect and read\n                                 timeout, default: ``10.0``\n\n        hls-timeout              (float) Timeout for reading data from\n                                 HLS streams, default: ``60.0``\n\n        http-proxy               (str) Specify a HTTP proxy to use for\n                                 all HTTP requests\n\n        https-proxy              (str) Specify a HTTPS proxy to use for\n                                 all HTTPS requests\n\n        http-cookies             (dict or str) A dict or a semi-colon (;)\n                                 delimited str of cookies to add to each\n                                 HTTP request, e.g. ``foo=bar;baz=qux``\n\n        http-headers             (dict or str) A dict or semi-colon (;)\n                                 delimited str of headers to add to each\n                                 HTTP request, e.g. ``foo=bar;baz=qux``\n\n        http-query-params        (dict or str) A dict or a ampersand (&)\n                                 delimited string of query parameters to\n                                 add to each HTTP request,\n                                 e.g. ``foo=bar&baz=qux``\n\n        http-trust-env           (bool) Trust HTTP settings set in the\n                                 environment, such as environment\n                                 variables (HTTP_PROXY, etc) and\n                                 ~/.netrc authentication\n\n        http-ssl-verify          (bool) Verify SSL certificates,\n                                 default: ``True``\n\n        http-ssl-cert            (str or tuple) SSL certificate to use,\n                                 can be either a .pem file (str) or a\n                                 .crt/.key pair (tuple)\n\n        http-timeout             (float) General timeout used by all HTTP\n                                 requests except the ones covered by\n                                 other options, default: ``20.0``\n\n        http-stream-timeout      (float) Timeout for reading data from\n                                 HTTP streams, default: ``60.0``\n\n        subprocess-errorlog      (bool) Log errors from subprocesses to\n                                 a file located in the temp directory\n\n        subprocess-errorlog-path (str) Log errors from subprocesses to\n                                 a specific file\n\n        ringbuffer-size          (int) The size of the internal ring\n                                 buffer used by most stream types,\n                                 default: ``16777216`` (16MB)\n\n        rtmp-proxy               (str) Specify a proxy (SOCKS) that RTMP\n                                 streams will use\n\n        rtmp-rtmpdump            (str) Specify the location of the\n                                 rtmpdump executable used by RTMP streams,\n                                 e.g. ``/usr/local/bin/rtmpdump``\n\n        rtmp-timeout             (float) Timeout for reading data from\n                                 RTMP streams, default: ``60.0``\n\n        ffmpeg-ffmpeg            (str) Specify the location of the\n                                 ffmpeg executable use by Muxing streams\n                                 e.g. ``/usr/local/bin/ffmpeg``\n\n        ffmpeg-verbose           (bool) Log stderr from ffmpeg to the\n                                 console\n\n        ffmpeg-verbose-path      (str) Specify the location of the\n                                 ffmpeg stderr log file\n\n        ffmpeg-video-transcode   (str) The codec to use if transcoding\n                                 video when muxing with ffmpeg\n                                 e.g. ``h264``\n\n        ffmpeg-audio-transcode   (str) The codec to use if transcoding\n                                 audio when muxing with ffmpeg\n                                 e.g. ``aac``\n\n        stream-segment-attempts  (int) How many attempts should be done\n                                 to download each segment, default: ``3``.\n                                 General option used by streams not\n                                 covered by other options.\n\n        stream-segment-threads   (int) The size of the thread pool used\n                                 to download segments, default: ``1``.\n                                 General option used by streams not\n                                 covered by other options.\n\n        stream-segment-timeout   (float) Segment connect and read\n                                 timeout, default: ``10.0``.\n                                 General option used by streams not\n                                 covered by other options.\n\n        stream-timeout           (float) Timeout for reading data from\n                                 stream, default: ``60.0``.\n                                 General option used by streams not\n                                 covered by other options.\n\n        locale                   (str) Locale setting, in the RFC 1766 format\n                                 eg. en_US or es_ES\n                                 default: ``system locale``.\n\n        user-input-requester     (UserInputRequester) instance of UserInputRequester\n                                 to collect input from the user at runtime. Must be\n                                 set before the plugins are loaded.\n                                 default: ``UserInputRequester``.\n        ======================== =========================================\n\n        \"\"\"\n\n        # Backwards compatibility\n        if key == \"rtmpdump\":\n            key = \"rtmp-rtmpdump\"\n        elif key == \"rtmpdump-proxy\":\n            key = \"rtmp-proxy\"\n        elif key == \"errorlog\":\n            key = \"subprocess-errorlog\"\n        elif key == \"errorlog-path\":\n            key = \"subprocess-errorlog-path\"\n\n        if key == \"http-proxy\":\n            self.http.proxies[\"http\"] = update_scheme(\"http://\", value)\n        elif key == \"https-proxy\":\n            self.http.proxies[\"https\"] = update_scheme(\"https://\", value)\n        elif key == \"http-cookies\":\n            if isinstance(value, dict):\n                self.http.cookies.update(value)\n            else:\n                self.http.parse_cookies(value)\n        elif key == \"http-headers\":\n            if isinstance(value, dict):\n                self.http.headers.update(value)\n            else:\n                self.http.parse_headers(value)\n        elif key == \"http-query-params\":\n            if isinstance(value, dict):\n                self.http.params.update(value)\n            else:\n                self.http.parse_query_params(value)\n        elif key == \"http-trust-env\":\n            self.http.trust_env = value\n        elif key == \"http-ssl-verify\":\n            self.http.verify = value\n        elif key == \"http-disable-dh\":\n            if value:\n                requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += ':!DH'\n                try:\n                    requests.packages.urllib3.contrib.pyopenssl.DEFAULT_SSL_CIPHER_LIST = \\\n                        requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS.encode(\"ascii\")\n                except AttributeError:\n                    # no ssl to disable the cipher on\n                    pass\n        elif key == \"http-ssl-cert\":\n            self.http.cert = value\n        elif key == \"http-timeout\":\n            self.http.timeout = value\n        else:\n            self.options.set(key, value)", "language": "python", "code": "def set_option(self, key, value):\n        \"\"\"Sets general options used by plugins and streams originating\n        from this session object.\n\n        :param key: key of the option\n        :param value: value to set the option to\n\n\n        **Available options**:\n\n        ======================== =========================================\n        hds-live-edge            ( float) Specify the time live HDS\n                                 streams will start from the edge of\n                                 stream, default: ``10.0``\n\n        hds-segment-attempts     (int) How many attempts should be done\n                                 to download each HDS segment, default: ``3``\n\n        hds-segment-threads      (int) The size of the thread pool used\n                                 to download segments, default: ``1``\n\n        hds-segment-timeout      (float) HDS segment connect and read\n                                 timeout, default: ``10.0``\n\n        hds-timeout              (float) Timeout for reading data from\n                                 HDS streams, default: ``60.0``\n\n        hls-live-edge            (int) How many segments from the end\n                                 to start live streams on, default: ``3``\n\n        hls-segment-attempts     (int) How many attempts should be done\n                                 to download each HLS segment, default: ``3``\n\n        hls-segment-threads      (int) The size of the thread pool used\n                                 to download segments, default: ``1``\n\n        hls-segment-timeout      (float) HLS segment connect and read\n                                 timeout, default: ``10.0``\n\n        hls-timeout              (float) Timeout for reading data from\n                                 HLS streams, default: ``60.0``\n\n        http-proxy               (str) Specify a HTTP proxy to use for\n                                 all HTTP requests\n\n        https-proxy              (str) Specify a HTTPS proxy to use for\n                                 all HTTPS requests\n\n        http-cookies             (dict or str) A dict or a semi-colon (;)\n                                 delimited str of cookies to add to each\n                                 HTTP request, e.g. ``foo=bar;baz=qux``\n\n        http-headers             (dict or str) A dict or semi-colon (;)\n                                 delimited str of headers to add to each\n                                 HTTP request, e.g. ``foo=bar;baz=qux``\n\n        http-query-params        (dict or str) A dict or a ampersand (&)\n                                 delimited string of query parameters to\n                                 add to each HTTP request,\n                                 e.g. ``foo=bar&baz=qux``\n\n        http-trust-env           (bool) Trust HTTP settings set in the\n                                 environment, such as environment\n                                 variables (HTTP_PROXY, etc) and\n                                 ~/.netrc authentication\n\n        http-ssl-verify          (bool) Verify SSL certificates,\n                                 default: ``True``\n\n        http-ssl-cert            (str or tuple) SSL certificate to use,\n                                 can be either a .pem file (str) or a\n                                 .crt/.key pair (tuple)\n\n        http-timeout             (float) General timeout used by all HTTP\n                                 requests except the ones covered by\n                                 other options, default: ``20.0``\n\n        http-stream-timeout      (float) Timeout for reading data from\n                                 HTTP streams, default: ``60.0``\n\n        subprocess-errorlog      (bool) Log errors from subprocesses to\n                                 a file located in the temp directory\n\n        subprocess-errorlog-path (str) Log errors from subprocesses to\n                                 a specific file\n\n        ringbuffer-size          (int) The size of the internal ring\n                                 buffer used by most stream types,\n                                 default: ``16777216`` (16MB)\n\n        rtmp-proxy               (str) Specify a proxy (SOCKS) that RTMP\n                                 streams will use\n\n        rtmp-rtmpdump            (str) Specify the location of the\n                                 rtmpdump executable used by RTMP streams,\n                                 e.g. ``/usr/local/bin/rtmpdump``\n\n        rtmp-timeout             (float) Timeout for reading data from\n                                 RTMP streams, default: ``60.0``\n\n        ffmpeg-ffmpeg            (str) Specify the location of the\n                                 ffmpeg executable use by Muxing streams\n                                 e.g. ``/usr/local/bin/ffmpeg``\n\n        ffmpeg-verbose           (bool) Log stderr from ffmpeg to the\n                                 console\n\n        ffmpeg-verbose-path      (str) Specify the location of the\n                                 ffmpeg stderr log file\n\n        ffmpeg-video-transcode   (str) The codec to use if transcoding\n                                 video when muxing with ffmpeg\n                                 e.g. ``h264``\n\n        ffmpeg-audio-transcode   (str) The codec to use if transcoding\n                                 audio when muxing with ffmpeg\n                                 e.g. ``aac``\n\n        stream-segment-attempts  (int) How many attempts should be done\n                                 to download each segment, default: ``3``.\n                                 General option used by streams not\n                                 covered by other options.\n\n        stream-segment-threads   (int) The size of the thread pool used\n                                 to download segments, default: ``1``.\n                                 General option used by streams not\n                                 covered by other options.\n\n        stream-segment-timeout   (float) Segment connect and read\n                                 timeout, default: ``10.0``.\n                                 General option used by streams not\n                                 covered by other options.\n\n        stream-timeout           (float) Timeout for reading data from\n                                 stream, default: ``60.0``.\n                                 General option used by streams not\n                                 covered by other options.\n\n        locale                   (str) Locale setting, in the RFC 1766 format\n                                 eg. en_US or es_ES\n                                 default: ``system locale``.\n\n        user-input-requester     (UserInputRequester) instance of UserInputRequester\n                                 to collect input from the user at runtime. Must be\n                                 set before the plugins are loaded.\n                                 default: ``UserInputRequester``.\n        ======================== =========================================\n\n        \"\"\"\n\n        # Backwards compatibility\n        if key == \"rtmpdump\":\n            key = \"rtmp-rtmpdump\"\n        elif key == \"rtmpdump-proxy\":\n            key = \"rtmp-proxy\"\n        elif key == \"errorlog\":\n            key = \"subprocess-errorlog\"\n        elif key == \"errorlog-path\":\n            key = \"subprocess-errorlog-path\"\n\n        if key == \"http-proxy\":\n            self.http.proxies[\"http\"] = update_scheme(\"http://\", value)\n        elif key == \"https-proxy\":\n            self.http.proxies[\"https\"] = update_scheme(\"https://\", value)\n        elif key == \"http-cookies\":\n            if isinstance(value, dict):\n                self.http.cookies.update(value)\n            else:\n                self.http.parse_cookies(value)\n        elif key == \"http-headers\":\n            if isinstance(value, dict):\n                self.http.headers.update(value)\n            else:\n                self.http.parse_headers(value)\n        elif key == \"http-query-params\":\n            if isinstance(value, dict):\n                self.http.params.update(value)\n            else:\n                self.http.parse_query_params(value)\n        elif key == \"http-trust-env\":\n            self.http.trust_env = value\n        elif key == \"http-ssl-verify\":\n            self.http.verify = value\n        elif key == \"http-disable-dh\":\n            if value:\n                requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += ':!DH'\n                try:\n                    requests.packages.urllib3.contrib.pyopenssl.DEFAULT_SSL_CIPHER_LIST = \\\n                        requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS.encode(\"ascii\")\n                except AttributeError:\n                    # no ssl to disable the cipher on\n                    pass\n        elif key == \"http-ssl-cert\":\n            self.http.cert = value\n        elif key == \"http-timeout\":\n            self.http.timeout = value\n        else:\n            self.options.set(key, value)", "code_tokens": ["def", "set_option", "(", "self", ",", "key", ",", "value", ")", ":", "# Backwards compatibility", "if", "key", "==", "\"rtmpdump\"", ":", "key", "=", "\"rtmp-rtmpdump\"", "elif", "key", "==", "\"rtmpdump-proxy\"", ":", "key", "=", "\"rtmp-proxy\"", "elif", "key", "==", "\"errorlog\"", ":", "key", "=", "\"subprocess-errorlog\"", "elif", "key", "==", "\"errorlog-path\"", ":", "key", "=", "\"subprocess-errorlog-path\"", "if", "key", "==", "\"http-proxy\"", ":", "self", ".", "http", ".", "proxies", "[", "\"http\"", "]", "=", "update_scheme", "(", "\"http://\"", ",", "value", ")", "elif", "key", "==", "\"https-proxy\"", ":", "self", ".", "http", ".", "proxies", "[", "\"https\"", "]", "=", "update_scheme", "(", "\"https://\"", ",", "value", ")", "elif", "key", "==", "\"http-cookies\"", ":", "if", "isinstance", "(", "value", ",", "dict", ")", ":", "self", ".", "http", ".", "cookies", ".", "update", "(", "value", ")", "else", ":", "self", ".", "http", ".", "parse_cookies", "(", "value", ")", "elif", "key", "==", "\"http-headers\"", ":", "if", "isinstance", "(", "value", ",", "dict", ")", ":", "self", ".", "http", ".", "headers", ".", "update", "(", "value", ")", "else", ":", "self", ".", "http", ".", "parse_headers", "(", "value", ")", "elif", "key", "==", "\"http-query-params\"", ":", "if", "isinstance", "(", "value", ",", "dict", ")", ":", "self", ".", "http", ".", "params", ".", "update", "(", "value", ")", "else", ":", "self", ".", "http", ".", "parse_query_params", "(", "value", ")", "elif", "key", "==", "\"http-trust-env\"", ":", "self", ".", "http", ".", "trust_env", "=", "value", "elif", "key", "==", "\"http-ssl-verify\"", ":", "self", ".", "http", ".", "verify", "=", "value", "elif", "key", "==", "\"http-disable-dh\"", ":", "if", "value", ":", "requests", ".", "packages", ".", "urllib3", ".", "util", ".", "ssl_", ".", "DEFAULT_CIPHERS", "+=", "':!DH'", "try", ":", "requests", ".", "packages", ".", "urllib3", ".", "contrib", ".", "pyopenssl", ".", "DEFAULT_SSL_CIPHER_LIST", "=", "requests", ".", "packages", ".", "urllib3", ".", "util", ".", "ssl_", ".", "DEFAULT_CIPHERS", ".", "encode", "(", "\"ascii\"", ")", "except", "AttributeError", ":", "# no ssl to disable the cipher on", "pass", "elif", "key", "==", "\"http-ssl-cert\"", ":", "self", ".", "http", ".", "cert", "=", "value", "elif", "key", "==", "\"http-timeout\"", ":", "self", ".", "http", ".", "timeout", "=", "value", "else", ":", "self", ".", "options", ".", "set", "(", "key", ",", "value", ")"], "docstring": "Sets general options used by plugins and streams originating\n        from this session object.\n\n        :param key: key of the option\n        :param value: value to set the option to\n\n\n        **Available options**:\n\n        ======================== =========================================\n        hds-live-edge            ( float) Specify the time live HDS\n                                 streams will start from the edge of\n                                 stream, default: ``10.0``\n\n        hds-segment-attempts     (int) How many attempts should be done\n                                 to download each HDS segment, default: ``3``\n\n        hds-segment-threads      (int) The size of the thread pool used\n                                 to download segments, default: ``1``\n\n        hds-segment-timeout      (float) HDS segment connect and read\n                                 timeout, default: ``10.0``\n\n        hds-timeout              (float) Timeout for reading data from\n                                 HDS streams, default: ``60.0``\n\n        hls-live-edge            (int) How many segments from the end\n                                 to start live streams on, default: ``3``\n\n        hls-segment-attempts     (int) How many attempts should be done\n                                 to download each HLS segment, default: ``3``\n\n        hls-segment-threads      (int) The size of the thread pool used\n                                 to download segments, default: ``1``\n\n        hls-segment-timeout      (float) HLS segment connect and read\n                                 timeout, default: ``10.0``\n\n        hls-timeout              (float) Timeout for reading data from\n                                 HLS streams, default: ``60.0``\n\n        http-proxy               (str) Specify a HTTP proxy to use for\n                                 all HTTP requests\n\n        https-proxy              (str) Specify a HTTPS proxy to use for\n                                 all HTTPS requests\n\n        http-cookies             (dict or str) A dict or a semi-colon (;)\n                                 delimited str of cookies to add to each\n                                 HTTP request, e.g. ``foo=bar;baz=qux``\n\n        http-headers             (dict or str) A dict or semi-colon (;)\n                                 delimited str of headers to add to each\n                                 HTTP request, e.g. ``foo=bar;baz=qux``\n\n        http-query-params        (dict or str) A dict or a ampersand (&)\n                                 delimited string of query parameters to\n                                 add to each HTTP request,\n                                 e.g. ``foo=bar&baz=qux``\n\n        http-trust-env           (bool) Trust HTTP settings set in the\n                                 environment, such as environment\n                                 variables (HTTP_PROXY, etc) and\n                                 ~/.netrc authentication\n\n        http-ssl-verify          (bool) Verify SSL certificates,\n                                 default: ``True``\n\n        http-ssl-cert            (str or tuple) SSL certificate to use,\n                                 can be either a .pem file (str) or a\n                                 .crt/.key pair (tuple)\n\n        http-timeout             (float) General timeout used by all HTTP\n                                 requests except the ones covered by\n                                 other options, default: ``20.0``\n\n        http-stream-timeout      (float) Timeout for reading data from\n                                 HTTP streams, default: ``60.0``\n\n        subprocess-errorlog      (bool) Log errors from subprocesses to\n                                 a file located in the temp directory\n\n        subprocess-errorlog-path (str) Log errors from subprocesses to\n                                 a specific file\n\n        ringbuffer-size          (int) The size of the internal ring\n                                 buffer used by most stream types,\n                                 default: ``16777216`` (16MB)\n\n        rtmp-proxy               (str) Specify a proxy (SOCKS) that RTMP\n                                 streams will use\n\n        rtmp-rtmpdump            (str) Specify the location of the\n                                 rtmpdump executable used by RTMP streams,\n                                 e.g. ``/usr/local/bin/rtmpdump``\n\n        rtmp-timeout             (float) Timeout for reading data from\n                                 RTMP streams, default: ``60.0``\n\n        ffmpeg-ffmpeg            (str) Specify the location of the\n                                 ffmpeg executable use by Muxing streams\n                                 e.g. ``/usr/local/bin/ffmpeg``\n\n        ffmpeg-verbose           (bool) Log stderr from ffmpeg to the\n                                 console\n\n        ffmpeg-verbose-path      (str) Specify the location of the\n                                 ffmpeg stderr log file\n\n        ffmpeg-video-transcode   (str) The codec to use if transcoding\n                                 video when muxing with ffmpeg\n                                 e.g. ``h264``\n\n        ffmpeg-audio-transcode   (str) The codec to use if transcoding\n                                 audio when muxing with ffmpeg\n                                 e.g. ``aac``\n\n        stream-segment-attempts  (int) How many attempts should be done\n                                 to download each segment, default: ``3``.\n                                 General option used by streams not\n                                 covered by other options.\n\n        stream-segment-threads   (int) The size of the thread pool used\n                                 to download segments, default: ``1``.\n                                 General option used by streams not\n                                 covered by other options.\n\n        stream-segment-timeout   (float) Segment connect and read\n                                 timeout, default: ``10.0``.\n                                 General option used by streams not\n                                 covered by other options.\n\n        stream-timeout           (float) Timeout for reading data from\n                                 stream, default: ``60.0``.\n                                 General option used by streams not\n                                 covered by other options.\n\n        locale                   (str) Locale setting, in the RFC 1766 format\n                                 eg. en_US or es_ES\n                                 default: ``system locale``.\n\n        user-input-requester     (UserInputRequester) instance of UserInputRequester\n                                 to collect input from the user at runtime. Must be\n                                 set before the plugins are loaded.\n                                 default: ``UserInputRequester``.\n        ======================== =========================================", "docstring_tokens": ["Sets", "general", "options", "used", "by", "plugins", "and", "streams", "originating", "from", "this", "session", "object", "."], "sha": "c8ed1daff14ac03195870238b9b900c1109dd5c1", "url": "https://github.com/streamlink/streamlink/blob/c8ed1daff14ac03195870238b9b900c1109dd5c1/src/streamlink/session.py#L98-L295", "partition": "test"}
{"repo": "chrisjrn/registrasion", "path": "registrasion/reporting/views.py", "func_name": "paid_invoices_by_date", "original_string": "def paid_invoices_by_date(request, form):\n    ''' Shows the number of paid invoices containing given products or\n    categories per day. '''\n\n    products = form.cleaned_data[\"product\"]\n    categories = form.cleaned_data[\"category\"]\n\n    invoices = commerce.Invoice.objects.filter(\n        (\n            Q(lineitem__product__in=products) |\n            Q(lineitem__product__category__in=categories)\n        ),\n        status=commerce.Invoice.STATUS_PAID,\n    )\n\n    # Invoices with payments will be paid at the time of their latest payment\n    payments = commerce.PaymentBase.objects.all()\n    payments = payments.filter(\n        invoice__in=invoices,\n    )\n    payments = payments.order_by(\"invoice\")\n    invoice_max_time = payments.values(\"invoice\").annotate(\n        max_time=Max(\"time\")\n    )\n\n    # Zero-value invoices will have no payments, so they're paid at issue time\n    zero_value_invoices = invoices.filter(value=0)\n\n    times = itertools.chain(\n        (line[\"max_time\"] for line in invoice_max_time),\n        (invoice.issue_time for invoice in zero_value_invoices),\n    )\n\n    by_date = collections.defaultdict(int)\n    for time in times:\n        date = datetime.datetime(\n            year=time.year, month=time.month, day=time.day\n        )\n        by_date[date] += 1\n\n    data = [(date_, count) for date_, count in sorted(by_date.items())]\n    data = [(date_.strftime(\"%Y-%m-%d\"), count) for date_, count in data]\n\n    return ListReport(\n        \"Paid Invoices By Date\",\n        [\"date\", \"count\"],\n        data,\n    )", "language": "python", "code": "def paid_invoices_by_date(request, form):\n    ''' Shows the number of paid invoices containing given products or\n    categories per day. '''\n\n    products = form.cleaned_data[\"product\"]\n    categories = form.cleaned_data[\"category\"]\n\n    invoices = commerce.Invoice.objects.filter(\n        (\n            Q(lineitem__product__in=products) |\n            Q(lineitem__product__category__in=categories)\n        ),\n        status=commerce.Invoice.STATUS_PAID,\n    )\n\n    # Invoices with payments will be paid at the time of their latest payment\n    payments = commerce.PaymentBase.objects.all()\n    payments = payments.filter(\n        invoice__in=invoices,\n    )\n    payments = payments.order_by(\"invoice\")\n    invoice_max_time = payments.values(\"invoice\").annotate(\n        max_time=Max(\"time\")\n    )\n\n    # Zero-value invoices will have no payments, so they're paid at issue time\n    zero_value_invoices = invoices.filter(value=0)\n\n    times = itertools.chain(\n        (line[\"max_time\"] for line in invoice_max_time),\n        (invoice.issue_time for invoice in zero_value_invoices),\n    )\n\n    by_date = collections.defaultdict(int)\n    for time in times:\n        date = datetime.datetime(\n            year=time.year, month=time.month, day=time.day\n        )\n        by_date[date] += 1\n\n    data = [(date_, count) for date_, count in sorted(by_date.items())]\n    data = [(date_.strftime(\"%Y-%m-%d\"), count) for date_, count in data]\n\n    return ListReport(\n        \"Paid Invoices By Date\",\n        [\"date\", \"count\"],\n        data,\n    )", "code_tokens": ["def", "paid_invoices_by_date", "(", "request", ",", "form", ")", ":", "products", "=", "form", ".", "cleaned_data", "[", "\"product\"", "]", "categories", "=", "form", ".", "cleaned_data", "[", "\"category\"", "]", "invoices", "=", "commerce", ".", "Invoice", ".", "objects", ".", "filter", "(", "(", "Q", "(", "lineitem__product__in", "=", "products", ")", "|", "Q", "(", "lineitem__product__category__in", "=", "categories", ")", ")", ",", "status", "=", "commerce", ".", "Invoice", ".", "STATUS_PAID", ",", ")", "# Invoices with payments will be paid at the time of their latest payment", "payments", "=", "commerce", ".", "PaymentBase", ".", "objects", ".", "all", "(", ")", "payments", "=", "payments", ".", "filter", "(", "invoice__in", "=", "invoices", ",", ")", "payments", "=", "payments", ".", "order_by", "(", "\"invoice\"", ")", "invoice_max_time", "=", "payments", ".", "values", "(", "\"invoice\"", ")", ".", "annotate", "(", "max_time", "=", "Max", "(", "\"time\"", ")", ")", "# Zero-value invoices will have no payments, so they're paid at issue time", "zero_value_invoices", "=", "invoices", ".", "filter", "(", "value", "=", "0", ")", "times", "=", "itertools", ".", "chain", "(", "(", "line", "[", "\"max_time\"", "]", "for", "line", "in", "invoice_max_time", ")", ",", "(", "invoice", ".", "issue_time", "for", "invoice", "in", "zero_value_invoices", ")", ",", ")", "by_date", "=", "collections", ".", "defaultdict", "(", "int", ")", "for", "time", "in", "times", ":", "date", "=", "datetime", ".", "datetime", "(", "year", "=", "time", ".", "year", ",", "month", "=", "time", ".", "month", ",", "day", "=", "time", ".", "day", ")", "by_date", "[", "date", "]", "+=", "1", "data", "=", "[", "(", "date_", ",", "count", ")", "for", "date_", ",", "count", "in", "sorted", "(", "by_date", ".", "items", "(", ")", ")", "]", "data", "=", "[", "(", "date_", ".", "strftime", "(", "\"%Y-%m-%d\"", ")", ",", "count", ")", "for", "date_", ",", "count", "in", "data", "]", "return", "ListReport", "(", "\"Paid Invoices By Date\"", ",", "[", "\"date\"", ",", "\"count\"", "]", ",", "data", ",", ")"], "docstring": "Shows the number of paid invoices containing given products or\n    categories per day.", "docstring_tokens": ["Shows", "the", "number", "of", "paid", "invoices", "containing", "given", "products", "or", "categories", "per", "day", "."], "sha": "461d5846c6f9f3b7099322a94f5d9911564448e4", "url": "https://github.com/chrisjrn/registrasion/blob/461d5846c6f9f3b7099322a94f5d9911564448e4/registrasion/reporting/views.py#L363-L410", "partition": "test"}
{"repo": "algofairness/BlackBoxAuditing", "path": "BlackBoxAuditing/model_factories/DecisionTree.py", "func_name": "expand_and_standardize_dataset", "original_string": "def expand_and_standardize_dataset(response_index, response_header, data_set, col_vals, headers, standardizers, feats_to_ignore, columns_to_expand, outcome_trans_dict):\n  \"\"\"\n  Standardizes continuous features and expands categorical features.\n  \"\"\"\n  # expand and standardize\n  modified_set = []\n  for row_index, row in enumerate(data_set):\n    new_row = []\n    for col_index, val in enumerate(row):\n      header = headers[col_index]\n\n      # Outcome feature -> index outcome\n      if col_index == response_index:\n        new_outcome = outcome_trans_dict[val]\n        new_row.append(new_outcome)\n\n      # Ignored feature -> pass\n      elif header in feats_to_ignore:\n        pass\n      \n      # Categorical feature -> create new binary column for each possible value of the column\n      elif header in columns_to_expand:\n        for poss_val in col_vals[header]:\n          if val == poss_val:\n            new_cat_val = 1.0\n          else:\n            new_cat_val = -1.0\n          new_row.append(new_cat_val)\n\n      # Continuous feature -> standardize value with respect to its column\n      else:\n        new_cont_val = float((val - standardizers[header]['mean']) / standardizers[header]['std_dev'])\n        new_row.append(new_cont_val)\n\n    modified_set.append(new_row)\n\n  # update headers to reflect column expansion\n  expanded_headers = []\n  for header in headers:\n    if header in feats_to_ignore:\n      pass\n    elif (header in columns_to_expand) and (header is not response_header):\n      for poss_val in col_vals[header]:\n        new_header = '{}_{}'.format(header,poss_val)\n        expanded_headers.append(new_header)\n    else:\n      expanded_headers.append(header)\n\n  return modified_set, expanded_headers", "language": "python", "code": "def expand_and_standardize_dataset(response_index, response_header, data_set, col_vals, headers, standardizers, feats_to_ignore, columns_to_expand, outcome_trans_dict):\n  \"\"\"\n  Standardizes continuous features and expands categorical features.\n  \"\"\"\n  # expand and standardize\n  modified_set = []\n  for row_index, row in enumerate(data_set):\n    new_row = []\n    for col_index, val in enumerate(row):\n      header = headers[col_index]\n\n      # Outcome feature -> index outcome\n      if col_index == response_index:\n        new_outcome = outcome_trans_dict[val]\n        new_row.append(new_outcome)\n\n      # Ignored feature -> pass\n      elif header in feats_to_ignore:\n        pass\n      \n      # Categorical feature -> create new binary column for each possible value of the column\n      elif header in columns_to_expand:\n        for poss_val in col_vals[header]:\n          if val == poss_val:\n            new_cat_val = 1.0\n          else:\n            new_cat_val = -1.0\n          new_row.append(new_cat_val)\n\n      # Continuous feature -> standardize value with respect to its column\n      else:\n        new_cont_val = float((val - standardizers[header]['mean']) / standardizers[header]['std_dev'])\n        new_row.append(new_cont_val)\n\n    modified_set.append(new_row)\n\n  # update headers to reflect column expansion\n  expanded_headers = []\n  for header in headers:\n    if header in feats_to_ignore:\n      pass\n    elif (header in columns_to_expand) and (header is not response_header):\n      for poss_val in col_vals[header]:\n        new_header = '{}_{}'.format(header,poss_val)\n        expanded_headers.append(new_header)\n    else:\n      expanded_headers.append(header)\n\n  return modified_set, expanded_headers", "code_tokens": ["def", "expand_and_standardize_dataset", "(", "response_index", ",", "response_header", ",", "data_set", ",", "col_vals", ",", "headers", ",", "standardizers", ",", "feats_to_ignore", ",", "columns_to_expand", ",", "outcome_trans_dict", ")", ":", "# expand and standardize", "modified_set", "=", "[", "]", "for", "row_index", ",", "row", "in", "enumerate", "(", "data_set", ")", ":", "new_row", "=", "[", "]", "for", "col_index", ",", "val", "in", "enumerate", "(", "row", ")", ":", "header", "=", "headers", "[", "col_index", "]", "# Outcome feature -> index outcome", "if", "col_index", "==", "response_index", ":", "new_outcome", "=", "outcome_trans_dict", "[", "val", "]", "new_row", ".", "append", "(", "new_outcome", ")", "# Ignored feature -> pass", "elif", "header", "in", "feats_to_ignore", ":", "pass", "# Categorical feature -> create new binary column for each possible value of the column", "elif", "header", "in", "columns_to_expand", ":", "for", "poss_val", "in", "col_vals", "[", "header", "]", ":", "if", "val", "==", "poss_val", ":", "new_cat_val", "=", "1.0", "else", ":", "new_cat_val", "=", "-", "1.0", "new_row", ".", "append", "(", "new_cat_val", ")", "# Continuous feature -> standardize value with respect to its column", "else", ":", "new_cont_val", "=", "float", "(", "(", "val", "-", "standardizers", "[", "header", "]", "[", "'mean'", "]", ")", "/", "standardizers", "[", "header", "]", "[", "'std_dev'", "]", ")", "new_row", ".", "append", "(", "new_cont_val", ")", "modified_set", ".", "append", "(", "new_row", ")", "# update headers to reflect column expansion", "expanded_headers", "=", "[", "]", "for", "header", "in", "headers", ":", "if", "header", "in", "feats_to_ignore", ":", "pass", "elif", "(", "header", "in", "columns_to_expand", ")", "and", "(", "header", "is", "not", "response_header", ")", ":", "for", "poss_val", "in", "col_vals", "[", "header", "]", ":", "new_header", "=", "'{}_{}'", ".", "format", "(", "header", ",", "poss_val", ")", "expanded_headers", ".", "append", "(", "new_header", ")", "else", ":", "expanded_headers", ".", "append", "(", "header", ")", "return", "modified_set", ",", "expanded_headers"], "docstring": "Standardizes continuous features and expands categorical features.", "docstring_tokens": ["Standardizes", "continuous", "features", "and", "expands", "categorical", "features", "."], "sha": "b06c4faed5591cd7088475b2a203127bc5820483", "url": "https://github.com/algofairness/BlackBoxAuditing/blob/b06c4faed5591cd7088475b2a203127bc5820483/BlackBoxAuditing/model_factories/DecisionTree.py#L142-L190", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/layers/distribution_layer.py", "func_name": "_get_convert_to_tensor_fn", "original_string": "def _get_convert_to_tensor_fn(identifier):\n  \"\"\"Return a convert-to-tensor func, given a name, config, callable, etc.\"\"\"\n  if identifier is None:\n    return None\n\n  if isinstance(identifier, six.string_types):\n    identifier = str(identifier)\n    return _deserialize(identifier)\n\n  if isinstance(identifier, dict):\n    return _deserialize(identifier)\n\n  if isinstance(identifier, property):\n    identifier = identifier.fget\n  if callable(identifier):\n    return identifier\n\n  raise ValueError('Could not interpret '\n                   'convert-to-tensor function identifier:', identifier)", "language": "python", "code": "def _get_convert_to_tensor_fn(identifier):\n  \"\"\"Return a convert-to-tensor func, given a name, config, callable, etc.\"\"\"\n  if identifier is None:\n    return None\n\n  if isinstance(identifier, six.string_types):\n    identifier = str(identifier)\n    return _deserialize(identifier)\n\n  if isinstance(identifier, dict):\n    return _deserialize(identifier)\n\n  if isinstance(identifier, property):\n    identifier = identifier.fget\n  if callable(identifier):\n    return identifier\n\n  raise ValueError('Could not interpret '\n                   'convert-to-tensor function identifier:', identifier)", "code_tokens": ["def", "_get_convert_to_tensor_fn", "(", "identifier", ")", ":", "if", "identifier", "is", "None", ":", "return", "None", "if", "isinstance", "(", "identifier", ",", "six", ".", "string_types", ")", ":", "identifier", "=", "str", "(", "identifier", ")", "return", "_deserialize", "(", "identifier", ")", "if", "isinstance", "(", "identifier", ",", "dict", ")", ":", "return", "_deserialize", "(", "identifier", ")", "if", "isinstance", "(", "identifier", ",", "property", ")", ":", "identifier", "=", "identifier", ".", "fget", "if", "callable", "(", "identifier", ")", ":", "return", "identifier", "raise", "ValueError", "(", "'Could not interpret '", "'convert-to-tensor function identifier:'", ",", "identifier", ")"], "docstring": "Return a convert-to-tensor func, given a name, config, callable, etc.", "docstring_tokens": ["Return", "a", "convert", "-", "to", "-", "tensor", "func", "given", "a", "name", "config", "callable", "etc", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/layers/distribution_layer.py#L1912-L1930", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/tools/qcvv/fitters.py", "func_name": "plot_rb_data", "original_string": "def plot_rb_data(xdata, ydatas, yavg, yerr, fit, survival_prob, ax=None,\n                 show_plt=True):\n    \"\"\"Plot randomized benchmarking data.\n\n    Args:\n        xdata (list): list of subsequence lengths\n        ydatas (list): list of lists of survival probabilities for each\n            sequence\n        yavg (list): mean of the survival probabilities at each sequence\n            length\n        yerr (list): error of the survival\n        fit (list): fit parameters\n        survival_prob (callable): function that computes survival probability\n        ax (Axes or None): plot axis (if passed in)\n        show_plt (bool): display the plot.\n\n    Raises:\n        ImportError: If matplotlib is not installed.\n    \"\"\"\n    # pylint: disable=invalid-name\n\n    if not HAS_MATPLOTLIB:\n        raise ImportError('The function plot_rb_data needs matplotlib. '\n                          'Run \"pip install matplotlib\" before.')\n    if ax is None:\n        plt.figure()\n        ax = plt.gca()\n\n    # Plot the result for each sequence\n    for ydata in ydatas:\n        ax.plot(xdata, ydata, color='gray', linestyle='none', marker='x')\n    # Plot the mean with error bars\n    ax.errorbar(xdata, yavg, yerr=yerr, color='r', linestyle='--', linewidth=3)\n\n    # Plot the fit\n    ax.plot(xdata, survival_prob(xdata, *fit), color='blue', linestyle='-', linewidth=2)\n    ax.tick_params(labelsize=14)\n    # ax.tick_params(axis='x',labelrotation=70)\n\n    ax.set_xlabel('Clifford Length', fontsize=16)\n    ax.set_ylabel('Z', fontsize=16)\n    ax.grid(True)\n\n    if show_plt:\n        plt.show()", "language": "python", "code": "def plot_rb_data(xdata, ydatas, yavg, yerr, fit, survival_prob, ax=None,\n                 show_plt=True):\n    \"\"\"Plot randomized benchmarking data.\n\n    Args:\n        xdata (list): list of subsequence lengths\n        ydatas (list): list of lists of survival probabilities for each\n            sequence\n        yavg (list): mean of the survival probabilities at each sequence\n            length\n        yerr (list): error of the survival\n        fit (list): fit parameters\n        survival_prob (callable): function that computes survival probability\n        ax (Axes or None): plot axis (if passed in)\n        show_plt (bool): display the plot.\n\n    Raises:\n        ImportError: If matplotlib is not installed.\n    \"\"\"\n    # pylint: disable=invalid-name\n\n    if not HAS_MATPLOTLIB:\n        raise ImportError('The function plot_rb_data needs matplotlib. '\n                          'Run \"pip install matplotlib\" before.')\n    if ax is None:\n        plt.figure()\n        ax = plt.gca()\n\n    # Plot the result for each sequence\n    for ydata in ydatas:\n        ax.plot(xdata, ydata, color='gray', linestyle='none', marker='x')\n    # Plot the mean with error bars\n    ax.errorbar(xdata, yavg, yerr=yerr, color='r', linestyle='--', linewidth=3)\n\n    # Plot the fit\n    ax.plot(xdata, survival_prob(xdata, *fit), color='blue', linestyle='-', linewidth=2)\n    ax.tick_params(labelsize=14)\n    # ax.tick_params(axis='x',labelrotation=70)\n\n    ax.set_xlabel('Clifford Length', fontsize=16)\n    ax.set_ylabel('Z', fontsize=16)\n    ax.grid(True)\n\n    if show_plt:\n        plt.show()", "code_tokens": ["def", "plot_rb_data", "(", "xdata", ",", "ydatas", ",", "yavg", ",", "yerr", ",", "fit", ",", "survival_prob", ",", "ax", "=", "None", ",", "show_plt", "=", "True", ")", ":", "# pylint: disable=invalid-name", "if", "not", "HAS_MATPLOTLIB", ":", "raise", "ImportError", "(", "'The function plot_rb_data needs matplotlib. '", "'Run \"pip install matplotlib\" before.'", ")", "if", "ax", "is", "None", ":", "plt", ".", "figure", "(", ")", "ax", "=", "plt", ".", "gca", "(", ")", "# Plot the result for each sequence", "for", "ydata", "in", "ydatas", ":", "ax", ".", "plot", "(", "xdata", ",", "ydata", ",", "color", "=", "'gray'", ",", "linestyle", "=", "'none'", ",", "marker", "=", "'x'", ")", "# Plot the mean with error bars", "ax", ".", "errorbar", "(", "xdata", ",", "yavg", ",", "yerr", "=", "yerr", ",", "color", "=", "'r'", ",", "linestyle", "=", "'--'", ",", "linewidth", "=", "3", ")", "# Plot the fit", "ax", ".", "plot", "(", "xdata", ",", "survival_prob", "(", "xdata", ",", "*", "fit", ")", ",", "color", "=", "'blue'", ",", "linestyle", "=", "'-'", ",", "linewidth", "=", "2", ")", "ax", ".", "tick_params", "(", "labelsize", "=", "14", ")", "# ax.tick_params(axis='x',labelrotation=70)", "ax", ".", "set_xlabel", "(", "'Clifford Length'", ",", "fontsize", "=", "16", ")", "ax", ".", "set_ylabel", "(", "'Z'", ",", "fontsize", "=", "16", ")", "ax", ".", "grid", "(", "True", ")", "if", "show_plt", ":", "plt", ".", "show", "(", ")"], "docstring": "Plot randomized benchmarking data.\n\n    Args:\n        xdata (list): list of subsequence lengths\n        ydatas (list): list of lists of survival probabilities for each\n            sequence\n        yavg (list): mean of the survival probabilities at each sequence\n            length\n        yerr (list): error of the survival\n        fit (list): fit parameters\n        survival_prob (callable): function that computes survival probability\n        ax (Axes or None): plot axis (if passed in)\n        show_plt (bool): display the plot.\n\n    Raises:\n        ImportError: If matplotlib is not installed.", "docstring_tokens": ["Plot", "randomized", "benchmarking", "data", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/tools/qcvv/fitters.py#L122-L166", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/pip/baseparser.py", "func_name": "ConfigOptionParser.get_environ_vars", "original_string": "def get_environ_vars(self):\n        \"\"\"Returns a generator with all environmental vars with prefix PIP_\"\"\"\n        for key, val in os.environ.items():\n            if _environ_prefix_re.search(key):\n                yield (_environ_prefix_re.sub(\"\", key).lower(), val)", "language": "python", "code": "def get_environ_vars(self):\n        \"\"\"Returns a generator with all environmental vars with prefix PIP_\"\"\"\n        for key, val in os.environ.items():\n            if _environ_prefix_re.search(key):\n                yield (_environ_prefix_re.sub(\"\", key).lower(), val)", "code_tokens": ["def", "get_environ_vars", "(", "self", ")", ":", "for", "key", ",", "val", "in", "os", ".", "environ", ".", "items", "(", ")", ":", "if", "_environ_prefix_re", ".", "search", "(", "key", ")", ":", "yield", "(", "_environ_prefix_re", ".", "sub", "(", "\"\"", ",", "key", ")", ".", "lower", "(", ")", ",", "val", ")"], "docstring": "Returns a generator with all environmental vars with prefix PIP_", "docstring_tokens": ["Returns", "a", "generator", "with", "all", "environmental", "vars", "with", "prefix", "PIP_"], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/pip/baseparser.py#L246-L250", "partition": "test"}
{"repo": "gtaylor/python-route53", "path": "route53/hosted_zone.py", "func_name": "HostedZone.create_cname_record", "original_string": "def create_cname_record(self, name, values, ttl=60, weight=None, region=None,\n                           set_identifier=None):\n        \"\"\"\n        Creates a CNAME record attached to this hosted zone.\n\n        :param str name: The fully qualified name of the record to add.\n        :param list values: A list of value strings for the record.\n        :keyword int ttl: The time-to-live of the record (in seconds).\n        :keyword int weight: *For weighted record sets only*. Among resource record\n            sets that have the same combination of DNS name and type, a value\n            that determines what portion of traffic for the current resource\n            record set is routed to the associated location. Ranges from 0-255.\n        :keyword str region: *For latency-based record sets*. The Amazon EC2 region\n            where the resource that is specified in this resource record set\n            resides.\n        :keyword str set_identifier: *For weighted and latency resource record\n            sets only*. An identifier that differentiates among multiple\n            resource record sets that have the same combination of DNS name\n            and type. 1-128 chars.\n        :rtype: tuple\n        :returns: A tuple in the form of ``(rrset, change_info)``, where\n            ``rrset`` is the newly created CNAMEResourceRecordSet instance.\n        \"\"\"\n\n        self._halt_if_already_deleted()\n\n        # Grab the params/kwargs here for brevity's sake.\n        values = locals()\n        del values['self']\n\n        return self._add_record(CNAMEResourceRecordSet, **values)", "language": "python", "code": "def create_cname_record(self, name, values, ttl=60, weight=None, region=None,\n                           set_identifier=None):\n        \"\"\"\n        Creates a CNAME record attached to this hosted zone.\n\n        :param str name: The fully qualified name of the record to add.\n        :param list values: A list of value strings for the record.\n        :keyword int ttl: The time-to-live of the record (in seconds).\n        :keyword int weight: *For weighted record sets only*. Among resource record\n            sets that have the same combination of DNS name and type, a value\n            that determines what portion of traffic for the current resource\n            record set is routed to the associated location. Ranges from 0-255.\n        :keyword str region: *For latency-based record sets*. The Amazon EC2 region\n            where the resource that is specified in this resource record set\n            resides.\n        :keyword str set_identifier: *For weighted and latency resource record\n            sets only*. An identifier that differentiates among multiple\n            resource record sets that have the same combination of DNS name\n            and type. 1-128 chars.\n        :rtype: tuple\n        :returns: A tuple in the form of ``(rrset, change_info)``, where\n            ``rrset`` is the newly created CNAMEResourceRecordSet instance.\n        \"\"\"\n\n        self._halt_if_already_deleted()\n\n        # Grab the params/kwargs here for brevity's sake.\n        values = locals()\n        del values['self']\n\n        return self._add_record(CNAMEResourceRecordSet, **values)", "code_tokens": ["def", "create_cname_record", "(", "self", ",", "name", ",", "values", ",", "ttl", "=", "60", ",", "weight", "=", "None", ",", "region", "=", "None", ",", "set_identifier", "=", "None", ")", ":", "self", ".", "_halt_if_already_deleted", "(", ")", "# Grab the params/kwargs here for brevity's sake.", "values", "=", "locals", "(", ")", "del", "values", "[", "'self'", "]", "return", "self", ".", "_add_record", "(", "CNAMEResourceRecordSet", ",", "*", "*", "values", ")"], "docstring": "Creates a CNAME record attached to this hosted zone.\n\n        :param str name: The fully qualified name of the record to add.\n        :param list values: A list of value strings for the record.\n        :keyword int ttl: The time-to-live of the record (in seconds).\n        :keyword int weight: *For weighted record sets only*. Among resource record\n            sets that have the same combination of DNS name and type, a value\n            that determines what portion of traffic for the current resource\n            record set is routed to the associated location. Ranges from 0-255.\n        :keyword str region: *For latency-based record sets*. The Amazon EC2 region\n            where the resource that is specified in this resource record set\n            resides.\n        :keyword str set_identifier: *For weighted and latency resource record\n            sets only*. An identifier that differentiates among multiple\n            resource record sets that have the same combination of DNS name\n            and type. 1-128 chars.\n        :rtype: tuple\n        :returns: A tuple in the form of ``(rrset, change_info)``, where\n            ``rrset`` is the newly created CNAMEResourceRecordSet instance.", "docstring_tokens": ["Creates", "a", "CNAME", "record", "attached", "to", "this", "hosted", "zone", "."], "sha": "b9fc7e258a79551c9ed61e4a71668b7f06f9e774", "url": "https://github.com/gtaylor/python-route53/blob/b9fc7e258a79551c9ed61e4a71668b7f06f9e774/route53/hosted_zone.py#L252-L282", "partition": "test"}
{"repo": "cmcginty/PyWeather", "path": "weather/stations/davis.py", "func_name": "VantagePro._wakeup", "original_string": "def _wakeup(self):\n        '''\n        issue wakeup command to device to take out of standby mode.\n        '''\n        log.info(\"send: WAKEUP\")\n        for i in xrange(3):\n            self.port.write('\\n')  # wakeup device\n            ack = self.port.read(len(self.WAKE_ACK))  # read wakeup string\n            log_raw('read', ack)\n            if ack == self.WAKE_ACK:\n                return\n        raise NoDeviceException('Can not access weather station')", "language": "python", "code": "def _wakeup(self):\n        '''\n        issue wakeup command to device to take out of standby mode.\n        '''\n        log.info(\"send: WAKEUP\")\n        for i in xrange(3):\n            self.port.write('\\n')  # wakeup device\n            ack = self.port.read(len(self.WAKE_ACK))  # read wakeup string\n            log_raw('read', ack)\n            if ack == self.WAKE_ACK:\n                return\n        raise NoDeviceException('Can not access weather station')", "code_tokens": ["def", "_wakeup", "(", "self", ")", ":", "log", ".", "info", "(", "\"send: WAKEUP\"", ")", "for", "i", "in", "xrange", "(", "3", ")", ":", "self", ".", "port", ".", "write", "(", "'\\n'", ")", "# wakeup device", "ack", "=", "self", ".", "port", ".", "read", "(", "len", "(", "self", ".", "WAKE_ACK", ")", ")", "# read wakeup string", "log_raw", "(", "'read'", ",", "ack", ")", "if", "ack", "==", "self", ".", "WAKE_ACK", ":", "return", "raise", "NoDeviceException", "(", "'Can not access weather station'", ")"], "docstring": "issue wakeup command to device to take out of standby mode.", "docstring_tokens": ["issue", "wakeup", "command", "to", "device", "to", "take", "out", "of", "standby", "mode", "."], "sha": "8c25d9cd1fa921e0a6e460d523656279cac045cb", "url": "https://github.com/cmcginty/PyWeather/blob/8c25d9cd1fa921e0a6e460d523656279cac045cb/weather/stations/davis.py#L382-L393", "partition": "test"}
{"repo": "SmokinCaterpillar/pypet", "path": "pypet/storageservice.py", "func_name": "HDF5StorageService._srvc_load_several_items", "original_string": "def _srvc_load_several_items(self, iterable, *args, **kwargs):\n        \"\"\"Loads several items from an iterable\n\n        Iterables are supposed to be of a format like `[(msg, item, args, kwarg),...]`\n        If `args` and `kwargs` are not part of a tuple, they are taken from the\n        current `args` and `kwargs` provided to this function.\n\n        \"\"\"\n        for input_tuple in iterable:\n            msg = input_tuple[0]\n            item = input_tuple[1]\n            if len(input_tuple) > 2:\n                args = input_tuple[2]\n            if len(input_tuple) > 3:\n                kwargs = input_tuple[3]\n            if len(input_tuple) > 4:\n                raise RuntimeError('You shall not pass!')\n\n            self.load(msg, item, *args, **kwargs)", "language": "python", "code": "def _srvc_load_several_items(self, iterable, *args, **kwargs):\n        \"\"\"Loads several items from an iterable\n\n        Iterables are supposed to be of a format like `[(msg, item, args, kwarg),...]`\n        If `args` and `kwargs` are not part of a tuple, they are taken from the\n        current `args` and `kwargs` provided to this function.\n\n        \"\"\"\n        for input_tuple in iterable:\n            msg = input_tuple[0]\n            item = input_tuple[1]\n            if len(input_tuple) > 2:\n                args = input_tuple[2]\n            if len(input_tuple) > 3:\n                kwargs = input_tuple[3]\n            if len(input_tuple) > 4:\n                raise RuntimeError('You shall not pass!')\n\n            self.load(msg, item, *args, **kwargs)", "code_tokens": ["def", "_srvc_load_several_items", "(", "self", ",", "iterable", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "for", "input_tuple", "in", "iterable", ":", "msg", "=", "input_tuple", "[", "0", "]", "item", "=", "input_tuple", "[", "1", "]", "if", "len", "(", "input_tuple", ")", ">", "2", ":", "args", "=", "input_tuple", "[", "2", "]", "if", "len", "(", "input_tuple", ")", ">", "3", ":", "kwargs", "=", "input_tuple", "[", "3", "]", "if", "len", "(", "input_tuple", ")", ">", "4", ":", "raise", "RuntimeError", "(", "'You shall not pass!'", ")", "self", ".", "load", "(", "msg", ",", "item", ",", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Loads several items from an iterable\n\n        Iterables are supposed to be of a format like `[(msg, item, args, kwarg),...]`\n        If `args` and `kwargs` are not part of a tuple, they are taken from the\n        current `args` and `kwargs` provided to this function.", "docstring_tokens": ["Loads", "several", "items", "from", "an", "iterable"], "sha": "97ad3e80d46dbdea02deeb98ea41f05a19565826", "url": "https://github.com/SmokinCaterpillar/pypet/blob/97ad3e80d46dbdea02deeb98ea41f05a19565826/pypet/storageservice.py#L1339-L1357", "partition": "test"}
{"repo": "urinieto/msaf", "path": "msaf/base.py", "func_name": "Features.estimate_beats", "original_string": "def estimate_beats(self):\n        \"\"\"Estimates the beats using librosa.\n\n        Returns\n        -------\n        times: np.array\n            Times of estimated beats in seconds.\n        frames: np.array\n            Frame indeces of estimated beats.\n        \"\"\"\n        # Compute harmonic-percussive source separation if needed\n        if self._audio_percussive is None:\n            self._audio_harmonic, self._audio_percussive = self.compute_HPSS()\n\n        # Compute beats\n        tempo, frames = librosa.beat.beat_track(\n            y=self._audio_percussive, sr=self.sr,\n            hop_length=self.hop_length)\n\n        # To times\n        times = librosa.frames_to_time(frames, sr=self.sr,\n                                       hop_length=self.hop_length)\n\n        # TODO: Is this really necessary?\n        if len(times) > 0 and times[0] == 0:\n            times = times[1:]\n            frames = frames[1:]\n\n        return times, frames", "language": "python", "code": "def estimate_beats(self):\n        \"\"\"Estimates the beats using librosa.\n\n        Returns\n        -------\n        times: np.array\n            Times of estimated beats in seconds.\n        frames: np.array\n            Frame indeces of estimated beats.\n        \"\"\"\n        # Compute harmonic-percussive source separation if needed\n        if self._audio_percussive is None:\n            self._audio_harmonic, self._audio_percussive = self.compute_HPSS()\n\n        # Compute beats\n        tempo, frames = librosa.beat.beat_track(\n            y=self._audio_percussive, sr=self.sr,\n            hop_length=self.hop_length)\n\n        # To times\n        times = librosa.frames_to_time(frames, sr=self.sr,\n                                       hop_length=self.hop_length)\n\n        # TODO: Is this really necessary?\n        if len(times) > 0 and times[0] == 0:\n            times = times[1:]\n            frames = frames[1:]\n\n        return times, frames", "code_tokens": ["def", "estimate_beats", "(", "self", ")", ":", "# Compute harmonic-percussive source separation if needed", "if", "self", ".", "_audio_percussive", "is", "None", ":", "self", ".", "_audio_harmonic", ",", "self", ".", "_audio_percussive", "=", "self", ".", "compute_HPSS", "(", ")", "# Compute beats", "tempo", ",", "frames", "=", "librosa", ".", "beat", ".", "beat_track", "(", "y", "=", "self", ".", "_audio_percussive", ",", "sr", "=", "self", ".", "sr", ",", "hop_length", "=", "self", ".", "hop_length", ")", "# To times", "times", "=", "librosa", ".", "frames_to_time", "(", "frames", ",", "sr", "=", "self", ".", "sr", ",", "hop_length", "=", "self", ".", "hop_length", ")", "# TODO: Is this really necessary?", "if", "len", "(", "times", ")", ">", "0", "and", "times", "[", "0", "]", "==", "0", ":", "times", "=", "times", "[", "1", ":", "]", "frames", "=", "frames", "[", "1", ":", "]", "return", "times", ",", "frames"], "docstring": "Estimates the beats using librosa.\n\n        Returns\n        -------\n        times: np.array\n            Times of estimated beats in seconds.\n        frames: np.array\n            Frame indeces of estimated beats.", "docstring_tokens": ["Estimates", "the", "beats", "using", "librosa", "."], "sha": "9dbb57d77a1310465a65cc40f1641d083ca74385", "url": "https://github.com/urinieto/msaf/blob/9dbb57d77a1310465a65cc40f1641d083ca74385/msaf/base.py#L112-L140", "partition": "test"}
{"repo": "ibm-watson-iot/iot-python", "path": "tmp/src/things/things.py", "func_name": "updateLogicalInterface", "original_string": "def updateLogicalInterface(self, logicalInterfaceId, name, schemaId, description=None):\n        \"\"\"\n        Updates a logical interface.\n        Parameters: logicalInterfaceId (string), name (string), schemaId (string), description (string, optional).\n        Throws APIException on failure.\n        \"\"\"\n        req = ApiClient.oneLogicalInterfaceUrl % (self.host, \"/draft\", logicalInterfaceId)\n        body = {\"name\" : name, \"schemaId\" : schemaId, \"id\" : logicalInterfaceId}\n        if description:\n            body[\"description\"] = description\n        resp = requests.put(req, auth=self.credentials, headers={\"Content-Type\":\"application/json\"},\n                            data=json.dumps(body),  verify=self.verify)\n        if resp.status_code == 200:\n            self.logger.debug(\"Logical interface updated\")\n        else:\n            raise ibmiotf.APIException(resp.status_code, \"HTTP error updating logical interface\", resp)\n        return resp.json()", "language": "python", "code": "def updateLogicalInterface(self, logicalInterfaceId, name, schemaId, description=None):\n        \"\"\"\n        Updates a logical interface.\n        Parameters: logicalInterfaceId (string), name (string), schemaId (string), description (string, optional).\n        Throws APIException on failure.\n        \"\"\"\n        req = ApiClient.oneLogicalInterfaceUrl % (self.host, \"/draft\", logicalInterfaceId)\n        body = {\"name\" : name, \"schemaId\" : schemaId, \"id\" : logicalInterfaceId}\n        if description:\n            body[\"description\"] = description\n        resp = requests.put(req, auth=self.credentials, headers={\"Content-Type\":\"application/json\"},\n                            data=json.dumps(body),  verify=self.verify)\n        if resp.status_code == 200:\n            self.logger.debug(\"Logical interface updated\")\n        else:\n            raise ibmiotf.APIException(resp.status_code, \"HTTP error updating logical interface\", resp)\n        return resp.json()", "code_tokens": ["def", "updateLogicalInterface", "(", "self", ",", "logicalInterfaceId", ",", "name", ",", "schemaId", ",", "description", "=", "None", ")", ":", "req", "=", "ApiClient", ".", "oneLogicalInterfaceUrl", "%", "(", "self", ".", "host", ",", "\"/draft\"", ",", "logicalInterfaceId", ")", "body", "=", "{", "\"name\"", ":", "name", ",", "\"schemaId\"", ":", "schemaId", ",", "\"id\"", ":", "logicalInterfaceId", "}", "if", "description", ":", "body", "[", "\"description\"", "]", "=", "description", "resp", "=", "requests", ".", "put", "(", "req", ",", "auth", "=", "self", ".", "credentials", ",", "headers", "=", "{", "\"Content-Type\"", ":", "\"application/json\"", "}", ",", "data", "=", "json", ".", "dumps", "(", "body", ")", ",", "verify", "=", "self", ".", "verify", ")", "if", "resp", ".", "status_code", "==", "200", ":", "self", ".", "logger", ".", "debug", "(", "\"Logical interface updated\"", ")", "else", ":", "raise", "ibmiotf", ".", "APIException", "(", "resp", ".", "status_code", ",", "\"HTTP error updating logical interface\"", ",", "resp", ")", "return", "resp", ".", "json", "(", ")"], "docstring": "Updates a logical interface.\n        Parameters: logicalInterfaceId (string), name (string), schemaId (string), description (string, optional).\n        Throws APIException on failure.", "docstring_tokens": ["Updates", "a", "logical", "interface", ".", "Parameters", ":", "logicalInterfaceId", "(", "string", ")", "name", "(", "string", ")", "schemaId", "(", "string", ")", "description", "(", "string", "optional", ")", ".", "Throws", "APIException", "on", "failure", "."], "sha": "195f05adce3fba4ec997017e41e02ebd85c0c4cc", "url": "https://github.com/ibm-watson-iot/iot-python/blob/195f05adce3fba4ec997017e41e02ebd85c0c4cc/tmp/src/things/things.py#L842-L858", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/nose/importer.py", "func_name": "add_path", "original_string": "def add_path(path, config=None):\n    \"\"\"Ensure that the path, or the root of the current package (if\n    path is in a package), is in sys.path.\n    \"\"\"\n\n    # FIXME add any src-looking dirs seen too... need to get config for that\n    \n    log.debug('Add path %s' % path)    \n    if not path:\n        return []\n    added = []\n    parent = os.path.dirname(path)\n    if (parent\n        and os.path.exists(os.path.join(path, '__init__.py'))):\n        added.extend(add_path(parent, config))\n    elif not path in sys.path:\n        log.debug(\"insert %s into sys.path\", path)\n        sys.path.insert(0, path)\n        added.append(path)\n    if config and config.srcDirs:\n        for dirname in config.srcDirs:\n            dirpath = os.path.join(path, dirname)\n            if os.path.isdir(dirpath):\n                sys.path.insert(0, dirpath)\n                added.append(dirpath)\n    return added", "language": "python", "code": "def add_path(path, config=None):\n    \"\"\"Ensure that the path, or the root of the current package (if\n    path is in a package), is in sys.path.\n    \"\"\"\n\n    # FIXME add any src-looking dirs seen too... need to get config for that\n    \n    log.debug('Add path %s' % path)    \n    if not path:\n        return []\n    added = []\n    parent = os.path.dirname(path)\n    if (parent\n        and os.path.exists(os.path.join(path, '__init__.py'))):\n        added.extend(add_path(parent, config))\n    elif not path in sys.path:\n        log.debug(\"insert %s into sys.path\", path)\n        sys.path.insert(0, path)\n        added.append(path)\n    if config and config.srcDirs:\n        for dirname in config.srcDirs:\n            dirpath = os.path.join(path, dirname)\n            if os.path.isdir(dirpath):\n                sys.path.insert(0, dirpath)\n                added.append(dirpath)\n    return added", "code_tokens": ["def", "add_path", "(", "path", ",", "config", "=", "None", ")", ":", "# FIXME add any src-looking dirs seen too... need to get config for that", "log", ".", "debug", "(", "'Add path %s'", "%", "path", ")", "if", "not", "path", ":", "return", "[", "]", "added", "=", "[", "]", "parent", "=", "os", ".", "path", ".", "dirname", "(", "path", ")", "if", "(", "parent", "and", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "path", ",", "'__init__.py'", ")", ")", ")", ":", "added", ".", "extend", "(", "add_path", "(", "parent", ",", "config", ")", ")", "elif", "not", "path", "in", "sys", ".", "path", ":", "log", ".", "debug", "(", "\"insert %s into sys.path\"", ",", "path", ")", "sys", ".", "path", ".", "insert", "(", "0", ",", "path", ")", "added", ".", "append", "(", "path", ")", "if", "config", "and", "config", ".", "srcDirs", ":", "for", "dirname", "in", "config", ".", "srcDirs", ":", "dirpath", "=", "os", ".", "path", ".", "join", "(", "path", ",", "dirname", ")", "if", "os", ".", "path", ".", "isdir", "(", "dirpath", ")", ":", "sys", ".", "path", ".", "insert", "(", "0", ",", "dirpath", ")", "added", ".", "append", "(", "dirpath", ")", "return", "added"], "docstring": "Ensure that the path, or the root of the current package (if\n    path is in a package), is in sys.path.", "docstring_tokens": ["Ensure", "that", "the", "path", "or", "the", "root", "of", "the", "current", "package", "(", "if", "path", "is", "in", "a", "package", ")", "is", "in", "sys", ".", "path", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/nose/importer.py#L123-L148", "partition": "test"}
{"repo": "Yelp/py_zipkin", "path": "py_zipkin/encoding/protobuf/__init__.py", "func_name": "_convert_endpoint", "original_string": "def _convert_endpoint(endpoint):\n    \"\"\"Converts py_zipkin's Endpoint to Protobuf's Endpoint.\n\n    :param endpoint: py_zipkins' endpoint to convert.\n    :type endpoint: py_zipkin.encoding.Endpoint\n    :return: corresponding protobuf's endpoint.\n    :rtype: zipkin_pb2.Endpoint\n    \"\"\"\n    pb_endpoint = zipkin_pb2.Endpoint()\n\n    if endpoint.service_name:\n        pb_endpoint.service_name = endpoint.service_name\n    if endpoint.port and endpoint.port != 0:\n        pb_endpoint.port = endpoint.port\n    if endpoint.ipv4:\n        pb_endpoint.ipv4 = socket.inet_pton(socket.AF_INET, endpoint.ipv4)\n    if endpoint.ipv6:\n        pb_endpoint.ipv6 = socket.inet_pton(socket.AF_INET6, endpoint.ipv6)\n\n    return pb_endpoint", "language": "python", "code": "def _convert_endpoint(endpoint):\n    \"\"\"Converts py_zipkin's Endpoint to Protobuf's Endpoint.\n\n    :param endpoint: py_zipkins' endpoint to convert.\n    :type endpoint: py_zipkin.encoding.Endpoint\n    :return: corresponding protobuf's endpoint.\n    :rtype: zipkin_pb2.Endpoint\n    \"\"\"\n    pb_endpoint = zipkin_pb2.Endpoint()\n\n    if endpoint.service_name:\n        pb_endpoint.service_name = endpoint.service_name\n    if endpoint.port and endpoint.port != 0:\n        pb_endpoint.port = endpoint.port\n    if endpoint.ipv4:\n        pb_endpoint.ipv4 = socket.inet_pton(socket.AF_INET, endpoint.ipv4)\n    if endpoint.ipv6:\n        pb_endpoint.ipv6 = socket.inet_pton(socket.AF_INET6, endpoint.ipv6)\n\n    return pb_endpoint", "code_tokens": ["def", "_convert_endpoint", "(", "endpoint", ")", ":", "pb_endpoint", "=", "zipkin_pb2", ".", "Endpoint", "(", ")", "if", "endpoint", ".", "service_name", ":", "pb_endpoint", ".", "service_name", "=", "endpoint", ".", "service_name", "if", "endpoint", ".", "port", "and", "endpoint", ".", "port", "!=", "0", ":", "pb_endpoint", ".", "port", "=", "endpoint", ".", "port", "if", "endpoint", ".", "ipv4", ":", "pb_endpoint", ".", "ipv4", "=", "socket", ".", "inet_pton", "(", "socket", ".", "AF_INET", ",", "endpoint", ".", "ipv4", ")", "if", "endpoint", ".", "ipv6", ":", "pb_endpoint", ".", "ipv6", "=", "socket", ".", "inet_pton", "(", "socket", ".", "AF_INET6", ",", "endpoint", ".", "ipv6", ")", "return", "pb_endpoint"], "docstring": "Converts py_zipkin's Endpoint to Protobuf's Endpoint.\n\n    :param endpoint: py_zipkins' endpoint to convert.\n    :type endpoint: py_zipkin.encoding.Endpoint\n    :return: corresponding protobuf's endpoint.\n    :rtype: zipkin_pb2.Endpoint", "docstring_tokens": ["Converts", "py_zipkin", "s", "Endpoint", "to", "Protobuf", "s", "Endpoint", "."], "sha": "0944d9a3fb1f1798dbb276694aeed99f2b4283ba", "url": "https://github.com/Yelp/py_zipkin/blob/0944d9a3fb1f1798dbb276694aeed99f2b4283ba/py_zipkin/encoding/protobuf/__init__.py#L134-L153", "partition": "test"}
{"repo": "Clinical-Genomics/scout", "path": "scout/server/app.py", "func_name": "register_blueprints", "original_string": "def register_blueprints(app):\n    \"\"\"Register Flask blueprints.\"\"\"\n    app.register_blueprint(public.public_bp)\n    app.register_blueprint(genes.genes_bp)\n    app.register_blueprint(cases.cases_bp)\n    app.register_blueprint(login.login_bp)\n    app.register_blueprint(variants.variants_bp)\n    app.register_blueprint(panels.panels_bp)\n    app.register_blueprint(dashboard.dashboard_bp)\n    app.register_blueprint(api.api_bp)\n    app.register_blueprint(alignviewers.alignviewers_bp)\n    app.register_blueprint(phenotypes.hpo_bp)\n    app.register_blueprint(institutes.overview)", "language": "python", "code": "def register_blueprints(app):\n    \"\"\"Register Flask blueprints.\"\"\"\n    app.register_blueprint(public.public_bp)\n    app.register_blueprint(genes.genes_bp)\n    app.register_blueprint(cases.cases_bp)\n    app.register_blueprint(login.login_bp)\n    app.register_blueprint(variants.variants_bp)\n    app.register_blueprint(panels.panels_bp)\n    app.register_blueprint(dashboard.dashboard_bp)\n    app.register_blueprint(api.api_bp)\n    app.register_blueprint(alignviewers.alignviewers_bp)\n    app.register_blueprint(phenotypes.hpo_bp)\n    app.register_blueprint(institutes.overview)", "code_tokens": ["def", "register_blueprints", "(", "app", ")", ":", "app", ".", "register_blueprint", "(", "public", ".", "public_bp", ")", "app", ".", "register_blueprint", "(", "genes", ".", "genes_bp", ")", "app", ".", "register_blueprint", "(", "cases", ".", "cases_bp", ")", "app", ".", "register_blueprint", "(", "login", ".", "login_bp", ")", "app", ".", "register_blueprint", "(", "variants", ".", "variants_bp", ")", "app", ".", "register_blueprint", "(", "panels", ".", "panels_bp", ")", "app", ".", "register_blueprint", "(", "dashboard", ".", "dashboard_bp", ")", "app", ".", "register_blueprint", "(", "api", ".", "api_bp", ")", "app", ".", "register_blueprint", "(", "alignviewers", ".", "alignviewers_bp", ")", "app", ".", "register_blueprint", "(", "phenotypes", ".", "hpo_bp", ")", "app", ".", "register_blueprint", "(", "institutes", ".", "overview", ")"], "docstring": "Register Flask blueprints.", "docstring_tokens": ["Register", "Flask", "blueprints", "."], "sha": "90a551e2e1653a319e654c2405c2866f93d0ebb9", "url": "https://github.com/Clinical-Genomics/scout/blob/90a551e2e1653a319e654c2405c2866f93d0ebb9/scout/server/app.py#L98-L110", "partition": "test"}
{"repo": "pydron/anycall", "path": "anycall/bytequeue.py", "func_name": "ByteQueue.peek", "original_string": "def peek(self, n):\n        \"\"\"\n        Return the first `n` characters from the queue without\n        removing them.\n        Throws an error if there are less than `n` characters in the queue.\n        \n        Equivalent to::\n        \n            s = queue[:n]\n            \n        if `queue` where a regular string.\n        \"\"\"\n        if self._len < n:\n            raise ValueError(\"Not enough bytes in the queue\")\n        \n        def part_generator(n):\n            \"\"\"\n            Returns the requested bytes in parts\n            \"\"\"\n            \n            remaining = n\n            \n            for part in self._parts:\n                if len(part) <= remaining:\n                    yield part\n                    remaining -= len(part)\n                else:\n                    yield part[:remaining]\n                    remaining = 0 \n                if remaining == 0:\n                    break\n                    \n        return \"\".join(part_generator(n))", "language": "python", "code": "def peek(self, n):\n        \"\"\"\n        Return the first `n` characters from the queue without\n        removing them.\n        Throws an error if there are less than `n` characters in the queue.\n        \n        Equivalent to::\n        \n            s = queue[:n]\n            \n        if `queue` where a regular string.\n        \"\"\"\n        if self._len < n:\n            raise ValueError(\"Not enough bytes in the queue\")\n        \n        def part_generator(n):\n            \"\"\"\n            Returns the requested bytes in parts\n            \"\"\"\n            \n            remaining = n\n            \n            for part in self._parts:\n                if len(part) <= remaining:\n                    yield part\n                    remaining -= len(part)\n                else:\n                    yield part[:remaining]\n                    remaining = 0 \n                if remaining == 0:\n                    break\n                    \n        return \"\".join(part_generator(n))", "code_tokens": ["def", "peek", "(", "self", ",", "n", ")", ":", "if", "self", ".", "_len", "<", "n", ":", "raise", "ValueError", "(", "\"Not enough bytes in the queue\"", ")", "def", "part_generator", "(", "n", ")", ":", "\"\"\"\n            Returns the requested bytes in parts\n            \"\"\"", "remaining", "=", "n", "for", "part", "in", "self", ".", "_parts", ":", "if", "len", "(", "part", ")", "<=", "remaining", ":", "yield", "part", "remaining", "-=", "len", "(", "part", ")", "else", ":", "yield", "part", "[", ":", "remaining", "]", "remaining", "=", "0", "if", "remaining", "==", "0", ":", "break", "return", "\"\"", ".", "join", "(", "part_generator", "(", "n", ")", ")"], "docstring": "Return the first `n` characters from the queue without\n        removing them.\n        Throws an error if there are less than `n` characters in the queue.\n        \n        Equivalent to::\n        \n            s = queue[:n]\n            \n        if `queue` where a regular string.", "docstring_tokens": ["Return", "the", "first", "n", "characters", "from", "the", "queue", "without", "removing", "them", ".", "Throws", "an", "error", "if", "there", "are", "less", "than", "n", "characters", "in", "the", "queue", ".", "Equivalent", "to", "::", "s", "=", "queue", "[", ":", "n", "]", "if", "queue", "where", "a", "regular", "string", "."], "sha": "43add96660258a14b24aa8e8413dffb1741b72d7", "url": "https://github.com/pydron/anycall/blob/43add96660258a14b24aa8e8413dffb1741b72d7/anycall/bytequeue.py#L93-L125", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/psutil/_psbsd.py", "func_name": "Process.get_memory_info", "original_string": "def get_memory_info(self):\n        \"\"\"Return a tuple with the process' RSS and VMS size.\"\"\"\n        rss, vms = _psutil_bsd.get_process_memory_info(self.pid)[:2]\n        return nt_meminfo(rss, vms)", "language": "python", "code": "def get_memory_info(self):\n        \"\"\"Return a tuple with the process' RSS and VMS size.\"\"\"\n        rss, vms = _psutil_bsd.get_process_memory_info(self.pid)[:2]\n        return nt_meminfo(rss, vms)", "code_tokens": ["def", "get_memory_info", "(", "self", ")", ":", "rss", ",", "vms", "=", "_psutil_bsd", ".", "get_process_memory_info", "(", "self", ".", "pid", ")", "[", ":", "2", "]", "return", "nt_meminfo", "(", "rss", ",", "vms", ")"], "docstring": "Return a tuple with the process' RSS and VMS size.", "docstring_tokens": ["Return", "a", "tuple", "with", "the", "process", "RSS", "and", "VMS", "size", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/psutil/_psbsd.py#L220-L223", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/pip/utils/__init__.py", "func_name": "dist_in_usersite", "original_string": "def dist_in_usersite(dist):\n    \"\"\"\n    Return True if given Distribution is installed in user site.\n    \"\"\"\n    norm_path = normalize_path(dist_location(dist))\n    return norm_path.startswith(normalize_path(user_site))", "language": "python", "code": "def dist_in_usersite(dist):\n    \"\"\"\n    Return True if given Distribution is installed in user site.\n    \"\"\"\n    norm_path = normalize_path(dist_location(dist))\n    return norm_path.startswith(normalize_path(user_site))", "code_tokens": ["def", "dist_in_usersite", "(", "dist", ")", ":", "norm_path", "=", "normalize_path", "(", "dist_location", "(", "dist", ")", ")", "return", "norm_path", ".", "startswith", "(", "normalize_path", "(", "user_site", ")", ")"], "docstring": "Return True if given Distribution is installed in user site.", "docstring_tokens": ["Return", "True", "if", "given", "Distribution", "is", "installed", "in", "user", "site", "."], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/pip/utils/__init__.py#L347-L352", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/vector_diffeomixture.py", "func_name": "quadrature_scheme_softmaxnormal_quantiles", "original_string": "def quadrature_scheme_softmaxnormal_quantiles(\n    normal_loc, normal_scale, quadrature_size,\n    validate_args=False, name=None):\n  \"\"\"Use SoftmaxNormal quantiles to form quadrature on `K - 1` simplex.\n\n  A `SoftmaxNormal` random variable `Y` may be generated via\n\n  ```\n  Y = SoftmaxCentered(X),\n  X = Normal(normal_loc, normal_scale)\n  ```\n\n  Args:\n    normal_loc: `float`-like `Tensor` with shape `[b1, ..., bB, K-1]`, B>=0.\n      The location parameter of the Normal used to construct the SoftmaxNormal.\n    normal_scale: `float`-like `Tensor`. Broadcastable with `normal_loc`.\n      The scale parameter of the Normal used to construct the SoftmaxNormal.\n    quadrature_size: Python `int` scalar representing the number of quadrature\n      points.\n    validate_args: Python `bool`, default `False`. When `True` distribution\n      parameters are checked for validity despite possibly degrading runtime\n      performance. When `False` invalid inputs may silently render incorrect\n      outputs.\n    name: Python `str` name prefixed to Ops created by this class.\n\n  Returns:\n    grid: Shape `[b1, ..., bB, K, quadrature_size]` `Tensor` representing the\n      convex combination of affine parameters for `K` components.\n      `grid[..., :, n]` is the `n`-th grid point, living in the `K - 1` simplex.\n    probs:  Shape `[b1, ..., bB, K, quadrature_size]` `Tensor` representing the\n      associated with each grid point.\n  \"\"\"\n  with tf.name_scope(name or \"softmax_normal_grid_and_probs\"):\n    normal_loc = tf.convert_to_tensor(value=normal_loc, name=\"normal_loc\")\n    dt = dtype_util.base_dtype(normal_loc.dtype)\n    normal_scale = tf.convert_to_tensor(\n        value=normal_scale, dtype=dt, name=\"normal_scale\")\n\n    normal_scale = maybe_check_quadrature_param(\n        normal_scale, \"normal_scale\", validate_args)\n\n    dist = normal.Normal(loc=normal_loc, scale=normal_scale)\n\n    def _get_batch_ndims():\n      \"\"\"Helper to get rank(dist.batch_shape), statically if possible.\"\"\"\n      ndims = tensorshape_util.rank(dist.batch_shape)\n      if ndims is None:\n        ndims = tf.shape(input=dist.batch_shape_tensor())[0]\n      return ndims\n    batch_ndims = _get_batch_ndims()\n\n    def _get_final_shape(qs):\n      \"\"\"Helper to build `TensorShape`.\"\"\"\n      bs = tensorshape_util.with_rank_at_least(dist.batch_shape, 1)\n      num_components = tf.compat.dimension_value(bs[-1])\n      if num_components is not None:\n        num_components += 1\n      tail = tf.TensorShape([num_components, qs])\n      return bs[:-1].concatenate(tail)\n\n    def _compute_quantiles():\n      \"\"\"Helper to build quantiles.\"\"\"\n      # Omit {0, 1} since they might lead to Inf/NaN.\n      zero = tf.zeros([], dtype=dist.dtype)\n      edges = tf.linspace(zero, 1., quadrature_size + 3)[1:-1]\n      # Expand edges so its broadcast across batch dims.\n      edges = tf.reshape(\n          edges,\n          shape=tf.concat(\n              [[-1], tf.ones([batch_ndims], dtype=tf.int32)], axis=0))\n      quantiles = dist.quantile(edges)\n      quantiles = softmax_centered_bijector.SoftmaxCentered().forward(quantiles)\n      # Cyclically permute left by one.\n      perm = tf.concat([tf.range(1, 1 + batch_ndims), [0]], axis=0)\n      quantiles = tf.transpose(a=quantiles, perm=perm)\n      tensorshape_util.set_shape(\n          quantiles, _get_final_shape(quadrature_size + 1))\n      return quantiles\n    quantiles = _compute_quantiles()\n\n    # Compute grid as quantile midpoints.\n    grid = (quantiles[..., :-1] + quantiles[..., 1:]) / 2.\n    # Set shape hints.\n    tensorshape_util.set_shape(grid, _get_final_shape(quadrature_size))\n\n    # By construction probs is constant, i.e., `1 / quadrature_size`. This is\n    # important, because non-constant probs leads to non-reparameterizable\n    # samples.\n    probs = tf.fill(\n        dims=[quadrature_size], value=1. / tf.cast(quadrature_size, dist.dtype))\n\n    return grid, probs", "language": "python", "code": "def quadrature_scheme_softmaxnormal_quantiles(\n    normal_loc, normal_scale, quadrature_size,\n    validate_args=False, name=None):\n  \"\"\"Use SoftmaxNormal quantiles to form quadrature on `K - 1` simplex.\n\n  A `SoftmaxNormal` random variable `Y` may be generated via\n\n  ```\n  Y = SoftmaxCentered(X),\n  X = Normal(normal_loc, normal_scale)\n  ```\n\n  Args:\n    normal_loc: `float`-like `Tensor` with shape `[b1, ..., bB, K-1]`, B>=0.\n      The location parameter of the Normal used to construct the SoftmaxNormal.\n    normal_scale: `float`-like `Tensor`. Broadcastable with `normal_loc`.\n      The scale parameter of the Normal used to construct the SoftmaxNormal.\n    quadrature_size: Python `int` scalar representing the number of quadrature\n      points.\n    validate_args: Python `bool`, default `False`. When `True` distribution\n      parameters are checked for validity despite possibly degrading runtime\n      performance. When `False` invalid inputs may silently render incorrect\n      outputs.\n    name: Python `str` name prefixed to Ops created by this class.\n\n  Returns:\n    grid: Shape `[b1, ..., bB, K, quadrature_size]` `Tensor` representing the\n      convex combination of affine parameters for `K` components.\n      `grid[..., :, n]` is the `n`-th grid point, living in the `K - 1` simplex.\n    probs:  Shape `[b1, ..., bB, K, quadrature_size]` `Tensor` representing the\n      associated with each grid point.\n  \"\"\"\n  with tf.name_scope(name or \"softmax_normal_grid_and_probs\"):\n    normal_loc = tf.convert_to_tensor(value=normal_loc, name=\"normal_loc\")\n    dt = dtype_util.base_dtype(normal_loc.dtype)\n    normal_scale = tf.convert_to_tensor(\n        value=normal_scale, dtype=dt, name=\"normal_scale\")\n\n    normal_scale = maybe_check_quadrature_param(\n        normal_scale, \"normal_scale\", validate_args)\n\n    dist = normal.Normal(loc=normal_loc, scale=normal_scale)\n\n    def _get_batch_ndims():\n      \"\"\"Helper to get rank(dist.batch_shape), statically if possible.\"\"\"\n      ndims = tensorshape_util.rank(dist.batch_shape)\n      if ndims is None:\n        ndims = tf.shape(input=dist.batch_shape_tensor())[0]\n      return ndims\n    batch_ndims = _get_batch_ndims()\n\n    def _get_final_shape(qs):\n      \"\"\"Helper to build `TensorShape`.\"\"\"\n      bs = tensorshape_util.with_rank_at_least(dist.batch_shape, 1)\n      num_components = tf.compat.dimension_value(bs[-1])\n      if num_components is not None:\n        num_components += 1\n      tail = tf.TensorShape([num_components, qs])\n      return bs[:-1].concatenate(tail)\n\n    def _compute_quantiles():\n      \"\"\"Helper to build quantiles.\"\"\"\n      # Omit {0, 1} since they might lead to Inf/NaN.\n      zero = tf.zeros([], dtype=dist.dtype)\n      edges = tf.linspace(zero, 1., quadrature_size + 3)[1:-1]\n      # Expand edges so its broadcast across batch dims.\n      edges = tf.reshape(\n          edges,\n          shape=tf.concat(\n              [[-1], tf.ones([batch_ndims], dtype=tf.int32)], axis=0))\n      quantiles = dist.quantile(edges)\n      quantiles = softmax_centered_bijector.SoftmaxCentered().forward(quantiles)\n      # Cyclically permute left by one.\n      perm = tf.concat([tf.range(1, 1 + batch_ndims), [0]], axis=0)\n      quantiles = tf.transpose(a=quantiles, perm=perm)\n      tensorshape_util.set_shape(\n          quantiles, _get_final_shape(quadrature_size + 1))\n      return quantiles\n    quantiles = _compute_quantiles()\n\n    # Compute grid as quantile midpoints.\n    grid = (quantiles[..., :-1] + quantiles[..., 1:]) / 2.\n    # Set shape hints.\n    tensorshape_util.set_shape(grid, _get_final_shape(quadrature_size))\n\n    # By construction probs is constant, i.e., `1 / quadrature_size`. This is\n    # important, because non-constant probs leads to non-reparameterizable\n    # samples.\n    probs = tf.fill(\n        dims=[quadrature_size], value=1. / tf.cast(quadrature_size, dist.dtype))\n\n    return grid, probs", "code_tokens": ["def", "quadrature_scheme_softmaxnormal_quantiles", "(", "normal_loc", ",", "normal_scale", ",", "quadrature_size", ",", "validate_args", "=", "False", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "name_scope", "(", "name", "or", "\"softmax_normal_grid_and_probs\"", ")", ":", "normal_loc", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "normal_loc", ",", "name", "=", "\"normal_loc\"", ")", "dt", "=", "dtype_util", ".", "base_dtype", "(", "normal_loc", ".", "dtype", ")", "normal_scale", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "normal_scale", ",", "dtype", "=", "dt", ",", "name", "=", "\"normal_scale\"", ")", "normal_scale", "=", "maybe_check_quadrature_param", "(", "normal_scale", ",", "\"normal_scale\"", ",", "validate_args", ")", "dist", "=", "normal", ".", "Normal", "(", "loc", "=", "normal_loc", ",", "scale", "=", "normal_scale", ")", "def", "_get_batch_ndims", "(", ")", ":", "\"\"\"Helper to get rank(dist.batch_shape), statically if possible.\"\"\"", "ndims", "=", "tensorshape_util", ".", "rank", "(", "dist", ".", "batch_shape", ")", "if", "ndims", "is", "None", ":", "ndims", "=", "tf", ".", "shape", "(", "input", "=", "dist", ".", "batch_shape_tensor", "(", ")", ")", "[", "0", "]", "return", "ndims", "batch_ndims", "=", "_get_batch_ndims", "(", ")", "def", "_get_final_shape", "(", "qs", ")", ":", "\"\"\"Helper to build `TensorShape`.\"\"\"", "bs", "=", "tensorshape_util", ".", "with_rank_at_least", "(", "dist", ".", "batch_shape", ",", "1", ")", "num_components", "=", "tf", ".", "compat", ".", "dimension_value", "(", "bs", "[", "-", "1", "]", ")", "if", "num_components", "is", "not", "None", ":", "num_components", "+=", "1", "tail", "=", "tf", ".", "TensorShape", "(", "[", "num_components", ",", "qs", "]", ")", "return", "bs", "[", ":", "-", "1", "]", ".", "concatenate", "(", "tail", ")", "def", "_compute_quantiles", "(", ")", ":", "\"\"\"Helper to build quantiles.\"\"\"", "# Omit {0, 1} since they might lead to Inf/NaN.", "zero", "=", "tf", ".", "zeros", "(", "[", "]", ",", "dtype", "=", "dist", ".", "dtype", ")", "edges", "=", "tf", ".", "linspace", "(", "zero", ",", "1.", ",", "quadrature_size", "+", "3", ")", "[", "1", ":", "-", "1", "]", "# Expand edges so its broadcast across batch dims.", "edges", "=", "tf", ".", "reshape", "(", "edges", ",", "shape", "=", "tf", ".", "concat", "(", "[", "[", "-", "1", "]", ",", "tf", ".", "ones", "(", "[", "batch_ndims", "]", ",", "dtype", "=", "tf", ".", "int32", ")", "]", ",", "axis", "=", "0", ")", ")", "quantiles", "=", "dist", ".", "quantile", "(", "edges", ")", "quantiles", "=", "softmax_centered_bijector", ".", "SoftmaxCentered", "(", ")", ".", "forward", "(", "quantiles", ")", "# Cyclically permute left by one.", "perm", "=", "tf", ".", "concat", "(", "[", "tf", ".", "range", "(", "1", ",", "1", "+", "batch_ndims", ")", ",", "[", "0", "]", "]", ",", "axis", "=", "0", ")", "quantiles", "=", "tf", ".", "transpose", "(", "a", "=", "quantiles", ",", "perm", "=", "perm", ")", "tensorshape_util", ".", "set_shape", "(", "quantiles", ",", "_get_final_shape", "(", "quadrature_size", "+", "1", ")", ")", "return", "quantiles", "quantiles", "=", "_compute_quantiles", "(", ")", "# Compute grid as quantile midpoints.", "grid", "=", "(", "quantiles", "[", "...", ",", ":", "-", "1", "]", "+", "quantiles", "[", "...", ",", "1", ":", "]", ")", "/", "2.", "# Set shape hints.", "tensorshape_util", ".", "set_shape", "(", "grid", ",", "_get_final_shape", "(", "quadrature_size", ")", ")", "# By construction probs is constant, i.e., `1 / quadrature_size`. This is", "# important, because non-constant probs leads to non-reparameterizable", "# samples.", "probs", "=", "tf", ".", "fill", "(", "dims", "=", "[", "quadrature_size", "]", ",", "value", "=", "1.", "/", "tf", ".", "cast", "(", "quadrature_size", ",", "dist", ".", "dtype", ")", ")", "return", "grid", ",", "probs"], "docstring": "Use SoftmaxNormal quantiles to form quadrature on `K - 1` simplex.\n\n  A `SoftmaxNormal` random variable `Y` may be generated via\n\n  ```\n  Y = SoftmaxCentered(X),\n  X = Normal(normal_loc, normal_scale)\n  ```\n\n  Args:\n    normal_loc: `float`-like `Tensor` with shape `[b1, ..., bB, K-1]`, B>=0.\n      The location parameter of the Normal used to construct the SoftmaxNormal.\n    normal_scale: `float`-like `Tensor`. Broadcastable with `normal_loc`.\n      The scale parameter of the Normal used to construct the SoftmaxNormal.\n    quadrature_size: Python `int` scalar representing the number of quadrature\n      points.\n    validate_args: Python `bool`, default `False`. When `True` distribution\n      parameters are checked for validity despite possibly degrading runtime\n      performance. When `False` invalid inputs may silently render incorrect\n      outputs.\n    name: Python `str` name prefixed to Ops created by this class.\n\n  Returns:\n    grid: Shape `[b1, ..., bB, K, quadrature_size]` `Tensor` representing the\n      convex combination of affine parameters for `K` components.\n      `grid[..., :, n]` is the `n`-th grid point, living in the `K - 1` simplex.\n    probs:  Shape `[b1, ..., bB, K, quadrature_size]` `Tensor` representing the\n      associated with each grid point.", "docstring_tokens": ["Use", "SoftmaxNormal", "quantiles", "to", "form", "quadrature", "on", "K", "-", "1", "simplex", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/vector_diffeomixture.py#L108-L199", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/core/history.py", "func_name": "HistoryAccessor._run_sql", "original_string": "def _run_sql(self, sql, params, raw=True, output=False):\n        \"\"\"Prepares and runs an SQL query for the history database.\n\n        Parameters\n        ----------\n        sql : str\n          Any filtering expressions to go after SELECT ... FROM ...\n        params : tuple\n          Parameters passed to the SQL query (to replace \"?\")\n        raw, output : bool\n          See :meth:`get_range`\n\n        Returns\n        -------\n        Tuples as :meth:`get_range`\n        \"\"\"\n        toget = 'source_raw' if raw else 'source'\n        sqlfrom = \"history\"\n        if output:\n            sqlfrom = \"history LEFT JOIN output_history USING (session, line)\"\n            toget = \"history.%s, output_history.output\" % toget\n        cur = self.db.execute(\"SELECT session, line, %s FROM %s \" %\\\n                                (toget, sqlfrom) + sql, params)\n        if output:    # Regroup into 3-tuples, and parse JSON\n            return ((ses, lin, (inp, out)) for ses, lin, inp, out in cur)\n        return cur", "language": "python", "code": "def _run_sql(self, sql, params, raw=True, output=False):\n        \"\"\"Prepares and runs an SQL query for the history database.\n\n        Parameters\n        ----------\n        sql : str\n          Any filtering expressions to go after SELECT ... FROM ...\n        params : tuple\n          Parameters passed to the SQL query (to replace \"?\")\n        raw, output : bool\n          See :meth:`get_range`\n\n        Returns\n        -------\n        Tuples as :meth:`get_range`\n        \"\"\"\n        toget = 'source_raw' if raw else 'source'\n        sqlfrom = \"history\"\n        if output:\n            sqlfrom = \"history LEFT JOIN output_history USING (session, line)\"\n            toget = \"history.%s, output_history.output\" % toget\n        cur = self.db.execute(\"SELECT session, line, %s FROM %s \" %\\\n                                (toget, sqlfrom) + sql, params)\n        if output:    # Regroup into 3-tuples, and parse JSON\n            return ((ses, lin, (inp, out)) for ses, lin, inp, out in cur)\n        return cur", "code_tokens": ["def", "_run_sql", "(", "self", ",", "sql", ",", "params", ",", "raw", "=", "True", ",", "output", "=", "False", ")", ":", "toget", "=", "'source_raw'", "if", "raw", "else", "'source'", "sqlfrom", "=", "\"history\"", "if", "output", ":", "sqlfrom", "=", "\"history LEFT JOIN output_history USING (session, line)\"", "toget", "=", "\"history.%s, output_history.output\"", "%", "toget", "cur", "=", "self", ".", "db", ".", "execute", "(", "\"SELECT session, line, %s FROM %s \"", "%", "(", "toget", ",", "sqlfrom", ")", "+", "sql", ",", "params", ")", "if", "output", ":", "# Regroup into 3-tuples, and parse JSON", "return", "(", "(", "ses", ",", "lin", ",", "(", "inp", ",", "out", ")", ")", "for", "ses", ",", "lin", ",", "inp", ",", "out", "in", "cur", ")", "return", "cur"], "docstring": "Prepares and runs an SQL query for the history database.\n\n        Parameters\n        ----------\n        sql : str\n          Any filtering expressions to go after SELECT ... FROM ...\n        params : tuple\n          Parameters passed to the SQL query (to replace \"?\")\n        raw, output : bool\n          See :meth:`get_range`\n\n        Returns\n        -------\n        Tuples as :meth:`get_range`", "docstring_tokens": ["Prepares", "and", "runs", "an", "SQL", "query", "for", "the", "history", "database", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/core/history.py#L175-L200", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/examples/sprites_dataset.py", "func_name": "read_image", "original_string": "def read_image(filepath):\n  \"\"\"Returns an image tensor.\"\"\"\n  im_bytes = tf.io.read_file(filepath)\n  im = tf.image.decode_image(im_bytes, channels=CHANNELS)\n  im = tf.image.convert_image_dtype(im, tf.float32)\n  return im", "language": "python", "code": "def read_image(filepath):\n  \"\"\"Returns an image tensor.\"\"\"\n  im_bytes = tf.io.read_file(filepath)\n  im = tf.image.decode_image(im_bytes, channels=CHANNELS)\n  im = tf.image.convert_image_dtype(im, tf.float32)\n  return im", "code_tokens": ["def", "read_image", "(", "filepath", ")", ":", "im_bytes", "=", "tf", ".", "io", ".", "read_file", "(", "filepath", ")", "im", "=", "tf", ".", "image", ".", "decode_image", "(", "im_bytes", ",", "channels", "=", "CHANNELS", ")", "im", "=", "tf", ".", "image", ".", "convert_image_dtype", "(", "im", ",", "tf", ".", "float32", ")", "return", "im"], "docstring": "Returns an image tensor.", "docstring_tokens": ["Returns", "an", "image", "tensor", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/examples/sprites_dataset.py#L113-L118", "partition": "test"}
{"repo": "un33k/django-toolware", "path": "toolware/templatetags/highlight.py", "func_name": "highlight", "original_string": "def highlight(string, keywords, cls_name='highlighted'):\n    \"\"\" Given an list of words, this function highlights the matched text in the given string. \"\"\"\n\n    if not keywords:\n        return string\n    if not string:\n        return ''\n    include, exclude = get_text_tokenizer(keywords)\n    highlighted = highlight_text(include, string, cls_name)\n    return highlighted", "language": "python", "code": "def highlight(string, keywords, cls_name='highlighted'):\n    \"\"\" Given an list of words, this function highlights the matched text in the given string. \"\"\"\n\n    if not keywords:\n        return string\n    if not string:\n        return ''\n    include, exclude = get_text_tokenizer(keywords)\n    highlighted = highlight_text(include, string, cls_name)\n    return highlighted", "code_tokens": ["def", "highlight", "(", "string", ",", "keywords", ",", "cls_name", "=", "'highlighted'", ")", ":", "if", "not", "keywords", ":", "return", "string", "if", "not", "string", ":", "return", "''", "include", ",", "exclude", "=", "get_text_tokenizer", "(", "keywords", ")", "highlighted", "=", "highlight_text", "(", "include", ",", "string", ",", "cls_name", ")", "return", "highlighted"], "docstring": "Given an list of words, this function highlights the matched text in the given string.", "docstring_tokens": ["Given", "an", "list", "of", "words", "this", "function", "highlights", "the", "matched", "text", "in", "the", "given", "string", "."], "sha": "973f3e003dc38b812897dab88455bee37dcaf931", "url": "https://github.com/un33k/django-toolware/blob/973f3e003dc38b812897dab88455bee37dcaf931/toolware/templatetags/highlight.py#L38-L47", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/distribution_util.py", "func_name": "expand_to_vector", "original_string": "def expand_to_vector(x, tensor_name=None, op_name=None, validate_args=False):\n  \"\"\"Transform a 0-D or 1-D `Tensor` to be 1-D.\n\n  For user convenience, many parts of the TensorFlow Probability API accept\n  inputs of rank 0 or 1 -- i.e., allowing an `event_shape` of `[5]` to be passed\n  to the API as either `5` or `[5]`.  This function can be used to transform\n  such an argument to always be 1-D.\n\n  NOTE: Python or NumPy values will be converted to `Tensor`s with standard type\n  inference/conversion.  In particular, an empty list or tuple will become an\n  empty `Tensor` with dtype `float32`.  Callers should convert values to\n  `Tensor`s before calling this function if different behavior is desired\n  (e.g. converting empty lists / other values to `Tensor`s with dtype `int32`).\n\n  Args:\n    x: A 0-D or 1-D `Tensor`.\n    tensor_name: Python `str` name for `Tensor`s created by this function.\n    op_name: Python `str` name for `Op`s created by this function.\n    validate_args: Python `bool, default `False`.  When `True`, arguments may be\n      checked for validity at execution time, possibly degrading runtime\n      performance.  When `False`, invalid inputs may silently render incorrect\n        outputs.\n  Returns:\n    vector: a 1-D `Tensor`.\n  \"\"\"\n  with tf.name_scope(op_name or \"expand_to_vector\"):\n    x = tf.convert_to_tensor(value=x, name=\"x\")\n    ndims = tensorshape_util.rank(x.shape)\n\n    if ndims is None:\n      # Maybe expand ndims from 0 to 1.\n      if validate_args:\n        x = with_dependencies([\n            assert_util.assert_rank_at_most(\n                x, 1, message=\"Input is neither scalar nor vector.\")\n        ], x)\n      ndims = tf.rank(x)\n      expanded_shape = pick_vector(\n          tf.equal(ndims, 0), np.array([1], dtype=np.int32), tf.shape(input=x))\n      return tf.reshape(x, expanded_shape)\n\n    elif ndims == 0:\n      # Definitely expand ndims from 0 to 1.\n      x_const = tf.get_static_value(x)\n      if x_const is not None:\n        return tf.convert_to_tensor(\n            value=dtype_util.as_numpy_dtype(x.dtype)([x_const]),\n            name=tensor_name)\n\n      else:\n        return tf.reshape(x, [1])\n\n    elif ndims != 1:\n      raise ValueError(\"Input is neither scalar nor vector.\")\n\n    # ndims == 1\n    return x", "language": "python", "code": "def expand_to_vector(x, tensor_name=None, op_name=None, validate_args=False):\n  \"\"\"Transform a 0-D or 1-D `Tensor` to be 1-D.\n\n  For user convenience, many parts of the TensorFlow Probability API accept\n  inputs of rank 0 or 1 -- i.e., allowing an `event_shape` of `[5]` to be passed\n  to the API as either `5` or `[5]`.  This function can be used to transform\n  such an argument to always be 1-D.\n\n  NOTE: Python or NumPy values will be converted to `Tensor`s with standard type\n  inference/conversion.  In particular, an empty list or tuple will become an\n  empty `Tensor` with dtype `float32`.  Callers should convert values to\n  `Tensor`s before calling this function if different behavior is desired\n  (e.g. converting empty lists / other values to `Tensor`s with dtype `int32`).\n\n  Args:\n    x: A 0-D or 1-D `Tensor`.\n    tensor_name: Python `str` name for `Tensor`s created by this function.\n    op_name: Python `str` name for `Op`s created by this function.\n    validate_args: Python `bool, default `False`.  When `True`, arguments may be\n      checked for validity at execution time, possibly degrading runtime\n      performance.  When `False`, invalid inputs may silently render incorrect\n        outputs.\n  Returns:\n    vector: a 1-D `Tensor`.\n  \"\"\"\n  with tf.name_scope(op_name or \"expand_to_vector\"):\n    x = tf.convert_to_tensor(value=x, name=\"x\")\n    ndims = tensorshape_util.rank(x.shape)\n\n    if ndims is None:\n      # Maybe expand ndims from 0 to 1.\n      if validate_args:\n        x = with_dependencies([\n            assert_util.assert_rank_at_most(\n                x, 1, message=\"Input is neither scalar nor vector.\")\n        ], x)\n      ndims = tf.rank(x)\n      expanded_shape = pick_vector(\n          tf.equal(ndims, 0), np.array([1], dtype=np.int32), tf.shape(input=x))\n      return tf.reshape(x, expanded_shape)\n\n    elif ndims == 0:\n      # Definitely expand ndims from 0 to 1.\n      x_const = tf.get_static_value(x)\n      if x_const is not None:\n        return tf.convert_to_tensor(\n            value=dtype_util.as_numpy_dtype(x.dtype)([x_const]),\n            name=tensor_name)\n\n      else:\n        return tf.reshape(x, [1])\n\n    elif ndims != 1:\n      raise ValueError(\"Input is neither scalar nor vector.\")\n\n    # ndims == 1\n    return x", "code_tokens": ["def", "expand_to_vector", "(", "x", ",", "tensor_name", "=", "None", ",", "op_name", "=", "None", ",", "validate_args", "=", "False", ")", ":", "with", "tf", ".", "name_scope", "(", "op_name", "or", "\"expand_to_vector\"", ")", ":", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "name", "=", "\"x\"", ")", "ndims", "=", "tensorshape_util", ".", "rank", "(", "x", ".", "shape", ")", "if", "ndims", "is", "None", ":", "# Maybe expand ndims from 0 to 1.", "if", "validate_args", ":", "x", "=", "with_dependencies", "(", "[", "assert_util", ".", "assert_rank_at_most", "(", "x", ",", "1", ",", "message", "=", "\"Input is neither scalar nor vector.\"", ")", "]", ",", "x", ")", "ndims", "=", "tf", ".", "rank", "(", "x", ")", "expanded_shape", "=", "pick_vector", "(", "tf", ".", "equal", "(", "ndims", ",", "0", ")", ",", "np", ".", "array", "(", "[", "1", "]", ",", "dtype", "=", "np", ".", "int32", ")", ",", "tf", ".", "shape", "(", "input", "=", "x", ")", ")", "return", "tf", ".", "reshape", "(", "x", ",", "expanded_shape", ")", "elif", "ndims", "==", "0", ":", "# Definitely expand ndims from 0 to 1.", "x_const", "=", "tf", ".", "get_static_value", "(", "x", ")", "if", "x_const", "is", "not", "None", ":", "return", "tf", ".", "convert_to_tensor", "(", "value", "=", "dtype_util", ".", "as_numpy_dtype", "(", "x", ".", "dtype", ")", "(", "[", "x_const", "]", ")", ",", "name", "=", "tensor_name", ")", "else", ":", "return", "tf", ".", "reshape", "(", "x", ",", "[", "1", "]", ")", "elif", "ndims", "!=", "1", ":", "raise", "ValueError", "(", "\"Input is neither scalar nor vector.\"", ")", "# ndims == 1", "return", "x"], "docstring": "Transform a 0-D or 1-D `Tensor` to be 1-D.\n\n  For user convenience, many parts of the TensorFlow Probability API accept\n  inputs of rank 0 or 1 -- i.e., allowing an `event_shape` of `[5]` to be passed\n  to the API as either `5` or `[5]`.  This function can be used to transform\n  such an argument to always be 1-D.\n\n  NOTE: Python or NumPy values will be converted to `Tensor`s with standard type\n  inference/conversion.  In particular, an empty list or tuple will become an\n  empty `Tensor` with dtype `float32`.  Callers should convert values to\n  `Tensor`s before calling this function if different behavior is desired\n  (e.g. converting empty lists / other values to `Tensor`s with dtype `int32`).\n\n  Args:\n    x: A 0-D or 1-D `Tensor`.\n    tensor_name: Python `str` name for `Tensor`s created by this function.\n    op_name: Python `str` name for `Op`s created by this function.\n    validate_args: Python `bool, default `False`.  When `True`, arguments may be\n      checked for validity at execution time, possibly degrading runtime\n      performance.  When `False`, invalid inputs may silently render incorrect\n        outputs.\n  Returns:\n    vector: a 1-D `Tensor`.", "docstring_tokens": ["Transform", "a", "0", "-", "D", "or", "1", "-", "D", "Tensor", "to", "be", "1", "-", "D", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/distribution_util.py#L2103-L2159", "partition": "test"}
{"repo": "rwl/godot", "path": "godot/ui/graph_view_model.py", "func_name": "GraphViewModel.godot_options", "original_string": "def godot_options(self, info):\n        \"\"\" Handles display of the options menu. \"\"\"\n\n        if info.initialized:\n            self.edit_traits( parent = info.ui.control,\n                              kind   = \"livemodal\",\n                              view   = \"options_view\" )", "language": "python", "code": "def godot_options(self, info):\n        \"\"\" Handles display of the options menu. \"\"\"\n\n        if info.initialized:\n            self.edit_traits( parent = info.ui.control,\n                              kind   = \"livemodal\",\n                              view   = \"options_view\" )", "code_tokens": ["def", "godot_options", "(", "self", ",", "info", ")", ":", "if", "info", ".", "initialized", ":", "self", ".", "edit_traits", "(", "parent", "=", "info", ".", "ui", ".", "control", ",", "kind", "=", "\"livemodal\"", ",", "view", "=", "\"options_view\"", ")"], "docstring": "Handles display of the options menu.", "docstring_tokens": ["Handles", "display", "of", "the", "options", "menu", "."], "sha": "013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f", "url": "https://github.com/rwl/godot/blob/013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f/godot/ui/graph_view_model.py#L445-L451", "partition": "test"}
{"repo": "treycucco/pyebnf", "path": "pyebnf/compiler.py", "func_name": "Compiler._get_rule_transform", "original_string": "def _get_rule_transform(self, rule):\n    \"\"\"The return value for each rule can be either retyped, compressed or left alone. This method\n    determines that and returns the source code text for accomplishing it.\n    \"\"\"\n    rd = self._find_directive(lambda d: d.name == \"rule\" and d.args.get(\"name\") == rule.name)\n\n    if rd:\n      args = rd.args\n    else:\n      args = {}\n\n    transform = args.get(\"transform\", \"retype\")\n\n    if transform == \"retype\":\n      new_name = args.get(\"to_type\", \"TokenType.{0}\".format(rule.name))\n      return \".retyped({0})\".format(new_name)\n    elif transform == \"compress\":\n      new_name = args.get(\"to_type\", \"TokenType.{0}\".format(rule.name))\n      if new_name == \"identity\":\n        return \".compressed()\"\n      else:\n        return \".compressed({0})\".format(new_name)\n    elif transform == \"identity\":\n      return \"\"", "language": "python", "code": "def _get_rule_transform(self, rule):\n    \"\"\"The return value for each rule can be either retyped, compressed or left alone. This method\n    determines that and returns the source code text for accomplishing it.\n    \"\"\"\n    rd = self._find_directive(lambda d: d.name == \"rule\" and d.args.get(\"name\") == rule.name)\n\n    if rd:\n      args = rd.args\n    else:\n      args = {}\n\n    transform = args.get(\"transform\", \"retype\")\n\n    if transform == \"retype\":\n      new_name = args.get(\"to_type\", \"TokenType.{0}\".format(rule.name))\n      return \".retyped({0})\".format(new_name)\n    elif transform == \"compress\":\n      new_name = args.get(\"to_type\", \"TokenType.{0}\".format(rule.name))\n      if new_name == \"identity\":\n        return \".compressed()\"\n      else:\n        return \".compressed({0})\".format(new_name)\n    elif transform == \"identity\":\n      return \"\"", "code_tokens": ["def", "_get_rule_transform", "(", "self", ",", "rule", ")", ":", "rd", "=", "self", ".", "_find_directive", "(", "lambda", "d", ":", "d", ".", "name", "==", "\"rule\"", "and", "d", ".", "args", ".", "get", "(", "\"name\"", ")", "==", "rule", ".", "name", ")", "if", "rd", ":", "args", "=", "rd", ".", "args", "else", ":", "args", "=", "{", "}", "transform", "=", "args", ".", "get", "(", "\"transform\"", ",", "\"retype\"", ")", "if", "transform", "==", "\"retype\"", ":", "new_name", "=", "args", ".", "get", "(", "\"to_type\"", ",", "\"TokenType.{0}\"", ".", "format", "(", "rule", ".", "name", ")", ")", "return", "\".retyped({0})\"", ".", "format", "(", "new_name", ")", "elif", "transform", "==", "\"compress\"", ":", "new_name", "=", "args", ".", "get", "(", "\"to_type\"", ",", "\"TokenType.{0}\"", ".", "format", "(", "rule", ".", "name", ")", ")", "if", "new_name", "==", "\"identity\"", ":", "return", "\".compressed()\"", "else", ":", "return", "\".compressed({0})\"", ".", "format", "(", "new_name", ")", "elif", "transform", "==", "\"identity\"", ":", "return", "\"\""], "docstring": "The return value for each rule can be either retyped, compressed or left alone. This method\n    determines that and returns the source code text for accomplishing it.", "docstring_tokens": ["The", "return", "value", "for", "each", "rule", "can", "be", "either", "retyped", "compressed", "or", "left", "alone", ".", "This", "method", "determines", "that", "and", "returns", "the", "source", "code", "text", "for", "accomplishing", "it", "."], "sha": "3634ddabbe5d73508bcc20f4a591f86a46634e1d", "url": "https://github.com/treycucco/pyebnf/blob/3634ddabbe5d73508bcc20f4a591f86a46634e1d/pyebnf/compiler.py#L208-L231", "partition": "test"}
{"repo": "BD2KGenomics/toil-lib", "path": "src/toil_lib/tools/preprocessing.py", "func_name": "picard_mark_duplicates", "original_string": "def picard_mark_duplicates(job, bam, bai, validation_stringency='LENIENT'):\n    \"\"\"\n    Runs Picard MarkDuplicates on a BAM file. Requires that the BAM file be coordinate sorted.\n\n    :param JobFunctionWrappingJob job: passed automatically by Toil\n    :param str bam: FileStoreID for BAM file\n    :param str bai: FileStoreID for BAM index file\n    :param str validation_stringency: BAM file validation stringency, default is LENIENT\n    :return: FileStoreIDs for BAM and BAI files\n    :rtype: tuple\n    \"\"\"\n    work_dir = job.fileStore.getLocalTempDir()\n\n    # Retrieve file path\n    job.fileStore.readGlobalFile(bam, os.path.join(work_dir, 'sorted.bam'))\n    job.fileStore.readGlobalFile(bai, os.path.join(work_dir, 'sorted.bai'))\n\n    # Call: picardtools\n    command = ['MarkDuplicates',\n               'INPUT=sorted.bam',\n               'OUTPUT=mkdups.bam',\n               'METRICS_FILE=metrics.txt',\n               'ASSUME_SORTED=true',\n               'CREATE_INDEX=true',\n               'VALIDATION_STRINGENCY=%s' % validation_stringency.upper()]\n\n    # picard-tools container doesn't have JAVA_OPTS variable\n    # Set TMPDIR to /data to prevent writing temporary files to /tmp\n    docker_parameters = ['--rm',\n                         '--log-driver', 'none',\n                         '-e', 'JAVA_OPTIONS=-Djava.io.tmpdir=/data/ -Xmx{}'.format(job.memory),\n                         '-v', '{}:/data'.format(work_dir)]\n\n    start_time = time.time()\n    dockerCall(job=job, workDir=work_dir,\n               parameters=command,\n               tool='quay.io/ucsc_cgl/picardtools:1.95--dd5ac549b95eb3e5d166a5e310417ef13651994e',\n               dockerParameters=docker_parameters)\n    end_time = time.time()\n    _log_runtime(job, start_time, end_time, \"Picard MarkDuplicates\")\n\n    bam = job.fileStore.writeGlobalFile(os.path.join(work_dir, 'mkdups.bam'))\n    bai = job.fileStore.writeGlobalFile(os.path.join(work_dir, 'mkdups.bai'))\n    return bam, bai", "language": "python", "code": "def picard_mark_duplicates(job, bam, bai, validation_stringency='LENIENT'):\n    \"\"\"\n    Runs Picard MarkDuplicates on a BAM file. Requires that the BAM file be coordinate sorted.\n\n    :param JobFunctionWrappingJob job: passed automatically by Toil\n    :param str bam: FileStoreID for BAM file\n    :param str bai: FileStoreID for BAM index file\n    :param str validation_stringency: BAM file validation stringency, default is LENIENT\n    :return: FileStoreIDs for BAM and BAI files\n    :rtype: tuple\n    \"\"\"\n    work_dir = job.fileStore.getLocalTempDir()\n\n    # Retrieve file path\n    job.fileStore.readGlobalFile(bam, os.path.join(work_dir, 'sorted.bam'))\n    job.fileStore.readGlobalFile(bai, os.path.join(work_dir, 'sorted.bai'))\n\n    # Call: picardtools\n    command = ['MarkDuplicates',\n               'INPUT=sorted.bam',\n               'OUTPUT=mkdups.bam',\n               'METRICS_FILE=metrics.txt',\n               'ASSUME_SORTED=true',\n               'CREATE_INDEX=true',\n               'VALIDATION_STRINGENCY=%s' % validation_stringency.upper()]\n\n    # picard-tools container doesn't have JAVA_OPTS variable\n    # Set TMPDIR to /data to prevent writing temporary files to /tmp\n    docker_parameters = ['--rm',\n                         '--log-driver', 'none',\n                         '-e', 'JAVA_OPTIONS=-Djava.io.tmpdir=/data/ -Xmx{}'.format(job.memory),\n                         '-v', '{}:/data'.format(work_dir)]\n\n    start_time = time.time()\n    dockerCall(job=job, workDir=work_dir,\n               parameters=command,\n               tool='quay.io/ucsc_cgl/picardtools:1.95--dd5ac549b95eb3e5d166a5e310417ef13651994e',\n               dockerParameters=docker_parameters)\n    end_time = time.time()\n    _log_runtime(job, start_time, end_time, \"Picard MarkDuplicates\")\n\n    bam = job.fileStore.writeGlobalFile(os.path.join(work_dir, 'mkdups.bam'))\n    bai = job.fileStore.writeGlobalFile(os.path.join(work_dir, 'mkdups.bai'))\n    return bam, bai", "code_tokens": ["def", "picard_mark_duplicates", "(", "job", ",", "bam", ",", "bai", ",", "validation_stringency", "=", "'LENIENT'", ")", ":", "work_dir", "=", "job", ".", "fileStore", ".", "getLocalTempDir", "(", ")", "# Retrieve file path", "job", ".", "fileStore", ".", "readGlobalFile", "(", "bam", ",", "os", ".", "path", ".", "join", "(", "work_dir", ",", "'sorted.bam'", ")", ")", "job", ".", "fileStore", ".", "readGlobalFile", "(", "bai", ",", "os", ".", "path", ".", "join", "(", "work_dir", ",", "'sorted.bai'", ")", ")", "# Call: picardtools", "command", "=", "[", "'MarkDuplicates'", ",", "'INPUT=sorted.bam'", ",", "'OUTPUT=mkdups.bam'", ",", "'METRICS_FILE=metrics.txt'", ",", "'ASSUME_SORTED=true'", ",", "'CREATE_INDEX=true'", ",", "'VALIDATION_STRINGENCY=%s'", "%", "validation_stringency", ".", "upper", "(", ")", "]", "# picard-tools container doesn't have JAVA_OPTS variable", "# Set TMPDIR to /data to prevent writing temporary files to /tmp", "docker_parameters", "=", "[", "'--rm'", ",", "'--log-driver'", ",", "'none'", ",", "'-e'", ",", "'JAVA_OPTIONS=-Djava.io.tmpdir=/data/ -Xmx{}'", ".", "format", "(", "job", ".", "memory", ")", ",", "'-v'", ",", "'{}:/data'", ".", "format", "(", "work_dir", ")", "]", "start_time", "=", "time", ".", "time", "(", ")", "dockerCall", "(", "job", "=", "job", ",", "workDir", "=", "work_dir", ",", "parameters", "=", "command", ",", "tool", "=", "'quay.io/ucsc_cgl/picardtools:1.95--dd5ac549b95eb3e5d166a5e310417ef13651994e'", ",", "dockerParameters", "=", "docker_parameters", ")", "end_time", "=", "time", ".", "time", "(", ")", "_log_runtime", "(", "job", ",", "start_time", ",", "end_time", ",", "\"Picard MarkDuplicates\"", ")", "bam", "=", "job", ".", "fileStore", ".", "writeGlobalFile", "(", "os", ".", "path", ".", "join", "(", "work_dir", ",", "'mkdups.bam'", ")", ")", "bai", "=", "job", ".", "fileStore", ".", "writeGlobalFile", "(", "os", ".", "path", ".", "join", "(", "work_dir", ",", "'mkdups.bai'", ")", ")", "return", "bam", ",", "bai"], "docstring": "Runs Picard MarkDuplicates on a BAM file. Requires that the BAM file be coordinate sorted.\n\n    :param JobFunctionWrappingJob job: passed automatically by Toil\n    :param str bam: FileStoreID for BAM file\n    :param str bai: FileStoreID for BAM index file\n    :param str validation_stringency: BAM file validation stringency, default is LENIENT\n    :return: FileStoreIDs for BAM and BAI files\n    :rtype: tuple", "docstring_tokens": ["Runs", "Picard", "MarkDuplicates", "on", "a", "BAM", "file", ".", "Requires", "that", "the", "BAM", "file", "be", "coordinate", "sorted", "."], "sha": "022a615fc3dc98fc1aaa7bfd232409962ca44fbd", "url": "https://github.com/BD2KGenomics/toil-lib/blob/022a615fc3dc98fc1aaa7bfd232409962ca44fbd/src/toil_lib/tools/preprocessing.py#L297-L340", "partition": "test"}
{"repo": "jerith/txfake", "path": "txfake/fake_connection.py", "func_name": "patch_transport_fake_push_producer", "original_string": "def patch_transport_fake_push_producer(transport):\n    \"\"\"\n    Patch the three methods belonging to IPushProducer onto the transport if it\n    doesn't already have them. (`Agent` assumes its transport has these.)\n    \"\"\"\n    patch_if_missing(transport, 'pauseProducing', lambda: None)\n    patch_if_missing(transport, 'resumeProducing', lambda: None)\n    patch_if_missing(transport, 'stopProducing', transport.loseConnection)", "language": "python", "code": "def patch_transport_fake_push_producer(transport):\n    \"\"\"\n    Patch the three methods belonging to IPushProducer onto the transport if it\n    doesn't already have them. (`Agent` assumes its transport has these.)\n    \"\"\"\n    patch_if_missing(transport, 'pauseProducing', lambda: None)\n    patch_if_missing(transport, 'resumeProducing', lambda: None)\n    patch_if_missing(transport, 'stopProducing', transport.loseConnection)", "code_tokens": ["def", "patch_transport_fake_push_producer", "(", "transport", ")", ":", "patch_if_missing", "(", "transport", ",", "'pauseProducing'", ",", "lambda", ":", "None", ")", "patch_if_missing", "(", "transport", ",", "'resumeProducing'", ",", "lambda", ":", "None", ")", "patch_if_missing", "(", "transport", ",", "'stopProducing'", ",", "transport", ".", "loseConnection", ")"], "docstring": "Patch the three methods belonging to IPushProducer onto the transport if it\n    doesn't already have them. (`Agent` assumes its transport has these.)", "docstring_tokens": ["Patch", "the", "three", "methods", "belonging", "to", "IPushProducer", "onto", "the", "transport", "if", "it", "doesn", "t", "already", "have", "them", ".", "(", "Agent", "assumes", "its", "transport", "has", "these", ".", ")"], "sha": "5c1cda2b9a56458c254d0d9476b6c426d57f5757", "url": "https://github.com/jerith/txfake/blob/5c1cda2b9a56458c254d0d9476b6c426d57f5757/txfake/fake_connection.py#L210-L217", "partition": "test"}
{"repo": "athre0z/wasm", "path": "wasm/formatter.py", "func_name": "format_function", "original_string": "def format_function(\n    func_body,\n    func_type=None,\n    indent=2,\n    format_locals=True,\n):\n    \"\"\"\n    Takes a `FunctionBody` and optionally a `FunctionType`, yielding the string \n    representation of the function line by line. The function type is required\n    for formatting function parameter and return value information.\n    \"\"\"\n    if func_type is None:\n        yield 'func'\n    else:\n        param_section = ' (param {})'.format(' '.join(\n            map(format_lang_type, func_type.param_types)\n        )) if func_type.param_types else ''\n        result_section = ' (result {})'.format(\n            format_lang_type(func_type.return_type)\n        ) if func_type.return_type else ''\n        yield 'func' + param_section + result_section\n\n    if format_locals and func_body.locals:\n        yield '(locals {})'.format(' '.join(itertools.chain.from_iterable(\n            itertools.repeat(format_lang_type(x.type), x.count)\n            for x in func_body.locals\n        )))\n\n    level = 1\n    for cur_insn in decode_bytecode(func_body.code):\n        if cur_insn.op.flags & INSN_LEAVE_BLOCK:\n            level -= 1\n        yield ' ' * (level * indent) + format_instruction(cur_insn)\n        if cur_insn.op.flags & INSN_ENTER_BLOCK:\n            level += 1", "language": "python", "code": "def format_function(\n    func_body,\n    func_type=None,\n    indent=2,\n    format_locals=True,\n):\n    \"\"\"\n    Takes a `FunctionBody` and optionally a `FunctionType`, yielding the string \n    representation of the function line by line. The function type is required\n    for formatting function parameter and return value information.\n    \"\"\"\n    if func_type is None:\n        yield 'func'\n    else:\n        param_section = ' (param {})'.format(' '.join(\n            map(format_lang_type, func_type.param_types)\n        )) if func_type.param_types else ''\n        result_section = ' (result {})'.format(\n            format_lang_type(func_type.return_type)\n        ) if func_type.return_type else ''\n        yield 'func' + param_section + result_section\n\n    if format_locals and func_body.locals:\n        yield '(locals {})'.format(' '.join(itertools.chain.from_iterable(\n            itertools.repeat(format_lang_type(x.type), x.count)\n            for x in func_body.locals\n        )))\n\n    level = 1\n    for cur_insn in decode_bytecode(func_body.code):\n        if cur_insn.op.flags & INSN_LEAVE_BLOCK:\n            level -= 1\n        yield ' ' * (level * indent) + format_instruction(cur_insn)\n        if cur_insn.op.flags & INSN_ENTER_BLOCK:\n            level += 1", "code_tokens": ["def", "format_function", "(", "func_body", ",", "func_type", "=", "None", ",", "indent", "=", "2", ",", "format_locals", "=", "True", ",", ")", ":", "if", "func_type", "is", "None", ":", "yield", "'func'", "else", ":", "param_section", "=", "' (param {})'", ".", "format", "(", "' '", ".", "join", "(", "map", "(", "format_lang_type", ",", "func_type", ".", "param_types", ")", ")", ")", "if", "func_type", ".", "param_types", "else", "''", "result_section", "=", "' (result {})'", ".", "format", "(", "format_lang_type", "(", "func_type", ".", "return_type", ")", ")", "if", "func_type", ".", "return_type", "else", "''", "yield", "'func'", "+", "param_section", "+", "result_section", "if", "format_locals", "and", "func_body", ".", "locals", ":", "yield", "'(locals {})'", ".", "format", "(", "' '", ".", "join", "(", "itertools", ".", "chain", ".", "from_iterable", "(", "itertools", ".", "repeat", "(", "format_lang_type", "(", "x", ".", "type", ")", ",", "x", ".", "count", ")", "for", "x", "in", "func_body", ".", "locals", ")", ")", ")", "level", "=", "1", "for", "cur_insn", "in", "decode_bytecode", "(", "func_body", ".", "code", ")", ":", "if", "cur_insn", ".", "op", ".", "flags", "&", "INSN_LEAVE_BLOCK", ":", "level", "-=", "1", "yield", "' '", "*", "(", "level", "*", "indent", ")", "+", "format_instruction", "(", "cur_insn", ")", "if", "cur_insn", ".", "op", ".", "flags", "&", "INSN_ENTER_BLOCK", ":", "level", "+=", "1"], "docstring": "Takes a `FunctionBody` and optionally a `FunctionType`, yielding the string \n    representation of the function line by line. The function type is required\n    for formatting function parameter and return value information.", "docstring_tokens": ["Takes", "a", "FunctionBody", "and", "optionally", "a", "FunctionType", "yielding", "the", "string", "representation", "of", "the", "function", "line", "by", "line", ".", "The", "function", "type", "is", "required", "for", "formatting", "function", "parameter", "and", "return", "value", "information", "."], "sha": "bc9c7e3f40242a2a8fc9650c4b994f0cddf8d755", "url": "https://github.com/athre0z/wasm/blob/bc9c7e3f40242a2a8fc9650c4b994f0cddf8d755/wasm/formatter.py#L46-L80", "partition": "test"}
{"repo": "Chilipp/sphinx-nbexamples", "path": "sphinx_nbexamples/__init__.py", "func_name": "NotebookProcessor.get_description", "original_string": "def get_description(self):\n        \"\"\"Get summary and description of this notebook\"\"\"\n        def split_header(s, get_header=True):\n            s = s.lstrip().rstrip()\n            parts = s.splitlines()\n            if parts[0].startswith('#'):\n                if get_header:\n                    header = re.sub('#+\\s*', '', parts.pop(0))\n                    if not parts:\n                        return header, ''\n                else:\n                    header = ''\n                rest = '\\n'.join(parts).lstrip().split('\\n\\n')\n                desc = rest[0].replace('\\n', ' ')\n                return header, desc\n            else:\n                if get_header:\n                    if parts[0].startswith(('=', '-')):\n                        parts = parts[1:]\n                    header = parts.pop(0)\n                    if parts and parts[0].startswith(('=', '-')):\n                        parts.pop(0)\n                    if not parts:\n                        return header, ''\n                else:\n                    header = ''\n                rest = '\\n'.join(parts).lstrip().split('\\n\\n')\n                desc = rest[0].replace('\\n', ' ')\n                return header, desc\n\n        first_cell = self.nb['cells'][0]\n\n        if not first_cell['cell_type'] == 'markdown':\n            return '', ''\n        header, desc = split_header(first_cell['source'])\n        if not desc and len(self.nb['cells']) > 1:\n            second_cell = self.nb['cells'][1]\n            if second_cell['cell_type'] == 'markdown':\n                _, desc = split_header(second_cell['source'], False)\n        return header, desc", "language": "python", "code": "def get_description(self):\n        \"\"\"Get summary and description of this notebook\"\"\"\n        def split_header(s, get_header=True):\n            s = s.lstrip().rstrip()\n            parts = s.splitlines()\n            if parts[0].startswith('#'):\n                if get_header:\n                    header = re.sub('#+\\s*', '', parts.pop(0))\n                    if not parts:\n                        return header, ''\n                else:\n                    header = ''\n                rest = '\\n'.join(parts).lstrip().split('\\n\\n')\n                desc = rest[0].replace('\\n', ' ')\n                return header, desc\n            else:\n                if get_header:\n                    if parts[0].startswith(('=', '-')):\n                        parts = parts[1:]\n                    header = parts.pop(0)\n                    if parts and parts[0].startswith(('=', '-')):\n                        parts.pop(0)\n                    if not parts:\n                        return header, ''\n                else:\n                    header = ''\n                rest = '\\n'.join(parts).lstrip().split('\\n\\n')\n                desc = rest[0].replace('\\n', ' ')\n                return header, desc\n\n        first_cell = self.nb['cells'][0]\n\n        if not first_cell['cell_type'] == 'markdown':\n            return '', ''\n        header, desc = split_header(first_cell['source'])\n        if not desc and len(self.nb['cells']) > 1:\n            second_cell = self.nb['cells'][1]\n            if second_cell['cell_type'] == 'markdown':\n                _, desc = split_header(second_cell['source'], False)\n        return header, desc", "code_tokens": ["def", "get_description", "(", "self", ")", ":", "def", "split_header", "(", "s", ",", "get_header", "=", "True", ")", ":", "s", "=", "s", ".", "lstrip", "(", ")", ".", "rstrip", "(", ")", "parts", "=", "s", ".", "splitlines", "(", ")", "if", "parts", "[", "0", "]", ".", "startswith", "(", "'#'", ")", ":", "if", "get_header", ":", "header", "=", "re", ".", "sub", "(", "'#+\\s*'", ",", "''", ",", "parts", ".", "pop", "(", "0", ")", ")", "if", "not", "parts", ":", "return", "header", ",", "''", "else", ":", "header", "=", "''", "rest", "=", "'\\n'", ".", "join", "(", "parts", ")", ".", "lstrip", "(", ")", ".", "split", "(", "'\\n\\n'", ")", "desc", "=", "rest", "[", "0", "]", ".", "replace", "(", "'\\n'", ",", "' '", ")", "return", "header", ",", "desc", "else", ":", "if", "get_header", ":", "if", "parts", "[", "0", "]", ".", "startswith", "(", "(", "'='", ",", "'-'", ")", ")", ":", "parts", "=", "parts", "[", "1", ":", "]", "header", "=", "parts", ".", "pop", "(", "0", ")", "if", "parts", "and", "parts", "[", "0", "]", ".", "startswith", "(", "(", "'='", ",", "'-'", ")", ")", ":", "parts", ".", "pop", "(", "0", ")", "if", "not", "parts", ":", "return", "header", ",", "''", "else", ":", "header", "=", "''", "rest", "=", "'\\n'", ".", "join", "(", "parts", ")", ".", "lstrip", "(", ")", ".", "split", "(", "'\\n\\n'", ")", "desc", "=", "rest", "[", "0", "]", ".", "replace", "(", "'\\n'", ",", "' '", ")", "return", "header", ",", "desc", "first_cell", "=", "self", ".", "nb", "[", "'cells'", "]", "[", "0", "]", "if", "not", "first_cell", "[", "'cell_type'", "]", "==", "'markdown'", ":", "return", "''", ",", "''", "header", ",", "desc", "=", "split_header", "(", "first_cell", "[", "'source'", "]", ")", "if", "not", "desc", "and", "len", "(", "self", ".", "nb", "[", "'cells'", "]", ")", ">", "1", ":", "second_cell", "=", "self", ".", "nb", "[", "'cells'", "]", "[", "1", "]", "if", "second_cell", "[", "'cell_type'", "]", "==", "'markdown'", ":", "_", ",", "desc", "=", "split_header", "(", "second_cell", "[", "'source'", "]", ",", "False", ")", "return", "header", ",", "desc"], "docstring": "Get summary and description of this notebook", "docstring_tokens": ["Get", "summary", "and", "description", "of", "this", "notebook"], "sha": "08e0319ff3c70f8a931dfa8890caf48add4d0470", "url": "https://github.com/Chilipp/sphinx-nbexamples/blob/08e0319ff3c70f8a931dfa8890caf48add4d0470/sphinx_nbexamples/__init__.py#L509-L548", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/core/inputsplitter.py", "func_name": "has_comment", "original_string": "def has_comment(src):\n    \"\"\"Indicate whether an input line has (i.e. ends in, or is) a comment.\n    \n    This uses tokenize, so it can distinguish comments from # inside strings.\n    \n    Parameters\n    ----------\n    src : string\n      A single line input string.\n    \n    Returns\n    -------\n    Boolean: True if source has a comment.\n    \"\"\"\n    readline = StringIO(src).readline\n    toktypes = set()\n    try:\n        for t in tokenize.generate_tokens(readline):\n            toktypes.add(t[0])\n    except tokenize.TokenError:\n        pass\n    return(tokenize.COMMENT in toktypes)", "language": "python", "code": "def has_comment(src):\n    \"\"\"Indicate whether an input line has (i.e. ends in, or is) a comment.\n    \n    This uses tokenize, so it can distinguish comments from # inside strings.\n    \n    Parameters\n    ----------\n    src : string\n      A single line input string.\n    \n    Returns\n    -------\n    Boolean: True if source has a comment.\n    \"\"\"\n    readline = StringIO(src).readline\n    toktypes = set()\n    try:\n        for t in tokenize.generate_tokens(readline):\n            toktypes.add(t[0])\n    except tokenize.TokenError:\n        pass\n    return(tokenize.COMMENT in toktypes)", "code_tokens": ["def", "has_comment", "(", "src", ")", ":", "readline", "=", "StringIO", "(", "src", ")", ".", "readline", "toktypes", "=", "set", "(", ")", "try", ":", "for", "t", "in", "tokenize", ".", "generate_tokens", "(", "readline", ")", ":", "toktypes", ".", "add", "(", "t", "[", "0", "]", ")", "except", "tokenize", ".", "TokenError", ":", "pass", "return", "(", "tokenize", ".", "COMMENT", "in", "toktypes", ")"], "docstring": "Indicate whether an input line has (i.e. ends in, or is) a comment.\n    \n    This uses tokenize, so it can distinguish comments from # inside strings.\n    \n    Parameters\n    ----------\n    src : string\n      A single line input string.\n    \n    Returns\n    -------\n    Boolean: True if source has a comment.", "docstring_tokens": ["Indicate", "whether", "an", "input", "line", "has", "(", "i", ".", "e", ".", "ends", "in", "or", "is", ")", "a", "comment", ".", "This", "uses", "tokenize", "so", "it", "can", "distinguish", "comments", "from", "#", "inside", "strings", ".", "Parameters", "----------", "src", ":", "string", "A", "single", "line", "input", "string", ".", "Returns", "-------", "Boolean", ":", "True", "if", "source", "has", "a", "comment", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/core/inputsplitter.py#L206-L227", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/zmq/ipkernel.py", "func_name": "Kernel._topic", "original_string": "def _topic(self, topic):\n        \"\"\"prefixed topic for IOPub messages\"\"\"\n        if self.int_id >= 0:\n            base = \"engine.%i\" % self.int_id\n        else:\n            base = \"kernel.%s\" % self.ident\n        \n        return py3compat.cast_bytes(\"%s.%s\" % (base, topic))", "language": "python", "code": "def _topic(self, topic):\n        \"\"\"prefixed topic for IOPub messages\"\"\"\n        if self.int_id >= 0:\n            base = \"engine.%i\" % self.int_id\n        else:\n            base = \"kernel.%s\" % self.ident\n        \n        return py3compat.cast_bytes(\"%s.%s\" % (base, topic))", "code_tokens": ["def", "_topic", "(", "self", ",", "topic", ")", ":", "if", "self", ".", "int_id", ">=", "0", ":", "base", "=", "\"engine.%i\"", "%", "self", ".", "int_id", "else", ":", "base", "=", "\"kernel.%s\"", "%", "self", ".", "ident", "return", "py3compat", ".", "cast_bytes", "(", "\"%s.%s\"", "%", "(", "base", ",", "topic", ")", ")"], "docstring": "prefixed topic for IOPub messages", "docstring_tokens": ["prefixed", "topic", "for", "IOPub", "messages"], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/zmq/ipkernel.py#L647-L654", "partition": "test"}
{"repo": "zinic/pynsive", "path": "pynsive/reflection.py", "func_name": "rdiscover_modules", "original_string": "def rdiscover_modules(directory):\n    \"\"\"\n    Attempts to list all of the modules and submodules found within a given\n    directory tree. This function recursively searches the directory tree\n    for potential python modules and returns a list of candidate names.\n\n    **Note:** This function returns a list of strings representing\n    discovered module names, not the actual, loaded modules.\n\n    :param directory: the directory to search for modules.\n    \"\"\"\n    found = list()\n\n    if os.path.isdir(directory):\n        for entry in os.listdir(directory):\n            next_dir = os.path.join(directory, entry)\n\n            # Scan only if there's an __init__.py file\n            if os.path.isfile(os.path.join(next_dir, MODULE_INIT_FILE)):\n                modules = _search_for_modules(next_dir, True, entry)\n                found.extend(modules)\n\n    return found", "language": "python", "code": "def rdiscover_modules(directory):\n    \"\"\"\n    Attempts to list all of the modules and submodules found within a given\n    directory tree. This function recursively searches the directory tree\n    for potential python modules and returns a list of candidate names.\n\n    **Note:** This function returns a list of strings representing\n    discovered module names, not the actual, loaded modules.\n\n    :param directory: the directory to search for modules.\n    \"\"\"\n    found = list()\n\n    if os.path.isdir(directory):\n        for entry in os.listdir(directory):\n            next_dir = os.path.join(directory, entry)\n\n            # Scan only if there's an __init__.py file\n            if os.path.isfile(os.path.join(next_dir, MODULE_INIT_FILE)):\n                modules = _search_for_modules(next_dir, True, entry)\n                found.extend(modules)\n\n    return found", "code_tokens": ["def", "rdiscover_modules", "(", "directory", ")", ":", "found", "=", "list", "(", ")", "if", "os", ".", "path", ".", "isdir", "(", "directory", ")", ":", "for", "entry", "in", "os", ".", "listdir", "(", "directory", ")", ":", "next_dir", "=", "os", ".", "path", ".", "join", "(", "directory", ",", "entry", ")", "# Scan only if there's an __init__.py file", "if", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "next_dir", ",", "MODULE_INIT_FILE", ")", ")", ":", "modules", "=", "_search_for_modules", "(", "next_dir", ",", "True", ",", "entry", ")", "found", ".", "extend", "(", "modules", ")", "return", "found"], "docstring": "Attempts to list all of the modules and submodules found within a given\n    directory tree. This function recursively searches the directory tree\n    for potential python modules and returns a list of candidate names.\n\n    **Note:** This function returns a list of strings representing\n    discovered module names, not the actual, loaded modules.\n\n    :param directory: the directory to search for modules.", "docstring_tokens": ["Attempts", "to", "list", "all", "of", "the", "modules", "and", "submodules", "found", "within", "a", "given", "directory", "tree", ".", "This", "function", "recursively", "searches", "the", "directory", "tree", "for", "potential", "python", "modules", "and", "returns", "a", "list", "of", "candidate", "names", "."], "sha": "15bc8b35a91be5817979eb327427b6235b1b411e", "url": "https://github.com/zinic/pynsive/blob/15bc8b35a91be5817979eb327427b6235b1b411e/pynsive/reflection.py#L114-L136", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/frontend/qt/console/console_widget.py", "func_name": "ConsoleWidget._event_filter_console_keypress", "original_string": "def _event_filter_console_keypress(self, event):\n        \"\"\" Filter key events for the underlying text widget to create a\n            console-like interface.\n        \"\"\"\n        intercepted = False\n        cursor = self._control.textCursor()\n        position = cursor.position()\n        key = event.key()\n        ctrl_down = self._control_key_down(event.modifiers())\n        alt_down = event.modifiers() & QtCore.Qt.AltModifier\n        shift_down = event.modifiers() & QtCore.Qt.ShiftModifier\n\n        #------ Special sequences ----------------------------------------------\n\n        if event.matches(QtGui.QKeySequence.Copy):\n            self.copy()\n            intercepted = True\n\n        elif event.matches(QtGui.QKeySequence.Cut):\n            self.cut()\n            intercepted = True\n\n        elif event.matches(QtGui.QKeySequence.Paste):\n            self.paste()\n            intercepted = True\n\n        #------ Special modifier logic -----------------------------------------\n\n        elif key in (QtCore.Qt.Key_Return, QtCore.Qt.Key_Enter):\n            intercepted = True\n\n            # Special handling when tab completing in text mode.\n            self._cancel_completion()\n\n            if self._in_buffer(position):\n                # Special handling when a reading a line of raw input.\n                if self._reading:\n                    self._append_plain_text('\\n')\n                    self._reading = False\n                    if self._reading_callback:\n                        self._reading_callback()\n\n                # If the input buffer is a single line or there is only\n                # whitespace after the cursor, execute. Otherwise, split the\n                # line with a continuation prompt.\n                elif not self._executing:\n                    cursor.movePosition(QtGui.QTextCursor.End,\n                                        QtGui.QTextCursor.KeepAnchor)\n                    at_end = len(cursor.selectedText().strip()) == 0\n                    single_line = (self._get_end_cursor().blockNumber() ==\n                                   self._get_prompt_cursor().blockNumber())\n                    if (at_end or shift_down or single_line) and not ctrl_down:\n                        self.execute(interactive = not shift_down)\n                    else:\n                        # Do this inside an edit block for clean undo/redo.\n                        cursor.beginEditBlock()\n                        cursor.setPosition(position)\n                        cursor.insertText('\\n')\n                        self._insert_continuation_prompt(cursor)\n                        cursor.endEditBlock()\n\n                        # Ensure that the whole input buffer is visible.\n                        # FIXME: This will not be usable if the input buffer is\n                        # taller than the console widget.\n                        self._control.moveCursor(QtGui.QTextCursor.End)\n                        self._control.setTextCursor(cursor)\n\n        #------ Control/Cmd modifier -------------------------------------------\n\n        elif ctrl_down:\n            if key == QtCore.Qt.Key_G:\n                self._keyboard_quit()\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_K:\n                if self._in_buffer(position):\n                    cursor.clearSelection()\n                    cursor.movePosition(QtGui.QTextCursor.EndOfLine,\n                                        QtGui.QTextCursor.KeepAnchor)\n                    if not cursor.hasSelection():\n                        # Line deletion (remove continuation prompt)\n                        cursor.movePosition(QtGui.QTextCursor.NextBlock,\n                                            QtGui.QTextCursor.KeepAnchor)\n                        cursor.movePosition(QtGui.QTextCursor.Right,\n                                            QtGui.QTextCursor.KeepAnchor,\n                                            len(self._continuation_prompt))\n                    self._kill_ring.kill_cursor(cursor)\n                    self._set_cursor(cursor)\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_L:\n                self.prompt_to_top()\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_O:\n                if self._page_control and self._page_control.isVisible():\n                    self._page_control.setFocus()\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_U:\n                if self._in_buffer(position):\n                    cursor.clearSelection()\n                    start_line = cursor.blockNumber()\n                    if start_line == self._get_prompt_cursor().blockNumber():\n                        offset = len(self._prompt)\n                    else:\n                        offset = len(self._continuation_prompt)\n                    cursor.movePosition(QtGui.QTextCursor.StartOfBlock,\n                                        QtGui.QTextCursor.KeepAnchor)\n                    cursor.movePosition(QtGui.QTextCursor.Right,\n                                        QtGui.QTextCursor.KeepAnchor, offset)\n                    self._kill_ring.kill_cursor(cursor)\n                    self._set_cursor(cursor)\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_Y:\n                self._keep_cursor_in_buffer()\n                self._kill_ring.yank()\n                intercepted = True\n\n            elif key in (QtCore.Qt.Key_Backspace, QtCore.Qt.Key_Delete):\n                if key == QtCore.Qt.Key_Backspace:\n                    cursor = self._get_word_start_cursor(position)\n                else: # key == QtCore.Qt.Key_Delete\n                    cursor = self._get_word_end_cursor(position)\n                cursor.setPosition(position, QtGui.QTextCursor.KeepAnchor)\n                self._kill_ring.kill_cursor(cursor)\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_D:\n                if len(self.input_buffer) == 0:\n                    self.exit_requested.emit(self)\n                else:\n                    new_event = QtGui.QKeyEvent(QtCore.QEvent.KeyPress,\n                                                QtCore.Qt.Key_Delete,\n                                                QtCore.Qt.NoModifier)\n                    QtGui.qApp.sendEvent(self._control, new_event)\n                    intercepted = True\n\n        #------ Alt modifier ---------------------------------------------------\n\n        elif alt_down:\n            if key == QtCore.Qt.Key_B:\n                self._set_cursor(self._get_word_start_cursor(position))\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_F:\n                self._set_cursor(self._get_word_end_cursor(position))\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_Y:\n                self._kill_ring.rotate()\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_Backspace:\n                cursor = self._get_word_start_cursor(position)\n                cursor.setPosition(position, QtGui.QTextCursor.KeepAnchor)\n                self._kill_ring.kill_cursor(cursor)\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_D:\n                cursor = self._get_word_end_cursor(position)\n                cursor.setPosition(position, QtGui.QTextCursor.KeepAnchor)\n                self._kill_ring.kill_cursor(cursor)\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_Delete:\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_Greater:\n                self._control.moveCursor(QtGui.QTextCursor.End)\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_Less:\n                self._control.setTextCursor(self._get_prompt_cursor())\n                intercepted = True\n\n        #------ No modifiers ---------------------------------------------------\n\n        else:\n            if shift_down:\n                anchormode = QtGui.QTextCursor.KeepAnchor\n            else:\n                anchormode = QtGui.QTextCursor.MoveAnchor\n\n            if key == QtCore.Qt.Key_Escape:\n                self._keyboard_quit()\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_Up:\n                if self._reading or not self._up_pressed(shift_down):\n                    intercepted = True\n                else:\n                    prompt_line = self._get_prompt_cursor().blockNumber()\n                    intercepted = cursor.blockNumber() <= prompt_line\n\n            elif key == QtCore.Qt.Key_Down:\n                if self._reading or not self._down_pressed(shift_down):\n                    intercepted = True\n                else:\n                    end_line = self._get_end_cursor().blockNumber()\n                    intercepted = cursor.blockNumber() == end_line\n\n            elif key == QtCore.Qt.Key_Tab:\n                if not self._reading:\n                    if self._tab_pressed():\n                        # real tab-key, insert four spaces\n                        cursor.insertText(' '*4)\n                    intercepted = True\n\n            elif key == QtCore.Qt.Key_Left:\n\n                # Move to the previous line\n                line, col = cursor.blockNumber(), cursor.columnNumber()\n                if line > self._get_prompt_cursor().blockNumber() and \\\n                        col == len(self._continuation_prompt):\n                    self._control.moveCursor(QtGui.QTextCursor.PreviousBlock,\n                                             mode=anchormode)\n                    self._control.moveCursor(QtGui.QTextCursor.EndOfBlock,\n                                             mode=anchormode)\n                    intercepted = True\n\n                # Regular left movement\n                else:\n                    intercepted = not self._in_buffer(position - 1)\n\n            elif key == QtCore.Qt.Key_Right:\n                original_block_number = cursor.blockNumber()\n                cursor.movePosition(QtGui.QTextCursor.Right,\n                                mode=anchormode)\n                if cursor.blockNumber() != original_block_number:\n                    cursor.movePosition(QtGui.QTextCursor.Right,\n                                        n=len(self._continuation_prompt),\n                                        mode=anchormode)\n                self._set_cursor(cursor)\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_Home:\n                start_line = cursor.blockNumber()\n                if start_line == self._get_prompt_cursor().blockNumber():\n                    start_pos = self._prompt_pos\n                else:\n                    cursor.movePosition(QtGui.QTextCursor.StartOfBlock,\n                                        QtGui.QTextCursor.KeepAnchor)\n                    start_pos = cursor.position()\n                    start_pos += len(self._continuation_prompt)\n                    cursor.setPosition(position)\n                if shift_down and self._in_buffer(position):\n                    cursor.setPosition(start_pos, QtGui.QTextCursor.KeepAnchor)\n                else:\n                    cursor.setPosition(start_pos)\n                self._set_cursor(cursor)\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_Backspace:\n\n                # Line deletion (remove continuation prompt)\n                line, col = cursor.blockNumber(), cursor.columnNumber()\n                if not self._reading and \\\n                        col == len(self._continuation_prompt) and \\\n                        line > self._get_prompt_cursor().blockNumber():\n                    cursor.beginEditBlock()\n                    cursor.movePosition(QtGui.QTextCursor.StartOfBlock,\n                                        QtGui.QTextCursor.KeepAnchor)\n                    cursor.removeSelectedText()\n                    cursor.deletePreviousChar()\n                    cursor.endEditBlock()\n                    intercepted = True\n\n                # Regular backwards deletion\n                else:\n                    anchor = cursor.anchor()\n                    if anchor == position:\n                        intercepted = not self._in_buffer(position - 1)\n                    else:\n                        intercepted = not self._in_buffer(min(anchor, position))\n\n            elif key == QtCore.Qt.Key_Delete:\n\n                # Line deletion (remove continuation prompt)\n                if not self._reading and self._in_buffer(position) and \\\n                        cursor.atBlockEnd() and not cursor.hasSelection():\n                    cursor.movePosition(QtGui.QTextCursor.NextBlock,\n                                        QtGui.QTextCursor.KeepAnchor)\n                    cursor.movePosition(QtGui.QTextCursor.Right,\n                                        QtGui.QTextCursor.KeepAnchor,\n                                        len(self._continuation_prompt))\n                    cursor.removeSelectedText()\n                    intercepted = True\n\n                # Regular forwards deletion:\n                else:\n                    anchor = cursor.anchor()\n                    intercepted = (not self._in_buffer(anchor) or\n                                   not self._in_buffer(position))\n\n        # Don't move the cursor if Control/Cmd is pressed to allow copy-paste\n        # using the keyboard in any part of the buffer. Also, permit scrolling\n        # with Page Up/Down keys. Finally, if we're executing, don't move the\n        # cursor (if even this made sense, we can't guarantee that the prompt\n        # position is still valid due to text truncation).\n        if not (self._control_key_down(event.modifiers(), include_command=True)\n                or key in (QtCore.Qt.Key_PageUp, QtCore.Qt.Key_PageDown)\n                or (self._executing and not self._reading)):\n            self._keep_cursor_in_buffer()\n\n        return intercepted", "language": "python", "code": "def _event_filter_console_keypress(self, event):\n        \"\"\" Filter key events for the underlying text widget to create a\n            console-like interface.\n        \"\"\"\n        intercepted = False\n        cursor = self._control.textCursor()\n        position = cursor.position()\n        key = event.key()\n        ctrl_down = self._control_key_down(event.modifiers())\n        alt_down = event.modifiers() & QtCore.Qt.AltModifier\n        shift_down = event.modifiers() & QtCore.Qt.ShiftModifier\n\n        #------ Special sequences ----------------------------------------------\n\n        if event.matches(QtGui.QKeySequence.Copy):\n            self.copy()\n            intercepted = True\n\n        elif event.matches(QtGui.QKeySequence.Cut):\n            self.cut()\n            intercepted = True\n\n        elif event.matches(QtGui.QKeySequence.Paste):\n            self.paste()\n            intercepted = True\n\n        #------ Special modifier logic -----------------------------------------\n\n        elif key in (QtCore.Qt.Key_Return, QtCore.Qt.Key_Enter):\n            intercepted = True\n\n            # Special handling when tab completing in text mode.\n            self._cancel_completion()\n\n            if self._in_buffer(position):\n                # Special handling when a reading a line of raw input.\n                if self._reading:\n                    self._append_plain_text('\\n')\n                    self._reading = False\n                    if self._reading_callback:\n                        self._reading_callback()\n\n                # If the input buffer is a single line or there is only\n                # whitespace after the cursor, execute. Otherwise, split the\n                # line with a continuation prompt.\n                elif not self._executing:\n                    cursor.movePosition(QtGui.QTextCursor.End,\n                                        QtGui.QTextCursor.KeepAnchor)\n                    at_end = len(cursor.selectedText().strip()) == 0\n                    single_line = (self._get_end_cursor().blockNumber() ==\n                                   self._get_prompt_cursor().blockNumber())\n                    if (at_end or shift_down or single_line) and not ctrl_down:\n                        self.execute(interactive = not shift_down)\n                    else:\n                        # Do this inside an edit block for clean undo/redo.\n                        cursor.beginEditBlock()\n                        cursor.setPosition(position)\n                        cursor.insertText('\\n')\n                        self._insert_continuation_prompt(cursor)\n                        cursor.endEditBlock()\n\n                        # Ensure that the whole input buffer is visible.\n                        # FIXME: This will not be usable if the input buffer is\n                        # taller than the console widget.\n                        self._control.moveCursor(QtGui.QTextCursor.End)\n                        self._control.setTextCursor(cursor)\n\n        #------ Control/Cmd modifier -------------------------------------------\n\n        elif ctrl_down:\n            if key == QtCore.Qt.Key_G:\n                self._keyboard_quit()\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_K:\n                if self._in_buffer(position):\n                    cursor.clearSelection()\n                    cursor.movePosition(QtGui.QTextCursor.EndOfLine,\n                                        QtGui.QTextCursor.KeepAnchor)\n                    if not cursor.hasSelection():\n                        # Line deletion (remove continuation prompt)\n                        cursor.movePosition(QtGui.QTextCursor.NextBlock,\n                                            QtGui.QTextCursor.KeepAnchor)\n                        cursor.movePosition(QtGui.QTextCursor.Right,\n                                            QtGui.QTextCursor.KeepAnchor,\n                                            len(self._continuation_prompt))\n                    self._kill_ring.kill_cursor(cursor)\n                    self._set_cursor(cursor)\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_L:\n                self.prompt_to_top()\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_O:\n                if self._page_control and self._page_control.isVisible():\n                    self._page_control.setFocus()\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_U:\n                if self._in_buffer(position):\n                    cursor.clearSelection()\n                    start_line = cursor.blockNumber()\n                    if start_line == self._get_prompt_cursor().blockNumber():\n                        offset = len(self._prompt)\n                    else:\n                        offset = len(self._continuation_prompt)\n                    cursor.movePosition(QtGui.QTextCursor.StartOfBlock,\n                                        QtGui.QTextCursor.KeepAnchor)\n                    cursor.movePosition(QtGui.QTextCursor.Right,\n                                        QtGui.QTextCursor.KeepAnchor, offset)\n                    self._kill_ring.kill_cursor(cursor)\n                    self._set_cursor(cursor)\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_Y:\n                self._keep_cursor_in_buffer()\n                self._kill_ring.yank()\n                intercepted = True\n\n            elif key in (QtCore.Qt.Key_Backspace, QtCore.Qt.Key_Delete):\n                if key == QtCore.Qt.Key_Backspace:\n                    cursor = self._get_word_start_cursor(position)\n                else: # key == QtCore.Qt.Key_Delete\n                    cursor = self._get_word_end_cursor(position)\n                cursor.setPosition(position, QtGui.QTextCursor.KeepAnchor)\n                self._kill_ring.kill_cursor(cursor)\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_D:\n                if len(self.input_buffer) == 0:\n                    self.exit_requested.emit(self)\n                else:\n                    new_event = QtGui.QKeyEvent(QtCore.QEvent.KeyPress,\n                                                QtCore.Qt.Key_Delete,\n                                                QtCore.Qt.NoModifier)\n                    QtGui.qApp.sendEvent(self._control, new_event)\n                    intercepted = True\n\n        #------ Alt modifier ---------------------------------------------------\n\n        elif alt_down:\n            if key == QtCore.Qt.Key_B:\n                self._set_cursor(self._get_word_start_cursor(position))\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_F:\n                self._set_cursor(self._get_word_end_cursor(position))\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_Y:\n                self._kill_ring.rotate()\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_Backspace:\n                cursor = self._get_word_start_cursor(position)\n                cursor.setPosition(position, QtGui.QTextCursor.KeepAnchor)\n                self._kill_ring.kill_cursor(cursor)\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_D:\n                cursor = self._get_word_end_cursor(position)\n                cursor.setPosition(position, QtGui.QTextCursor.KeepAnchor)\n                self._kill_ring.kill_cursor(cursor)\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_Delete:\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_Greater:\n                self._control.moveCursor(QtGui.QTextCursor.End)\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_Less:\n                self._control.setTextCursor(self._get_prompt_cursor())\n                intercepted = True\n\n        #------ No modifiers ---------------------------------------------------\n\n        else:\n            if shift_down:\n                anchormode = QtGui.QTextCursor.KeepAnchor\n            else:\n                anchormode = QtGui.QTextCursor.MoveAnchor\n\n            if key == QtCore.Qt.Key_Escape:\n                self._keyboard_quit()\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_Up:\n                if self._reading or not self._up_pressed(shift_down):\n                    intercepted = True\n                else:\n                    prompt_line = self._get_prompt_cursor().blockNumber()\n                    intercepted = cursor.blockNumber() <= prompt_line\n\n            elif key == QtCore.Qt.Key_Down:\n                if self._reading or not self._down_pressed(shift_down):\n                    intercepted = True\n                else:\n                    end_line = self._get_end_cursor().blockNumber()\n                    intercepted = cursor.blockNumber() == end_line\n\n            elif key == QtCore.Qt.Key_Tab:\n                if not self._reading:\n                    if self._tab_pressed():\n                        # real tab-key, insert four spaces\n                        cursor.insertText(' '*4)\n                    intercepted = True\n\n            elif key == QtCore.Qt.Key_Left:\n\n                # Move to the previous line\n                line, col = cursor.blockNumber(), cursor.columnNumber()\n                if line > self._get_prompt_cursor().blockNumber() and \\\n                        col == len(self._continuation_prompt):\n                    self._control.moveCursor(QtGui.QTextCursor.PreviousBlock,\n                                             mode=anchormode)\n                    self._control.moveCursor(QtGui.QTextCursor.EndOfBlock,\n                                             mode=anchormode)\n                    intercepted = True\n\n                # Regular left movement\n                else:\n                    intercepted = not self._in_buffer(position - 1)\n\n            elif key == QtCore.Qt.Key_Right:\n                original_block_number = cursor.blockNumber()\n                cursor.movePosition(QtGui.QTextCursor.Right,\n                                mode=anchormode)\n                if cursor.blockNumber() != original_block_number:\n                    cursor.movePosition(QtGui.QTextCursor.Right,\n                                        n=len(self._continuation_prompt),\n                                        mode=anchormode)\n                self._set_cursor(cursor)\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_Home:\n                start_line = cursor.blockNumber()\n                if start_line == self._get_prompt_cursor().blockNumber():\n                    start_pos = self._prompt_pos\n                else:\n                    cursor.movePosition(QtGui.QTextCursor.StartOfBlock,\n                                        QtGui.QTextCursor.KeepAnchor)\n                    start_pos = cursor.position()\n                    start_pos += len(self._continuation_prompt)\n                    cursor.setPosition(position)\n                if shift_down and self._in_buffer(position):\n                    cursor.setPosition(start_pos, QtGui.QTextCursor.KeepAnchor)\n                else:\n                    cursor.setPosition(start_pos)\n                self._set_cursor(cursor)\n                intercepted = True\n\n            elif key == QtCore.Qt.Key_Backspace:\n\n                # Line deletion (remove continuation prompt)\n                line, col = cursor.blockNumber(), cursor.columnNumber()\n                if not self._reading and \\\n                        col == len(self._continuation_prompt) and \\\n                        line > self._get_prompt_cursor().blockNumber():\n                    cursor.beginEditBlock()\n                    cursor.movePosition(QtGui.QTextCursor.StartOfBlock,\n                                        QtGui.QTextCursor.KeepAnchor)\n                    cursor.removeSelectedText()\n                    cursor.deletePreviousChar()\n                    cursor.endEditBlock()\n                    intercepted = True\n\n                # Regular backwards deletion\n                else:\n                    anchor = cursor.anchor()\n                    if anchor == position:\n                        intercepted = not self._in_buffer(position - 1)\n                    else:\n                        intercepted = not self._in_buffer(min(anchor, position))\n\n            elif key == QtCore.Qt.Key_Delete:\n\n                # Line deletion (remove continuation prompt)\n                if not self._reading and self._in_buffer(position) and \\\n                        cursor.atBlockEnd() and not cursor.hasSelection():\n                    cursor.movePosition(QtGui.QTextCursor.NextBlock,\n                                        QtGui.QTextCursor.KeepAnchor)\n                    cursor.movePosition(QtGui.QTextCursor.Right,\n                                        QtGui.QTextCursor.KeepAnchor,\n                                        len(self._continuation_prompt))\n                    cursor.removeSelectedText()\n                    intercepted = True\n\n                # Regular forwards deletion:\n                else:\n                    anchor = cursor.anchor()\n                    intercepted = (not self._in_buffer(anchor) or\n                                   not self._in_buffer(position))\n\n        # Don't move the cursor if Control/Cmd is pressed to allow copy-paste\n        # using the keyboard in any part of the buffer. Also, permit scrolling\n        # with Page Up/Down keys. Finally, if we're executing, don't move the\n        # cursor (if even this made sense, we can't guarantee that the prompt\n        # position is still valid due to text truncation).\n        if not (self._control_key_down(event.modifiers(), include_command=True)\n                or key in (QtCore.Qt.Key_PageUp, QtCore.Qt.Key_PageDown)\n                or (self._executing and not self._reading)):\n            self._keep_cursor_in_buffer()\n\n        return intercepted", "code_tokens": ["def", "_event_filter_console_keypress", "(", "self", ",", "event", ")", ":", "intercepted", "=", "False", "cursor", "=", "self", ".", "_control", ".", "textCursor", "(", ")", "position", "=", "cursor", ".", "position", "(", ")", "key", "=", "event", ".", "key", "(", ")", "ctrl_down", "=", "self", ".", "_control_key_down", "(", "event", ".", "modifiers", "(", ")", ")", "alt_down", "=", "event", ".", "modifiers", "(", ")", "&", "QtCore", ".", "Qt", ".", "AltModifier", "shift_down", "=", "event", ".", "modifiers", "(", ")", "&", "QtCore", ".", "Qt", ".", "ShiftModifier", "#------ Special sequences ----------------------------------------------", "if", "event", ".", "matches", "(", "QtGui", ".", "QKeySequence", ".", "Copy", ")", ":", "self", ".", "copy", "(", ")", "intercepted", "=", "True", "elif", "event", ".", "matches", "(", "QtGui", ".", "QKeySequence", ".", "Cut", ")", ":", "self", ".", "cut", "(", ")", "intercepted", "=", "True", "elif", "event", ".", "matches", "(", "QtGui", ".", "QKeySequence", ".", "Paste", ")", ":", "self", ".", "paste", "(", ")", "intercepted", "=", "True", "#------ Special modifier logic -----------------------------------------", "elif", "key", "in", "(", "QtCore", ".", "Qt", ".", "Key_Return", ",", "QtCore", ".", "Qt", ".", "Key_Enter", ")", ":", "intercepted", "=", "True", "# Special handling when tab completing in text mode.", "self", ".", "_cancel_completion", "(", ")", "if", "self", ".", "_in_buffer", "(", "position", ")", ":", "# Special handling when a reading a line of raw input.", "if", "self", ".", "_reading", ":", "self", ".", "_append_plain_text", "(", "'\\n'", ")", "self", ".", "_reading", "=", "False", "if", "self", ".", "_reading_callback", ":", "self", ".", "_reading_callback", "(", ")", "# If the input buffer is a single line or there is only", "# whitespace after the cursor, execute. Otherwise, split the", "# line with a continuation prompt.", "elif", "not", "self", ".", "_executing", ":", "cursor", ".", "movePosition", "(", "QtGui", ".", "QTextCursor", ".", "End", ",", "QtGui", ".", "QTextCursor", ".", "KeepAnchor", ")", "at_end", "=", "len", "(", "cursor", ".", "selectedText", "(", ")", ".", "strip", "(", ")", ")", "==", "0", "single_line", "=", "(", "self", ".", "_get_end_cursor", "(", ")", ".", "blockNumber", "(", ")", "==", "self", ".", "_get_prompt_cursor", "(", ")", ".", "blockNumber", "(", ")", ")", "if", "(", "at_end", "or", "shift_down", "or", "single_line", ")", "and", "not", "ctrl_down", ":", "self", ".", "execute", "(", "interactive", "=", "not", "shift_down", ")", "else", ":", "# Do this inside an edit block for clean undo/redo.", "cursor", ".", "beginEditBlock", "(", ")", "cursor", ".", "setPosition", "(", "position", ")", "cursor", ".", "insertText", "(", "'\\n'", ")", "self", ".", "_insert_continuation_prompt", "(", "cursor", ")", "cursor", ".", "endEditBlock", "(", ")", "# Ensure that the whole input buffer is visible.", "# FIXME: This will not be usable if the input buffer is", "# taller than the console widget.", "self", ".", "_control", ".", "moveCursor", "(", "QtGui", ".", "QTextCursor", ".", "End", ")", "self", ".", "_control", ".", "setTextCursor", "(", "cursor", ")", "#------ Control/Cmd modifier -------------------------------------------", "elif", "ctrl_down", ":", "if", "key", "==", "QtCore", ".", "Qt", ".", "Key_G", ":", "self", ".", "_keyboard_quit", "(", ")", "intercepted", "=", "True", "elif", "key", "==", "QtCore", ".", "Qt", ".", "Key_K", ":", "if", "self", ".", "_in_buffer", "(", "position", ")", ":", "cursor", ".", "clearSelection", "(", ")", "cursor", ".", "movePosition", "(", "QtGui", ".", "QTextCursor", ".", "EndOfLine", ",", "QtGui", ".", "QTextCursor", ".", "KeepAnchor", ")", "if", "not", "cursor", ".", "hasSelection", "(", ")", ":", "# Line deletion (remove continuation prompt)", "cursor", ".", "movePosition", "(", "QtGui", ".", "QTextCursor", ".", "NextBlock", ",", "QtGui", ".", "QTextCursor", ".", "KeepAnchor", ")", "cursor", ".", "movePosition", "(", "QtGui", ".", "QTextCursor", ".", "Right", ",", "QtGui", ".", "QTextCursor", ".", "KeepAnchor", ",", "len", "(", "self", ".", "_continuation_prompt", ")", ")", "self", ".", "_kill_ring", ".", "kill_cursor", "(", "cursor", ")", "self", ".", "_set_cursor", "(", "cursor", ")", "intercepted", "=", "True", "elif", "key", "==", "QtCore", ".", "Qt", ".", "Key_L", ":", "self", ".", "prompt_to_top", "(", ")", "intercepted", "=", "True", "elif", "key", "==", "QtCore", ".", "Qt", ".", "Key_O", ":", "if", "self", ".", "_page_control", "and", "self", ".", "_page_control", ".", "isVisible", "(", ")", ":", "self", ".", "_page_control", ".", "setFocus", "(", ")", "intercepted", "=", "True", "elif", "key", "==", "QtCore", ".", "Qt", ".", "Key_U", ":", "if", "self", ".", "_in_buffer", "(", "position", ")", ":", "cursor", ".", "clearSelection", "(", ")", "start_line", "=", "cursor", ".", "blockNumber", "(", ")", "if", "start_line", "==", "self", ".", "_get_prompt_cursor", "(", ")", ".", "blockNumber", "(", ")", ":", "offset", "=", "len", "(", "self", ".", "_prompt", ")", "else", ":", "offset", "=", "len", "(", "self", ".", "_continuation_prompt", ")", "cursor", ".", "movePosition", "(", "QtGui", ".", "QTextCursor", ".", "StartOfBlock", ",", "QtGui", ".", "QTextCursor", ".", "KeepAnchor", ")", "cursor", ".", "movePosition", "(", "QtGui", ".", "QTextCursor", ".", "Right", ",", "QtGui", ".", "QTextCursor", ".", "KeepAnchor", ",", "offset", ")", "self", ".", "_kill_ring", ".", "kill_cursor", "(", "cursor", ")", "self", ".", "_set_cursor", "(", "cursor", ")", "intercepted", "=", "True", "elif", "key", "==", "QtCore", ".", "Qt", ".", "Key_Y", ":", "self", ".", "_keep_cursor_in_buffer", "(", ")", "self", ".", "_kill_ring", ".", "yank", "(", ")", "intercepted", "=", "True", "elif", "key", "in", "(", "QtCore", ".", "Qt", ".", "Key_Backspace", ",", "QtCore", ".", "Qt", ".", "Key_Delete", ")", ":", "if", "key", "==", "QtCore", ".", "Qt", ".", "Key_Backspace", ":", "cursor", "=", "self", ".", "_get_word_start_cursor", "(", "position", ")", "else", ":", "# key == QtCore.Qt.Key_Delete", "cursor", "=", "self", ".", "_get_word_end_cursor", "(", "position", ")", "cursor", ".", "setPosition", "(", "position", ",", "QtGui", ".", "QTextCursor", ".", "KeepAnchor", ")", "self", ".", "_kill_ring", ".", "kill_cursor", "(", "cursor", ")", "intercepted", "=", "True", "elif", "key", "==", "QtCore", ".", "Qt", ".", "Key_D", ":", "if", "len", "(", "self", ".", "input_buffer", ")", "==", "0", ":", "self", ".", "exit_requested", ".", "emit", "(", "self", ")", "else", ":", "new_event", "=", "QtGui", ".", "QKeyEvent", "(", "QtCore", ".", "QEvent", ".", "KeyPress", ",", "QtCore", ".", "Qt", ".", "Key_Delete", ",", "QtCore", ".", "Qt", ".", "NoModifier", ")", "QtGui", ".", "qApp", ".", "sendEvent", "(", "self", ".", "_control", ",", "new_event", ")", "intercepted", "=", "True", "#------ Alt modifier ---------------------------------------------------", "elif", "alt_down", ":", "if", "key", "==", "QtCore", ".", "Qt", ".", "Key_B", ":", "self", ".", "_set_cursor", "(", "self", ".", "_get_word_start_cursor", "(", "position", ")", ")", "intercepted", "=", "True", "elif", "key", "==", "QtCore", ".", "Qt", ".", "Key_F", ":", "self", ".", "_set_cursor", "(", "self", ".", "_get_word_end_cursor", "(", "position", ")", ")", "intercepted", "=", "True", "elif", "key", "==", "QtCore", ".", "Qt", ".", "Key_Y", ":", "self", ".", "_kill_ring", ".", "rotate", "(", ")", "intercepted", "=", "True", "elif", "key", "==", "QtCore", ".", "Qt", ".", "Key_Backspace", ":", "cursor", "=", "self", ".", "_get_word_start_cursor", "(", "position", ")", "cursor", ".", "setPosition", "(", "position", ",", "QtGui", ".", "QTextCursor", ".", "KeepAnchor", ")", "self", ".", "_kill_ring", ".", "kill_cursor", "(", "cursor", ")", "intercepted", "=", "True", "elif", "key", "==", "QtCore", ".", "Qt", ".", "Key_D", ":", "cursor", "=", "self", ".", "_get_word_end_cursor", "(", "position", ")", "cursor", ".", "setPosition", "(", "position", ",", "QtGui", ".", "QTextCursor", ".", "KeepAnchor", ")", "self", ".", "_kill_ring", ".", "kill_cursor", "(", "cursor", ")", "intercepted", "=", "True", "elif", "key", "==", "QtCore", ".", "Qt", ".", "Key_Delete", ":", "intercepted", "=", "True", "elif", "key", "==", "QtCore", ".", "Qt", ".", "Key_Greater", ":", "self", ".", "_control", ".", "moveCursor", "(", "QtGui", ".", "QTextCursor", ".", "End", ")", "intercepted", "=", "True", "elif", "key", "==", "QtCore", ".", "Qt", ".", "Key_Less", ":", "self", ".", "_control", ".", "setTextCursor", "(", "self", ".", "_get_prompt_cursor", "(", ")", ")", "intercepted", "=", "True", "#------ No modifiers ---------------------------------------------------", "else", ":", "if", "shift_down", ":", "anchormode", "=", "QtGui", ".", "QTextCursor", ".", "KeepAnchor", "else", ":", "anchormode", "=", "QtGui", ".", "QTextCursor", ".", "MoveAnchor", "if", "key", "==", "QtCore", ".", "Qt", ".", "Key_Escape", ":", "self", ".", "_keyboard_quit", "(", ")", "intercepted", "=", "True", "elif", "key", "==", "QtCore", ".", "Qt", ".", "Key_Up", ":", "if", "self", ".", "_reading", "or", "not", "self", ".", "_up_pressed", "(", "shift_down", ")", ":", "intercepted", "=", "True", "else", ":", "prompt_line", "=", "self", ".", "_get_prompt_cursor", "(", ")", ".", "blockNumber", "(", ")", "intercepted", "=", "cursor", ".", "blockNumber", "(", ")", "<=", "prompt_line", "elif", "key", "==", "QtCore", ".", "Qt", ".", "Key_Down", ":", "if", "self", ".", "_reading", "or", "not", "self", ".", "_down_pressed", "(", "shift_down", ")", ":", "intercepted", "=", "True", "else", ":", "end_line", "=", "self", ".", "_get_end_cursor", "(", ")", ".", "blockNumber", "(", ")", "intercepted", "=", "cursor", ".", "blockNumber", "(", ")", "==", "end_line", "elif", "key", "==", "QtCore", ".", "Qt", ".", "Key_Tab", ":", "if", "not", "self", ".", "_reading", ":", "if", "self", ".", "_tab_pressed", "(", ")", ":", "# real tab-key, insert four spaces", "cursor", ".", "insertText", "(", "' '", "*", "4", ")", "intercepted", "=", "True", "elif", "key", "==", "QtCore", ".", "Qt", ".", "Key_Left", ":", "# Move to the previous line", "line", ",", "col", "=", "cursor", ".", "blockNumber", "(", ")", ",", "cursor", ".", "columnNumber", "(", ")", "if", "line", ">", "self", ".", "_get_prompt_cursor", "(", ")", ".", "blockNumber", "(", ")", "and", "col", "==", "len", "(", "self", ".", "_continuation_prompt", ")", ":", "self", ".", "_control", ".", "moveCursor", "(", "QtGui", ".", "QTextCursor", ".", "PreviousBlock", ",", "mode", "=", "anchormode", ")", "self", ".", "_control", ".", "moveCursor", "(", "QtGui", ".", "QTextCursor", ".", "EndOfBlock", ",", "mode", "=", "anchormode", ")", "intercepted", "=", "True", "# Regular left movement", "else", ":", "intercepted", "=", "not", "self", ".", "_in_buffer", "(", "position", "-", "1", ")", "elif", "key", "==", "QtCore", ".", "Qt", ".", "Key_Right", ":", "original_block_number", "=", "cursor", ".", "blockNumber", "(", ")", "cursor", ".", "movePosition", "(", "QtGui", ".", "QTextCursor", ".", "Right", ",", "mode", "=", "anchormode", ")", "if", "cursor", ".", "blockNumber", "(", ")", "!=", "original_block_number", ":", "cursor", ".", "movePosition", "(", "QtGui", ".", "QTextCursor", ".", "Right", ",", "n", "=", "len", "(", "self", ".", "_continuation_prompt", ")", ",", "mode", "=", "anchormode", ")", "self", ".", "_set_cursor", "(", "cursor", ")", "intercepted", "=", "True", "elif", "key", "==", "QtCore", ".", "Qt", ".", "Key_Home", ":", "start_line", "=", "cursor", ".", "blockNumber", "(", ")", "if", "start_line", "==", "self", ".", "_get_prompt_cursor", "(", ")", ".", "blockNumber", "(", ")", ":", "start_pos", "=", "self", ".", "_prompt_pos", "else", ":", "cursor", ".", "movePosition", "(", "QtGui", ".", "QTextCursor", ".", "StartOfBlock", ",", "QtGui", ".", "QTextCursor", ".", "KeepAnchor", ")", "start_pos", "=", "cursor", ".", "position", "(", ")", "start_pos", "+=", "len", "(", "self", ".", "_continuation_prompt", ")", "cursor", ".", "setPosition", "(", "position", ")", "if", "shift_down", "and", "self", ".", "_in_buffer", "(", "position", ")", ":", "cursor", ".", "setPosition", "(", "start_pos", ",", "QtGui", ".", "QTextCursor", ".", "KeepAnchor", ")", "else", ":", "cursor", ".", "setPosition", "(", "start_pos", ")", "self", ".", "_set_cursor", "(", "cursor", ")", "intercepted", "=", "True", "elif", "key", "==", "QtCore", ".", "Qt", ".", "Key_Backspace", ":", "# Line deletion (remove continuation prompt)", "line", ",", "col", "=", "cursor", ".", "blockNumber", "(", ")", ",", "cursor", ".", "columnNumber", "(", ")", "if", "not", "self", ".", "_reading", "and", "col", "==", "len", "(", "self", ".", "_continuation_prompt", ")", "and", "line", ">", "self", ".", "_get_prompt_cursor", "(", ")", ".", "blockNumber", "(", ")", ":", "cursor", ".", "beginEditBlock", "(", ")", "cursor", ".", "movePosition", "(", "QtGui", ".", "QTextCursor", ".", "StartOfBlock", ",", "QtGui", ".", "QTextCursor", ".", "KeepAnchor", ")", "cursor", ".", "removeSelectedText", "(", ")", "cursor", ".", "deletePreviousChar", "(", ")", "cursor", ".", "endEditBlock", "(", ")", "intercepted", "=", "True", "# Regular backwards deletion", "else", ":", "anchor", "=", "cursor", ".", "anchor", "(", ")", "if", "anchor", "==", "position", ":", "intercepted", "=", "not", "self", ".", "_in_buffer", "(", "position", "-", "1", ")", "else", ":", "intercepted", "=", "not", "self", ".", "_in_buffer", "(", "min", "(", "anchor", ",", "position", ")", ")", "elif", "key", "==", "QtCore", ".", "Qt", ".", "Key_Delete", ":", "# Line deletion (remove continuation prompt)", "if", "not", "self", ".", "_reading", "and", "self", ".", "_in_buffer", "(", "position", ")", "and", "cursor", ".", "atBlockEnd", "(", ")", "and", "not", "cursor", ".", "hasSelection", "(", ")", ":", "cursor", ".", "movePosition", "(", "QtGui", ".", "QTextCursor", ".", "NextBlock", ",", "QtGui", ".", "QTextCursor", ".", "KeepAnchor", ")", "cursor", ".", "movePosition", "(", "QtGui", ".", "QTextCursor", ".", "Right", ",", "QtGui", ".", "QTextCursor", ".", "KeepAnchor", ",", "len", "(", "self", ".", "_continuation_prompt", ")", ")", "cursor", ".", "removeSelectedText", "(", ")", "intercepted", "=", "True", "# Regular forwards deletion:", "else", ":", "anchor", "=", "cursor", ".", "anchor", "(", ")", "intercepted", "=", "(", "not", "self", ".", "_in_buffer", "(", "anchor", ")", "or", "not", "self", ".", "_in_buffer", "(", "position", ")", ")", "# Don't move the cursor if Control/Cmd is pressed to allow copy-paste", "# using the keyboard in any part of the buffer. Also, permit scrolling", "# with Page Up/Down keys. Finally, if we're executing, don't move the", "# cursor (if even this made sense, we can't guarantee that the prompt", "# position is still valid due to text truncation).", "if", "not", "(", "self", ".", "_control_key_down", "(", "event", ".", "modifiers", "(", ")", ",", "include_command", "=", "True", ")", "or", "key", "in", "(", "QtCore", ".", "Qt", ".", "Key_PageUp", ",", "QtCore", ".", "Qt", ".", "Key_PageDown", ")", "or", "(", "self", ".", "_executing", "and", "not", "self", ".", "_reading", ")", ")", ":", "self", ".", "_keep_cursor_in_buffer", "(", ")", "return", "intercepted"], "docstring": "Filter key events for the underlying text widget to create a\n            console-like interface.", "docstring_tokens": ["Filter", "key", "events", "for", "the", "underlying", "text", "widget", "to", "create", "a", "console", "-", "like", "interface", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/frontend/qt/console/console_widget.py#L1053-L1359", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/nbformat/v1/nbbase.py", "func_name": "new_code_cell", "original_string": "def new_code_cell(code=None, prompt_number=None):\n    \"\"\"Create a new code cell with input and output\"\"\"\n    cell = NotebookNode()\n    cell.cell_type = u'code'\n    if code is not None:\n        cell.code = unicode(code)\n    if prompt_number is not None:\n        cell.prompt_number = int(prompt_number)\n    return cell", "language": "python", "code": "def new_code_cell(code=None, prompt_number=None):\n    \"\"\"Create a new code cell with input and output\"\"\"\n    cell = NotebookNode()\n    cell.cell_type = u'code'\n    if code is not None:\n        cell.code = unicode(code)\n    if prompt_number is not None:\n        cell.prompt_number = int(prompt_number)\n    return cell", "code_tokens": ["def", "new_code_cell", "(", "code", "=", "None", ",", "prompt_number", "=", "None", ")", ":", "cell", "=", "NotebookNode", "(", ")", "cell", ".", "cell_type", "=", "u'code'", "if", "code", "is", "not", "None", ":", "cell", ".", "code", "=", "unicode", "(", "code", ")", "if", "prompt_number", "is", "not", "None", ":", "cell", ".", "prompt_number", "=", "int", "(", "prompt_number", ")", "return", "cell"], "docstring": "Create a new code cell with input and output", "docstring_tokens": ["Create", "a", "new", "code", "cell", "with", "input", "and", "output"], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/nbformat/v1/nbbase.py#L44-L52", "partition": "test"}
{"repo": "deepmipt/DeepPavlov", "path": "deeppavlov/metrics/roc_auc_score.py", "func_name": "roc_auc_score", "original_string": "def roc_auc_score(y_true: Union[List[List[float]], List[List[int]], np.ndarray],\n                  y_pred: Union[List[List[float]], List[List[int]], np.ndarray]) -> float:\n    \"\"\"\n    Compute Area Under the Curve (AUC) from prediction scores.\n\n    Args:\n        y_true: true binary labels\n        y_pred: target scores, can either be probability estimates of the positive class\n\n    Returns:\n        Area Under the Curve (AUC) from prediction scores\n    \"\"\"\n    try:\n        return sklearn.metrics.roc_auc_score(np.squeeze(np.array(y_true)),\n                                             np.squeeze(np.array(y_pred)), average=\"macro\")\n    except ValueError:\n        return 0.", "language": "python", "code": "def roc_auc_score(y_true: Union[List[List[float]], List[List[int]], np.ndarray],\n                  y_pred: Union[List[List[float]], List[List[int]], np.ndarray]) -> float:\n    \"\"\"\n    Compute Area Under the Curve (AUC) from prediction scores.\n\n    Args:\n        y_true: true binary labels\n        y_pred: target scores, can either be probability estimates of the positive class\n\n    Returns:\n        Area Under the Curve (AUC) from prediction scores\n    \"\"\"\n    try:\n        return sklearn.metrics.roc_auc_score(np.squeeze(np.array(y_true)),\n                                             np.squeeze(np.array(y_pred)), average=\"macro\")\n    except ValueError:\n        return 0.", "code_tokens": ["def", "roc_auc_score", "(", "y_true", ":", "Union", "[", "List", "[", "List", "[", "float", "]", "]", ",", "List", "[", "List", "[", "int", "]", "]", ",", "np", ".", "ndarray", "]", ",", "y_pred", ":", "Union", "[", "List", "[", "List", "[", "float", "]", "]", ",", "List", "[", "List", "[", "int", "]", "]", ",", "np", ".", "ndarray", "]", ")", "->", "float", ":", "try", ":", "return", "sklearn", ".", "metrics", ".", "roc_auc_score", "(", "np", ".", "squeeze", "(", "np", ".", "array", "(", "y_true", ")", ")", ",", "np", ".", "squeeze", "(", "np", ".", "array", "(", "y_pred", ")", ")", ",", "average", "=", "\"macro\"", ")", "except", "ValueError", ":", "return", "0."], "docstring": "Compute Area Under the Curve (AUC) from prediction scores.\n\n    Args:\n        y_true: true binary labels\n        y_pred: target scores, can either be probability estimates of the positive class\n\n    Returns:\n        Area Under the Curve (AUC) from prediction scores", "docstring_tokens": ["Compute", "Area", "Under", "the", "Curve", "(", "AUC", ")", "from", "prediction", "scores", "."], "sha": "f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c", "url": "https://github.com/deepmipt/DeepPavlov/blob/f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c/deeppavlov/metrics/roc_auc_score.py#L25-L41", "partition": "test"}
{"repo": "penguinmenac3/opendatalake", "path": "opendatalake/detection/utils.py", "func_name": "crop_image", "original_string": "def crop_image(img, start_y, start_x, h, w):\n    \"\"\"\n    Crop an image given the top left corner.\n    :param img: The image\n    :param start_y: The top left corner y coord\n    :param start_x: The top left corner x coord\n    :param h: The result height\n    :param w: The result width\n    :return: The cropped image.\n    \"\"\"\n    return img[start_y:start_y + h, start_x:start_x + w, :].copy()", "language": "python", "code": "def crop_image(img, start_y, start_x, h, w):\n    \"\"\"\n    Crop an image given the top left corner.\n    :param img: The image\n    :param start_y: The top left corner y coord\n    :param start_x: The top left corner x coord\n    :param h: The result height\n    :param w: The result width\n    :return: The cropped image.\n    \"\"\"\n    return img[start_y:start_y + h, start_x:start_x + w, :].copy()", "code_tokens": ["def", "crop_image", "(", "img", ",", "start_y", ",", "start_x", ",", "h", ",", "w", ")", ":", "return", "img", "[", "start_y", ":", "start_y", "+", "h", ",", "start_x", ":", "start_x", "+", "w", ",", ":", "]", ".", "copy", "(", ")"], "docstring": "Crop an image given the top left corner.\n    :param img: The image\n    :param start_y: The top left corner y coord\n    :param start_x: The top left corner x coord\n    :param h: The result height\n    :param w: The result width\n    :return: The cropped image.", "docstring_tokens": ["Crop", "an", "image", "given", "the", "top", "left", "corner", ".", ":", "param", "img", ":", "The", "image", ":", "param", "start_y", ":", "The", "top", "left", "corner", "y", "coord", ":", "param", "start_x", ":", "The", "top", "left", "corner", "x", "coord", ":", "param", "h", ":", "The", "result", "height", ":", "param", "w", ":", "The", "result", "width", ":", "return", ":", "The", "cropped", "image", "."], "sha": "77c888377095e1812a16982c8efbd2f6b1697a33", "url": "https://github.com/penguinmenac3/opendatalake/blob/77c888377095e1812a16982c8efbd2f6b1697a33/opendatalake/detection/utils.py#L758-L768", "partition": "test"}
{"repo": "ingolemo/python-lenses", "path": "examples/robots.py", "func_name": "GameState.advance_robots", "original_string": "def advance_robots(self):\n        '''Produces a new game state in which the robots have advanced\n        towards the player by one step. Handles the robots crashing into\n        one another too.'''\n\n        # move the robots towards the player\n        self = lens.robots.Each().call_step_towards(self.player)(self)\n        # robots in the same place are crashes\n        self = lens.crashes.call_union(duplicates(self.robots))(self)\n        # remove crashed robots\n        self = lens.robots.modify(lambda r: list(set(r) - self.crashes))(self)\n\n        return self", "language": "python", "code": "def advance_robots(self):\n        '''Produces a new game state in which the robots have advanced\n        towards the player by one step. Handles the robots crashing into\n        one another too.'''\n\n        # move the robots towards the player\n        self = lens.robots.Each().call_step_towards(self.player)(self)\n        # robots in the same place are crashes\n        self = lens.crashes.call_union(duplicates(self.robots))(self)\n        # remove crashed robots\n        self = lens.robots.modify(lambda r: list(set(r) - self.crashes))(self)\n\n        return self", "code_tokens": ["def", "advance_robots", "(", "self", ")", ":", "# move the robots towards the player", "self", "=", "lens", ".", "robots", ".", "Each", "(", ")", ".", "call_step_towards", "(", "self", ".", "player", ")", "(", "self", ")", "# robots in the same place are crashes", "self", "=", "lens", ".", "crashes", ".", "call_union", "(", "duplicates", "(", "self", ".", "robots", ")", ")", "(", "self", ")", "# remove crashed robots", "self", "=", "lens", ".", "robots", ".", "modify", "(", "lambda", "r", ":", "list", "(", "set", "(", "r", ")", "-", "self", ".", "crashes", ")", ")", "(", "self", ")", "return", "self"], "docstring": "Produces a new game state in which the robots have advanced\n        towards the player by one step. Handles the robots crashing into\n        one another too.", "docstring_tokens": ["Produces", "a", "new", "game", "state", "in", "which", "the", "robots", "have", "advanced", "towards", "the", "player", "by", "one", "step", ".", "Handles", "the", "robots", "crashing", "into", "one", "another", "too", "."], "sha": "a3a6ed0a31f6674451e542e7380a8aa16e6f8edf", "url": "https://github.com/ingolemo/python-lenses/blob/a3a6ed0a31f6674451e542e7380a8aa16e6f8edf/examples/robots.py#L113-L125", "partition": "test"}
{"repo": "nvdv/vprof", "path": "vprof/code_heatmap.py", "func_name": "check_standard_dir", "original_string": "def check_standard_dir(module_path):\n    \"\"\"Checks whether path belongs to standard library or installed modules.\"\"\"\n    if 'site-packages' in module_path:\n        return True\n    for stdlib_path in _STDLIB_PATHS:\n        if fnmatch.fnmatchcase(module_path, stdlib_path + '*'):\n            return True\n    return False", "language": "python", "code": "def check_standard_dir(module_path):\n    \"\"\"Checks whether path belongs to standard library or installed modules.\"\"\"\n    if 'site-packages' in module_path:\n        return True\n    for stdlib_path in _STDLIB_PATHS:\n        if fnmatch.fnmatchcase(module_path, stdlib_path + '*'):\n            return True\n    return False", "code_tokens": ["def", "check_standard_dir", "(", "module_path", ")", ":", "if", "'site-packages'", "in", "module_path", ":", "return", "True", "for", "stdlib_path", "in", "_STDLIB_PATHS", ":", "if", "fnmatch", ".", "fnmatchcase", "(", "module_path", ",", "stdlib_path", "+", "'*'", ")", ":", "return", "True", "return", "False"], "docstring": "Checks whether path belongs to standard library or installed modules.", "docstring_tokens": ["Checks", "whether", "path", "belongs", "to", "standard", "library", "or", "installed", "modules", "."], "sha": "4c3ff78f8920ab10cb9c00b14143452aa09ff6bb", "url": "https://github.com/nvdv/vprof/blob/4c3ff78f8920ab10cb9c00b14143452aa09ff6bb/vprof/code_heatmap.py#L18-L25", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/hooks/hive_hooks.py", "func_name": "HiveMetastoreHook.table_exists", "original_string": "def table_exists(self, table_name, db='default'):\n        \"\"\"\n        Check if table exists\n\n        >>> hh = HiveMetastoreHook()\n        >>> hh.table_exists(db='airflow', table_name='static_babynames')\n        True\n        >>> hh.table_exists(db='airflow', table_name='does_not_exist')\n        False\n        \"\"\"\n        try:\n            self.get_table(table_name, db)\n            return True\n        except Exception:\n            return False", "language": "python", "code": "def table_exists(self, table_name, db='default'):\n        \"\"\"\n        Check if table exists\n\n        >>> hh = HiveMetastoreHook()\n        >>> hh.table_exists(db='airflow', table_name='static_babynames')\n        True\n        >>> hh.table_exists(db='airflow', table_name='does_not_exist')\n        False\n        \"\"\"\n        try:\n            self.get_table(table_name, db)\n            return True\n        except Exception:\n            return False", "code_tokens": ["def", "table_exists", "(", "self", ",", "table_name", ",", "db", "=", "'default'", ")", ":", "try", ":", "self", ".", "get_table", "(", "table_name", ",", "db", ")", "return", "True", "except", "Exception", ":", "return", "False"], "docstring": "Check if table exists\n\n        >>> hh = HiveMetastoreHook()\n        >>> hh.table_exists(db='airflow', table_name='static_babynames')\n        True\n        >>> hh.table_exists(db='airflow', table_name='does_not_exist')\n        False", "docstring_tokens": ["Check", "if", "table", "exists"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/hive_hooks.py#L731-L745", "partition": "test"}
{"repo": "nkgilley/python-ecobee-api", "path": "pyecobee/__init__.py", "func_name": "Ecobee.delete_vacation", "original_string": "def delete_vacation(self, index, vacation):\n        ''' Delete the vacation with name vacation '''\n        body = {\"selection\": {\n                    \"selectionType\": \"thermostats\",\n                    \"selectionMatch\": self.thermostats[index]['identifier']},\n                \"functions\": [{\"type\": \"deleteVacation\", \"params\": {\n                    \"name\": vacation\n                }}]}\n\n        log_msg_action = \"delete a vacation\"\n        return self.make_request(body, log_msg_action)", "language": "python", "code": "def delete_vacation(self, index, vacation):\n        ''' Delete the vacation with name vacation '''\n        body = {\"selection\": {\n                    \"selectionType\": \"thermostats\",\n                    \"selectionMatch\": self.thermostats[index]['identifier']},\n                \"functions\": [{\"type\": \"deleteVacation\", \"params\": {\n                    \"name\": vacation\n                }}]}\n\n        log_msg_action = \"delete a vacation\"\n        return self.make_request(body, log_msg_action)", "code_tokens": ["def", "delete_vacation", "(", "self", ",", "index", ",", "vacation", ")", ":", "body", "=", "{", "\"selection\"", ":", "{", "\"selectionType\"", ":", "\"thermostats\"", ",", "\"selectionMatch\"", ":", "self", ".", "thermostats", "[", "index", "]", "[", "'identifier'", "]", "}", ",", "\"functions\"", ":", "[", "{", "\"type\"", ":", "\"deleteVacation\"", ",", "\"params\"", ":", "{", "\"name\"", ":", "vacation", "}", "}", "]", "}", "log_msg_action", "=", "\"delete a vacation\"", "return", "self", ".", "make_request", "(", "body", ",", "log_msg_action", ")"], "docstring": "Delete the vacation with name vacation", "docstring_tokens": ["Delete", "the", "vacation", "with", "name", "vacation"], "sha": "cc8d90d20abcb9ef5b66ec9cb035bae2f06ba174", "url": "https://github.com/nkgilley/python-ecobee-api/blob/cc8d90d20abcb9ef5b66ec9cb035bae2f06ba174/pyecobee/__init__.py#L271-L281", "partition": "test"}
{"repo": "iotaledger/iota.lib.py", "path": "iota/api.py", "func_name": "Iota.get_transfers", "original_string": "def get_transfers(self, start=0, stop=None, inclusion_states=False):\n        # type: (int, Optional[int], bool) -> dict\n        \"\"\"\n        Returns all transfers associated with the seed.\n\n        :param start:\n            Starting key index.\n\n        :param stop:\n            Stop before this index.\n\n            Note that this parameter behaves like the ``stop`` attribute\n            in a :py:class:`slice` object; the stop index is *not*\n            included in the result.\n\n            If ``None`` (default), then this method will check every\n            address until it finds one without any transfers.\n\n        :param inclusion_states:\n            Whether to also fetch the inclusion states of the transfers.\n\n            This requires an additional API call to the node, so it is\n            disabled by default.\n\n        :return:\n            Dict with the following structure::\n\n                {\n                    'bundles': List[Bundle],\n                        Matching bundles, sorted by tail transaction\n                        timestamp.\n\n                        This value is always a list, even if only one\n                        bundle was found.\n                }\n\n        References:\n\n        - https://github.com/iotaledger/wiki/blob/master/api-proposal.md#gettransfers\n        \"\"\"\n        return extended.GetTransfersCommand(self.adapter)(\n            seed=self.seed,\n            start=start,\n            stop=stop,\n            inclusionStates=inclusion_states,\n        )", "language": "python", "code": "def get_transfers(self, start=0, stop=None, inclusion_states=False):\n        # type: (int, Optional[int], bool) -> dict\n        \"\"\"\n        Returns all transfers associated with the seed.\n\n        :param start:\n            Starting key index.\n\n        :param stop:\n            Stop before this index.\n\n            Note that this parameter behaves like the ``stop`` attribute\n            in a :py:class:`slice` object; the stop index is *not*\n            included in the result.\n\n            If ``None`` (default), then this method will check every\n            address until it finds one without any transfers.\n\n        :param inclusion_states:\n            Whether to also fetch the inclusion states of the transfers.\n\n            This requires an additional API call to the node, so it is\n            disabled by default.\n\n        :return:\n            Dict with the following structure::\n\n                {\n                    'bundles': List[Bundle],\n                        Matching bundles, sorted by tail transaction\n                        timestamp.\n\n                        This value is always a list, even if only one\n                        bundle was found.\n                }\n\n        References:\n\n        - https://github.com/iotaledger/wiki/blob/master/api-proposal.md#gettransfers\n        \"\"\"\n        return extended.GetTransfersCommand(self.adapter)(\n            seed=self.seed,\n            start=start,\n            stop=stop,\n            inclusionStates=inclusion_states,\n        )", "code_tokens": ["def", "get_transfers", "(", "self", ",", "start", "=", "0", ",", "stop", "=", "None", ",", "inclusion_states", "=", "False", ")", ":", "# type: (int, Optional[int], bool) -> dict", "return", "extended", ".", "GetTransfersCommand", "(", "self", ".", "adapter", ")", "(", "seed", "=", "self", ".", "seed", ",", "start", "=", "start", ",", "stop", "=", "stop", ",", "inclusionStates", "=", "inclusion_states", ",", ")"], "docstring": "Returns all transfers associated with the seed.\n\n        :param start:\n            Starting key index.\n\n        :param stop:\n            Stop before this index.\n\n            Note that this parameter behaves like the ``stop`` attribute\n            in a :py:class:`slice` object; the stop index is *not*\n            included in the result.\n\n            If ``None`` (default), then this method will check every\n            address until it finds one without any transfers.\n\n        :param inclusion_states:\n            Whether to also fetch the inclusion states of the transfers.\n\n            This requires an additional API call to the node, so it is\n            disabled by default.\n\n        :return:\n            Dict with the following structure::\n\n                {\n                    'bundles': List[Bundle],\n                        Matching bundles, sorted by tail transaction\n                        timestamp.\n\n                        This value is always a list, even if only one\n                        bundle was found.\n                }\n\n        References:\n\n        - https://github.com/iotaledger/wiki/blob/master/api-proposal.md#gettransfers", "docstring_tokens": ["Returns", "all", "transfers", "associated", "with", "the", "seed", "."], "sha": "97cdd1e241498446b46157b79b2a1ea2ec6d387a", "url": "https://github.com/iotaledger/iota.lib.py/blob/97cdd1e241498446b46157b79b2a1ea2ec6d387a/iota/api.py#L791-L836", "partition": "test"}
{"repo": "OpenKMIP/PyKMIP", "path": "kmip/core/messages/payloads/query.py", "func_name": "QueryResponsePayload.write", "original_string": "def write(self, output_buffer, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Write the data encoding the QueryResponsePayload object to a stream.\n\n        Args:\n            output_buffer (Stream): A data stream in which to encode object\n                data, supporting a write method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be encoded. Optional,\n                defaults to KMIP 1.0.\n        \"\"\"\n        local_buffer = utils.BytearrayStream()\n\n        if self._operations:\n            for operation in self._operations:\n                operation.write(local_buffer, kmip_version=kmip_version)\n\n        if self._object_types:\n            for object_type in self._object_types:\n                object_type.write(local_buffer, kmip_version=kmip_version)\n\n        if self._vendor_identification:\n            self._vendor_identification.write(\n                local_buffer,\n                kmip_version=kmip_version\n            )\n\n        if self._server_information:\n            self._server_information.write(\n                local_buffer,\n                kmip_version=kmip_version\n            )\n\n        if self._application_namespaces:\n            for application_namespace in self._application_namespaces:\n                application_namespace.write(\n                    local_buffer,\n                    kmip_version=kmip_version\n                )\n\n        if kmip_version >= enums.KMIPVersion.KMIP_1_1:\n            if self._extension_information:\n                for extension_information in self._extension_information:\n                    extension_information.write(\n                        local_buffer,\n                        kmip_version=kmip_version\n                    )\n\n        if kmip_version >= enums.KMIPVersion.KMIP_1_2:\n            if self._attestation_types:\n                for attestation_type in self._attestation_types:\n                    attestation_type.write(\n                        local_buffer,\n                        kmip_version=kmip_version\n                    )\n\n        if kmip_version >= enums.KMIPVersion.KMIP_1_3:\n            if self._rng_parameters:\n                for rng_parameters in self._rng_parameters:\n                    rng_parameters.write(\n                        local_buffer,\n                        kmip_version=kmip_version\n                    )\n            if self._profile_information:\n                for profile_information in self._profile_information:\n                    profile_information.write(\n                        local_buffer,\n                        kmip_version=kmip_version\n                    )\n            if self._validation_information:\n                for validation_information in self._validation_information:\n                    validation_information.write(\n                        local_buffer,\n                        kmip_version=kmip_version\n                    )\n            if self._capability_information:\n                for capability_information in self._capability_information:\n                    capability_information.write(\n                        local_buffer,\n                        kmip_version=kmip_version\n                    )\n            if self._client_registration_methods:\n                for client_reg_method in self._client_registration_methods:\n                    client_reg_method.write(\n                        local_buffer,\n                        kmip_version=kmip_version\n                    )\n\n        if kmip_version >= enums.KMIPVersion.KMIP_2_0:\n            if self._defaults_information:\n                self._defaults_information.write(\n                    local_buffer,\n                    kmip_version=kmip_version\n                )\n            if self._storage_protection_masks:\n                for storage_protection_mask in self._storage_protection_masks:\n                    storage_protection_mask.write(\n                        local_buffer,\n                        kmip_version=kmip_version\n                    )\n\n        self.length = local_buffer.length()\n        super(QueryResponsePayload, self).write(\n            output_buffer,\n            kmip_version=kmip_version\n        )\n        output_buffer.write(local_buffer.buffer)", "language": "python", "code": "def write(self, output_buffer, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Write the data encoding the QueryResponsePayload object to a stream.\n\n        Args:\n            output_buffer (Stream): A data stream in which to encode object\n                data, supporting a write method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be encoded. Optional,\n                defaults to KMIP 1.0.\n        \"\"\"\n        local_buffer = utils.BytearrayStream()\n\n        if self._operations:\n            for operation in self._operations:\n                operation.write(local_buffer, kmip_version=kmip_version)\n\n        if self._object_types:\n            for object_type in self._object_types:\n                object_type.write(local_buffer, kmip_version=kmip_version)\n\n        if self._vendor_identification:\n            self._vendor_identification.write(\n                local_buffer,\n                kmip_version=kmip_version\n            )\n\n        if self._server_information:\n            self._server_information.write(\n                local_buffer,\n                kmip_version=kmip_version\n            )\n\n        if self._application_namespaces:\n            for application_namespace in self._application_namespaces:\n                application_namespace.write(\n                    local_buffer,\n                    kmip_version=kmip_version\n                )\n\n        if kmip_version >= enums.KMIPVersion.KMIP_1_1:\n            if self._extension_information:\n                for extension_information in self._extension_information:\n                    extension_information.write(\n                        local_buffer,\n                        kmip_version=kmip_version\n                    )\n\n        if kmip_version >= enums.KMIPVersion.KMIP_1_2:\n            if self._attestation_types:\n                for attestation_type in self._attestation_types:\n                    attestation_type.write(\n                        local_buffer,\n                        kmip_version=kmip_version\n                    )\n\n        if kmip_version >= enums.KMIPVersion.KMIP_1_3:\n            if self._rng_parameters:\n                for rng_parameters in self._rng_parameters:\n                    rng_parameters.write(\n                        local_buffer,\n                        kmip_version=kmip_version\n                    )\n            if self._profile_information:\n                for profile_information in self._profile_information:\n                    profile_information.write(\n                        local_buffer,\n                        kmip_version=kmip_version\n                    )\n            if self._validation_information:\n                for validation_information in self._validation_information:\n                    validation_information.write(\n                        local_buffer,\n                        kmip_version=kmip_version\n                    )\n            if self._capability_information:\n                for capability_information in self._capability_information:\n                    capability_information.write(\n                        local_buffer,\n                        kmip_version=kmip_version\n                    )\n            if self._client_registration_methods:\n                for client_reg_method in self._client_registration_methods:\n                    client_reg_method.write(\n                        local_buffer,\n                        kmip_version=kmip_version\n                    )\n\n        if kmip_version >= enums.KMIPVersion.KMIP_2_0:\n            if self._defaults_information:\n                self._defaults_information.write(\n                    local_buffer,\n                    kmip_version=kmip_version\n                )\n            if self._storage_protection_masks:\n                for storage_protection_mask in self._storage_protection_masks:\n                    storage_protection_mask.write(\n                        local_buffer,\n                        kmip_version=kmip_version\n                    )\n\n        self.length = local_buffer.length()\n        super(QueryResponsePayload, self).write(\n            output_buffer,\n            kmip_version=kmip_version\n        )\n        output_buffer.write(local_buffer.buffer)", "code_tokens": ["def", "write", "(", "self", ",", "output_buffer", ",", "kmip_version", "=", "enums", ".", "KMIPVersion", ".", "KMIP_1_0", ")", ":", "local_buffer", "=", "utils", ".", "BytearrayStream", "(", ")", "if", "self", ".", "_operations", ":", "for", "operation", "in", "self", ".", "_operations", ":", "operation", ".", "write", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "if", "self", ".", "_object_types", ":", "for", "object_type", "in", "self", ".", "_object_types", ":", "object_type", ".", "write", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "if", "self", ".", "_vendor_identification", ":", "self", ".", "_vendor_identification", ".", "write", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "if", "self", ".", "_server_information", ":", "self", ".", "_server_information", ".", "write", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "if", "self", ".", "_application_namespaces", ":", "for", "application_namespace", "in", "self", ".", "_application_namespaces", ":", "application_namespace", ".", "write", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "if", "kmip_version", ">=", "enums", ".", "KMIPVersion", ".", "KMIP_1_1", ":", "if", "self", ".", "_extension_information", ":", "for", "extension_information", "in", "self", ".", "_extension_information", ":", "extension_information", ".", "write", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "if", "kmip_version", ">=", "enums", ".", "KMIPVersion", ".", "KMIP_1_2", ":", "if", "self", ".", "_attestation_types", ":", "for", "attestation_type", "in", "self", ".", "_attestation_types", ":", "attestation_type", ".", "write", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "if", "kmip_version", ">=", "enums", ".", "KMIPVersion", ".", "KMIP_1_3", ":", "if", "self", ".", "_rng_parameters", ":", "for", "rng_parameters", "in", "self", ".", "_rng_parameters", ":", "rng_parameters", ".", "write", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "if", "self", ".", "_profile_information", ":", "for", "profile_information", "in", "self", ".", "_profile_information", ":", "profile_information", ".", "write", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "if", "self", ".", "_validation_information", ":", "for", "validation_information", "in", "self", ".", "_validation_information", ":", "validation_information", ".", "write", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "if", "self", ".", "_capability_information", ":", "for", "capability_information", "in", "self", ".", "_capability_information", ":", "capability_information", ".", "write", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "if", "self", ".", "_client_registration_methods", ":", "for", "client_reg_method", "in", "self", ".", "_client_registration_methods", ":", "client_reg_method", ".", "write", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "if", "kmip_version", ">=", "enums", ".", "KMIPVersion", ".", "KMIP_2_0", ":", "if", "self", ".", "_defaults_information", ":", "self", ".", "_defaults_information", ".", "write", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "if", "self", ".", "_storage_protection_masks", ":", "for", "storage_protection_mask", "in", "self", ".", "_storage_protection_masks", ":", "storage_protection_mask", ".", "write", "(", "local_buffer", ",", "kmip_version", "=", "kmip_version", ")", "self", ".", "length", "=", "local_buffer", ".", "length", "(", ")", "super", "(", "QueryResponsePayload", ",", "self", ")", ".", "write", "(", "output_buffer", ",", "kmip_version", "=", "kmip_version", ")", "output_buffer", ".", "write", "(", "local_buffer", ".", "buffer", ")"], "docstring": "Write the data encoding the QueryResponsePayload object to a stream.\n\n        Args:\n            output_buffer (Stream): A data stream in which to encode object\n                data, supporting a write method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be encoded. Optional,\n                defaults to KMIP 1.0.", "docstring_tokens": ["Write", "the", "data", "encoding", "the", "QueryResponsePayload", "object", "to", "a", "stream", "."], "sha": "b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e", "url": "https://github.com/OpenKMIP/PyKMIP/blob/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e/kmip/core/messages/payloads/query.py#L884-L991", "partition": "test"}
{"repo": "LordSputnik/mutagen", "path": "mutagen/_util.py", "func_name": "insert_bytes", "original_string": "def insert_bytes(fobj, size, offset, BUFFER_SIZE=2**16):\n    \"\"\"Insert size bytes of empty space starting at offset.\n\n    fobj must be an open file object, open rb+ or\n    equivalent. Mutagen tries to use mmap to resize the file, but\n    falls back to a significantly slower method if mmap fails.\n    \"\"\"\n\n    assert 0 < size\n    assert 0 <= offset\n    locked = False\n    fobj.seek(0, 2)\n    filesize = fobj.tell()\n    movesize = filesize - offset\n    fobj.write(b'\\x00' * size)\n    fobj.flush()\n    try:\n        try:\n            import mmap\n            file_map = mmap.mmap(fobj.fileno(), filesize + size)\n            try:\n                file_map.move(offset + size, offset, movesize)\n            finally:\n                file_map.close()\n        except (ValueError, EnvironmentError, ImportError):\n            # handle broken mmap scenarios\n            locked = lock(fobj)\n            fobj.truncate(filesize)\n\n            fobj.seek(0, 2)\n            padsize = size\n            # Don't generate an enormous string if we need to pad\n            # the file out several megs.\n            while padsize:\n                addsize = min(BUFFER_SIZE, padsize)\n                fobj.write(b\"\\x00\" * addsize)\n                padsize -= addsize\n\n            fobj.seek(filesize, 0)\n            while movesize:\n                # At the start of this loop, fobj is pointing at the end\n                # of the data we need to move, which is of movesize length.\n                thismove = min(BUFFER_SIZE, movesize)\n                # Seek back however much we're going to read this frame.\n                fobj.seek(-thismove, 1)\n                nextpos = fobj.tell()\n                # Read it, so we're back at the end.\n                data = fobj.read(thismove)\n                # Seek back to where we need to write it.\n                fobj.seek(-thismove + size, 1)\n                # Write it.\n                fobj.write(data)\n                # And seek back to the end of the unmoved data.\n                fobj.seek(nextpos)\n                movesize -= thismove\n\n            fobj.flush()\n    finally:\n        if locked:\n            unlock(fobj)", "language": "python", "code": "def insert_bytes(fobj, size, offset, BUFFER_SIZE=2**16):\n    \"\"\"Insert size bytes of empty space starting at offset.\n\n    fobj must be an open file object, open rb+ or\n    equivalent. Mutagen tries to use mmap to resize the file, but\n    falls back to a significantly slower method if mmap fails.\n    \"\"\"\n\n    assert 0 < size\n    assert 0 <= offset\n    locked = False\n    fobj.seek(0, 2)\n    filesize = fobj.tell()\n    movesize = filesize - offset\n    fobj.write(b'\\x00' * size)\n    fobj.flush()\n    try:\n        try:\n            import mmap\n            file_map = mmap.mmap(fobj.fileno(), filesize + size)\n            try:\n                file_map.move(offset + size, offset, movesize)\n            finally:\n                file_map.close()\n        except (ValueError, EnvironmentError, ImportError):\n            # handle broken mmap scenarios\n            locked = lock(fobj)\n            fobj.truncate(filesize)\n\n            fobj.seek(0, 2)\n            padsize = size\n            # Don't generate an enormous string if we need to pad\n            # the file out several megs.\n            while padsize:\n                addsize = min(BUFFER_SIZE, padsize)\n                fobj.write(b\"\\x00\" * addsize)\n                padsize -= addsize\n\n            fobj.seek(filesize, 0)\n            while movesize:\n                # At the start of this loop, fobj is pointing at the end\n                # of the data we need to move, which is of movesize length.\n                thismove = min(BUFFER_SIZE, movesize)\n                # Seek back however much we're going to read this frame.\n                fobj.seek(-thismove, 1)\n                nextpos = fobj.tell()\n                # Read it, so we're back at the end.\n                data = fobj.read(thismove)\n                # Seek back to where we need to write it.\n                fobj.seek(-thismove + size, 1)\n                # Write it.\n                fobj.write(data)\n                # And seek back to the end of the unmoved data.\n                fobj.seek(nextpos)\n                movesize -= thismove\n\n            fobj.flush()\n    finally:\n        if locked:\n            unlock(fobj)", "code_tokens": ["def", "insert_bytes", "(", "fobj", ",", "size", ",", "offset", ",", "BUFFER_SIZE", "=", "2", "**", "16", ")", ":", "assert", "0", "<", "size", "assert", "0", "<=", "offset", "locked", "=", "False", "fobj", ".", "seek", "(", "0", ",", "2", ")", "filesize", "=", "fobj", ".", "tell", "(", ")", "movesize", "=", "filesize", "-", "offset", "fobj", ".", "write", "(", "b'\\x00'", "*", "size", ")", "fobj", ".", "flush", "(", ")", "try", ":", "try", ":", "import", "mmap", "file_map", "=", "mmap", ".", "mmap", "(", "fobj", ".", "fileno", "(", ")", ",", "filesize", "+", "size", ")", "try", ":", "file_map", ".", "move", "(", "offset", "+", "size", ",", "offset", ",", "movesize", ")", "finally", ":", "file_map", ".", "close", "(", ")", "except", "(", "ValueError", ",", "EnvironmentError", ",", "ImportError", ")", ":", "# handle broken mmap scenarios", "locked", "=", "lock", "(", "fobj", ")", "fobj", ".", "truncate", "(", "filesize", ")", "fobj", ".", "seek", "(", "0", ",", "2", ")", "padsize", "=", "size", "# Don't generate an enormous string if we need to pad", "# the file out several megs.", "while", "padsize", ":", "addsize", "=", "min", "(", "BUFFER_SIZE", ",", "padsize", ")", "fobj", ".", "write", "(", "b\"\\x00\"", "*", "addsize", ")", "padsize", "-=", "addsize", "fobj", ".", "seek", "(", "filesize", ",", "0", ")", "while", "movesize", ":", "# At the start of this loop, fobj is pointing at the end", "# of the data we need to move, which is of movesize length.", "thismove", "=", "min", "(", "BUFFER_SIZE", ",", "movesize", ")", "# Seek back however much we're going to read this frame.", "fobj", ".", "seek", "(", "-", "thismove", ",", "1", ")", "nextpos", "=", "fobj", ".", "tell", "(", ")", "# Read it, so we're back at the end.", "data", "=", "fobj", ".", "read", "(", "thismove", ")", "# Seek back to where we need to write it.", "fobj", ".", "seek", "(", "-", "thismove", "+", "size", ",", "1", ")", "# Write it.", "fobj", ".", "write", "(", "data", ")", "# And seek back to the end of the unmoved data.", "fobj", ".", "seek", "(", "nextpos", ")", "movesize", "-=", "thismove", "fobj", ".", "flush", "(", ")", "finally", ":", "if", "locked", ":", "unlock", "(", "fobj", ")"], "docstring": "Insert size bytes of empty space starting at offset.\n\n    fobj must be an open file object, open rb+ or\n    equivalent. Mutagen tries to use mmap to resize the file, but\n    falls back to a significantly slower method if mmap fails.", "docstring_tokens": ["Insert", "size", "bytes", "of", "empty", "space", "starting", "at", "offset", "."], "sha": "38e62c8dc35c72b16554f5dbe7c0fde91acc3411", "url": "https://github.com/LordSputnik/mutagen/blob/38e62c8dc35c72b16554f5dbe7c0fde91acc3411/mutagen/_util.py#L148-L207", "partition": "test"}
{"repo": "dagster-io/dagster", "path": "python_modules/dagster/dagster/core/types/decorator.py", "func_name": "as_dagster_type", "original_string": "def as_dagster_type(\n    existing_type,\n    name=None,\n    description=None,\n    input_schema=None,\n    output_schema=None,\n    serialization_strategy=None,\n    storage_plugins=None,\n):\n    '''\n    Takes a python cls and creates a type for it in the Dagster domain.\n\n    Args:\n        existing_type (cls)\n            The python type you want to project in to the Dagster type system.\n        name (Optional[str]):\n        description (Optiona[str]):\n        input_schema (Optional[InputSchema]):\n            An instance of a class that inherits from :py:class:`InputSchema` that\n            can map config data to a value of this type.\n\n        output_schema (Optiona[OutputSchema]):\n            An instance of a class that inherits from :py:class:`OutputSchema` that\n            can map config data to persisting values of this type.\n\n        serialization_strategy (Optional[SerializationStrategy]):\n            The default behavior for how to serialize this value for\n            persisting between execution steps.\n\n        storage_plugins (Optional[Dict[RunStorageMode, TypeStoragePlugin]]):\n            Storage type specific overrides for the serialization strategy.\n            This allows for storage specific optimzations such as effecient\n            distributed storage on S3.\n    '''\n    check.type_param(existing_type, 'existing_type')\n    check.opt_str_param(name, 'name')\n    check.opt_str_param(description, 'description')\n    check.opt_inst_param(input_schema, 'input_schema', InputSchema)\n    check.opt_inst_param(output_schema, 'output_schema', OutputSchema)\n    check.opt_inst_param(serialization_strategy, 'serialization_strategy', SerializationStrategy)\n    storage_plugins = check.opt_dict_param(storage_plugins, 'storage_plugins')\n\n    if serialization_strategy is None:\n        serialization_strategy = PickleSerializationStrategy()\n\n    name = existing_type.__name__ if name is None else name\n\n    return _decorate_as_dagster_type(\n        existing_type,\n        key=name,\n        name=name,\n        description=description,\n        input_schema=input_schema,\n        output_schema=output_schema,\n        serialization_strategy=serialization_strategy,\n        storage_plugins=storage_plugins,\n    )", "language": "python", "code": "def as_dagster_type(\n    existing_type,\n    name=None,\n    description=None,\n    input_schema=None,\n    output_schema=None,\n    serialization_strategy=None,\n    storage_plugins=None,\n):\n    '''\n    Takes a python cls and creates a type for it in the Dagster domain.\n\n    Args:\n        existing_type (cls)\n            The python type you want to project in to the Dagster type system.\n        name (Optional[str]):\n        description (Optiona[str]):\n        input_schema (Optional[InputSchema]):\n            An instance of a class that inherits from :py:class:`InputSchema` that\n            can map config data to a value of this type.\n\n        output_schema (Optiona[OutputSchema]):\n            An instance of a class that inherits from :py:class:`OutputSchema` that\n            can map config data to persisting values of this type.\n\n        serialization_strategy (Optional[SerializationStrategy]):\n            The default behavior for how to serialize this value for\n            persisting between execution steps.\n\n        storage_plugins (Optional[Dict[RunStorageMode, TypeStoragePlugin]]):\n            Storage type specific overrides for the serialization strategy.\n            This allows for storage specific optimzations such as effecient\n            distributed storage on S3.\n    '''\n    check.type_param(existing_type, 'existing_type')\n    check.opt_str_param(name, 'name')\n    check.opt_str_param(description, 'description')\n    check.opt_inst_param(input_schema, 'input_schema', InputSchema)\n    check.opt_inst_param(output_schema, 'output_schema', OutputSchema)\n    check.opt_inst_param(serialization_strategy, 'serialization_strategy', SerializationStrategy)\n    storage_plugins = check.opt_dict_param(storage_plugins, 'storage_plugins')\n\n    if serialization_strategy is None:\n        serialization_strategy = PickleSerializationStrategy()\n\n    name = existing_type.__name__ if name is None else name\n\n    return _decorate_as_dagster_type(\n        existing_type,\n        key=name,\n        name=name,\n        description=description,\n        input_schema=input_schema,\n        output_schema=output_schema,\n        serialization_strategy=serialization_strategy,\n        storage_plugins=storage_plugins,\n    )", "code_tokens": ["def", "as_dagster_type", "(", "existing_type", ",", "name", "=", "None", ",", "description", "=", "None", ",", "input_schema", "=", "None", ",", "output_schema", "=", "None", ",", "serialization_strategy", "=", "None", ",", "storage_plugins", "=", "None", ",", ")", ":", "check", ".", "type_param", "(", "existing_type", ",", "'existing_type'", ")", "check", ".", "opt_str_param", "(", "name", ",", "'name'", ")", "check", ".", "opt_str_param", "(", "description", ",", "'description'", ")", "check", ".", "opt_inst_param", "(", "input_schema", ",", "'input_schema'", ",", "InputSchema", ")", "check", ".", "opt_inst_param", "(", "output_schema", ",", "'output_schema'", ",", "OutputSchema", ")", "check", ".", "opt_inst_param", "(", "serialization_strategy", ",", "'serialization_strategy'", ",", "SerializationStrategy", ")", "storage_plugins", "=", "check", ".", "opt_dict_param", "(", "storage_plugins", ",", "'storage_plugins'", ")", "if", "serialization_strategy", "is", "None", ":", "serialization_strategy", "=", "PickleSerializationStrategy", "(", ")", "name", "=", "existing_type", ".", "__name__", "if", "name", "is", "None", "else", "name", "return", "_decorate_as_dagster_type", "(", "existing_type", ",", "key", "=", "name", ",", "name", "=", "name", ",", "description", "=", "description", ",", "input_schema", "=", "input_schema", ",", "output_schema", "=", "output_schema", ",", "serialization_strategy", "=", "serialization_strategy", ",", "storage_plugins", "=", "storage_plugins", ",", ")"], "docstring": "Takes a python cls and creates a type for it in the Dagster domain.\n\n    Args:\n        existing_type (cls)\n            The python type you want to project in to the Dagster type system.\n        name (Optional[str]):\n        description (Optiona[str]):\n        input_schema (Optional[InputSchema]):\n            An instance of a class that inherits from :py:class:`InputSchema` that\n            can map config data to a value of this type.\n\n        output_schema (Optiona[OutputSchema]):\n            An instance of a class that inherits from :py:class:`OutputSchema` that\n            can map config data to persisting values of this type.\n\n        serialization_strategy (Optional[SerializationStrategy]):\n            The default behavior for how to serialize this value for\n            persisting between execution steps.\n\n        storage_plugins (Optional[Dict[RunStorageMode, TypeStoragePlugin]]):\n            Storage type specific overrides for the serialization strategy.\n            This allows for storage specific optimzations such as effecient\n            distributed storage on S3.", "docstring_tokens": ["Takes", "a", "python", "cls", "and", "creates", "a", "type", "for", "it", "in", "the", "Dagster", "domain", "."], "sha": "4119f8c773089de64831b1dfb9e168e353d401dc", "url": "https://github.com/dagster-io/dagster/blob/4119f8c773089de64831b1dfb9e168e353d401dc/python_modules/dagster/dagster/core/types/decorator.py#L98-L154", "partition": "test"}
{"repo": "Chilipp/sphinx-nbexamples", "path": "sphinx_nbexamples/__init__.py", "func_name": "NotebookProcessor.url", "original_string": "def url(self):\n        \"\"\"The url on jupyter nbviewer for this notebook or None if unknown\"\"\"\n        if self._url is not None:\n            url = self._url\n        else:\n            url = getattr(self.nb.metadata, 'url', None)\n        if url is not None:\n            return nbviewer_link(url)", "language": "python", "code": "def url(self):\n        \"\"\"The url on jupyter nbviewer for this notebook or None if unknown\"\"\"\n        if self._url is not None:\n            url = self._url\n        else:\n            url = getattr(self.nb.metadata, 'url', None)\n        if url is not None:\n            return nbviewer_link(url)", "code_tokens": ["def", "url", "(", "self", ")", ":", "if", "self", ".", "_url", "is", "not", "None", ":", "url", "=", "self", ".", "_url", "else", ":", "url", "=", "getattr", "(", "self", ".", "nb", ".", "metadata", ",", "'url'", ",", "None", ")", "if", "url", "is", "not", "None", ":", "return", "nbviewer_link", "(", "url", ")"], "docstring": "The url on jupyter nbviewer for this notebook or None if unknown", "docstring_tokens": ["The", "url", "on", "jupyter", "nbviewer", "for", "this", "notebook", "or", "None", "if", "unknown"], "sha": "08e0319ff3c70f8a931dfa8890caf48add4d0470", "url": "https://github.com/Chilipp/sphinx-nbexamples/blob/08e0319ff3c70f8a931dfa8890caf48add4d0470/sphinx_nbexamples/__init__.py#L244-L251", "partition": "test"}
{"repo": "xmartlabs/benderthon", "path": "benderthon/tf_freeze.py", "func_name": "save_graph_only_from_checkpoint", "original_string": "def save_graph_only_from_checkpoint(input_checkpoint, output_file_path, output_node_names, as_text=False):\n    \"\"\"Save a small version of the graph based on a checkpoint and the output node names.\"\"\"\n    check_input_checkpoint(input_checkpoint)\n\n    output_node_names = output_node_names_string_as_list(output_node_names)\n\n    with tf.Session() as sess:\n        restore_from_checkpoint(sess, input_checkpoint)\n        save_graph_only(sess, output_file_path, output_node_names, as_text=as_text)", "language": "python", "code": "def save_graph_only_from_checkpoint(input_checkpoint, output_file_path, output_node_names, as_text=False):\n    \"\"\"Save a small version of the graph based on a checkpoint and the output node names.\"\"\"\n    check_input_checkpoint(input_checkpoint)\n\n    output_node_names = output_node_names_string_as_list(output_node_names)\n\n    with tf.Session() as sess:\n        restore_from_checkpoint(sess, input_checkpoint)\n        save_graph_only(sess, output_file_path, output_node_names, as_text=as_text)", "code_tokens": ["def", "save_graph_only_from_checkpoint", "(", "input_checkpoint", ",", "output_file_path", ",", "output_node_names", ",", "as_text", "=", "False", ")", ":", "check_input_checkpoint", "(", "input_checkpoint", ")", "output_node_names", "=", "output_node_names_string_as_list", "(", "output_node_names", ")", "with", "tf", ".", "Session", "(", ")", "as", "sess", ":", "restore_from_checkpoint", "(", "sess", ",", "input_checkpoint", ")", "save_graph_only", "(", "sess", ",", "output_file_path", ",", "output_node_names", ",", "as_text", "=", "as_text", ")"], "docstring": "Save a small version of the graph based on a checkpoint and the output node names.", "docstring_tokens": ["Save", "a", "small", "version", "of", "the", "graph", "based", "on", "a", "checkpoint", "and", "the", "output", "node", "names", "."], "sha": "810b6fb90f56136257e7ed12e5a30d17ad7ce6ba", "url": "https://github.com/xmartlabs/benderthon/blob/810b6fb90f56136257e7ed12e5a30d17ad7ce6ba/benderthon/tf_freeze.py#L51-L59", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/frontend/html/notebook/notebookmanager.py", "func_name": "NotebookManager.copy_notebook", "original_string": "def copy_notebook(self, notebook_id):\n        \"\"\"Copy an existing notebook and return its notebook_id.\"\"\"\n        last_mod, nb = self.get_notebook_object(notebook_id)\n        name = nb.metadata.name + '-Copy'\n        path, name = self.increment_filename(name)\n        nb.metadata.name = name\n        notebook_id = self.new_notebook_id(name)\n        self.save_notebook_object(notebook_id, nb)\n        return notebook_id", "language": "python", "code": "def copy_notebook(self, notebook_id):\n        \"\"\"Copy an existing notebook and return its notebook_id.\"\"\"\n        last_mod, nb = self.get_notebook_object(notebook_id)\n        name = nb.metadata.name + '-Copy'\n        path, name = self.increment_filename(name)\n        nb.metadata.name = name\n        notebook_id = self.new_notebook_id(name)\n        self.save_notebook_object(notebook_id, nb)\n        return notebook_id", "code_tokens": ["def", "copy_notebook", "(", "self", ",", "notebook_id", ")", ":", "last_mod", ",", "nb", "=", "self", ".", "get_notebook_object", "(", "notebook_id", ")", "name", "=", "nb", ".", "metadata", ".", "name", "+", "'-Copy'", "path", ",", "name", "=", "self", ".", "increment_filename", "(", "name", ")", "nb", ".", "metadata", ".", "name", "=", "name", "notebook_id", "=", "self", ".", "new_notebook_id", "(", "name", ")", "self", ".", "save_notebook_object", "(", "notebook_id", ",", "nb", ")", "return", "notebook_id"], "docstring": "Copy an existing notebook and return its notebook_id.", "docstring_tokens": ["Copy", "an", "existing", "notebook", "and", "return", "its", "notebook_id", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/frontend/html/notebook/notebookmanager.py#L278-L286", "partition": "test"}
{"repo": "MeirKriheli/python-bidi", "path": "bidi/algorithm.py", "func_name": "debug_storage", "original_string": "def debug_storage(storage, base_info=False, chars=True, runs=False):\n    \"Display debug information for the storage\"\n\n    import codecs\n    import locale\n    import sys\n\n    if six.PY2:\n        stderr = codecs.getwriter(locale.getpreferredencoding())(sys.stderr)\n    else:\n        stderr = sys.stderr\n\n    caller = inspect.stack()[1][3]\n    stderr.write('in %s\\n' % caller)\n\n    if base_info:\n        stderr.write(u'  base level  : %d\\n' % storage['base_level'])\n        stderr.write(u'  base dir    : %s\\n' % storage['base_dir'])\n\n    if runs:\n        stderr.write(u'  runs        : %s\\n' % list(storage['runs']))\n\n    if chars:\n        output = u'  Chars       : '\n        for _ch in storage['chars']:\n            if _ch != '\\n':\n                output += _ch['ch']\n            else:\n                output += 'C'\n        stderr.write(output + u'\\n')\n\n        output = u'  Res. levels : %s\\n' % u''.join(\n            [six.text_type(_ch['level']) for _ch in storage['chars']])\n        stderr.write(output)\n\n        _types = [_ch['type'].ljust(3) for _ch in storage['chars']]\n\n        for i in range(3):\n            if i:\n                output = u'                %s\\n'\n            else:\n                output = u'  Res. types  : %s\\n'\n            stderr.write(output % u''.join([_t[i] for _t in _types]))", "language": "python", "code": "def debug_storage(storage, base_info=False, chars=True, runs=False):\n    \"Display debug information for the storage\"\n\n    import codecs\n    import locale\n    import sys\n\n    if six.PY2:\n        stderr = codecs.getwriter(locale.getpreferredencoding())(sys.stderr)\n    else:\n        stderr = sys.stderr\n\n    caller = inspect.stack()[1][3]\n    stderr.write('in %s\\n' % caller)\n\n    if base_info:\n        stderr.write(u'  base level  : %d\\n' % storage['base_level'])\n        stderr.write(u'  base dir    : %s\\n' % storage['base_dir'])\n\n    if runs:\n        stderr.write(u'  runs        : %s\\n' % list(storage['runs']))\n\n    if chars:\n        output = u'  Chars       : '\n        for _ch in storage['chars']:\n            if _ch != '\\n':\n                output += _ch['ch']\n            else:\n                output += 'C'\n        stderr.write(output + u'\\n')\n\n        output = u'  Res. levels : %s\\n' % u''.join(\n            [six.text_type(_ch['level']) for _ch in storage['chars']])\n        stderr.write(output)\n\n        _types = [_ch['type'].ljust(3) for _ch in storage['chars']]\n\n        for i in range(3):\n            if i:\n                output = u'                %s\\n'\n            else:\n                output = u'  Res. types  : %s\\n'\n            stderr.write(output % u''.join([_t[i] for _t in _types]))", "code_tokens": ["def", "debug_storage", "(", "storage", ",", "base_info", "=", "False", ",", "chars", "=", "True", ",", "runs", "=", "False", ")", ":", "import", "codecs", "import", "locale", "import", "sys", "if", "six", ".", "PY2", ":", "stderr", "=", "codecs", ".", "getwriter", "(", "locale", ".", "getpreferredencoding", "(", ")", ")", "(", "sys", ".", "stderr", ")", "else", ":", "stderr", "=", "sys", ".", "stderr", "caller", "=", "inspect", ".", "stack", "(", ")", "[", "1", "]", "[", "3", "]", "stderr", ".", "write", "(", "'in %s\\n'", "%", "caller", ")", "if", "base_info", ":", "stderr", ".", "write", "(", "u'  base level  : %d\\n'", "%", "storage", "[", "'base_level'", "]", ")", "stderr", ".", "write", "(", "u'  base dir    : %s\\n'", "%", "storage", "[", "'base_dir'", "]", ")", "if", "runs", ":", "stderr", ".", "write", "(", "u'  runs        : %s\\n'", "%", "list", "(", "storage", "[", "'runs'", "]", ")", ")", "if", "chars", ":", "output", "=", "u'  Chars       : '", "for", "_ch", "in", "storage", "[", "'chars'", "]", ":", "if", "_ch", "!=", "'\\n'", ":", "output", "+=", "_ch", "[", "'ch'", "]", "else", ":", "output", "+=", "'C'", "stderr", ".", "write", "(", "output", "+", "u'\\n'", ")", "output", "=", "u'  Res. levels : %s\\n'", "%", "u''", ".", "join", "(", "[", "six", ".", "text_type", "(", "_ch", "[", "'level'", "]", ")", "for", "_ch", "in", "storage", "[", "'chars'", "]", "]", ")", "stderr", ".", "write", "(", "output", ")", "_types", "=", "[", "_ch", "[", "'type'", "]", ".", "ljust", "(", "3", ")", "for", "_ch", "in", "storage", "[", "'chars'", "]", "]", "for", "i", "in", "range", "(", "3", ")", ":", "if", "i", ":", "output", "=", "u'                %s\\n'", "else", ":", "output", "=", "u'  Res. types  : %s\\n'", "stderr", ".", "write", "(", "output", "%", "u''", ".", "join", "(", "[", "_t", "[", "i", "]", "for", "_t", "in", "_types", "]", ")", ")"], "docstring": "Display debug information for the storage", "docstring_tokens": ["Display", "debug", "information", "for", "the", "storage"], "sha": "a0e265bb465c1b7ad628487991e33b5ebe364641", "url": "https://github.com/MeirKriheli/python-bidi/blob/a0e265bb465c1b7ad628487991e33b5ebe364641/bidi/algorithm.py#L62-L104", "partition": "test"}
{"repo": "ambitioninc/python-logentries-api", "path": "logentries_api/resources.py", "func_name": "Alerts.get", "original_string": "def get(self, alert_type, alert_args=None):\n        \"\"\"\n        Get alerts that match the alert type and args.\n\n        :param alert_type: The type of the alert. Must be one of 'pagerduty',\n            'mailto', 'webhook', 'slack', or 'hipchat'\n        :type alert_type: str\n\n        :param alert_args: The args for the alert. The provided args must be a\n            subset of the actual alert args. If no args are provided, all\n            alerts matching the ``alert_type`` are returned. For example:\n            ``.get('mailto', alert_args={'direct': 'me@mydomain.com'})`` or\n            ``.get('slack', {'url': 'https://hooks.slack.com/services...'})``\n\n        :return: A list of matching alerts. An empty list is returned if there\n            are not any matches\n        :rtype: list of dict\n\n        :raises: This will raise a\n            :class:`ServerException<logentries_api.exceptions.ServerException>`\n            if there is an error from Logentries\n        \"\"\"\n        alert_args = alert_args or {}\n\n        alerts = self.list()\n        return [\n            alert\n            for alert\n            in alerts\n            if alert.get('type') == alert_type\n            and dict_is_subset(alert_args, alert.get('args'))\n        ]", "language": "python", "code": "def get(self, alert_type, alert_args=None):\n        \"\"\"\n        Get alerts that match the alert type and args.\n\n        :param alert_type: The type of the alert. Must be one of 'pagerduty',\n            'mailto', 'webhook', 'slack', or 'hipchat'\n        :type alert_type: str\n\n        :param alert_args: The args for the alert. The provided args must be a\n            subset of the actual alert args. If no args are provided, all\n            alerts matching the ``alert_type`` are returned. For example:\n            ``.get('mailto', alert_args={'direct': 'me@mydomain.com'})`` or\n            ``.get('slack', {'url': 'https://hooks.slack.com/services...'})``\n\n        :return: A list of matching alerts. An empty list is returned if there\n            are not any matches\n        :rtype: list of dict\n\n        :raises: This will raise a\n            :class:`ServerException<logentries_api.exceptions.ServerException>`\n            if there is an error from Logentries\n        \"\"\"\n        alert_args = alert_args or {}\n\n        alerts = self.list()\n        return [\n            alert\n            for alert\n            in alerts\n            if alert.get('type') == alert_type\n            and dict_is_subset(alert_args, alert.get('args'))\n        ]", "code_tokens": ["def", "get", "(", "self", ",", "alert_type", ",", "alert_args", "=", "None", ")", ":", "alert_args", "=", "alert_args", "or", "{", "}", "alerts", "=", "self", ".", "list", "(", ")", "return", "[", "alert", "for", "alert", "in", "alerts", "if", "alert", ".", "get", "(", "'type'", ")", "==", "alert_type", "and", "dict_is_subset", "(", "alert_args", ",", "alert", ".", "get", "(", "'args'", ")", ")", "]"], "docstring": "Get alerts that match the alert type and args.\n\n        :param alert_type: The type of the alert. Must be one of 'pagerduty',\n            'mailto', 'webhook', 'slack', or 'hipchat'\n        :type alert_type: str\n\n        :param alert_args: The args for the alert. The provided args must be a\n            subset of the actual alert args. If no args are provided, all\n            alerts matching the ``alert_type`` are returned. For example:\n            ``.get('mailto', alert_args={'direct': 'me@mydomain.com'})`` or\n            ``.get('slack', {'url': 'https://hooks.slack.com/services...'})``\n\n        :return: A list of matching alerts. An empty list is returned if there\n            are not any matches\n        :rtype: list of dict\n\n        :raises: This will raise a\n            :class:`ServerException<logentries_api.exceptions.ServerException>`\n            if there is an error from Logentries", "docstring_tokens": ["Get", "alerts", "that", "match", "the", "alert", "type", "and", "args", "."], "sha": "77ff1a7a2995d7ea2725b74e34c0f880f4ee23bc", "url": "https://github.com/ambitioninc/python-logentries-api/blob/77ff1a7a2995d7ea2725b74e34c0f880f4ee23bc/logentries_api/resources.py#L560-L591", "partition": "test"}
{"repo": "SmokinCaterpillar/pypet", "path": "pypet/trajectory.py", "func_name": "Trajectory._make_reversed_wildcards", "original_string": "def _make_reversed_wildcards(self, old_length=-1):\n        \"\"\"Creates a full mapping from all wildcard translations to the corresponding wildcards\"\"\"\n        if len(self._reversed_wildcards) > 0:\n            # We already created reversed wildcards, so we don't need to do all of them\n            # again\n            start = old_length\n        else:\n            start = -1\n        for wildcards, func in self._wildcard_functions.items():\n            for irun in range(start, len(self)):\n                translated_name = func(irun)\n                if not translated_name in self._reversed_wildcards:\n                    self._reversed_wildcards[translated_name] = ([], wildcards)\n                self._reversed_wildcards[translated_name][0].append(irun)", "language": "python", "code": "def _make_reversed_wildcards(self, old_length=-1):\n        \"\"\"Creates a full mapping from all wildcard translations to the corresponding wildcards\"\"\"\n        if len(self._reversed_wildcards) > 0:\n            # We already created reversed wildcards, so we don't need to do all of them\n            # again\n            start = old_length\n        else:\n            start = -1\n        for wildcards, func in self._wildcard_functions.items():\n            for irun in range(start, len(self)):\n                translated_name = func(irun)\n                if not translated_name in self._reversed_wildcards:\n                    self._reversed_wildcards[translated_name] = ([], wildcards)\n                self._reversed_wildcards[translated_name][0].append(irun)", "code_tokens": ["def", "_make_reversed_wildcards", "(", "self", ",", "old_length", "=", "-", "1", ")", ":", "if", "len", "(", "self", ".", "_reversed_wildcards", ")", ">", "0", ":", "# We already created reversed wildcards, so we don't need to do all of them", "# again", "start", "=", "old_length", "else", ":", "start", "=", "-", "1", "for", "wildcards", ",", "func", "in", "self", ".", "_wildcard_functions", ".", "items", "(", ")", ":", "for", "irun", "in", "range", "(", "start", ",", "len", "(", "self", ")", ")", ":", "translated_name", "=", "func", "(", "irun", ")", "if", "not", "translated_name", "in", "self", ".", "_reversed_wildcards", ":", "self", ".", "_reversed_wildcards", "[", "translated_name", "]", "=", "(", "[", "]", ",", "wildcards", ")", "self", ".", "_reversed_wildcards", "[", "translated_name", "]", "[", "0", "]", ".", "append", "(", "irun", ")"], "docstring": "Creates a full mapping from all wildcard translations to the corresponding wildcards", "docstring_tokens": ["Creates", "a", "full", "mapping", "from", "all", "wildcard", "translations", "to", "the", "corresponding", "wildcards"], "sha": "97ad3e80d46dbdea02deeb98ea41f05a19565826", "url": "https://github.com/SmokinCaterpillar/pypet/blob/97ad3e80d46dbdea02deeb98ea41f05a19565826/pypet/trajectory.py#L1806-L1819", "partition": "test"}
{"repo": "ubyssey/dispatch", "path": "dispatch/api/serializers.py", "func_name": "ContentSerializer.sanitize_block", "original_string": "def sanitize_block(self, block):\n        \"\"\"Santizes the data for the given block.\n        If block has a matching embed serializer, use the `to_internal_value` method.\"\"\"\n\n        embed_type = block.get('type', None)\n        data = block.get('data', {})\n        serializer = self.serializers.get(embed_type, None)\n\n        if serializer is None:\n            return block\n\n        block['data'] = serializer.to_internal_value(data)\n\n        return block", "language": "python", "code": "def sanitize_block(self, block):\n        \"\"\"Santizes the data for the given block.\n        If block has a matching embed serializer, use the `to_internal_value` method.\"\"\"\n\n        embed_type = block.get('type', None)\n        data = block.get('data', {})\n        serializer = self.serializers.get(embed_type, None)\n\n        if serializer is None:\n            return block\n\n        block['data'] = serializer.to_internal_value(data)\n\n        return block", "code_tokens": ["def", "sanitize_block", "(", "self", ",", "block", ")", ":", "embed_type", "=", "block", ".", "get", "(", "'type'", ",", "None", ")", "data", "=", "block", ".", "get", "(", "'data'", ",", "{", "}", ")", "serializer", "=", "self", ".", "serializers", ".", "get", "(", "embed_type", ",", "None", ")", "if", "serializer", "is", "None", ":", "return", "block", "block", "[", "'data'", "]", "=", "serializer", ".", "to_internal_value", "(", "data", ")", "return", "block"], "docstring": "Santizes the data for the given block.\n        If block has a matching embed serializer, use the `to_internal_value` method.", "docstring_tokens": ["Santizes", "the", "data", "for", "the", "given", "block", ".", "If", "block", "has", "a", "matching", "embed", "serializer", "use", "the", "to_internal_value", "method", "."], "sha": "8da6084fe61726f20e9cf675190480cfc45ee764", "url": "https://github.com/ubyssey/dispatch/blob/8da6084fe61726f20e9cf675190480cfc45ee764/dispatch/api/serializers.py#L449-L462", "partition": "test"}
{"repo": "rocky/python3-trepan", "path": "trepan/lib/sighandler.py", "func_name": "SignalManager.handle_stop", "original_string": "def handle_stop(self, signame, set_stop):\n        \"\"\"Set whether we stop or not when this signal is caught.\n        If 'set_stop' is True your program will stop when this signal\n        happens.\"\"\"\n        if set_stop:\n            self.sigs[signame].b_stop       = True\n            # stop keyword implies print AND nopass\n            self.sigs[signame].print_method = self.dbgr.intf[-1].msg\n            self.sigs[signame].pass_along   = False\n        else:\n            self.sigs[signame].b_stop       = False\n            pass\n        return set_stop", "language": "python", "code": "def handle_stop(self, signame, set_stop):\n        \"\"\"Set whether we stop or not when this signal is caught.\n        If 'set_stop' is True your program will stop when this signal\n        happens.\"\"\"\n        if set_stop:\n            self.sigs[signame].b_stop       = True\n            # stop keyword implies print AND nopass\n            self.sigs[signame].print_method = self.dbgr.intf[-1].msg\n            self.sigs[signame].pass_along   = False\n        else:\n            self.sigs[signame].b_stop       = False\n            pass\n        return set_stop", "code_tokens": ["def", "handle_stop", "(", "self", ",", "signame", ",", "set_stop", ")", ":", "if", "set_stop", ":", "self", ".", "sigs", "[", "signame", "]", ".", "b_stop", "=", "True", "# stop keyword implies print AND nopass", "self", ".", "sigs", "[", "signame", "]", ".", "print_method", "=", "self", ".", "dbgr", ".", "intf", "[", "-", "1", "]", ".", "msg", "self", ".", "sigs", "[", "signame", "]", ".", "pass_along", "=", "False", "else", ":", "self", ".", "sigs", "[", "signame", "]", ".", "b_stop", "=", "False", "pass", "return", "set_stop"], "docstring": "Set whether we stop or not when this signal is caught.\n        If 'set_stop' is True your program will stop when this signal\n        happens.", "docstring_tokens": ["Set", "whether", "we", "stop", "or", "not", "when", "this", "signal", "is", "caught", ".", "If", "set_stop", "is", "True", "your", "program", "will", "stop", "when", "this", "signal", "happens", "."], "sha": "14e91bc0acce090d67be145b1ac040cab92ac5f3", "url": "https://github.com/rocky/python3-trepan/blob/14e91bc0acce090d67be145b1ac040cab92ac5f3/trepan/lib/sighandler.py#L396-L408", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/pyreverse/diagrams.py", "func_name": "ClassDiagram.classe", "original_string": "def classe(self, name):\n        \"\"\"return a class by its name, raise KeyError if not found\n        \"\"\"\n        for klass in self.classes():\n            if klass.node.name == name:\n                return klass\n        raise KeyError(name)", "language": "python", "code": "def classe(self, name):\n        \"\"\"return a class by its name, raise KeyError if not found\n        \"\"\"\n        for klass in self.classes():\n            if klass.node.name == name:\n                return klass\n        raise KeyError(name)", "code_tokens": ["def", "classe", "(", "self", ",", "name", ")", ":", "for", "klass", "in", "self", ".", "classes", "(", ")", ":", "if", "klass", ".", "node", ".", "name", "==", "name", ":", "return", "klass", "raise", "KeyError", "(", "name", ")"], "docstring": "return a class by its name, raise KeyError if not found", "docstring_tokens": ["return", "a", "class", "by", "its", "name", "raise", "KeyError", "if", "not", "found"], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/pyreverse/diagrams.py#L157-L163", "partition": "test"}
{"repo": "Toblerity/rtree", "path": "rtree/index.py", "func_name": "Index.delete", "original_string": "def delete(self, id, coordinates):\n        \"\"\"Deletes items from the index with the given ``'id'`` within the\n        specified coordinates.\n\n        :param id: long integer\n            A long integer that is the identifier for this index entry.  IDs\n            need not be unique to be inserted into the index, and it is up\n            to the user to ensure they are unique if this is a requirement.\n\n        :param coordinates: sequence or array\n            Dimension * 2 coordinate pairs, representing the min\n            and max coordinates in each dimension of the item to be\n            deleted from the index. Their ordering will depend on the\n            index's :attr:`interleaved` data member.\n            These are not the coordinates of a space containing the\n            item, but those of the item itself. Together with the\n            id parameter, they determine which item will be deleted.\n            This may be an object that satisfies the numpy array protocol.\n\n        Example::\n\n            >>> from rtree import index\n            >>> idx = index.Index()\n            >>> idx.delete(4321,\n            ...            (34.3776829412, 26.7375853734, 49.3776829412,\n            ...             41.7375853734))\n\n        \"\"\"\n        p_mins, p_maxs = self.get_coordinate_pointers(coordinates)\n        core.rt.Index_DeleteData(\n            self.handle, id, p_mins, p_maxs, self.properties.dimension)", "language": "python", "code": "def delete(self, id, coordinates):\n        \"\"\"Deletes items from the index with the given ``'id'`` within the\n        specified coordinates.\n\n        :param id: long integer\n            A long integer that is the identifier for this index entry.  IDs\n            need not be unique to be inserted into the index, and it is up\n            to the user to ensure they are unique if this is a requirement.\n\n        :param coordinates: sequence or array\n            Dimension * 2 coordinate pairs, representing the min\n            and max coordinates in each dimension of the item to be\n            deleted from the index. Their ordering will depend on the\n            index's :attr:`interleaved` data member.\n            These are not the coordinates of a space containing the\n            item, but those of the item itself. Together with the\n            id parameter, they determine which item will be deleted.\n            This may be an object that satisfies the numpy array protocol.\n\n        Example::\n\n            >>> from rtree import index\n            >>> idx = index.Index()\n            >>> idx.delete(4321,\n            ...            (34.3776829412, 26.7375853734, 49.3776829412,\n            ...             41.7375853734))\n\n        \"\"\"\n        p_mins, p_maxs = self.get_coordinate_pointers(coordinates)\n        core.rt.Index_DeleteData(\n            self.handle, id, p_mins, p_maxs, self.properties.dimension)", "code_tokens": ["def", "delete", "(", "self", ",", "id", ",", "coordinates", ")", ":", "p_mins", ",", "p_maxs", "=", "self", ".", "get_coordinate_pointers", "(", "coordinates", ")", "core", ".", "rt", ".", "Index_DeleteData", "(", "self", ".", "handle", ",", "id", ",", "p_mins", ",", "p_maxs", ",", "self", ".", "properties", ".", "dimension", ")"], "docstring": "Deletes items from the index with the given ``'id'`` within the\n        specified coordinates.\n\n        :param id: long integer\n            A long integer that is the identifier for this index entry.  IDs\n            need not be unique to be inserted into the index, and it is up\n            to the user to ensure they are unique if this is a requirement.\n\n        :param coordinates: sequence or array\n            Dimension * 2 coordinate pairs, representing the min\n            and max coordinates in each dimension of the item to be\n            deleted from the index. Their ordering will depend on the\n            index's :attr:`interleaved` data member.\n            These are not the coordinates of a space containing the\n            item, but those of the item itself. Together with the\n            id parameter, they determine which item will be deleted.\n            This may be an object that satisfies the numpy array protocol.\n\n        Example::\n\n            >>> from rtree import index\n            >>> idx = index.Index()\n            >>> idx.delete(4321,\n            ...            (34.3776829412, 26.7375853734, 49.3776829412,\n            ...             41.7375853734))", "docstring_tokens": ["Deletes", "items", "from", "the", "index", "with", "the", "given", "id", "within", "the", "specified", "coordinates", "."], "sha": "5d33357c8e88f1a8344415dc15a7d2440211b281", "url": "https://github.com/Toblerity/rtree/blob/5d33357c8e88f1a8344415dc15a7d2440211b281/rtree/index.py#L622-L652", "partition": "test"}
{"repo": "sergei-maertens/django-systemjs", "path": "systemjs/base.py", "func_name": "SystemTracer.trace", "original_string": "def trace(self, app):\n        \"\"\"\n        Trace the dependencies for app.\n\n        A tracer-instance is shortlived, and re-tracing the same app should\n        yield the same results. Since tracing is an expensive process, cache\n        the result on the tracer instance.\n        \"\"\"\n        if app not in self._trace_cache:\n            process = subprocess.Popen(\n                \"trace-deps.js {}\".format(app), shell=True,\n                stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n                env=self.env, universal_newlines=True, cwd=self._package_json_dir\n            )\n            out, err = process.communicate()\n            if err:\n                raise TraceError(err)\n            self._trace_cache[app] = json.loads(out)\n        return self._trace_cache[app]", "language": "python", "code": "def trace(self, app):\n        \"\"\"\n        Trace the dependencies for app.\n\n        A tracer-instance is shortlived, and re-tracing the same app should\n        yield the same results. Since tracing is an expensive process, cache\n        the result on the tracer instance.\n        \"\"\"\n        if app not in self._trace_cache:\n            process = subprocess.Popen(\n                \"trace-deps.js {}\".format(app), shell=True,\n                stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n                env=self.env, universal_newlines=True, cwd=self._package_json_dir\n            )\n            out, err = process.communicate()\n            if err:\n                raise TraceError(err)\n            self._trace_cache[app] = json.loads(out)\n        return self._trace_cache[app]", "code_tokens": ["def", "trace", "(", "self", ",", "app", ")", ":", "if", "app", "not", "in", "self", ".", "_trace_cache", ":", "process", "=", "subprocess", ".", "Popen", "(", "\"trace-deps.js {}\"", ".", "format", "(", "app", ")", ",", "shell", "=", "True", ",", "stdout", "=", "subprocess", ".", "PIPE", ",", "stderr", "=", "subprocess", ".", "PIPE", ",", "env", "=", "self", ".", "env", ",", "universal_newlines", "=", "True", ",", "cwd", "=", "self", ".", "_package_json_dir", ")", "out", ",", "err", "=", "process", ".", "communicate", "(", ")", "if", "err", ":", "raise", "TraceError", "(", "err", ")", "self", ".", "_trace_cache", "[", "app", "]", "=", "json", ".", "loads", "(", "out", ")", "return", "self", ".", "_trace_cache", "[", "app", "]"], "docstring": "Trace the dependencies for app.\n\n        A tracer-instance is shortlived, and re-tracing the same app should\n        yield the same results. Since tracing is an expensive process, cache\n        the result on the tracer instance.", "docstring_tokens": ["Trace", "the", "dependencies", "for", "app", "."], "sha": "efd4a3862a39d9771609a25a5556f36023cf6e5c", "url": "https://github.com/sergei-maertens/django-systemjs/blob/efd4a3862a39d9771609a25a5556f36023cf6e5c/systemjs/base.py#L210-L228", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/utils/_process_win32.py", "func_name": "getoutput", "original_string": "def getoutput(cmd):\n    \"\"\"Return standard output of executing cmd in a shell.\n\n    Accepts the same arguments as os.system().\n\n    Parameters\n    ----------\n    cmd : str\n      A command to be executed in the system shell.\n\n    Returns\n    -------\n    stdout : str\n    \"\"\"\n\n    with AvoidUNCPath() as path:\n        if path is not None:\n            cmd = '\"pushd %s &&\"%s' % (path, cmd)\n        out = process_handler(cmd, lambda p: p.communicate()[0], STDOUT)\n\n    if out is None:\n        out = b''\n    return py3compat.bytes_to_str(out)", "language": "python", "code": "def getoutput(cmd):\n    \"\"\"Return standard output of executing cmd in a shell.\n\n    Accepts the same arguments as os.system().\n\n    Parameters\n    ----------\n    cmd : str\n      A command to be executed in the system shell.\n\n    Returns\n    -------\n    stdout : str\n    \"\"\"\n\n    with AvoidUNCPath() as path:\n        if path is not None:\n            cmd = '\"pushd %s &&\"%s' % (path, cmd)\n        out = process_handler(cmd, lambda p: p.communicate()[0], STDOUT)\n\n    if out is None:\n        out = b''\n    return py3compat.bytes_to_str(out)", "code_tokens": ["def", "getoutput", "(", "cmd", ")", ":", "with", "AvoidUNCPath", "(", ")", "as", "path", ":", "if", "path", "is", "not", "None", ":", "cmd", "=", "'\"pushd %s &&\"%s'", "%", "(", "path", ",", "cmd", ")", "out", "=", "process_handler", "(", "cmd", ",", "lambda", "p", ":", "p", ".", "communicate", "(", ")", "[", "0", "]", ",", "STDOUT", ")", "if", "out", "is", "None", ":", "out", "=", "b''", "return", "py3compat", ".", "bytes_to_str", "(", "out", ")"], "docstring": "Return standard output of executing cmd in a shell.\n\n    Accepts the same arguments as os.system().\n\n    Parameters\n    ----------\n    cmd : str\n      A command to be executed in the system shell.\n\n    Returns\n    -------\n    stdout : str", "docstring_tokens": ["Return", "standard", "output", "of", "executing", "cmd", "in", "a", "shell", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/utils/_process_win32.py#L136-L158", "partition": "test"}
{"repo": "gnosis/gnosis-py", "path": "gnosis/eth/contracts.py", "func_name": "get_safe_contract", "original_string": "def get_safe_contract(w3: Web3, address=None):\n    \"\"\"\n    Get Gnosis Safe Master contract. It should be used to access Safe methods on Proxy contracts.\n    :param w3: Web3 instance\n    :param address: address of the safe contract/proxy contract\n    :return: Safe Contract\n    \"\"\"\n    return w3.eth.contract(address,\n                           abi=GNOSIS_SAFE_INTERFACE['abi'],\n                           bytecode=GNOSIS_SAFE_INTERFACE['bytecode'])", "language": "python", "code": "def get_safe_contract(w3: Web3, address=None):\n    \"\"\"\n    Get Gnosis Safe Master contract. It should be used to access Safe methods on Proxy contracts.\n    :param w3: Web3 instance\n    :param address: address of the safe contract/proxy contract\n    :return: Safe Contract\n    \"\"\"\n    return w3.eth.contract(address,\n                           abi=GNOSIS_SAFE_INTERFACE['abi'],\n                           bytecode=GNOSIS_SAFE_INTERFACE['bytecode'])", "code_tokens": ["def", "get_safe_contract", "(", "w3", ":", "Web3", ",", "address", "=", "None", ")", ":", "return", "w3", ".", "eth", ".", "contract", "(", "address", ",", "abi", "=", "GNOSIS_SAFE_INTERFACE", "[", "'abi'", "]", ",", "bytecode", "=", "GNOSIS_SAFE_INTERFACE", "[", "'bytecode'", "]", ")"], "docstring": "Get Gnosis Safe Master contract. It should be used to access Safe methods on Proxy contracts.\n    :param w3: Web3 instance\n    :param address: address of the safe contract/proxy contract\n    :return: Safe Contract", "docstring_tokens": ["Get", "Gnosis", "Safe", "Master", "contract", ".", "It", "should", "be", "used", "to", "access", "Safe", "methods", "on", "Proxy", "contracts", ".", ":", "param", "w3", ":", "Web3", "instance", ":", "param", "address", ":", "address", "of", "the", "safe", "contract", "/", "proxy", "contract", ":", "return", ":", "Safe", "Contract"], "sha": "2a9a5d75a375fc9813ac04df133e6910c82f9d49", "url": "https://github.com/gnosis/gnosis-py/blob/2a9a5d75a375fc9813ac04df133e6910c82f9d49/gnosis/eth/contracts.py#L29-L38", "partition": "test"}
{"repo": "h2non/pook", "path": "pook/mock.py", "func_name": "Mock.headers_present", "original_string": "def headers_present(self, headers):\n        \"\"\"\n        Defines a list of headers that must be present in the\n        outgoing request in order to satisfy the matcher, no matter what value\n        the headers hosts.\n\n        Header keys are case insensitive.\n\n        Arguments:\n            headers (list|tuple): header keys to match.\n\n        Returns:\n            self: current Mock instance.\n\n        Example::\n\n            (pook.get('server.com/api')\n                .headers_present(['content-type', 'Authorization']))\n        \"\"\"\n        headers = {name: re.compile('(.*)') for name in headers}\n        self.add_matcher(matcher('HeadersMatcher', headers))", "language": "python", "code": "def headers_present(self, headers):\n        \"\"\"\n        Defines a list of headers that must be present in the\n        outgoing request in order to satisfy the matcher, no matter what value\n        the headers hosts.\n\n        Header keys are case insensitive.\n\n        Arguments:\n            headers (list|tuple): header keys to match.\n\n        Returns:\n            self: current Mock instance.\n\n        Example::\n\n            (pook.get('server.com/api')\n                .headers_present(['content-type', 'Authorization']))\n        \"\"\"\n        headers = {name: re.compile('(.*)') for name in headers}\n        self.add_matcher(matcher('HeadersMatcher', headers))", "code_tokens": ["def", "headers_present", "(", "self", ",", "headers", ")", ":", "headers", "=", "{", "name", ":", "re", ".", "compile", "(", "'(.*)'", ")", "for", "name", "in", "headers", "}", "self", ".", "add_matcher", "(", "matcher", "(", "'HeadersMatcher'", ",", "headers", ")", ")"], "docstring": "Defines a list of headers that must be present in the\n        outgoing request in order to satisfy the matcher, no matter what value\n        the headers hosts.\n\n        Header keys are case insensitive.\n\n        Arguments:\n            headers (list|tuple): header keys to match.\n\n        Returns:\n            self: current Mock instance.\n\n        Example::\n\n            (pook.get('server.com/api')\n                .headers_present(['content-type', 'Authorization']))", "docstring_tokens": ["Defines", "a", "list", "of", "headers", "that", "must", "be", "present", "in", "the", "outgoing", "request", "in", "order", "to", "satisfy", "the", "matcher", "no", "matter", "what", "value", "the", "headers", "hosts", "."], "sha": "e64094e41e4d89d98d2d29af7608ef27dc50cf19", "url": "https://github.com/h2non/pook/blob/e64094e41e4d89d98d2d29af7608ef27dc50cf19/pook/mock.py#L247-L267", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/checkers/logging.py", "func_name": "is_method_call", "original_string": "def is_method_call(func, types=(), methods=()):\n    \"\"\"Determines if a BoundMethod node represents a method call.\n\n    Args:\n      func (astroid.BoundMethod): The BoundMethod AST node to check.\n      types (Optional[String]): Optional sequence of caller type names to restrict check.\n      methods (Optional[String]): Optional sequence of method names to restrict check.\n\n    Returns:\n      bool: true if the node represents a method call for the given type and\n      method names, False otherwise.\n    \"\"\"\n    return (\n        isinstance(func, astroid.BoundMethod)\n        and isinstance(func.bound, astroid.Instance)\n        and (func.bound.name in types if types else True)\n        and (func.name in methods if methods else True)\n    )", "language": "python", "code": "def is_method_call(func, types=(), methods=()):\n    \"\"\"Determines if a BoundMethod node represents a method call.\n\n    Args:\n      func (astroid.BoundMethod): The BoundMethod AST node to check.\n      types (Optional[String]): Optional sequence of caller type names to restrict check.\n      methods (Optional[String]): Optional sequence of method names to restrict check.\n\n    Returns:\n      bool: true if the node represents a method call for the given type and\n      method names, False otherwise.\n    \"\"\"\n    return (\n        isinstance(func, astroid.BoundMethod)\n        and isinstance(func.bound, astroid.Instance)\n        and (func.bound.name in types if types else True)\n        and (func.name in methods if methods else True)\n    )", "code_tokens": ["def", "is_method_call", "(", "func", ",", "types", "=", "(", ")", ",", "methods", "=", "(", ")", ")", ":", "return", "(", "isinstance", "(", "func", ",", "astroid", ".", "BoundMethod", ")", "and", "isinstance", "(", "func", ".", "bound", ",", "astroid", ".", "Instance", ")", "and", "(", "func", ".", "bound", ".", "name", "in", "types", "if", "types", "else", "True", ")", "and", "(", "func", ".", "name", "in", "methods", "if", "methods", "else", "True", ")", ")"], "docstring": "Determines if a BoundMethod node represents a method call.\n\n    Args:\n      func (astroid.BoundMethod): The BoundMethod AST node to check.\n      types (Optional[String]): Optional sequence of caller type names to restrict check.\n      methods (Optional[String]): Optional sequence of method names to restrict check.\n\n    Returns:\n      bool: true if the node represents a method call for the given type and\n      method names, False otherwise.", "docstring_tokens": ["Determines", "if", "a", "BoundMethod", "node", "represents", "a", "method", "call", "."], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/checkers/logging.py#L99-L116", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/lkj.py", "func_name": "_uniform_unit_norm", "original_string": "def _uniform_unit_norm(dimension, shape, dtype, seed):\n  \"\"\"Returns a batch of points chosen uniformly from the unit hypersphere.\"\"\"\n  # This works because the Gaussian distribution is spherically symmetric.\n  # raw shape: shape + [dimension]\n  raw = normal.Normal(\n      loc=dtype_util.as_numpy_dtype(dtype)(0),\n      scale=dtype_util.as_numpy_dtype(dtype)(1)).sample(\n          tf.concat([shape, [dimension]], axis=0), seed=seed())\n  unit_norm = raw / tf.norm(tensor=raw, ord=2, axis=-1)[..., tf.newaxis]\n  return unit_norm", "language": "python", "code": "def _uniform_unit_norm(dimension, shape, dtype, seed):\n  \"\"\"Returns a batch of points chosen uniformly from the unit hypersphere.\"\"\"\n  # This works because the Gaussian distribution is spherically symmetric.\n  # raw shape: shape + [dimension]\n  raw = normal.Normal(\n      loc=dtype_util.as_numpy_dtype(dtype)(0),\n      scale=dtype_util.as_numpy_dtype(dtype)(1)).sample(\n          tf.concat([shape, [dimension]], axis=0), seed=seed())\n  unit_norm = raw / tf.norm(tensor=raw, ord=2, axis=-1)[..., tf.newaxis]\n  return unit_norm", "code_tokens": ["def", "_uniform_unit_norm", "(", "dimension", ",", "shape", ",", "dtype", ",", "seed", ")", ":", "# This works because the Gaussian distribution is spherically symmetric.", "# raw shape: shape + [dimension]", "raw", "=", "normal", ".", "Normal", "(", "loc", "=", "dtype_util", ".", "as_numpy_dtype", "(", "dtype", ")", "(", "0", ")", ",", "scale", "=", "dtype_util", ".", "as_numpy_dtype", "(", "dtype", ")", "(", "1", ")", ")", ".", "sample", "(", "tf", ".", "concat", "(", "[", "shape", ",", "[", "dimension", "]", "]", ",", "axis", "=", "0", ")", ",", "seed", "=", "seed", "(", ")", ")", "unit_norm", "=", "raw", "/", "tf", ".", "norm", "(", "tensor", "=", "raw", ",", "ord", "=", "2", ",", "axis", "=", "-", "1", ")", "[", "...", ",", "tf", ".", "newaxis", "]", "return", "unit_norm"], "docstring": "Returns a batch of points chosen uniformly from the unit hypersphere.", "docstring_tokens": ["Returns", "a", "batch", "of", "points", "chosen", "uniformly", "from", "the", "unit", "hypersphere", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/lkj.py#L47-L56", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/checkers/utils.py", "func_name": "is_from_fallback_block", "original_string": "def is_from_fallback_block(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"Check if the given node is from a fallback import block.\"\"\"\n    context = find_try_except_wrapper_node(node)\n    if not context:\n        return False\n\n    if isinstance(context, astroid.ExceptHandler):\n        other_body = context.parent.body\n        handlers = context.parent.handlers\n    else:\n        other_body = itertools.chain.from_iterable(\n            handler.body for handler in context.handlers\n        )\n        handlers = context.handlers\n\n    has_fallback_imports = any(\n        isinstance(import_node, (astroid.ImportFrom, astroid.Import))\n        for import_node in other_body\n    )\n    ignores_import_error = _except_handlers_ignores_exception(handlers, ImportError)\n    return ignores_import_error or has_fallback_imports", "language": "python", "code": "def is_from_fallback_block(node: astroid.node_classes.NodeNG) -> bool:\n    \"\"\"Check if the given node is from a fallback import block.\"\"\"\n    context = find_try_except_wrapper_node(node)\n    if not context:\n        return False\n\n    if isinstance(context, astroid.ExceptHandler):\n        other_body = context.parent.body\n        handlers = context.parent.handlers\n    else:\n        other_body = itertools.chain.from_iterable(\n            handler.body for handler in context.handlers\n        )\n        handlers = context.handlers\n\n    has_fallback_imports = any(\n        isinstance(import_node, (astroid.ImportFrom, astroid.Import))\n        for import_node in other_body\n    )\n    ignores_import_error = _except_handlers_ignores_exception(handlers, ImportError)\n    return ignores_import_error or has_fallback_imports", "code_tokens": ["def", "is_from_fallback_block", "(", "node", ":", "astroid", ".", "node_classes", ".", "NodeNG", ")", "->", "bool", ":", "context", "=", "find_try_except_wrapper_node", "(", "node", ")", "if", "not", "context", ":", "return", "False", "if", "isinstance", "(", "context", ",", "astroid", ".", "ExceptHandler", ")", ":", "other_body", "=", "context", ".", "parent", ".", "body", "handlers", "=", "context", ".", "parent", ".", "handlers", "else", ":", "other_body", "=", "itertools", ".", "chain", ".", "from_iterable", "(", "handler", ".", "body", "for", "handler", "in", "context", ".", "handlers", ")", "handlers", "=", "context", ".", "handlers", "has_fallback_imports", "=", "any", "(", "isinstance", "(", "import_node", ",", "(", "astroid", ".", "ImportFrom", ",", "astroid", ".", "Import", ")", ")", "for", "import_node", "in", "other_body", ")", "ignores_import_error", "=", "_except_handlers_ignores_exception", "(", "handlers", ",", "ImportError", ")", "return", "ignores_import_error", "or", "has_fallback_imports"], "docstring": "Check if the given node is from a fallback import block.", "docstring_tokens": ["Check", "if", "the", "given", "node", "is", "from", "a", "fallback", "import", "block", "."], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/checkers/utils.py#L826-L846", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/flask/blueprints.py", "func_name": "Blueprint.before_app_request", "original_string": "def before_app_request(self, f):\n        \"\"\"Like :meth:`Flask.before_request`.  Such a function is executed\n        before each request, even if outside of a blueprint.\n        \"\"\"\n        self.record_once(lambda s: s.app.before_request_funcs\n            .setdefault(None, []).append(f))\n        return f", "language": "python", "code": "def before_app_request(self, f):\n        \"\"\"Like :meth:`Flask.before_request`.  Such a function is executed\n        before each request, even if outside of a blueprint.\n        \"\"\"\n        self.record_once(lambda s: s.app.before_request_funcs\n            .setdefault(None, []).append(f))\n        return f", "code_tokens": ["def", "before_app_request", "(", "self", ",", "f", ")", ":", "self", ".", "record_once", "(", "lambda", "s", ":", "s", ".", "app", ".", "before_request_funcs", ".", "setdefault", "(", "None", ",", "[", "]", ")", ".", "append", "(", "f", ")", ")", "return", "f"], "docstring": "Like :meth:`Flask.before_request`.  Such a function is executed\n        before each request, even if outside of a blueprint.", "docstring_tokens": ["Like", ":", "meth", ":", "Flask", ".", "before_request", ".", "Such", "a", "function", "is", "executed", "before", "each", "request", "even", "if", "outside", "of", "a", "blueprint", "."], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/flask/blueprints.py#L277-L283", "partition": "test"}
{"repo": "deepmipt/DeepPavlov", "path": "deeppavlov/core/layers/tf_layers.py", "func_name": "cudnn_bi_gru", "original_string": "def cudnn_bi_gru(units,\n                 n_hidden,\n                 seq_lengths=None,\n                 n_layers=1,\n                 trainable_initial_states=False,\n                 name='cudnn_bi_gru',\n                 reuse=False):\n    \"\"\" Fast CuDNN Bi-GRU implementation\n\n    Args:\n        units: tf.Tensor with dimensions [B x T x F], where\n            B - batch size\n            T - number of tokens\n            F - features\n        n_hidden: dimensionality of hidden state\n        seq_lengths: number of tokens in each sample in the batch\n        n_layers: number of layers\n        trainable_initial_states: whether to create a special trainable variable\n                to initialize the hidden states of the network or use just zeros\n        name: name of the variable scope to use\n        reuse:whether to reuse already initialized variable\n\n\n    Returns:\n        h - all hidden states along T dimension,\n            tf.Tensor with dimensionality [B x T x F]\n        h_last - last hidden state, tf.Tensor with dimensionality [B x H * 2]\n            where H - number of hidden units\n    \"\"\"\n\n    with tf.variable_scope(name, reuse=reuse):\n        if seq_lengths is None:\n            seq_lengths = tf.ones([tf.shape(units)[0]], dtype=tf.int32) * tf.shape(units)[1]\n        with tf.variable_scope('Forward'):\n            h_fw, h_last_fw = cudnn_gru_wrapper(units,\n                                                n_hidden,\n                                                n_layers=n_layers,\n                                                trainable_initial_states=trainable_initial_states,\n                                                seq_lengths=seq_lengths,\n                                                reuse=reuse)\n\n        with tf.variable_scope('Backward'):\n            reversed_units = tf.reverse_sequence(units, seq_lengths=seq_lengths, seq_dim=1, batch_dim=0)\n            h_bw, h_last_bw = cudnn_gru_wrapper(reversed_units,\n                                                n_hidden,\n                                                n_layers=n_layers,\n                                                trainable_initial_states=trainable_initial_states,\n                                                seq_lengths=seq_lengths,\n                                                reuse=reuse)\n            h_bw = tf.reverse_sequence(h_bw, seq_lengths=seq_lengths, seq_dim=1, batch_dim=0)\n\n    return (h_fw, h_bw), (h_last_fw, h_last_bw)", "language": "python", "code": "def cudnn_bi_gru(units,\n                 n_hidden,\n                 seq_lengths=None,\n                 n_layers=1,\n                 trainable_initial_states=False,\n                 name='cudnn_bi_gru',\n                 reuse=False):\n    \"\"\" Fast CuDNN Bi-GRU implementation\n\n    Args:\n        units: tf.Tensor with dimensions [B x T x F], where\n            B - batch size\n            T - number of tokens\n            F - features\n        n_hidden: dimensionality of hidden state\n        seq_lengths: number of tokens in each sample in the batch\n        n_layers: number of layers\n        trainable_initial_states: whether to create a special trainable variable\n                to initialize the hidden states of the network or use just zeros\n        name: name of the variable scope to use\n        reuse:whether to reuse already initialized variable\n\n\n    Returns:\n        h - all hidden states along T dimension,\n            tf.Tensor with dimensionality [B x T x F]\n        h_last - last hidden state, tf.Tensor with dimensionality [B x H * 2]\n            where H - number of hidden units\n    \"\"\"\n\n    with tf.variable_scope(name, reuse=reuse):\n        if seq_lengths is None:\n            seq_lengths = tf.ones([tf.shape(units)[0]], dtype=tf.int32) * tf.shape(units)[1]\n        with tf.variable_scope('Forward'):\n            h_fw, h_last_fw = cudnn_gru_wrapper(units,\n                                                n_hidden,\n                                                n_layers=n_layers,\n                                                trainable_initial_states=trainable_initial_states,\n                                                seq_lengths=seq_lengths,\n                                                reuse=reuse)\n\n        with tf.variable_scope('Backward'):\n            reversed_units = tf.reverse_sequence(units, seq_lengths=seq_lengths, seq_dim=1, batch_dim=0)\n            h_bw, h_last_bw = cudnn_gru_wrapper(reversed_units,\n                                                n_hidden,\n                                                n_layers=n_layers,\n                                                trainable_initial_states=trainable_initial_states,\n                                                seq_lengths=seq_lengths,\n                                                reuse=reuse)\n            h_bw = tf.reverse_sequence(h_bw, seq_lengths=seq_lengths, seq_dim=1, batch_dim=0)\n\n    return (h_fw, h_bw), (h_last_fw, h_last_bw)", "code_tokens": ["def", "cudnn_bi_gru", "(", "units", ",", "n_hidden", ",", "seq_lengths", "=", "None", ",", "n_layers", "=", "1", ",", "trainable_initial_states", "=", "False", ",", "name", "=", "'cudnn_bi_gru'", ",", "reuse", "=", "False", ")", ":", "with", "tf", ".", "variable_scope", "(", "name", ",", "reuse", "=", "reuse", ")", ":", "if", "seq_lengths", "is", "None", ":", "seq_lengths", "=", "tf", ".", "ones", "(", "[", "tf", ".", "shape", "(", "units", ")", "[", "0", "]", "]", ",", "dtype", "=", "tf", ".", "int32", ")", "*", "tf", ".", "shape", "(", "units", ")", "[", "1", "]", "with", "tf", ".", "variable_scope", "(", "'Forward'", ")", ":", "h_fw", ",", "h_last_fw", "=", "cudnn_gru_wrapper", "(", "units", ",", "n_hidden", ",", "n_layers", "=", "n_layers", ",", "trainable_initial_states", "=", "trainable_initial_states", ",", "seq_lengths", "=", "seq_lengths", ",", "reuse", "=", "reuse", ")", "with", "tf", ".", "variable_scope", "(", "'Backward'", ")", ":", "reversed_units", "=", "tf", ".", "reverse_sequence", "(", "units", ",", "seq_lengths", "=", "seq_lengths", ",", "seq_dim", "=", "1", ",", "batch_dim", "=", "0", ")", "h_bw", ",", "h_last_bw", "=", "cudnn_gru_wrapper", "(", "reversed_units", ",", "n_hidden", ",", "n_layers", "=", "n_layers", ",", "trainable_initial_states", "=", "trainable_initial_states", ",", "seq_lengths", "=", "seq_lengths", ",", "reuse", "=", "reuse", ")", "h_bw", "=", "tf", ".", "reverse_sequence", "(", "h_bw", ",", "seq_lengths", "=", "seq_lengths", ",", "seq_dim", "=", "1", ",", "batch_dim", "=", "0", ")", "return", "(", "h_fw", ",", "h_bw", ")", ",", "(", "h_last_fw", ",", "h_last_bw", ")"], "docstring": "Fast CuDNN Bi-GRU implementation\n\n    Args:\n        units: tf.Tensor with dimensions [B x T x F], where\n            B - batch size\n            T - number of tokens\n            F - features\n        n_hidden: dimensionality of hidden state\n        seq_lengths: number of tokens in each sample in the batch\n        n_layers: number of layers\n        trainable_initial_states: whether to create a special trainable variable\n                to initialize the hidden states of the network or use just zeros\n        name: name of the variable scope to use\n        reuse:whether to reuse already initialized variable\n\n\n    Returns:\n        h - all hidden states along T dimension,\n            tf.Tensor with dimensionality [B x T x F]\n        h_last - last hidden state, tf.Tensor with dimensionality [B x H * 2]\n            where H - number of hidden units", "docstring_tokens": ["Fast", "CuDNN", "Bi", "-", "GRU", "implementation"], "sha": "f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c", "url": "https://github.com/deepmipt/DeepPavlov/blob/f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c/deeppavlov/core/layers/tf_layers.py#L766-L817", "partition": "test"}
{"repo": "tnkteja/myhelp", "path": "virtualEnvironment/lib/python2.7/site-packages/coverage/control.py", "func_name": "coverage.xml_report", "original_string": "def xml_report(self, morfs=None, outfile=None, ignore_errors=None,\n                    omit=None, include=None):\n        \"\"\"Generate an XML report of coverage results.\n\n        The report is compatible with Cobertura reports.\n\n        Each module in `morfs` is included in the report.  `outfile` is the\n        path to write the file to, \"-\" will write to stdout.\n\n        See `coverage.report()` for other arguments.\n\n        Returns a float, the total percentage covered.\n\n        \"\"\"\n        self._harvest_data()\n        self.config.from_args(\n            ignore_errors=ignore_errors, omit=omit, include=include,\n            xml_output=outfile,\n            )\n        file_to_close = None\n        delete_file = False\n        if self.config.xml_output:\n            if self.config.xml_output == '-':\n                outfile = sys.stdout\n            else:\n                outfile = open(self.config.xml_output, \"w\")\n                file_to_close = outfile\n        try:\n            try:\n                reporter = XmlReporter(self, self.config)\n                return reporter.report(morfs, outfile=outfile)\n            except CoverageException:\n                delete_file = True\n                raise\n        finally:\n            if file_to_close:\n                file_to_close.close()\n                if delete_file:\n                    file_be_gone(self.config.xml_output)", "language": "python", "code": "def xml_report(self, morfs=None, outfile=None, ignore_errors=None,\n                    omit=None, include=None):\n        \"\"\"Generate an XML report of coverage results.\n\n        The report is compatible with Cobertura reports.\n\n        Each module in `morfs` is included in the report.  `outfile` is the\n        path to write the file to, \"-\" will write to stdout.\n\n        See `coverage.report()` for other arguments.\n\n        Returns a float, the total percentage covered.\n\n        \"\"\"\n        self._harvest_data()\n        self.config.from_args(\n            ignore_errors=ignore_errors, omit=omit, include=include,\n            xml_output=outfile,\n            )\n        file_to_close = None\n        delete_file = False\n        if self.config.xml_output:\n            if self.config.xml_output == '-':\n                outfile = sys.stdout\n            else:\n                outfile = open(self.config.xml_output, \"w\")\n                file_to_close = outfile\n        try:\n            try:\n                reporter = XmlReporter(self, self.config)\n                return reporter.report(morfs, outfile=outfile)\n            except CoverageException:\n                delete_file = True\n                raise\n        finally:\n            if file_to_close:\n                file_to_close.close()\n                if delete_file:\n                    file_be_gone(self.config.xml_output)", "code_tokens": ["def", "xml_report", "(", "self", ",", "morfs", "=", "None", ",", "outfile", "=", "None", ",", "ignore_errors", "=", "None", ",", "omit", "=", "None", ",", "include", "=", "None", ")", ":", "self", ".", "_harvest_data", "(", ")", "self", ".", "config", ".", "from_args", "(", "ignore_errors", "=", "ignore_errors", ",", "omit", "=", "omit", ",", "include", "=", "include", ",", "xml_output", "=", "outfile", ",", ")", "file_to_close", "=", "None", "delete_file", "=", "False", "if", "self", ".", "config", ".", "xml_output", ":", "if", "self", ".", "config", ".", "xml_output", "==", "'-'", ":", "outfile", "=", "sys", ".", "stdout", "else", ":", "outfile", "=", "open", "(", "self", ".", "config", ".", "xml_output", ",", "\"w\"", ")", "file_to_close", "=", "outfile", "try", ":", "try", ":", "reporter", "=", "XmlReporter", "(", "self", ",", "self", ".", "config", ")", "return", "reporter", ".", "report", "(", "morfs", ",", "outfile", "=", "outfile", ")", "except", "CoverageException", ":", "delete_file", "=", "True", "raise", "finally", ":", "if", "file_to_close", ":", "file_to_close", ".", "close", "(", ")", "if", "delete_file", ":", "file_be_gone", "(", "self", ".", "config", ".", "xml_output", ")"], "docstring": "Generate an XML report of coverage results.\n\n        The report is compatible with Cobertura reports.\n\n        Each module in `morfs` is included in the report.  `outfile` is the\n        path to write the file to, \"-\" will write to stdout.\n\n        See `coverage.report()` for other arguments.\n\n        Returns a float, the total percentage covered.", "docstring_tokens": ["Generate", "an", "XML", "report", "of", "coverage", "results", "."], "sha": "fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb", "url": "https://github.com/tnkteja/myhelp/blob/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb/virtualEnvironment/lib/python2.7/site-packages/coverage/control.py#L664-L702", "partition": "test"}
{"repo": "intel-analytics/BigDL", "path": "pyspark/bigdl/nn/layer.py", "func_name": "SharedStaticUtils.of", "original_string": "def of(jvalue, bigdl_type=\"float\"):\n        \"\"\"\n        Create a Python Layer base on the given java value and the real type.\n        :param jvalue: Java object create by Py4j\n        :return: A Python Layer\n        \"\"\"\n        def get_py_name(jclass_name):\n            if jclass_name == \"StaticGraph\" or jclass_name == \"DynamicGraph\":\n                return \"Model\"\n            elif jclass_name == \"Input\":\n                return \"Layer\"\n            else:\n                return jclass_name\n\n        jname = callBigDlFunc(bigdl_type,\n                                      \"getRealClassNameOfJValue\",\n                                      jvalue)\n\n        jpackage_name = \".\".join(jname.split(\".\")[:-1])\n        pclass_name = get_py_name(jname.split(\".\")[-1])\n\n        if \"com.intel.analytics.bigdl.nn.keras.Model\" == jname or \\\n                        \"com.intel.analytics.bigdl.nn.keras.Sequential\" == jname:\n            base_module = importlib.import_module('bigdl.nn.keras.topology')\n        elif \"com.intel.analytics.bigdl.nn.keras\" == jpackage_name:\n            base_module = importlib.import_module('bigdl.nn.keras.layer')\n        else:\n            base_module = importlib.import_module('bigdl.nn.layer')\n\n        realClassName = \"Layer\" # The top base class\n        if pclass_name in dir(base_module):\n            realClassName = pclass_name\n        module = getattr(base_module, realClassName)\n        jvalue_creator = getattr(module, \"from_jvalue\")\n        model = jvalue_creator(jvalue, bigdl_type)\n        return model", "language": "python", "code": "def of(jvalue, bigdl_type=\"float\"):\n        \"\"\"\n        Create a Python Layer base on the given java value and the real type.\n        :param jvalue: Java object create by Py4j\n        :return: A Python Layer\n        \"\"\"\n        def get_py_name(jclass_name):\n            if jclass_name == \"StaticGraph\" or jclass_name == \"DynamicGraph\":\n                return \"Model\"\n            elif jclass_name == \"Input\":\n                return \"Layer\"\n            else:\n                return jclass_name\n\n        jname = callBigDlFunc(bigdl_type,\n                                      \"getRealClassNameOfJValue\",\n                                      jvalue)\n\n        jpackage_name = \".\".join(jname.split(\".\")[:-1])\n        pclass_name = get_py_name(jname.split(\".\")[-1])\n\n        if \"com.intel.analytics.bigdl.nn.keras.Model\" == jname or \\\n                        \"com.intel.analytics.bigdl.nn.keras.Sequential\" == jname:\n            base_module = importlib.import_module('bigdl.nn.keras.topology')\n        elif \"com.intel.analytics.bigdl.nn.keras\" == jpackage_name:\n            base_module = importlib.import_module('bigdl.nn.keras.layer')\n        else:\n            base_module = importlib.import_module('bigdl.nn.layer')\n\n        realClassName = \"Layer\" # The top base class\n        if pclass_name in dir(base_module):\n            realClassName = pclass_name\n        module = getattr(base_module, realClassName)\n        jvalue_creator = getattr(module, \"from_jvalue\")\n        model = jvalue_creator(jvalue, bigdl_type)\n        return model", "code_tokens": ["def", "of", "(", "jvalue", ",", "bigdl_type", "=", "\"float\"", ")", ":", "def", "get_py_name", "(", "jclass_name", ")", ":", "if", "jclass_name", "==", "\"StaticGraph\"", "or", "jclass_name", "==", "\"DynamicGraph\"", ":", "return", "\"Model\"", "elif", "jclass_name", "==", "\"Input\"", ":", "return", "\"Layer\"", "else", ":", "return", "jclass_name", "jname", "=", "callBigDlFunc", "(", "bigdl_type", ",", "\"getRealClassNameOfJValue\"", ",", "jvalue", ")", "jpackage_name", "=", "\".\"", ".", "join", "(", "jname", ".", "split", "(", "\".\"", ")", "[", ":", "-", "1", "]", ")", "pclass_name", "=", "get_py_name", "(", "jname", ".", "split", "(", "\".\"", ")", "[", "-", "1", "]", ")", "if", "\"com.intel.analytics.bigdl.nn.keras.Model\"", "==", "jname", "or", "\"com.intel.analytics.bigdl.nn.keras.Sequential\"", "==", "jname", ":", "base_module", "=", "importlib", ".", "import_module", "(", "'bigdl.nn.keras.topology'", ")", "elif", "\"com.intel.analytics.bigdl.nn.keras\"", "==", "jpackage_name", ":", "base_module", "=", "importlib", ".", "import_module", "(", "'bigdl.nn.keras.layer'", ")", "else", ":", "base_module", "=", "importlib", ".", "import_module", "(", "'bigdl.nn.layer'", ")", "realClassName", "=", "\"Layer\"", "# The top base class", "if", "pclass_name", "in", "dir", "(", "base_module", ")", ":", "realClassName", "=", "pclass_name", "module", "=", "getattr", "(", "base_module", ",", "realClassName", ")", "jvalue_creator", "=", "getattr", "(", "module", ",", "\"from_jvalue\"", ")", "model", "=", "jvalue_creator", "(", "jvalue", ",", "bigdl_type", ")", "return", "model"], "docstring": "Create a Python Layer base on the given java value and the real type.\n        :param jvalue: Java object create by Py4j\n        :return: A Python Layer", "docstring_tokens": ["Create", "a", "Python", "Layer", "base", "on", "the", "given", "java", "value", "and", "the", "real", "type", ".", ":", "param", "jvalue", ":", "Java", "object", "create", "by", "Py4j", ":", "return", ":", "A", "Python", "Layer"], "sha": "e9c19788285986ab789a2e2998f9a85d7524779f", "url": "https://github.com/intel-analytics/BigDL/blob/e9c19788285986ab789a2e2998f9a85d7524779f/pyspark/bigdl/nn/layer.py#L80-L115", "partition": "test"}
{"repo": "mental32/spotify.py", "path": "spotify/models/player.py", "func_name": "Player.resume", "original_string": "async def resume(self, *, device: Optional[SomeDevice] = None):\n        \"\"\"Resume playback on the user's account.\n\n        Parameters\n        ----------\n        device : Optional[:obj:`SomeDevice`]\n            The Device object or id of the device this command is targeting.\n            If not supplied, the user\u2019s currently active device is the target.\n        \"\"\"\n        await self._user.http.play_playback(None, device_id=str(device))", "language": "python", "code": "async def resume(self, *, device: Optional[SomeDevice] = None):\n        \"\"\"Resume playback on the user's account.\n\n        Parameters\n        ----------\n        device : Optional[:obj:`SomeDevice`]\n            The Device object or id of the device this command is targeting.\n            If not supplied, the user\u2019s currently active device is the target.\n        \"\"\"\n        await self._user.http.play_playback(None, device_id=str(device))", "code_tokens": ["async", "def", "resume", "(", "self", ",", "*", ",", "device", ":", "Optional", "[", "SomeDevice", "]", "=", "None", ")", ":", "await", "self", ".", "_user", ".", "http", ".", "play_playback", "(", "None", ",", "device_id", "=", "str", "(", "device", ")", ")"], "docstring": "Resume playback on the user's account.\n\n        Parameters\n        ----------\n        device : Optional[:obj:`SomeDevice`]\n            The Device object or id of the device this command is targeting.\n            If not supplied, the user\u2019s currently active device is the target.", "docstring_tokens": ["Resume", "playback", "on", "the", "user", "s", "account", "."], "sha": "bb296cac7c3dd289908906b7069bd80f43950515", "url": "https://github.com/mental32/spotify.py/blob/bb296cac7c3dd289908906b7069bd80f43950515/spotify/models/player.py#L85-L94", "partition": "test"}
{"repo": "CiscoDevNet/webexteamssdk", "path": "webexteamssdk/environment.py", "func_name": "_get_access_token", "original_string": "def _get_access_token():\n    \"\"\"Attempt to get the access token from the environment.\n\n    Try using the current and legacy environment variables. If the access token\n    is found in a legacy environment variable, raise a deprecation warning.\n\n    Returns:\n        The access token found in the environment (str), or None.\n    \"\"\"\n    access_token = os.environ.get(ACCESS_TOKEN_ENVIRONMENT_VARIABLE)\n    if access_token:\n        return access_token\n\n    else:\n        for access_token_variable in LEGACY_ACCESS_TOKEN_ENVIRONMENT_VARIABLES:\n            access_token = os.environ.get(access_token_variable)\n            if access_token:\n                env_var_deprecation_warning = PendingDeprecationWarning(\n                    \"Use of the `{legacy}` environment variable will be \"\n                    \"deprecated in the future.  Please update your \"\n                    \"environment(s) to use the new `{new}` environment \"\n                    \"variable.\".format(\n                        legacy=access_token,\n                        new=ACCESS_TOKEN_ENVIRONMENT_VARIABLE,\n                    )\n                )\n                warnings.warn(env_var_deprecation_warning)\n                return access_token", "language": "python", "code": "def _get_access_token():\n    \"\"\"Attempt to get the access token from the environment.\n\n    Try using the current and legacy environment variables. If the access token\n    is found in a legacy environment variable, raise a deprecation warning.\n\n    Returns:\n        The access token found in the environment (str), or None.\n    \"\"\"\n    access_token = os.environ.get(ACCESS_TOKEN_ENVIRONMENT_VARIABLE)\n    if access_token:\n        return access_token\n\n    else:\n        for access_token_variable in LEGACY_ACCESS_TOKEN_ENVIRONMENT_VARIABLES:\n            access_token = os.environ.get(access_token_variable)\n            if access_token:\n                env_var_deprecation_warning = PendingDeprecationWarning(\n                    \"Use of the `{legacy}` environment variable will be \"\n                    \"deprecated in the future.  Please update your \"\n                    \"environment(s) to use the new `{new}` environment \"\n                    \"variable.\".format(\n                        legacy=access_token,\n                        new=ACCESS_TOKEN_ENVIRONMENT_VARIABLE,\n                    )\n                )\n                warnings.warn(env_var_deprecation_warning)\n                return access_token", "code_tokens": ["def", "_get_access_token", "(", ")", ":", "access_token", "=", "os", ".", "environ", ".", "get", "(", "ACCESS_TOKEN_ENVIRONMENT_VARIABLE", ")", "if", "access_token", ":", "return", "access_token", "else", ":", "for", "access_token_variable", "in", "LEGACY_ACCESS_TOKEN_ENVIRONMENT_VARIABLES", ":", "access_token", "=", "os", ".", "environ", ".", "get", "(", "access_token_variable", ")", "if", "access_token", ":", "env_var_deprecation_warning", "=", "PendingDeprecationWarning", "(", "\"Use of the `{legacy}` environment variable will be \"", "\"deprecated in the future.  Please update your \"", "\"environment(s) to use the new `{new}` environment \"", "\"variable.\"", ".", "format", "(", "legacy", "=", "access_token", ",", "new", "=", "ACCESS_TOKEN_ENVIRONMENT_VARIABLE", ",", ")", ")", "warnings", ".", "warn", "(", "env_var_deprecation_warning", ")", "return", "access_token"], "docstring": "Attempt to get the access token from the environment.\n\n    Try using the current and legacy environment variables. If the access token\n    is found in a legacy environment variable, raise a deprecation warning.\n\n    Returns:\n        The access token found in the environment (str), or None.", "docstring_tokens": ["Attempt", "to", "get", "the", "access", "token", "from", "the", "environment", "."], "sha": "6fc2cc3557e080ba4b2a380664cb2a0532ae45cd", "url": "https://github.com/CiscoDevNet/webexteamssdk/blob/6fc2cc3557e080ba4b2a380664cb2a0532ae45cd/webexteamssdk/environment.py#L36-L63", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval", "path": "perceval/backends/core/telegram.py", "func_name": "TelegramBotClient.sanitize_for_archive", "original_string": "def sanitize_for_archive(url, headers, payload):\n        \"\"\"Sanitize URL of a HTTP request by removing the token information\n        before storing/retrieving archived items\n\n        :param: url: HTTP url request\n        :param: headers: HTTP headers request\n        :param: payload: HTTP payload request\n\n        :returns the sanitized url, plus the headers and payload\n        \"\"\"\n        url = re.sub('bot.*/', 'botXXXXX/', url)\n\n        return url, headers, payload", "language": "python", "code": "def sanitize_for_archive(url, headers, payload):\n        \"\"\"Sanitize URL of a HTTP request by removing the token information\n        before storing/retrieving archived items\n\n        :param: url: HTTP url request\n        :param: headers: HTTP headers request\n        :param: payload: HTTP payload request\n\n        :returns the sanitized url, plus the headers and payload\n        \"\"\"\n        url = re.sub('bot.*/', 'botXXXXX/', url)\n\n        return url, headers, payload", "code_tokens": ["def", "sanitize_for_archive", "(", "url", ",", "headers", ",", "payload", ")", ":", "url", "=", "re", ".", "sub", "(", "'bot.*/'", ",", "'botXXXXX/'", ",", "url", ")", "return", "url", ",", "headers", ",", "payload"], "docstring": "Sanitize URL of a HTTP request by removing the token information\n        before storing/retrieving archived items\n\n        :param: url: HTTP url request\n        :param: headers: HTTP headers request\n        :param: payload: HTTP payload request\n\n        :returns the sanitized url, plus the headers and payload", "docstring_tokens": ["Sanitize", "URL", "of", "a", "HTTP", "request", "by", "removing", "the", "token", "information", "before", "storing", "/", "retrieving", "archived", "items"], "sha": "41c908605e88b7ebc3a536c643fa0f212eaf9e0e", "url": "https://github.com/chaoss/grimoirelab-perceval/blob/41c908605e88b7ebc3a536c643fa0f212eaf9e0e/perceval/backends/core/telegram.py#L332-L344", "partition": "test"}
{"repo": "mozilla-iot/webthing-python", "path": "webthing/thing.py", "func_name": "Thing.remove_subscriber", "original_string": "def remove_subscriber(self, ws):\n        \"\"\"\n        Remove a websocket subscriber.\n\n        ws -- the websocket\n        \"\"\"\n        if ws in self.subscribers:\n            self.subscribers.remove(ws)\n\n        for name in self.available_events:\n            self.remove_event_subscriber(name, ws)", "language": "python", "code": "def remove_subscriber(self, ws):\n        \"\"\"\n        Remove a websocket subscriber.\n\n        ws -- the websocket\n        \"\"\"\n        if ws in self.subscribers:\n            self.subscribers.remove(ws)\n\n        for name in self.available_events:\n            self.remove_event_subscriber(name, ws)", "code_tokens": ["def", "remove_subscriber", "(", "self", ",", "ws", ")", ":", "if", "ws", "in", "self", ".", "subscribers", ":", "self", ".", "subscribers", ".", "remove", "(", "ws", ")", "for", "name", "in", "self", ".", "available_events", ":", "self", ".", "remove_event_subscriber", "(", "name", ",", "ws", ")"], "docstring": "Remove a websocket subscriber.\n\n        ws -- the websocket", "docstring_tokens": ["Remove", "a", "websocket", "subscriber", "."], "sha": "65d467c89ed79d0bbc42b8b3c8f9e5a320edd237", "url": "https://github.com/mozilla-iot/webthing-python/blob/65d467c89ed79d0bbc42b8b3c8f9e5a320edd237/webthing/thing.py#L390-L400", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/linear_gaussian_ssm.py", "func_name": "backward_smoothing_update", "original_string": "def backward_smoothing_update(filtered_mean,\n                              filtered_cov,\n                              predicted_mean,\n                              predicted_cov,\n                              next_posterior_mean,\n                              next_posterior_cov,\n                              transition_matrix):\n  \"\"\"Backward update for a Kalman smoother.\n\n  Give the `filtered_mean` mu(t | t), `filtered_cov` sigma(t | t),\n  `predicted_mean` mu(t+1 | t) and `predicted_cov` sigma(t+1 | t),\n  as returns from the `forward_filter` function, as well as\n  `next_posterior_mean` mu(t+1 | 1:T) and `next_posterior_cov` sigma(t+1 | 1:T),\n  if the `transition_matrix` of states from time t to time t+1\n  is given as A(t+1), the 1 step backward smoothed distribution parameter\n  could be calculated as:\n  p(z(t) | Obs(1:T)) = N( mu(t | 1:T), sigma(t | 1:T)),\n  mu(t | 1:T) = mu(t | t) + J(t) * (mu(t+1 | 1:T) - mu(t+1 | t)),\n  sigma(t | 1:T) = sigma(t | t)\n                   + J(t) * (sigma(t+1 | 1:T) - sigma(t+1 | t) * J(t)',\n  J(t) = sigma(t | t) * A(t+1)' / sigma(t+1 | t),\n  where all the multiplications are matrix multiplication, and `/` is\n  the matrix inverse. J(t) is the backward Kalman gain matrix.\n\n  The algorithm can be intialized from mu(T | 1:T) and sigma(T | 1:T),\n  which are the last step parameters returned by forward_filter.\n\n\n  Args:\n    filtered_mean: `Tensor` with event shape `[latent_size, 1]` and\n      batch shape `B`, containing mu(t | t).\n    filtered_cov: `Tensor` with event shape `[latent_size, latent_size]` and\n      batch shape `B`, containing sigma(t | t).\n    predicted_mean: `Tensor` with event shape `[latent_size, 1]` and\n      batch shape `B`, containing mu(t+1 | t).\n    predicted_cov: `Tensor` with event shape `[latent_size, latent_size]` and\n      batch shape `B`, containing sigma(t+1 | t).\n    next_posterior_mean: `Tensor` with event shape `[latent_size, 1]` and\n      batch shape `B`, containing mu(t+1 | 1:T).\n    next_posterior_cov: `Tensor` with event shape `[latent_size, latent_size]`\n      and batch shape `B`, containing sigma(t+1 | 1:T).\n    transition_matrix: `LinearOperator` with shape\n      `[latent_size, latent_size]` and batch shape broadcastable\n      to `B`.\n\n  Returns:\n    posterior_mean: `Tensor` with event shape `[latent_size, 1]` and\n      batch shape `B`, containing mu(t | 1:T).\n    posterior_cov: `Tensor` with event shape `[latent_size, latent_size]` and\n      batch shape `B`, containing sigma(t | 1:T).\n  \"\"\"\n  # Compute backward Kalman gain:\n  # J = F * T' * P^{-1}\n  # Since both F(iltered) and P(redictive) are cov matrices,\n  # thus self-adjoint, we can take the transpose.\n  # computation:\n  #      = (P^{-1} * T * F)'\n  #      = (P^{-1} * tmp_gain_cov) '\n  #      = (P \\ tmp_gain_cov)'\n  tmp_gain_cov = transition_matrix.matmul(filtered_cov)\n  predicted_cov_chol = tf.linalg.cholesky(predicted_cov)\n  gain_transpose = tf.linalg.cholesky_solve(predicted_cov_chol, tmp_gain_cov)\n\n  posterior_mean = (filtered_mean +\n                    tf.linalg.matmul(gain_transpose,\n                                     next_posterior_mean - predicted_mean,\n                                     adjoint_a=True))\n  posterior_cov = (\n      filtered_cov +\n      tf.linalg.matmul(gain_transpose,\n                       tf.linalg.matmul(\n                           next_posterior_cov - predicted_cov, gain_transpose),\n                       adjoint_a=True))\n\n  return (posterior_mean, posterior_cov)", "language": "python", "code": "def backward_smoothing_update(filtered_mean,\n                              filtered_cov,\n                              predicted_mean,\n                              predicted_cov,\n                              next_posterior_mean,\n                              next_posterior_cov,\n                              transition_matrix):\n  \"\"\"Backward update for a Kalman smoother.\n\n  Give the `filtered_mean` mu(t | t), `filtered_cov` sigma(t | t),\n  `predicted_mean` mu(t+1 | t) and `predicted_cov` sigma(t+1 | t),\n  as returns from the `forward_filter` function, as well as\n  `next_posterior_mean` mu(t+1 | 1:T) and `next_posterior_cov` sigma(t+1 | 1:T),\n  if the `transition_matrix` of states from time t to time t+1\n  is given as A(t+1), the 1 step backward smoothed distribution parameter\n  could be calculated as:\n  p(z(t) | Obs(1:T)) = N( mu(t | 1:T), sigma(t | 1:T)),\n  mu(t | 1:T) = mu(t | t) + J(t) * (mu(t+1 | 1:T) - mu(t+1 | t)),\n  sigma(t | 1:T) = sigma(t | t)\n                   + J(t) * (sigma(t+1 | 1:T) - sigma(t+1 | t) * J(t)',\n  J(t) = sigma(t | t) * A(t+1)' / sigma(t+1 | t),\n  where all the multiplications are matrix multiplication, and `/` is\n  the matrix inverse. J(t) is the backward Kalman gain matrix.\n\n  The algorithm can be intialized from mu(T | 1:T) and sigma(T | 1:T),\n  which are the last step parameters returned by forward_filter.\n\n\n  Args:\n    filtered_mean: `Tensor` with event shape `[latent_size, 1]` and\n      batch shape `B`, containing mu(t | t).\n    filtered_cov: `Tensor` with event shape `[latent_size, latent_size]` and\n      batch shape `B`, containing sigma(t | t).\n    predicted_mean: `Tensor` with event shape `[latent_size, 1]` and\n      batch shape `B`, containing mu(t+1 | t).\n    predicted_cov: `Tensor` with event shape `[latent_size, latent_size]` and\n      batch shape `B`, containing sigma(t+1 | t).\n    next_posterior_mean: `Tensor` with event shape `[latent_size, 1]` and\n      batch shape `B`, containing mu(t+1 | 1:T).\n    next_posterior_cov: `Tensor` with event shape `[latent_size, latent_size]`\n      and batch shape `B`, containing sigma(t+1 | 1:T).\n    transition_matrix: `LinearOperator` with shape\n      `[latent_size, latent_size]` and batch shape broadcastable\n      to `B`.\n\n  Returns:\n    posterior_mean: `Tensor` with event shape `[latent_size, 1]` and\n      batch shape `B`, containing mu(t | 1:T).\n    posterior_cov: `Tensor` with event shape `[latent_size, latent_size]` and\n      batch shape `B`, containing sigma(t | 1:T).\n  \"\"\"\n  # Compute backward Kalman gain:\n  # J = F * T' * P^{-1}\n  # Since both F(iltered) and P(redictive) are cov matrices,\n  # thus self-adjoint, we can take the transpose.\n  # computation:\n  #      = (P^{-1} * T * F)'\n  #      = (P^{-1} * tmp_gain_cov) '\n  #      = (P \\ tmp_gain_cov)'\n  tmp_gain_cov = transition_matrix.matmul(filtered_cov)\n  predicted_cov_chol = tf.linalg.cholesky(predicted_cov)\n  gain_transpose = tf.linalg.cholesky_solve(predicted_cov_chol, tmp_gain_cov)\n\n  posterior_mean = (filtered_mean +\n                    tf.linalg.matmul(gain_transpose,\n                                     next_posterior_mean - predicted_mean,\n                                     adjoint_a=True))\n  posterior_cov = (\n      filtered_cov +\n      tf.linalg.matmul(gain_transpose,\n                       tf.linalg.matmul(\n                           next_posterior_cov - predicted_cov, gain_transpose),\n                       adjoint_a=True))\n\n  return (posterior_mean, posterior_cov)", "code_tokens": ["def", "backward_smoothing_update", "(", "filtered_mean", ",", "filtered_cov", ",", "predicted_mean", ",", "predicted_cov", ",", "next_posterior_mean", ",", "next_posterior_cov", ",", "transition_matrix", ")", ":", "# Compute backward Kalman gain:", "# J = F * T' * P^{-1}", "# Since both F(iltered) and P(redictive) are cov matrices,", "# thus self-adjoint, we can take the transpose.", "# computation:", "#      = (P^{-1} * T * F)'", "#      = (P^{-1} * tmp_gain_cov) '", "#      = (P \\ tmp_gain_cov)'", "tmp_gain_cov", "=", "transition_matrix", ".", "matmul", "(", "filtered_cov", ")", "predicted_cov_chol", "=", "tf", ".", "linalg", ".", "cholesky", "(", "predicted_cov", ")", "gain_transpose", "=", "tf", ".", "linalg", ".", "cholesky_solve", "(", "predicted_cov_chol", ",", "tmp_gain_cov", ")", "posterior_mean", "=", "(", "filtered_mean", "+", "tf", ".", "linalg", ".", "matmul", "(", "gain_transpose", ",", "next_posterior_mean", "-", "predicted_mean", ",", "adjoint_a", "=", "True", ")", ")", "posterior_cov", "=", "(", "filtered_cov", "+", "tf", ".", "linalg", ".", "matmul", "(", "gain_transpose", ",", "tf", ".", "linalg", ".", "matmul", "(", "next_posterior_cov", "-", "predicted_cov", ",", "gain_transpose", ")", ",", "adjoint_a", "=", "True", ")", ")", "return", "(", "posterior_mean", ",", "posterior_cov", ")"], "docstring": "Backward update for a Kalman smoother.\n\n  Give the `filtered_mean` mu(t | t), `filtered_cov` sigma(t | t),\n  `predicted_mean` mu(t+1 | t) and `predicted_cov` sigma(t+1 | t),\n  as returns from the `forward_filter` function, as well as\n  `next_posterior_mean` mu(t+1 | 1:T) and `next_posterior_cov` sigma(t+1 | 1:T),\n  if the `transition_matrix` of states from time t to time t+1\n  is given as A(t+1), the 1 step backward smoothed distribution parameter\n  could be calculated as:\n  p(z(t) | Obs(1:T)) = N( mu(t | 1:T), sigma(t | 1:T)),\n  mu(t | 1:T) = mu(t | t) + J(t) * (mu(t+1 | 1:T) - mu(t+1 | t)),\n  sigma(t | 1:T) = sigma(t | t)\n                   + J(t) * (sigma(t+1 | 1:T) - sigma(t+1 | t) * J(t)',\n  J(t) = sigma(t | t) * A(t+1)' / sigma(t+1 | t),\n  where all the multiplications are matrix multiplication, and `/` is\n  the matrix inverse. J(t) is the backward Kalman gain matrix.\n\n  The algorithm can be intialized from mu(T | 1:T) and sigma(T | 1:T),\n  which are the last step parameters returned by forward_filter.\n\n\n  Args:\n    filtered_mean: `Tensor` with event shape `[latent_size, 1]` and\n      batch shape `B`, containing mu(t | t).\n    filtered_cov: `Tensor` with event shape `[latent_size, latent_size]` and\n      batch shape `B`, containing sigma(t | t).\n    predicted_mean: `Tensor` with event shape `[latent_size, 1]` and\n      batch shape `B`, containing mu(t+1 | t).\n    predicted_cov: `Tensor` with event shape `[latent_size, latent_size]` and\n      batch shape `B`, containing sigma(t+1 | t).\n    next_posterior_mean: `Tensor` with event shape `[latent_size, 1]` and\n      batch shape `B`, containing mu(t+1 | 1:T).\n    next_posterior_cov: `Tensor` with event shape `[latent_size, latent_size]`\n      and batch shape `B`, containing sigma(t+1 | 1:T).\n    transition_matrix: `LinearOperator` with shape\n      `[latent_size, latent_size]` and batch shape broadcastable\n      to `B`.\n\n  Returns:\n    posterior_mean: `Tensor` with event shape `[latent_size, 1]` and\n      batch shape `B`, containing mu(t | 1:T).\n    posterior_cov: `Tensor` with event shape `[latent_size, latent_size]` and\n      batch shape `B`, containing sigma(t | 1:T).", "docstring_tokens": ["Backward", "update", "for", "a", "Kalman", "smoother", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/linear_gaussian_ssm.py#L1198-L1272", "partition": "test"}
{"repo": "h2oai/h2o-3", "path": "h2o-py/h2o/model/binomial.py", "func_name": "H2OBinomialModel.plot", "original_string": "def plot(self, timestep=\"AUTO\", metric=\"AUTO\", server=False, **kwargs):\n        \"\"\"\n        Plot training set (and validation set if available) scoring history for an H2OBinomialModel.\n\n        The timestep and metric arguments are restricted to what is available in its scoring history.\n\n        :param str timestep: A unit of measurement for the x-axis.\n        :param str metric: A unit of measurement for the y-axis.\n        :param bool server: if True, then generate the image inline (using matplotlib's \"Agg\" backend)\n        \"\"\"\n        assert_is_type(metric, \"AUTO\", \"logloss\", \"auc\", \"classification_error\", \"rmse\")\n        if self._model_json[\"algo\"] in (\"deeplearning\", \"deepwater\", \"xgboost\", \"drf\", \"gbm\"):\n            if metric == \"AUTO\":\n                metric = \"logloss\"\n        self._plot(timestep=timestep, metric=metric, server=server)", "language": "python", "code": "def plot(self, timestep=\"AUTO\", metric=\"AUTO\", server=False, **kwargs):\n        \"\"\"\n        Plot training set (and validation set if available) scoring history for an H2OBinomialModel.\n\n        The timestep and metric arguments are restricted to what is available in its scoring history.\n\n        :param str timestep: A unit of measurement for the x-axis.\n        :param str metric: A unit of measurement for the y-axis.\n        :param bool server: if True, then generate the image inline (using matplotlib's \"Agg\" backend)\n        \"\"\"\n        assert_is_type(metric, \"AUTO\", \"logloss\", \"auc\", \"classification_error\", \"rmse\")\n        if self._model_json[\"algo\"] in (\"deeplearning\", \"deepwater\", \"xgboost\", \"drf\", \"gbm\"):\n            if metric == \"AUTO\":\n                metric = \"logloss\"\n        self._plot(timestep=timestep, metric=metric, server=server)", "code_tokens": ["def", "plot", "(", "self", ",", "timestep", "=", "\"AUTO\"", ",", "metric", "=", "\"AUTO\"", ",", "server", "=", "False", ",", "*", "*", "kwargs", ")", ":", "assert_is_type", "(", "metric", ",", "\"AUTO\"", ",", "\"logloss\"", ",", "\"auc\"", ",", "\"classification_error\"", ",", "\"rmse\"", ")", "if", "self", ".", "_model_json", "[", "\"algo\"", "]", "in", "(", "\"deeplearning\"", ",", "\"deepwater\"", ",", "\"xgboost\"", ",", "\"drf\"", ",", "\"gbm\"", ")", ":", "if", "metric", "==", "\"AUTO\"", ":", "metric", "=", "\"logloss\"", "self", ".", "_plot", "(", "timestep", "=", "timestep", ",", "metric", "=", "metric", ",", "server", "=", "server", ")"], "docstring": "Plot training set (and validation set if available) scoring history for an H2OBinomialModel.\n\n        The timestep and metric arguments are restricted to what is available in its scoring history.\n\n        :param str timestep: A unit of measurement for the x-axis.\n        :param str metric: A unit of measurement for the y-axis.\n        :param bool server: if True, then generate the image inline (using matplotlib's \"Agg\" backend)", "docstring_tokens": ["Plot", "training", "set", "(", "and", "validation", "set", "if", "available", ")", "scoring", "history", "for", "an", "H2OBinomialModel", "."], "sha": "dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8", "url": "https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/h2o-py/h2o/model/binomial.py#L445-L459", "partition": "test"}
{"repo": "Chilipp/sphinx-nbexamples", "path": "sphinx_nbexamples/__init__.py", "func_name": "NotebookProcessor.create_py", "original_string": "def create_py(self, nb, force=False):\n        \"\"\"Create the python script from the notebook node\"\"\"\n        # Although we would love to simply use ``nbconvert.export_python(nb)``\n        # this causes troubles in other cells processed by the ipython\n        # directive. Instead of getting something like ``Out [5]:``, we get\n        # some weird like '[0;31mOut[\u001b[1;31m5\u001b[0;31m]: \u001b[0m' which look like\n        # color information if we allow the call of nbconvert.export_python\n        if list(map(int, re.findall('\\d+', nbconvert.__version__))) >= [4, 2]:\n            py_file = os.path.basename(self.py_file)\n        else:\n            py_file = self.py_file\n        try:\n            level = logger.logger.level\n        except AttributeError:\n            level = logger.level\n        spr.call(['jupyter', 'nbconvert', '--to=python',\n                  '--output=' + py_file, '--log-level=%s' % level,\n                  self.outfile])\n        with open(self.py_file) as f:\n            py_content = f.read()\n        # comment out ipython magics\n        py_content = re.sub('^\\s*get_ipython\\(\\).magic.*', '# \\g<0>',\n                            py_content, flags=re.MULTILINE)\n        with open(self.py_file, 'w') as f:\n            f.write(py_content)", "language": "python", "code": "def create_py(self, nb, force=False):\n        \"\"\"Create the python script from the notebook node\"\"\"\n        # Although we would love to simply use ``nbconvert.export_python(nb)``\n        # this causes troubles in other cells processed by the ipython\n        # directive. Instead of getting something like ``Out [5]:``, we get\n        # some weird like '[0;31mOut[\u001b[1;31m5\u001b[0;31m]: \u001b[0m' which look like\n        # color information if we allow the call of nbconvert.export_python\n        if list(map(int, re.findall('\\d+', nbconvert.__version__))) >= [4, 2]:\n            py_file = os.path.basename(self.py_file)\n        else:\n            py_file = self.py_file\n        try:\n            level = logger.logger.level\n        except AttributeError:\n            level = logger.level\n        spr.call(['jupyter', 'nbconvert', '--to=python',\n                  '--output=' + py_file, '--log-level=%s' % level,\n                  self.outfile])\n        with open(self.py_file) as f:\n            py_content = f.read()\n        # comment out ipython magics\n        py_content = re.sub('^\\s*get_ipython\\(\\).magic.*', '# \\g<0>',\n                            py_content, flags=re.MULTILINE)\n        with open(self.py_file, 'w') as f:\n            f.write(py_content)", "code_tokens": ["def", "create_py", "(", "self", ",", "nb", ",", "force", "=", "False", ")", ":", "# Although we would love to simply use ``nbconvert.export_python(nb)``", "# this causes troubles in other cells processed by the ipython", "# directive. Instead of getting something like ``Out [5]:``, we get", "# some weird like '[0;31mOut[\u001b[1;31m5\u001b[0;31m]: \u001b[0m' which look like", "# color information if we allow the call of nbconvert.export_python", "if", "list", "(", "map", "(", "int", ",", "re", ".", "findall", "(", "'\\d+'", ",", "nbconvert", ".", "__version__", ")", ")", ")", ">=", "[", "4", ",", "2", "]", ":", "py_file", "=", "os", ".", "path", ".", "basename", "(", "self", ".", "py_file", ")", "else", ":", "py_file", "=", "self", ".", "py_file", "try", ":", "level", "=", "logger", ".", "logger", ".", "level", "except", "AttributeError", ":", "level", "=", "logger", ".", "level", "spr", ".", "call", "(", "[", "'jupyter'", ",", "'nbconvert'", ",", "'--to=python'", ",", "'--output='", "+", "py_file", ",", "'--log-level=%s'", "%", "level", ",", "self", ".", "outfile", "]", ")", "with", "open", "(", "self", ".", "py_file", ")", "as", "f", ":", "py_content", "=", "f", ".", "read", "(", ")", "# comment out ipython magics", "py_content", "=", "re", ".", "sub", "(", "'^\\s*get_ipython\\(\\).magic.*'", ",", "'# \\g<0>'", ",", "py_content", ",", "flags", "=", "re", ".", "MULTILINE", ")", "with", "open", "(", "self", ".", "py_file", ",", "'w'", ")", "as", "f", ":", "f", ".", "write", "(", "py_content", ")"], "docstring": "Create the python script from the notebook node", "docstring_tokens": ["Create", "the", "python", "script", "from", "the", "notebook", "node"], "sha": "08e0319ff3c70f8a931dfa8890caf48add4d0470", "url": "https://github.com/Chilipp/sphinx-nbexamples/blob/08e0319ff3c70f8a931dfa8890caf48add4d0470/sphinx_nbexamples/__init__.py#L460-L484", "partition": "test"}
{"repo": "MaxStrange/AudioSegment", "path": "algorithms/asa.py", "func_name": "_remove_overlaps", "original_string": "def _remove_overlaps(segmentation_mask, fronts):\n    \"\"\"\n    Removes all points in the fronts that overlap with the segmentation mask.\n    \"\"\"\n    fidxs, sidxs = np.where((segmentation_mask != fronts) & (segmentation_mask != 0) & (fronts != 0))\n    fronts[fidxs, sidxs] = 0", "language": "python", "code": "def _remove_overlaps(segmentation_mask, fronts):\n    \"\"\"\n    Removes all points in the fronts that overlap with the segmentation mask.\n    \"\"\"\n    fidxs, sidxs = np.where((segmentation_mask != fronts) & (segmentation_mask != 0) & (fronts != 0))\n    fronts[fidxs, sidxs] = 0", "code_tokens": ["def", "_remove_overlaps", "(", "segmentation_mask", ",", "fronts", ")", ":", "fidxs", ",", "sidxs", "=", "np", ".", "where", "(", "(", "segmentation_mask", "!=", "fronts", ")", "&", "(", "segmentation_mask", "!=", "0", ")", "&", "(", "fronts", "!=", "0", ")", ")", "fronts", "[", "fidxs", ",", "sidxs", "]", "=", "0"], "docstring": "Removes all points in the fronts that overlap with the segmentation mask.", "docstring_tokens": ["Removes", "all", "points", "in", "the", "fronts", "that", "overlap", "with", "the", "segmentation", "mask", "."], "sha": "1daefb8de626ddff3ff7016697c3ad31d262ecd6", "url": "https://github.com/MaxStrange/AudioSegment/blob/1daefb8de626ddff3ff7016697c3ad31d262ecd6/algorithms/asa.py#L632-L637", "partition": "test"}
{"repo": "SeabornGames/Logger", "path": "seaborn_logger/skip_traceback.py", "func_name": "skip_module", "original_string": "def skip_module(*modules):\n    \"\"\"\n        This will exclude all of the \"modules\" from the traceback\n    :param modules: list of modules to exclude\n    :return:        None\n    \"\"\"\n    modules = (modules and isinstance(modules[0], list)) and \\\n              modules[0] or modules\n\n    for module in modules:\n        if not module in SKIPPED_MODULES:\n            SKIPPED_MODULES.append(module)\n    traceback.extract_tb = _new_extract_tb", "language": "python", "code": "def skip_module(*modules):\n    \"\"\"\n        This will exclude all of the \"modules\" from the traceback\n    :param modules: list of modules to exclude\n    :return:        None\n    \"\"\"\n    modules = (modules and isinstance(modules[0], list)) and \\\n              modules[0] or modules\n\n    for module in modules:\n        if not module in SKIPPED_MODULES:\n            SKIPPED_MODULES.append(module)\n    traceback.extract_tb = _new_extract_tb", "code_tokens": ["def", "skip_module", "(", "*", "modules", ")", ":", "modules", "=", "(", "modules", "and", "isinstance", "(", "modules", "[", "0", "]", ",", "list", ")", ")", "and", "modules", "[", "0", "]", "or", "modules", "for", "module", "in", "modules", ":", "if", "not", "module", "in", "SKIPPED_MODULES", ":", "SKIPPED_MODULES", ".", "append", "(", "module", ")", "traceback", ".", "extract_tb", "=", "_new_extract_tb"], "docstring": "This will exclude all of the \"modules\" from the traceback\n    :param modules: list of modules to exclude\n    :return:        None", "docstring_tokens": ["This", "will", "exclude", "all", "of", "the", "modules", "from", "the", "traceback", ":", "param", "modules", ":", "list", "of", "modules", "to", "exclude", ":", "return", ":", "None"], "sha": "fb8b1700557aaea8d3216bd4c4df33c302bece7f", "url": "https://github.com/SeabornGames/Logger/blob/fb8b1700557aaea8d3216bd4c4df33c302bece7f/seaborn_logger/skip_traceback.py#L17-L29", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/layers/initializers.py", "func_name": "BlockwiseInitializer.get_config", "original_string": "def get_config(self):\n    \"\"\"Returns initializer configuration as a JSON-serializable dict.\"\"\"\n    return {\n        'initializers': [\n            tf.compat.v2.initializers.serialize(\n                tf.keras.initializers.get(init))\n            for init in self.initializers\n        ],\n        'sizes': self.sizes,\n        'validate_args': self.validate_args,\n    }", "language": "python", "code": "def get_config(self):\n    \"\"\"Returns initializer configuration as a JSON-serializable dict.\"\"\"\n    return {\n        'initializers': [\n            tf.compat.v2.initializers.serialize(\n                tf.keras.initializers.get(init))\n            for init in self.initializers\n        ],\n        'sizes': self.sizes,\n        'validate_args': self.validate_args,\n    }", "code_tokens": ["def", "get_config", "(", "self", ")", ":", "return", "{", "'initializers'", ":", "[", "tf", ".", "compat", ".", "v2", ".", "initializers", ".", "serialize", "(", "tf", ".", "keras", ".", "initializers", ".", "get", "(", "init", ")", ")", "for", "init", "in", "self", ".", "initializers", "]", ",", "'sizes'", ":", "self", ".", "sizes", ",", "'validate_args'", ":", "self", ".", "validate_args", ",", "}"], "docstring": "Returns initializer configuration as a JSON-serializable dict.", "docstring_tokens": ["Returns", "initializer", "configuration", "as", "a", "JSON", "-", "serializable", "dict", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/layers/initializers.py#L106-L116", "partition": "test"}
{"repo": "deepmipt/DeepPavlov", "path": "deeppavlov/core/data/utils.py", "func_name": "set_query_parameter", "original_string": "def set_query_parameter(url, param_name, param_value):\n    \"\"\"Given a URL, set or replace a query parameter and return the modified URL.\n\n    Args:\n        url: a given  URL\n        param_name: the parameter name to add\n        param_value: the parameter value\n    Returns:\n        URL with the added parameter\n\n    \"\"\"\n    scheme, netloc, path, query_string, fragment = urlsplit(url)\n    query_params = parse_qs(query_string)\n\n    query_params[param_name] = [param_value]\n    new_query_string = urlencode(query_params, doseq=True)\n\n    return urlunsplit((scheme, netloc, path, new_query_string, fragment))", "language": "python", "code": "def set_query_parameter(url, param_name, param_value):\n    \"\"\"Given a URL, set or replace a query parameter and return the modified URL.\n\n    Args:\n        url: a given  URL\n        param_name: the parameter name to add\n        param_value: the parameter value\n    Returns:\n        URL with the added parameter\n\n    \"\"\"\n    scheme, netloc, path, query_string, fragment = urlsplit(url)\n    query_params = parse_qs(query_string)\n\n    query_params[param_name] = [param_value]\n    new_query_string = urlencode(query_params, doseq=True)\n\n    return urlunsplit((scheme, netloc, path, new_query_string, fragment))", "code_tokens": ["def", "set_query_parameter", "(", "url", ",", "param_name", ",", "param_value", ")", ":", "scheme", ",", "netloc", ",", "path", ",", "query_string", ",", "fragment", "=", "urlsplit", "(", "url", ")", "query_params", "=", "parse_qs", "(", "query_string", ")", "query_params", "[", "param_name", "]", "=", "[", "param_value", "]", "new_query_string", "=", "urlencode", "(", "query_params", ",", "doseq", "=", "True", ")", "return", "urlunsplit", "(", "(", "scheme", ",", "netloc", ",", "path", ",", "new_query_string", ",", "fragment", ")", ")"], "docstring": "Given a URL, set or replace a query parameter and return the modified URL.\n\n    Args:\n        url: a given  URL\n        param_name: the parameter name to add\n        param_value: the parameter value\n    Returns:\n        URL with the added parameter", "docstring_tokens": ["Given", "a", "URL", "set", "or", "replace", "a", "query", "parameter", "and", "return", "the", "modified", "URL", "."], "sha": "f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c", "url": "https://github.com/deepmipt/DeepPavlov/blob/f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c/deeppavlov/core/data/utils.py#L463-L480", "partition": "test"}
{"repo": "keflavich/plfit", "path": "plfit/plfit.py", "func_name": "plfit.alphavsks", "original_string": "def alphavsks(self,autozoom=True,**kwargs):\n        \"\"\"\n        Plot alpha versus the ks value for derived alpha.  This plot can be used\n        as a diagnostic of whether you have derived the 'best' fit: if there are\n        multiple local minima, your data set may be well suited to a broken\n        powerlaw or a different function.\n        \"\"\"\n\n        pylab.plot(self._alpha_values, self._xmin_kstest, '.')\n        pylab.errorbar(self._alpha, self._ks, xerr=self._alphaerr, fmt='+')\n\n        ax=pylab.gca()\n        if autozoom:\n            ax.set_ylim(0.8*(self._ks),3*(self._ks))\n            ax.set_xlim((self._alpha)-5*self._alphaerr,(self._alpha)+5*self._alphaerr)\n        ax.set_ylabel(\"KS statistic\")\n        ax.set_xlabel(r'$\\alpha$')\n        pylab.draw()\n\n        return ax", "language": "python", "code": "def alphavsks(self,autozoom=True,**kwargs):\n        \"\"\"\n        Plot alpha versus the ks value for derived alpha.  This plot can be used\n        as a diagnostic of whether you have derived the 'best' fit: if there are\n        multiple local minima, your data set may be well suited to a broken\n        powerlaw or a different function.\n        \"\"\"\n\n        pylab.plot(self._alpha_values, self._xmin_kstest, '.')\n        pylab.errorbar(self._alpha, self._ks, xerr=self._alphaerr, fmt='+')\n\n        ax=pylab.gca()\n        if autozoom:\n            ax.set_ylim(0.8*(self._ks),3*(self._ks))\n            ax.set_xlim((self._alpha)-5*self._alphaerr,(self._alpha)+5*self._alphaerr)\n        ax.set_ylabel(\"KS statistic\")\n        ax.set_xlabel(r'$\\alpha$')\n        pylab.draw()\n\n        return ax", "code_tokens": ["def", "alphavsks", "(", "self", ",", "autozoom", "=", "True", ",", "*", "*", "kwargs", ")", ":", "pylab", ".", "plot", "(", "self", ".", "_alpha_values", ",", "self", ".", "_xmin_kstest", ",", "'.'", ")", "pylab", ".", "errorbar", "(", "self", ".", "_alpha", ",", "self", ".", "_ks", ",", "xerr", "=", "self", ".", "_alphaerr", ",", "fmt", "=", "'+'", ")", "ax", "=", "pylab", ".", "gca", "(", ")", "if", "autozoom", ":", "ax", ".", "set_ylim", "(", "0.8", "*", "(", "self", ".", "_ks", ")", ",", "3", "*", "(", "self", ".", "_ks", ")", ")", "ax", ".", "set_xlim", "(", "(", "self", ".", "_alpha", ")", "-", "5", "*", "self", ".", "_alphaerr", ",", "(", "self", ".", "_alpha", ")", "+", "5", "*", "self", ".", "_alphaerr", ")", "ax", ".", "set_ylabel", "(", "\"KS statistic\"", ")", "ax", ".", "set_xlabel", "(", "r'$\\alpha$'", ")", "pylab", ".", "draw", "(", ")", "return", "ax"], "docstring": "Plot alpha versus the ks value for derived alpha.  This plot can be used\n        as a diagnostic of whether you have derived the 'best' fit: if there are\n        multiple local minima, your data set may be well suited to a broken\n        powerlaw or a different function.", "docstring_tokens": ["Plot", "alpha", "versus", "the", "ks", "value", "for", "derived", "alpha", ".", "This", "plot", "can", "be", "used", "as", "a", "diagnostic", "of", "whether", "you", "have", "derived", "the", "best", "fit", ":", "if", "there", "are", "multiple", "local", "minima", "your", "data", "set", "may", "be", "well", "suited", "to", "a", "broken", "powerlaw", "or", "a", "different", "function", "."], "sha": "7dafa6302b427ba8c89651148e3e9d29add436c3", "url": "https://github.com/keflavich/plfit/blob/7dafa6302b427ba8c89651148e3e9d29add436c3/plfit/plfit.py#L474-L493", "partition": "test"}
{"repo": "gholt/swiftly", "path": "swiftly/client/client.py", "func_name": "Client.put_container", "original_string": "def put_container(self, container, headers=None, query=None, cdn=False,\n                      body=None):\n        \"\"\"\n        PUTs the container and returns the results. This is usually\n        done to create new containers and can also be used to set\n        X-Container-Meta-xxx headers. Note that if the container\n        already exists, any existing X-Container-Meta-xxx headers will\n        remain untouched. To remove an X-Container-Meta-xxx header,\n        send the header with an empty string as its value.\n\n        :param container: The name of the container.\n        :param headers: Additional headers to send with the request.\n        :param query: Set to a dict of query values to send on the\n            query string of the request.\n        :param cdn: If set True, the CDN management interface will be\n            used.\n        :param body: Some container PUT requests, like the\n            extract-archive bulk upload request, take a body.\n        :returns: A tuple of (status, reason, headers, contents).\n\n            :status: is an int for the HTTP status code.\n            :reason: is the str for the HTTP status (ex: \"Ok\").\n            :headers: is a dict with all lowercase keys of the HTTP\n                headers; if a header has multiple values, it will be a\n                list.\n            :contents: is the str for the HTTP body.\n        \"\"\"\n        path = self._container_path(container)\n        return self.request(\n            'PUT', path, body or '', headers, query=query, cdn=cdn)", "language": "python", "code": "def put_container(self, container, headers=None, query=None, cdn=False,\n                      body=None):\n        \"\"\"\n        PUTs the container and returns the results. This is usually\n        done to create new containers and can also be used to set\n        X-Container-Meta-xxx headers. Note that if the container\n        already exists, any existing X-Container-Meta-xxx headers will\n        remain untouched. To remove an X-Container-Meta-xxx header,\n        send the header with an empty string as its value.\n\n        :param container: The name of the container.\n        :param headers: Additional headers to send with the request.\n        :param query: Set to a dict of query values to send on the\n            query string of the request.\n        :param cdn: If set True, the CDN management interface will be\n            used.\n        :param body: Some container PUT requests, like the\n            extract-archive bulk upload request, take a body.\n        :returns: A tuple of (status, reason, headers, contents).\n\n            :status: is an int for the HTTP status code.\n            :reason: is the str for the HTTP status (ex: \"Ok\").\n            :headers: is a dict with all lowercase keys of the HTTP\n                headers; if a header has multiple values, it will be a\n                list.\n            :contents: is the str for the HTTP body.\n        \"\"\"\n        path = self._container_path(container)\n        return self.request(\n            'PUT', path, body or '', headers, query=query, cdn=cdn)", "code_tokens": ["def", "put_container", "(", "self", ",", "container", ",", "headers", "=", "None", ",", "query", "=", "None", ",", "cdn", "=", "False", ",", "body", "=", "None", ")", ":", "path", "=", "self", ".", "_container_path", "(", "container", ")", "return", "self", ".", "request", "(", "'PUT'", ",", "path", ",", "body", "or", "''", ",", "headers", ",", "query", "=", "query", ",", "cdn", "=", "cdn", ")"], "docstring": "PUTs the container and returns the results. This is usually\n        done to create new containers and can also be used to set\n        X-Container-Meta-xxx headers. Note that if the container\n        already exists, any existing X-Container-Meta-xxx headers will\n        remain untouched. To remove an X-Container-Meta-xxx header,\n        send the header with an empty string as its value.\n\n        :param container: The name of the container.\n        :param headers: Additional headers to send with the request.\n        :param query: Set to a dict of query values to send on the\n            query string of the request.\n        :param cdn: If set True, the CDN management interface will be\n            used.\n        :param body: Some container PUT requests, like the\n            extract-archive bulk upload request, take a body.\n        :returns: A tuple of (status, reason, headers, contents).\n\n            :status: is an int for the HTTP status code.\n            :reason: is the str for the HTTP status (ex: \"Ok\").\n            :headers: is a dict with all lowercase keys of the HTTP\n                headers; if a header has multiple values, it will be a\n                list.\n            :contents: is the str for the HTTP body.", "docstring_tokens": ["PUTs", "the", "container", "and", "returns", "the", "results", ".", "This", "is", "usually", "done", "to", "create", "new", "containers", "and", "can", "also", "be", "used", "to", "set", "X", "-", "Container", "-", "Meta", "-", "xxx", "headers", ".", "Note", "that", "if", "the", "container", "already", "exists", "any", "existing", "X", "-", "Container", "-", "Meta", "-", "xxx", "headers", "will", "remain", "untouched", ".", "To", "remove", "an", "X", "-", "Container", "-", "Meta", "-", "xxx", "header", "send", "the", "header", "with", "an", "empty", "string", "as", "its", "value", "."], "sha": "5bcc1c65323b1caf1f85adbefd9fc4988c072149", "url": "https://github.com/gholt/swiftly/blob/5bcc1c65323b1caf1f85adbefd9fc4988c072149/swiftly/client/client.py#L457-L486", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/internal/moving_stats.py", "func_name": "assign_moving_mean_variance", "original_string": "def assign_moving_mean_variance(\n    mean_var, variance_var, value, decay, name=None):\n  \"\"\"Compute exponentially weighted moving {mean,variance} of a streaming value.\n\n  The `value` updated exponentially weighted moving `mean_var` and\n  `variance_var` are given by the following recurrence relations:\n\n  ```python\n  variance_var = decay * (variance_var + (1 - decay) * (value - mean_var)**2)\n  mean_var     = decay * mean_var + (1 - decay) * value\n  ```\n\n  Note: `mean_var` is updated *after* `variance_var`, i.e., `variance_var` uses\n  the lag-1 mean.\n\n  For derivation justification, see [Finch (2009; Eq. 143)][1].\n  Parameterization: Finch's `alpha` is `1 - decay`.\n\n  Args:\n    mean_var: `float`-like `Variable` representing the exponentially weighted\n      moving mean. Same shape as `variance_var` and `value`.\n    variance_var: `float`-like `Variable` representing the\n      exponentially weighted moving variance. Same shape as `mean_var` and\n      `value`.\n    value: `float`-like `Tensor`. Same shape as `mean_var` and `variance_var`.\n    decay: A `float`-like `Tensor`. The moving mean decay. Typically close to\n      `1.`, e.g., `0.999`.\n    name: Optional name of the returned operation.\n\n  Returns:\n    mean_var: `Variable` representing the `value`-updated exponentially weighted\n      moving mean.\n    variance_var: `Variable` representing the `value`-updated\n      exponentially weighted moving variance.\n\n  Raises:\n    TypeError: if `mean_var` does not have float type `dtype`.\n    TypeError: if `mean_var`, `variance_var`, `value`, `decay` have different\n      `base_dtype`.\n\n  #### References\n\n  [1]: Tony Finch. Incremental calculation of weighted mean and variance.\n       _Technical Report_, 2009.\n       http://people.ds.cam.ac.uk/fanf2/hermes/doc/antiforgery/stats.pdf\n  \"\"\"\n  with tf.compat.v1.name_scope(name, \"assign_moving_mean_variance\",\n                               [variance_var, mean_var, value, decay]):\n    with tf.compat.v1.colocate_with(variance_var):\n      with tf.compat.v1.colocate_with(mean_var):\n        base_dtype = mean_var.dtype.base_dtype\n        if not base_dtype.is_floating:\n          raise TypeError(\n              \"mean_var.base_dtype({}) does not have float type \"\n              \"`dtype`.\".format(base_dtype.name))\n        if base_dtype != variance_var.dtype.base_dtype:\n          raise TypeError(\n              \"mean_var.base_dtype({}) != variance_var.base_dtype({})\".format(\n                  base_dtype.name,\n                  variance_var.dtype.base_dtype.name))\n        value = tf.convert_to_tensor(\n            value=value, dtype=base_dtype, name=\"value\")\n        decay = tf.convert_to_tensor(\n            value=decay, dtype=base_dtype, name=\"decay\")\n        delta = value - mean_var\n        with tf.control_dependencies([delta]):\n          # We want mean_{t+1} = decay * mean_t + (1. - decay) * value\n          # We compute mean += decay * mean_t - mean_t + (1. - decay) * value =\n          #   = (1. - decay) * (value - mean_t)\n          mean_var = mean_var.assign_add((1. - decay) * delta)\n          # We want variance_{t+1} = decay * (variance_t +\n          #   + (1 - decay) * (value - mean_var)**2).\n          # We compute variance -= variance_t - decay * (variance_t +\n          #     + (1 - decay) * (value - mean_var)**2) =\n          #   = (1 - decay) * variance_t\n          #     - decay * (1 - decay) * (value - mean_var)**2\n          #   = (1 - decay) * (variance_t - decay * (value - mean_var)**2).\n          variance_var = variance_var.assign_sub(\n              (1. - decay) * (variance_var - decay * tf.square(delta)))\n        return mean_var, variance_var", "language": "python", "code": "def assign_moving_mean_variance(\n    mean_var, variance_var, value, decay, name=None):\n  \"\"\"Compute exponentially weighted moving {mean,variance} of a streaming value.\n\n  The `value` updated exponentially weighted moving `mean_var` and\n  `variance_var` are given by the following recurrence relations:\n\n  ```python\n  variance_var = decay * (variance_var + (1 - decay) * (value - mean_var)**2)\n  mean_var     = decay * mean_var + (1 - decay) * value\n  ```\n\n  Note: `mean_var` is updated *after* `variance_var`, i.e., `variance_var` uses\n  the lag-1 mean.\n\n  For derivation justification, see [Finch (2009; Eq. 143)][1].\n  Parameterization: Finch's `alpha` is `1 - decay`.\n\n  Args:\n    mean_var: `float`-like `Variable` representing the exponentially weighted\n      moving mean. Same shape as `variance_var` and `value`.\n    variance_var: `float`-like `Variable` representing the\n      exponentially weighted moving variance. Same shape as `mean_var` and\n      `value`.\n    value: `float`-like `Tensor`. Same shape as `mean_var` and `variance_var`.\n    decay: A `float`-like `Tensor`. The moving mean decay. Typically close to\n      `1.`, e.g., `0.999`.\n    name: Optional name of the returned operation.\n\n  Returns:\n    mean_var: `Variable` representing the `value`-updated exponentially weighted\n      moving mean.\n    variance_var: `Variable` representing the `value`-updated\n      exponentially weighted moving variance.\n\n  Raises:\n    TypeError: if `mean_var` does not have float type `dtype`.\n    TypeError: if `mean_var`, `variance_var`, `value`, `decay` have different\n      `base_dtype`.\n\n  #### References\n\n  [1]: Tony Finch. Incremental calculation of weighted mean and variance.\n       _Technical Report_, 2009.\n       http://people.ds.cam.ac.uk/fanf2/hermes/doc/antiforgery/stats.pdf\n  \"\"\"\n  with tf.compat.v1.name_scope(name, \"assign_moving_mean_variance\",\n                               [variance_var, mean_var, value, decay]):\n    with tf.compat.v1.colocate_with(variance_var):\n      with tf.compat.v1.colocate_with(mean_var):\n        base_dtype = mean_var.dtype.base_dtype\n        if not base_dtype.is_floating:\n          raise TypeError(\n              \"mean_var.base_dtype({}) does not have float type \"\n              \"`dtype`.\".format(base_dtype.name))\n        if base_dtype != variance_var.dtype.base_dtype:\n          raise TypeError(\n              \"mean_var.base_dtype({}) != variance_var.base_dtype({})\".format(\n                  base_dtype.name,\n                  variance_var.dtype.base_dtype.name))\n        value = tf.convert_to_tensor(\n            value=value, dtype=base_dtype, name=\"value\")\n        decay = tf.convert_to_tensor(\n            value=decay, dtype=base_dtype, name=\"decay\")\n        delta = value - mean_var\n        with tf.control_dependencies([delta]):\n          # We want mean_{t+1} = decay * mean_t + (1. - decay) * value\n          # We compute mean += decay * mean_t - mean_t + (1. - decay) * value =\n          #   = (1. - decay) * (value - mean_t)\n          mean_var = mean_var.assign_add((1. - decay) * delta)\n          # We want variance_{t+1} = decay * (variance_t +\n          #   + (1 - decay) * (value - mean_var)**2).\n          # We compute variance -= variance_t - decay * (variance_t +\n          #     + (1 - decay) * (value - mean_var)**2) =\n          #   = (1 - decay) * variance_t\n          #     - decay * (1 - decay) * (value - mean_var)**2\n          #   = (1 - decay) * (variance_t - decay * (value - mean_var)**2).\n          variance_var = variance_var.assign_sub(\n              (1. - decay) * (variance_var - decay * tf.square(delta)))\n        return mean_var, variance_var", "code_tokens": ["def", "assign_moving_mean_variance", "(", "mean_var", ",", "variance_var", ",", "value", ",", "decay", ",", "name", "=", "None", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "\"assign_moving_mean_variance\"", ",", "[", "variance_var", ",", "mean_var", ",", "value", ",", "decay", "]", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "colocate_with", "(", "variance_var", ")", ":", "with", "tf", ".", "compat", ".", "v1", ".", "colocate_with", "(", "mean_var", ")", ":", "base_dtype", "=", "mean_var", ".", "dtype", ".", "base_dtype", "if", "not", "base_dtype", ".", "is_floating", ":", "raise", "TypeError", "(", "\"mean_var.base_dtype({}) does not have float type \"", "\"`dtype`.\"", ".", "format", "(", "base_dtype", ".", "name", ")", ")", "if", "base_dtype", "!=", "variance_var", ".", "dtype", ".", "base_dtype", ":", "raise", "TypeError", "(", "\"mean_var.base_dtype({}) != variance_var.base_dtype({})\"", ".", "format", "(", "base_dtype", ".", "name", ",", "variance_var", ".", "dtype", ".", "base_dtype", ".", "name", ")", ")", "value", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "value", ",", "dtype", "=", "base_dtype", ",", "name", "=", "\"value\"", ")", "decay", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "decay", ",", "dtype", "=", "base_dtype", ",", "name", "=", "\"decay\"", ")", "delta", "=", "value", "-", "mean_var", "with", "tf", ".", "control_dependencies", "(", "[", "delta", "]", ")", ":", "# We want mean_{t+1} = decay * mean_t + (1. - decay) * value", "# We compute mean += decay * mean_t - mean_t + (1. - decay) * value =", "#   = (1. - decay) * (value - mean_t)", "mean_var", "=", "mean_var", ".", "assign_add", "(", "(", "1.", "-", "decay", ")", "*", "delta", ")", "# We want variance_{t+1} = decay * (variance_t +", "#   + (1 - decay) * (value - mean_var)**2).", "# We compute variance -= variance_t - decay * (variance_t +", "#     + (1 - decay) * (value - mean_var)**2) =", "#   = (1 - decay) * variance_t", "#     - decay * (1 - decay) * (value - mean_var)**2", "#   = (1 - decay) * (variance_t - decay * (value - mean_var)**2).", "variance_var", "=", "variance_var", ".", "assign_sub", "(", "(", "1.", "-", "decay", ")", "*", "(", "variance_var", "-", "decay", "*", "tf", ".", "square", "(", "delta", ")", ")", ")", "return", "mean_var", ",", "variance_var"], "docstring": "Compute exponentially weighted moving {mean,variance} of a streaming value.\n\n  The `value` updated exponentially weighted moving `mean_var` and\n  `variance_var` are given by the following recurrence relations:\n\n  ```python\n  variance_var = decay * (variance_var + (1 - decay) * (value - mean_var)**2)\n  mean_var     = decay * mean_var + (1 - decay) * value\n  ```\n\n  Note: `mean_var` is updated *after* `variance_var`, i.e., `variance_var` uses\n  the lag-1 mean.\n\n  For derivation justification, see [Finch (2009; Eq. 143)][1].\n  Parameterization: Finch's `alpha` is `1 - decay`.\n\n  Args:\n    mean_var: `float`-like `Variable` representing the exponentially weighted\n      moving mean. Same shape as `variance_var` and `value`.\n    variance_var: `float`-like `Variable` representing the\n      exponentially weighted moving variance. Same shape as `mean_var` and\n      `value`.\n    value: `float`-like `Tensor`. Same shape as `mean_var` and `variance_var`.\n    decay: A `float`-like `Tensor`. The moving mean decay. Typically close to\n      `1.`, e.g., `0.999`.\n    name: Optional name of the returned operation.\n\n  Returns:\n    mean_var: `Variable` representing the `value`-updated exponentially weighted\n      moving mean.\n    variance_var: `Variable` representing the `value`-updated\n      exponentially weighted moving variance.\n\n  Raises:\n    TypeError: if `mean_var` does not have float type `dtype`.\n    TypeError: if `mean_var`, `variance_var`, `value`, `decay` have different\n      `base_dtype`.\n\n  #### References\n\n  [1]: Tony Finch. Incremental calculation of weighted mean and variance.\n       _Technical Report_, 2009.\n       http://people.ds.cam.ac.uk/fanf2/hermes/doc/antiforgery/stats.pdf", "docstring_tokens": ["Compute", "exponentially", "weighted", "moving", "{", "mean", "variance", "}", "of", "a", "streaming", "value", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/internal/moving_stats.py#L31-L110", "partition": "test"}
{"repo": "neurosynth/neurosynth", "path": "neurosynth/analysis/decode.py", "func_name": "Decoder._dot_product", "original_string": "def _dot_product(self, imgs_to_decode):\n        \"\"\" Decoding using the dot product.\n        \"\"\"\n        return np.dot(imgs_to_decode.T, self.feature_images).T", "language": "python", "code": "def _dot_product(self, imgs_to_decode):\n        \"\"\" Decoding using the dot product.\n        \"\"\"\n        return np.dot(imgs_to_decode.T, self.feature_images).T", "code_tokens": ["def", "_dot_product", "(", "self", ",", "imgs_to_decode", ")", ":", "return", "np", ".", "dot", "(", "imgs_to_decode", ".", "T", ",", "self", ".", "feature_images", ")", ".", "T"], "docstring": "Decoding using the dot product.", "docstring_tokens": ["Decoding", "using", "the", "dot", "product", "."], "sha": "948ce7edce15d7df693446e76834e0c23bfe8f11", "url": "https://github.com/neurosynth/neurosynth/blob/948ce7edce15d7df693446e76834e0c23bfe8f11/neurosynth/analysis/decode.py#L216-L219", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/pip/_vendor/requests/packages/urllib3/response.py", "func_name": "HTTPResponse._decode", "original_string": "def _decode(self, data, decode_content, flush_decoder):\n        \"\"\"\n        Decode the data passed in and potentially flush the decoder.\n        \"\"\"\n        try:\n            if decode_content and self._decoder:\n                data = self._decoder.decompress(data)\n        except (IOError, zlib.error) as e:\n            content_encoding = self.headers.get('content-encoding', '').lower()\n            raise DecodeError(\n                \"Received response with content-encoding: %s, but \"\n                \"failed to decode it.\" % content_encoding, e)\n\n        if flush_decoder and decode_content and self._decoder:\n            buf = self._decoder.decompress(binary_type())\n            data += buf + self._decoder.flush()\n\n        return data", "language": "python", "code": "def _decode(self, data, decode_content, flush_decoder):\n        \"\"\"\n        Decode the data passed in and potentially flush the decoder.\n        \"\"\"\n        try:\n            if decode_content and self._decoder:\n                data = self._decoder.decompress(data)\n        except (IOError, zlib.error) as e:\n            content_encoding = self.headers.get('content-encoding', '').lower()\n            raise DecodeError(\n                \"Received response with content-encoding: %s, but \"\n                \"failed to decode it.\" % content_encoding, e)\n\n        if flush_decoder and decode_content and self._decoder:\n            buf = self._decoder.decompress(binary_type())\n            data += buf + self._decoder.flush()\n\n        return data", "code_tokens": ["def", "_decode", "(", "self", ",", "data", ",", "decode_content", ",", "flush_decoder", ")", ":", "try", ":", "if", "decode_content", "and", "self", ".", "_decoder", ":", "data", "=", "self", ".", "_decoder", ".", "decompress", "(", "data", ")", "except", "(", "IOError", ",", "zlib", ".", "error", ")", "as", "e", ":", "content_encoding", "=", "self", ".", "headers", ".", "get", "(", "'content-encoding'", ",", "''", ")", ".", "lower", "(", ")", "raise", "DecodeError", "(", "\"Received response with content-encoding: %s, but \"", "\"failed to decode it.\"", "%", "content_encoding", ",", "e", ")", "if", "flush_decoder", "and", "decode_content", "and", "self", ".", "_decoder", ":", "buf", "=", "self", ".", "_decoder", ".", "decompress", "(", "binary_type", "(", ")", ")", "data", "+=", "buf", "+", "self", ".", "_decoder", ".", "flush", "(", ")", "return", "data"], "docstring": "Decode the data passed in and potentially flush the decoder.", "docstring_tokens": ["Decode", "the", "data", "passed", "in", "and", "potentially", "flush", "the", "decoder", "."], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/pip/_vendor/requests/packages/urllib3/response.py#L186-L203", "partition": "test"}
{"repo": "kennethreitz/twitter-scraper", "path": "twitter_scraper.py", "func_name": "get_tweets", "original_string": "def get_tweets(user, pages=25):\n    \"\"\"Gets tweets for a given user, via the Twitter frontend API.\"\"\"\n\n    url = f'https://twitter.com/i/profiles/show/{user}/timeline/tweets?include_available_features=1&include_entities=1&include_new_items_bar=true'\n    headers = {\n        'Accept': 'application/json, text/javascript, */*; q=0.01',\n        'Referer': f'https://twitter.com/{user}',\n        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/603.3.8 (KHTML, like Gecko) Version/10.1.2 Safari/603.3.8',\n        'X-Twitter-Active-User': 'yes',\n        'X-Requested-With': 'XMLHttpRequest',\n        'Accept-Language': 'en-US'\n    }\n\n    def gen_tweets(pages):\n        r = session.get(url, headers=headers)\n\n        while pages > 0:\n            try:\n                html = HTML(html=r.json()['items_html'],\n                            url='bunk', default_encoding='utf-8')\n            except KeyError:\n                raise ValueError(\n                    f'Oops! Either \"{user}\" does not exist or is private.')\n\n            comma = \",\"\n            dot = \".\"\n            tweets = []\n            for tweet in html.find('html > .stream-item'):\n                # 10~11 html elements have `.stream-item` class and also their `data-item-type` is `tweet`\n                # but their content doesn't look like a tweet's content\n                try:\n                    text = tweet.find('.tweet-text')[0].full_text\n                except IndexError:  # issue #50\n                    continue\n\n                tweet_id = tweet.find('.js-permalink')[0].attrs['data-conversation-id']\n\n                time = datetime.fromtimestamp(int(tweet.find('._timestamp')[0].attrs['data-time-ms']) / 1000.0)\n\n                interactions = [\n                    x.text\n                    for x in tweet.find('.ProfileTweet-actionCount')\n                ]\n\n                replies = int(\n                    interactions[0].split(' ')[0].replace(comma, '').replace(dot, '')\n                    or interactions[3]\n                )\n\n                retweets = int(\n                    interactions[1].split(' ')[0].replace(comma, '').replace(dot, '')\n                    or interactions[4]\n                    or interactions[5]\n                )\n\n                likes = int(\n                    interactions[2].split(' ')[0].replace(comma, '').replace(dot, '')\n                    or interactions[6]\n                    or interactions[7]\n                )\n\n                hashtags = [\n                    hashtag_node.full_text\n                    for hashtag_node in tweet.find('.twitter-hashtag')\n                ]\n                urls = [\n                    url_node.attrs['data-expanded-url']\n                    for url_node in tweet.find('a.twitter-timeline-link:not(.u-hidden)')\n                ]\n                photos = [\n                    photo_node.attrs['data-image-url']\n                    for photo_node in tweet.find('.AdaptiveMedia-photoContainer')\n                ]\n\n                videos = []\n                video_nodes = tweet.find(\".PlayableMedia-player\")\n                for node in video_nodes:\n                    styles = node.attrs['style'].split()\n                    for style in styles:\n                        if style.startswith('background'):\n                            tmp = style.split('/')[-1]\n                            video_id = tmp[:tmp.index('.jpg')]\n                            videos.append({'id': video_id})\n\n                tweets.append({\n                    'tweetId': tweet_id,\n                    'time': time,\n                    'text': text,\n                    'replies': replies,\n                    'retweets': retweets,\n                    'likes': likes,\n                    'entries': {\n                        'hashtags': hashtags, 'urls': urls,\n                        'photos': photos, 'videos': videos\n                    }\n                })\n\n            last_tweet = html.find('.stream-item')[-1].attrs['data-item-id']\n\n            for tweet in tweets:\n                if tweet:\n                    tweet['text'] = re.sub('http', ' http', tweet['text'], 1)\n                    yield tweet\n\n            r = session.get(url, params={'max_position': last_tweet}, headers=headers)\n            pages += -1\n\n    yield from gen_tweets(pages)", "language": "python", "code": "def get_tweets(user, pages=25):\n    \"\"\"Gets tweets for a given user, via the Twitter frontend API.\"\"\"\n\n    url = f'https://twitter.com/i/profiles/show/{user}/timeline/tweets?include_available_features=1&include_entities=1&include_new_items_bar=true'\n    headers = {\n        'Accept': 'application/json, text/javascript, */*; q=0.01',\n        'Referer': f'https://twitter.com/{user}',\n        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/603.3.8 (KHTML, like Gecko) Version/10.1.2 Safari/603.3.8',\n        'X-Twitter-Active-User': 'yes',\n        'X-Requested-With': 'XMLHttpRequest',\n        'Accept-Language': 'en-US'\n    }\n\n    def gen_tweets(pages):\n        r = session.get(url, headers=headers)\n\n        while pages > 0:\n            try:\n                html = HTML(html=r.json()['items_html'],\n                            url='bunk', default_encoding='utf-8')\n            except KeyError:\n                raise ValueError(\n                    f'Oops! Either \"{user}\" does not exist or is private.')\n\n            comma = \",\"\n            dot = \".\"\n            tweets = []\n            for tweet in html.find('html > .stream-item'):\n                # 10~11 html elements have `.stream-item` class and also their `data-item-type` is `tweet`\n                # but their content doesn't look like a tweet's content\n                try:\n                    text = tweet.find('.tweet-text')[0].full_text\n                except IndexError:  # issue #50\n                    continue\n\n                tweet_id = tweet.find('.js-permalink')[0].attrs['data-conversation-id']\n\n                time = datetime.fromtimestamp(int(tweet.find('._timestamp')[0].attrs['data-time-ms']) / 1000.0)\n\n                interactions = [\n                    x.text\n                    for x in tweet.find('.ProfileTweet-actionCount')\n                ]\n\n                replies = int(\n                    interactions[0].split(' ')[0].replace(comma, '').replace(dot, '')\n                    or interactions[3]\n                )\n\n                retweets = int(\n                    interactions[1].split(' ')[0].replace(comma, '').replace(dot, '')\n                    or interactions[4]\n                    or interactions[5]\n                )\n\n                likes = int(\n                    interactions[2].split(' ')[0].replace(comma, '').replace(dot, '')\n                    or interactions[6]\n                    or interactions[7]\n                )\n\n                hashtags = [\n                    hashtag_node.full_text\n                    for hashtag_node in tweet.find('.twitter-hashtag')\n                ]\n                urls = [\n                    url_node.attrs['data-expanded-url']\n                    for url_node in tweet.find('a.twitter-timeline-link:not(.u-hidden)')\n                ]\n                photos = [\n                    photo_node.attrs['data-image-url']\n                    for photo_node in tweet.find('.AdaptiveMedia-photoContainer')\n                ]\n\n                videos = []\n                video_nodes = tweet.find(\".PlayableMedia-player\")\n                for node in video_nodes:\n                    styles = node.attrs['style'].split()\n                    for style in styles:\n                        if style.startswith('background'):\n                            tmp = style.split('/')[-1]\n                            video_id = tmp[:tmp.index('.jpg')]\n                            videos.append({'id': video_id})\n\n                tweets.append({\n                    'tweetId': tweet_id,\n                    'time': time,\n                    'text': text,\n                    'replies': replies,\n                    'retweets': retweets,\n                    'likes': likes,\n                    'entries': {\n                        'hashtags': hashtags, 'urls': urls,\n                        'photos': photos, 'videos': videos\n                    }\n                })\n\n            last_tweet = html.find('.stream-item')[-1].attrs['data-item-id']\n\n            for tweet in tweets:\n                if tweet:\n                    tweet['text'] = re.sub('http', ' http', tweet['text'], 1)\n                    yield tweet\n\n            r = session.get(url, params={'max_position': last_tweet}, headers=headers)\n            pages += -1\n\n    yield from gen_tweets(pages)", "code_tokens": ["def", "get_tweets", "(", "user", ",", "pages", "=", "25", ")", ":", "url", "=", "f'https://twitter.com/i/profiles/show/{user}/timeline/tweets?include_available_features=1&include_entities=1&include_new_items_bar=true'", "headers", "=", "{", "'Accept'", ":", "'application/json, text/javascript, */*; q=0.01'", ",", "'Referer'", ":", "f'https://twitter.com/{user}'", ",", "'User-Agent'", ":", "'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/603.3.8 (KHTML, like Gecko) Version/10.1.2 Safari/603.3.8'", ",", "'X-Twitter-Active-User'", ":", "'yes'", ",", "'X-Requested-With'", ":", "'XMLHttpRequest'", ",", "'Accept-Language'", ":", "'en-US'", "}", "def", "gen_tweets", "(", "pages", ")", ":", "r", "=", "session", ".", "get", "(", "url", ",", "headers", "=", "headers", ")", "while", "pages", ">", "0", ":", "try", ":", "html", "=", "HTML", "(", "html", "=", "r", ".", "json", "(", ")", "[", "'items_html'", "]", ",", "url", "=", "'bunk'", ",", "default_encoding", "=", "'utf-8'", ")", "except", "KeyError", ":", "raise", "ValueError", "(", "f'Oops! Either \"{user}\" does not exist or is private.'", ")", "comma", "=", "\",\"", "dot", "=", "\".\"", "tweets", "=", "[", "]", "for", "tweet", "in", "html", ".", "find", "(", "'html > .stream-item'", ")", ":", "# 10~11 html elements have `.stream-item` class and also their `data-item-type` is `tweet`", "# but their content doesn't look like a tweet's content", "try", ":", "text", "=", "tweet", ".", "find", "(", "'.tweet-text'", ")", "[", "0", "]", ".", "full_text", "except", "IndexError", ":", "# issue #50", "continue", "tweet_id", "=", "tweet", ".", "find", "(", "'.js-permalink'", ")", "[", "0", "]", ".", "attrs", "[", "'data-conversation-id'", "]", "time", "=", "datetime", ".", "fromtimestamp", "(", "int", "(", "tweet", ".", "find", "(", "'._timestamp'", ")", "[", "0", "]", ".", "attrs", "[", "'data-time-ms'", "]", ")", "/", "1000.0", ")", "interactions", "=", "[", "x", ".", "text", "for", "x", "in", "tweet", ".", "find", "(", "'.ProfileTweet-actionCount'", ")", "]", "replies", "=", "int", "(", "interactions", "[", "0", "]", ".", "split", "(", "' '", ")", "[", "0", "]", ".", "replace", "(", "comma", ",", "''", ")", ".", "replace", "(", "dot", ",", "''", ")", "or", "interactions", "[", "3", "]", ")", "retweets", "=", "int", "(", "interactions", "[", "1", "]", ".", "split", "(", "' '", ")", "[", "0", "]", ".", "replace", "(", "comma", ",", "''", ")", ".", "replace", "(", "dot", ",", "''", ")", "or", "interactions", "[", "4", "]", "or", "interactions", "[", "5", "]", ")", "likes", "=", "int", "(", "interactions", "[", "2", "]", ".", "split", "(", "' '", ")", "[", "0", "]", ".", "replace", "(", "comma", ",", "''", ")", ".", "replace", "(", "dot", ",", "''", ")", "or", "interactions", "[", "6", "]", "or", "interactions", "[", "7", "]", ")", "hashtags", "=", "[", "hashtag_node", ".", "full_text", "for", "hashtag_node", "in", "tweet", ".", "find", "(", "'.twitter-hashtag'", ")", "]", "urls", "=", "[", "url_node", ".", "attrs", "[", "'data-expanded-url'", "]", "for", "url_node", "in", "tweet", ".", "find", "(", "'a.twitter-timeline-link:not(.u-hidden)'", ")", "]", "photos", "=", "[", "photo_node", ".", "attrs", "[", "'data-image-url'", "]", "for", "photo_node", "in", "tweet", ".", "find", "(", "'.AdaptiveMedia-photoContainer'", ")", "]", "videos", "=", "[", "]", "video_nodes", "=", "tweet", ".", "find", "(", "\".PlayableMedia-player\"", ")", "for", "node", "in", "video_nodes", ":", "styles", "=", "node", ".", "attrs", "[", "'style'", "]", ".", "split", "(", ")", "for", "style", "in", "styles", ":", "if", "style", ".", "startswith", "(", "'background'", ")", ":", "tmp", "=", "style", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", "video_id", "=", "tmp", "[", ":", "tmp", ".", "index", "(", "'.jpg'", ")", "]", "videos", ".", "append", "(", "{", "'id'", ":", "video_id", "}", ")", "tweets", ".", "append", "(", "{", "'tweetId'", ":", "tweet_id", ",", "'time'", ":", "time", ",", "'text'", ":", "text", ",", "'replies'", ":", "replies", ",", "'retweets'", ":", "retweets", ",", "'likes'", ":", "likes", ",", "'entries'", ":", "{", "'hashtags'", ":", "hashtags", ",", "'urls'", ":", "urls", ",", "'photos'", ":", "photos", ",", "'videos'", ":", "videos", "}", "}", ")", "last_tweet", "=", "html", ".", "find", "(", "'.stream-item'", ")", "[", "-", "1", "]", ".", "attrs", "[", "'data-item-id'", "]", "for", "tweet", "in", "tweets", ":", "if", "tweet", ":", "tweet", "[", "'text'", "]", "=", "re", ".", "sub", "(", "'http'", ",", "' http'", ",", "tweet", "[", "'text'", "]", ",", "1", ")", "yield", "tweet", "r", "=", "session", ".", "get", "(", "url", ",", "params", "=", "{", "'max_position'", ":", "last_tweet", "}", ",", "headers", "=", "headers", ")", "pages", "+=", "-", "1", "yield", "from", "gen_tweets", "(", "pages", ")"], "docstring": "Gets tweets for a given user, via the Twitter frontend API.", "docstring_tokens": ["Gets", "tweets", "for", "a", "given", "user", "via", "the", "Twitter", "frontend", "API", "."], "sha": "6e5631ab478e1066f6cb1bb9e9633d4928f6a550", "url": "https://github.com/kennethreitz/twitter-scraper/blob/6e5631ab478e1066f6cb1bb9e9633d4928f6a550/twitter_scraper.py#L8-L115", "partition": "test"}
{"repo": "greenbender/pynntp", "path": "nntp/nntp.py", "func_name": "BaseNNTPClient.__info_yenczlib_gen", "original_string": "def __info_yenczlib_gen(self):\n        \"\"\"Generator for the lines of a compressed info (textual) response.\n\n        Compressed responses are an extension to the NNTP protocol supported by\n        some usenet servers to reduce the bandwidth of heavily used range style\n        commands that can return large amounts of textual data. The server\n        returns that same data as it would for the uncompressed versions of the\n        command the difference being that the data is zlib deflated and then\n        yEnc encoded.\n\n        This function will produce that same output as the info_gen()\n        function. In other words it takes care of decoding and decompression.\n\n        Yields:\n            A line of the info response.\n\n        Raises:\n            NNTPError: If data is required to be read from the socket and fails.\n            NNTPDataError: When there is an error parsing the yEnc header or\n                trailer, if the CRC check fails or decompressing data fails.\n        \"\"\"\n\n        escape = 0\n        dcrc32 = 0\n        inflate = zlib.decompressobj(-15)\n\n        # header\n        header = next(self.__info_plain_gen())\n        if not header.startswith(\"=ybegin\"):\n            raise NNTPDataError(\"Bad yEnc header\")\n\n        # data\n        buf, trailer = fifo.Fifo(), \"\"\n        for line in self.__info_plain_gen():\n            if line.startswith(\"=yend\"):\n                trailer = line\n                continue\n            data, escape, dcrc32 = yenc.decode(line, escape, dcrc32)\n            try:\n                data = inflate.decompress(data)\n            except zlib.error:\n                raise NNTPDataError(\"Decompression failed\")\n            if not data:\n                continue\n            buf.write(data)\n            for l in buf:\n                yield l\n\n        # trailer\n        if not trailer:\n            raise NNTPDataError(\"Missing yEnc trailer\")\n\n        # expected crc32\n        ecrc32 = yenc.crc32(trailer)\n        if ecrc32 is None:\n            raise NNTPDataError(\"Bad yEnc trailer\")\n\n        # check crc32\n        if ecrc32 != dcrc32 & 0xffffffff:\n            raise NNTPDataError(\"Bad yEnc CRC\")", "language": "python", "code": "def __info_yenczlib_gen(self):\n        \"\"\"Generator for the lines of a compressed info (textual) response.\n\n        Compressed responses are an extension to the NNTP protocol supported by\n        some usenet servers to reduce the bandwidth of heavily used range style\n        commands that can return large amounts of textual data. The server\n        returns that same data as it would for the uncompressed versions of the\n        command the difference being that the data is zlib deflated and then\n        yEnc encoded.\n\n        This function will produce that same output as the info_gen()\n        function. In other words it takes care of decoding and decompression.\n\n        Yields:\n            A line of the info response.\n\n        Raises:\n            NNTPError: If data is required to be read from the socket and fails.\n            NNTPDataError: When there is an error parsing the yEnc header or\n                trailer, if the CRC check fails or decompressing data fails.\n        \"\"\"\n\n        escape = 0\n        dcrc32 = 0\n        inflate = zlib.decompressobj(-15)\n\n        # header\n        header = next(self.__info_plain_gen())\n        if not header.startswith(\"=ybegin\"):\n            raise NNTPDataError(\"Bad yEnc header\")\n\n        # data\n        buf, trailer = fifo.Fifo(), \"\"\n        for line in self.__info_plain_gen():\n            if line.startswith(\"=yend\"):\n                trailer = line\n                continue\n            data, escape, dcrc32 = yenc.decode(line, escape, dcrc32)\n            try:\n                data = inflate.decompress(data)\n            except zlib.error:\n                raise NNTPDataError(\"Decompression failed\")\n            if not data:\n                continue\n            buf.write(data)\n            for l in buf:\n                yield l\n\n        # trailer\n        if not trailer:\n            raise NNTPDataError(\"Missing yEnc trailer\")\n\n        # expected crc32\n        ecrc32 = yenc.crc32(trailer)\n        if ecrc32 is None:\n            raise NNTPDataError(\"Bad yEnc trailer\")\n\n        # check crc32\n        if ecrc32 != dcrc32 & 0xffffffff:\n            raise NNTPDataError(\"Bad yEnc CRC\")", "code_tokens": ["def", "__info_yenczlib_gen", "(", "self", ")", ":", "escape", "=", "0", "dcrc32", "=", "0", "inflate", "=", "zlib", ".", "decompressobj", "(", "-", "15", ")", "# header", "header", "=", "next", "(", "self", ".", "__info_plain_gen", "(", ")", ")", "if", "not", "header", ".", "startswith", "(", "\"=ybegin\"", ")", ":", "raise", "NNTPDataError", "(", "\"Bad yEnc header\"", ")", "# data", "buf", ",", "trailer", "=", "fifo", ".", "Fifo", "(", ")", ",", "\"\"", "for", "line", "in", "self", ".", "__info_plain_gen", "(", ")", ":", "if", "line", ".", "startswith", "(", "\"=yend\"", ")", ":", "trailer", "=", "line", "continue", "data", ",", "escape", ",", "dcrc32", "=", "yenc", ".", "decode", "(", "line", ",", "escape", ",", "dcrc32", ")", "try", ":", "data", "=", "inflate", ".", "decompress", "(", "data", ")", "except", "zlib", ".", "error", ":", "raise", "NNTPDataError", "(", "\"Decompression failed\"", ")", "if", "not", "data", ":", "continue", "buf", ".", "write", "(", "data", ")", "for", "l", "in", "buf", ":", "yield", "l", "# trailer", "if", "not", "trailer", ":", "raise", "NNTPDataError", "(", "\"Missing yEnc trailer\"", ")", "# expected crc32", "ecrc32", "=", "yenc", ".", "crc32", "(", "trailer", ")", "if", "ecrc32", "is", "None", ":", "raise", "NNTPDataError", "(", "\"Bad yEnc trailer\"", ")", "# check crc32", "if", "ecrc32", "!=", "dcrc32", "&", "0xffffffff", ":", "raise", "NNTPDataError", "(", "\"Bad yEnc CRC\"", ")"], "docstring": "Generator for the lines of a compressed info (textual) response.\n\n        Compressed responses are an extension to the NNTP protocol supported by\n        some usenet servers to reduce the bandwidth of heavily used range style\n        commands that can return large amounts of textual data. The server\n        returns that same data as it would for the uncompressed versions of the\n        command the difference being that the data is zlib deflated and then\n        yEnc encoded.\n\n        This function will produce that same output as the info_gen()\n        function. In other words it takes care of decoding and decompression.\n\n        Yields:\n            A line of the info response.\n\n        Raises:\n            NNTPError: If data is required to be read from the socket and fails.\n            NNTPDataError: When there is an error parsing the yEnc header or\n                trailer, if the CRC check fails or decompressing data fails.", "docstring_tokens": ["Generator", "for", "the", "lines", "of", "a", "compressed", "info", "(", "textual", ")", "response", "."], "sha": "991a76331cdf5d8f9dbf5b18f6e29adc80749a2f", "url": "https://github.com/greenbender/pynntp/blob/991a76331cdf5d8f9dbf5b18f6e29adc80749a2f/nntp/nntp.py#L303-L362", "partition": "test"}
{"repo": "chrisrink10/basilisp", "path": "src/basilisp/lang/compiler/generator.py", "func_name": "_load_attr", "original_string": "def _load_attr(name: str, ctx: ast.AST = ast.Load()) -> ast.Attribute:\n    \"\"\"Generate recursive Python Attribute AST nodes for resolving nested\n    names.\"\"\"\n    attrs = name.split(\".\")\n\n    def attr_node(node, idx):\n        if idx >= len(attrs):\n            node.ctx = ctx\n            return node\n        return attr_node(\n            ast.Attribute(value=node, attr=attrs[idx], ctx=ast.Load()), idx + 1\n        )\n\n    return attr_node(ast.Name(id=attrs[0], ctx=ast.Load()), 1)", "language": "python", "code": "def _load_attr(name: str, ctx: ast.AST = ast.Load()) -> ast.Attribute:\n    \"\"\"Generate recursive Python Attribute AST nodes for resolving nested\n    names.\"\"\"\n    attrs = name.split(\".\")\n\n    def attr_node(node, idx):\n        if idx >= len(attrs):\n            node.ctx = ctx\n            return node\n        return attr_node(\n            ast.Attribute(value=node, attr=attrs[idx], ctx=ast.Load()), idx + 1\n        )\n\n    return attr_node(ast.Name(id=attrs[0], ctx=ast.Load()), 1)", "code_tokens": ["def", "_load_attr", "(", "name", ":", "str", ",", "ctx", ":", "ast", ".", "AST", "=", "ast", ".", "Load", "(", ")", ")", "->", "ast", ".", "Attribute", ":", "attrs", "=", "name", ".", "split", "(", "\".\"", ")", "def", "attr_node", "(", "node", ",", "idx", ")", ":", "if", "idx", ">=", "len", "(", "attrs", ")", ":", "node", ".", "ctx", "=", "ctx", "return", "node", "return", "attr_node", "(", "ast", ".", "Attribute", "(", "value", "=", "node", ",", "attr", "=", "attrs", "[", "idx", "]", ",", "ctx", "=", "ast", ".", "Load", "(", ")", ")", ",", "idx", "+", "1", ")", "return", "attr_node", "(", "ast", ".", "Name", "(", "id", "=", "attrs", "[", "0", "]", ",", "ctx", "=", "ast", ".", "Load", "(", ")", ")", ",", "1", ")"], "docstring": "Generate recursive Python Attribute AST nodes for resolving nested\n    names.", "docstring_tokens": ["Generate", "recursive", "Python", "Attribute", "AST", "nodes", "for", "resolving", "nested", "names", "."], "sha": "3d82670ee218ec64eb066289c82766d14d18cc92", "url": "https://github.com/chrisrink10/basilisp/blob/3d82670ee218ec64eb066289c82766d14d18cc92/src/basilisp/lang/compiler/generator.py#L301-L314", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/lxml/html/__init__.py", "func_name": "open_in_browser", "original_string": "def open_in_browser(doc, encoding=None):\n    \"\"\"\n    Open the HTML document in a web browser, saving it to a temporary\n    file to open it.  Note that this does not delete the file after\n    use.  This is mainly meant for debugging.\n    \"\"\"\n    import os\n    import webbrowser\n    import tempfile\n    if not isinstance(doc, etree._ElementTree):\n        doc = etree.ElementTree(doc)\n    handle, fn = tempfile.mkstemp(suffix='.html')\n    f = os.fdopen(handle, 'wb')\n    try:\n        doc.write(f, method=\"html\", encoding=encoding or doc.docinfo.encoding or \"UTF-8\")\n    finally:\n        # we leak the file itself here, but we should at least close it\n        f.close()\n    url = 'file://' + fn.replace(os.path.sep, '/')\n    print(url)\n    webbrowser.open(url)", "language": "python", "code": "def open_in_browser(doc, encoding=None):\n    \"\"\"\n    Open the HTML document in a web browser, saving it to a temporary\n    file to open it.  Note that this does not delete the file after\n    use.  This is mainly meant for debugging.\n    \"\"\"\n    import os\n    import webbrowser\n    import tempfile\n    if not isinstance(doc, etree._ElementTree):\n        doc = etree.ElementTree(doc)\n    handle, fn = tempfile.mkstemp(suffix='.html')\n    f = os.fdopen(handle, 'wb')\n    try:\n        doc.write(f, method=\"html\", encoding=encoding or doc.docinfo.encoding or \"UTF-8\")\n    finally:\n        # we leak the file itself here, but we should at least close it\n        f.close()\n    url = 'file://' + fn.replace(os.path.sep, '/')\n    print(url)\n    webbrowser.open(url)", "code_tokens": ["def", "open_in_browser", "(", "doc", ",", "encoding", "=", "None", ")", ":", "import", "os", "import", "webbrowser", "import", "tempfile", "if", "not", "isinstance", "(", "doc", ",", "etree", ".", "_ElementTree", ")", ":", "doc", "=", "etree", ".", "ElementTree", "(", "doc", ")", "handle", ",", "fn", "=", "tempfile", ".", "mkstemp", "(", "suffix", "=", "'.html'", ")", "f", "=", "os", ".", "fdopen", "(", "handle", ",", "'wb'", ")", "try", ":", "doc", ".", "write", "(", "f", ",", "method", "=", "\"html\"", ",", "encoding", "=", "encoding", "or", "doc", ".", "docinfo", ".", "encoding", "or", "\"UTF-8\"", ")", "finally", ":", "# we leak the file itself here, but we should at least close it", "f", ".", "close", "(", ")", "url", "=", "'file://'", "+", "fn", ".", "replace", "(", "os", ".", "path", ".", "sep", ",", "'/'", ")", "print", "(", "url", ")", "webbrowser", ".", "open", "(", "url", ")"], "docstring": "Open the HTML document in a web browser, saving it to a temporary\n    file to open it.  Note that this does not delete the file after\n    use.  This is mainly meant for debugging.", "docstring_tokens": ["Open", "the", "HTML", "document", "in", "a", "web", "browser", "saving", "it", "to", "a", "temporary", "file", "to", "open", "it", ".", "Note", "that", "this", "does", "not", "delete", "the", "file", "after", "use", ".", "This", "is", "mainly", "meant", "for", "debugging", "."], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/lxml/html/__init__.py#L1653-L1673", "partition": "test"}
{"repo": "deepmipt/DeepPavlov", "path": "deeppavlov/core/common/check_gpu.py", "func_name": "check_gpu_existence", "original_string": "def check_gpu_existence():\n    r\"\"\"Return True if at least one GPU is available\"\"\"\n    global _gpu_available\n    if _gpu_available is None:\n        sess_config = tf.ConfigProto()\n        sess_config.gpu_options.allow_growth = True\n        try:\n            with tf.Session(config=sess_config):\n                device_list = device_lib.list_local_devices()\n                _gpu_available = any(device.device_type == 'GPU' for device in device_list)\n        except AttributeError as e:\n            log.warning(f'Got an AttributeError `{e}`, assuming documentation building')\n            _gpu_available = False\n    return _gpu_available", "language": "python", "code": "def check_gpu_existence():\n    r\"\"\"Return True if at least one GPU is available\"\"\"\n    global _gpu_available\n    if _gpu_available is None:\n        sess_config = tf.ConfigProto()\n        sess_config.gpu_options.allow_growth = True\n        try:\n            with tf.Session(config=sess_config):\n                device_list = device_lib.list_local_devices()\n                _gpu_available = any(device.device_type == 'GPU' for device in device_list)\n        except AttributeError as e:\n            log.warning(f'Got an AttributeError `{e}`, assuming documentation building')\n            _gpu_available = False\n    return _gpu_available", "code_tokens": ["def", "check_gpu_existence", "(", ")", ":", "global", "_gpu_available", "if", "_gpu_available", "is", "None", ":", "sess_config", "=", "tf", ".", "ConfigProto", "(", ")", "sess_config", ".", "gpu_options", ".", "allow_growth", "=", "True", "try", ":", "with", "tf", ".", "Session", "(", "config", "=", "sess_config", ")", ":", "device_list", "=", "device_lib", ".", "list_local_devices", "(", ")", "_gpu_available", "=", "any", "(", "device", ".", "device_type", "==", "'GPU'", "for", "device", "in", "device_list", ")", "except", "AttributeError", "as", "e", ":", "log", ".", "warning", "(", "f'Got an AttributeError `{e}`, assuming documentation building'", ")", "_gpu_available", "=", "False", "return", "_gpu_available"], "docstring": "r\"\"\"Return True if at least one GPU is available", "docstring_tokens": ["r", "Return", "True", "if", "at", "least", "one", "GPU", "is", "available"], "sha": "f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c", "url": "https://github.com/deepmipt/DeepPavlov/blob/f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c/deeppavlov/core/common/check_gpu.py#L26-L39", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval-mozilla", "path": "perceval/backends/mozilla/crates.py", "func_name": "Crates.__fetch_crate_data", "original_string": "def __fetch_crate_data(self, crate_id):\n        \"\"\"Get crate data\"\"\"\n\n        raw_crate = self.client.crate(crate_id)\n\n        crate = json.loads(raw_crate)\n        return crate['crate']", "language": "python", "code": "def __fetch_crate_data(self, crate_id):\n        \"\"\"Get crate data\"\"\"\n\n        raw_crate = self.client.crate(crate_id)\n\n        crate = json.loads(raw_crate)\n        return crate['crate']", "code_tokens": ["def", "__fetch_crate_data", "(", "self", ",", "crate_id", ")", ":", "raw_crate", "=", "self", ".", "client", ".", "crate", "(", "crate_id", ")", "crate", "=", "json", ".", "loads", "(", "raw_crate", ")", "return", "crate", "[", "'crate'", "]"], "docstring": "Get crate data", "docstring_tokens": ["Get", "crate", "data"], "sha": "4514f8d3d609d3cb79d83c72d51fcc4b4a7daeb4", "url": "https://github.com/chaoss/grimoirelab-perceval-mozilla/blob/4514f8d3d609d3cb79d83c72d51fcc4b4a7daeb4/perceval/backends/mozilla/crates.py#L240-L246", "partition": "test"}
{"repo": "open-mmlab/mmcv", "path": "mmcv/runner/runner.py", "func_name": "Runner.current_lr", "original_string": "def current_lr(self):\n        \"\"\"Get current learning rates.\n\n        Returns:\n            list: Current learning rate of all param groups.\n        \"\"\"\n        if self.optimizer is None:\n            raise RuntimeError(\n                'lr is not applicable because optimizer does not exist.')\n        return [group['lr'] for group in self.optimizer.param_groups]", "language": "python", "code": "def current_lr(self):\n        \"\"\"Get current learning rates.\n\n        Returns:\n            list: Current learning rate of all param groups.\n        \"\"\"\n        if self.optimizer is None:\n            raise RuntimeError(\n                'lr is not applicable because optimizer does not exist.')\n        return [group['lr'] for group in self.optimizer.param_groups]", "code_tokens": ["def", "current_lr", "(", "self", ")", ":", "if", "self", ".", "optimizer", "is", "None", ":", "raise", "RuntimeError", "(", "'lr is not applicable because optimizer does not exist.'", ")", "return", "[", "group", "[", "'lr'", "]", "for", "group", "in", "self", ".", "optimizer", ".", "param_groups", "]"], "docstring": "Get current learning rates.\n\n        Returns:\n            list: Current learning rate of all param groups.", "docstring_tokens": ["Get", "current", "learning", "rates", "."], "sha": "0d77f61450aab4dde8b8585a577cc496acb95d7f", "url": "https://github.com/open-mmlab/mmcv/blob/0d77f61450aab4dde8b8585a577cc496acb95d7f/mmcv/runner/runner.py#L183-L192", "partition": "test"}
{"repo": "draios/python-sdc-client", "path": "sdcclient/_common.py", "func_name": "_SdcCommon.delete_team", "original_string": "def delete_team(self, name):\n        '''**Description**\n            Deletes a team from Sysdig Monitor.\n\n        **Arguments**\n            - **name**: the name of the team that will be deleted from Sysdig Monitor\n\n        **Example**\n            `examples/user_team_mgmt.py <https://github.com/draios/python-sdc-client/blob/master/examples/user_team_mgmt.py>`_\n        '''\n        res = self.get_team(name)\n        if res[0] == False:\n            return res\n\n        t = res[1]\n        res = requests.delete(self.url + '/api/teams/' + str(t['id']), headers=self.hdrs, verify=self.ssl_verify)\n        if not self._checkResponse(res):\n            return [False, self.lasterr]\n        return [True, None]", "language": "python", "code": "def delete_team(self, name):\n        '''**Description**\n            Deletes a team from Sysdig Monitor.\n\n        **Arguments**\n            - **name**: the name of the team that will be deleted from Sysdig Monitor\n\n        **Example**\n            `examples/user_team_mgmt.py <https://github.com/draios/python-sdc-client/blob/master/examples/user_team_mgmt.py>`_\n        '''\n        res = self.get_team(name)\n        if res[0] == False:\n            return res\n\n        t = res[1]\n        res = requests.delete(self.url + '/api/teams/' + str(t['id']), headers=self.hdrs, verify=self.ssl_verify)\n        if not self._checkResponse(res):\n            return [False, self.lasterr]\n        return [True, None]", "code_tokens": ["def", "delete_team", "(", "self", ",", "name", ")", ":", "res", "=", "self", ".", "get_team", "(", "name", ")", "if", "res", "[", "0", "]", "==", "False", ":", "return", "res", "t", "=", "res", "[", "1", "]", "res", "=", "requests", ".", "delete", "(", "self", ".", "url", "+", "'/api/teams/'", "+", "str", "(", "t", "[", "'id'", "]", ")", ",", "headers", "=", "self", ".", "hdrs", ",", "verify", "=", "self", ".", "ssl_verify", ")", "if", "not", "self", ".", "_checkResponse", "(", "res", ")", ":", "return", "[", "False", ",", "self", ".", "lasterr", "]", "return", "[", "True", ",", "None", "]"], "docstring": "**Description**\n            Deletes a team from Sysdig Monitor.\n\n        **Arguments**\n            - **name**: the name of the team that will be deleted from Sysdig Monitor\n\n        **Example**\n            `examples/user_team_mgmt.py <https://github.com/draios/python-sdc-client/blob/master/examples/user_team_mgmt.py>`_", "docstring_tokens": ["**", "Description", "**", "Deletes", "a", "team", "from", "Sysdig", "Monitor", "."], "sha": "47f83415842048778939b90944f64386a3bcb205", "url": "https://github.com/draios/python-sdc-client/blob/47f83415842048778939b90944f64386a3bcb205/sdcclient/_common.py#L877-L895", "partition": "test"}
{"repo": "MisterY/price-database", "path": "pricedb/app.py", "func_name": "PriceDbApplication.__download_price", "original_string": "def __download_price(self, symbol: str, currency: str, agent: str):\n        \"\"\" Downloads and parses the price \"\"\"\n        from finance_quote_python import Quote\n\n        assert isinstance(symbol, str)\n        assert isinstance(currency, str)\n        assert isinstance(agent, str)\n\n        if not symbol:\n            return None\n\n        #self.logger.info(f\"Downloading {symbol}... \")\n\n        dl = Quote()\n        dl.logger = self.logger\n\n        dl.set_source(agent)\n        dl.set_currency(currency)\n\n        result = dl.fetch(agent, [symbol])\n\n        if not result:\n            raise ValueError(f\"Did not receive a response for {symbol}.\")\n\n        price = result[0]\n\n        if not price:\n            raise ValueError(f\"Price not downloaded/parsed for {symbol}.\")\n        else:\n            # Create price data entity, to be inserted.\n            self.add_price(price)\n\n        return price", "language": "python", "code": "def __download_price(self, symbol: str, currency: str, agent: str):\n        \"\"\" Downloads and parses the price \"\"\"\n        from finance_quote_python import Quote\n\n        assert isinstance(symbol, str)\n        assert isinstance(currency, str)\n        assert isinstance(agent, str)\n\n        if not symbol:\n            return None\n\n        #self.logger.info(f\"Downloading {symbol}... \")\n\n        dl = Quote()\n        dl.logger = self.logger\n\n        dl.set_source(agent)\n        dl.set_currency(currency)\n\n        result = dl.fetch(agent, [symbol])\n\n        if not result:\n            raise ValueError(f\"Did not receive a response for {symbol}.\")\n\n        price = result[0]\n\n        if not price:\n            raise ValueError(f\"Price not downloaded/parsed for {symbol}.\")\n        else:\n            # Create price data entity, to be inserted.\n            self.add_price(price)\n\n        return price", "code_tokens": ["def", "__download_price", "(", "self", ",", "symbol", ":", "str", ",", "currency", ":", "str", ",", "agent", ":", "str", ")", ":", "from", "finance_quote_python", "import", "Quote", "assert", "isinstance", "(", "symbol", ",", "str", ")", "assert", "isinstance", "(", "currency", ",", "str", ")", "assert", "isinstance", "(", "agent", ",", "str", ")", "if", "not", "symbol", ":", "return", "None", "#self.logger.info(f\"Downloading {symbol}... \")", "dl", "=", "Quote", "(", ")", "dl", ".", "logger", "=", "self", ".", "logger", "dl", ".", "set_source", "(", "agent", ")", "dl", ".", "set_currency", "(", "currency", ")", "result", "=", "dl", ".", "fetch", "(", "agent", ",", "[", "symbol", "]", ")", "if", "not", "result", ":", "raise", "ValueError", "(", "f\"Did not receive a response for {symbol}.\"", ")", "price", "=", "result", "[", "0", "]", "if", "not", "price", ":", "raise", "ValueError", "(", "f\"Price not downloaded/parsed for {symbol}.\"", ")", "else", ":", "# Create price data entity, to be inserted.", "self", ".", "add_price", "(", "price", ")", "return", "price"], "docstring": "Downloads and parses the price", "docstring_tokens": ["Downloads", "and", "parses", "the", "price"], "sha": "b4fd366b7763891c690fe3000b8840e656da023e", "url": "https://github.com/MisterY/price-database/blob/b4fd366b7763891c690fe3000b8840e656da023e/pricedb/app.py#L289-L321", "partition": "test"}
{"repo": "codeghar/brokerlso", "path": "brokerlso/qmfv2.py", "func_name": "RequestCmd.purge_queue", "original_string": "def purge_queue(self, name):\n        \"\"\"Create message content and properties to purge queue with QMFv2\n\n        :param name: Name of queue to purge\n        :type name: str\n\n        :returns: Tuple containing content and method properties\n        \"\"\"\n        content = {\"_object_id\": {\"_object_name\": \"org.apache.qpid.broker:queue:{0}\".format(name)},\n                   \"_method_name\": \"purge\",\n                   \"_arguments\": {\"type\": \"queue\",\n                                  \"name\": name,\n                                  \"filter\": dict()}}\n        logger.debug(\"Message content -> {0}\".format(content))\n\n        return content, self.method_properties", "language": "python", "code": "def purge_queue(self, name):\n        \"\"\"Create message content and properties to purge queue with QMFv2\n\n        :param name: Name of queue to purge\n        :type name: str\n\n        :returns: Tuple containing content and method properties\n        \"\"\"\n        content = {\"_object_id\": {\"_object_name\": \"org.apache.qpid.broker:queue:{0}\".format(name)},\n                   \"_method_name\": \"purge\",\n                   \"_arguments\": {\"type\": \"queue\",\n                                  \"name\": name,\n                                  \"filter\": dict()}}\n        logger.debug(\"Message content -> {0}\".format(content))\n\n        return content, self.method_properties", "code_tokens": ["def", "purge_queue", "(", "self", ",", "name", ")", ":", "content", "=", "{", "\"_object_id\"", ":", "{", "\"_object_name\"", ":", "\"org.apache.qpid.broker:queue:{0}\"", ".", "format", "(", "name", ")", "}", ",", "\"_method_name\"", ":", "\"purge\"", ",", "\"_arguments\"", ":", "{", "\"type\"", ":", "\"queue\"", ",", "\"name\"", ":", "name", ",", "\"filter\"", ":", "dict", "(", ")", "}", "}", "logger", ".", "debug", "(", "\"Message content -> {0}\"", ".", "format", "(", "content", ")", ")", "return", "content", ",", "self", ".", "method_properties"], "docstring": "Create message content and properties to purge queue with QMFv2\n\n        :param name: Name of queue to purge\n        :type name: str\n\n        :returns: Tuple containing content and method properties", "docstring_tokens": ["Create", "message", "content", "and", "properties", "to", "purge", "queue", "with", "QMFv2"], "sha": "e110e12502b090e12b06c7615dd0a96a14a92585", "url": "https://github.com/codeghar/brokerlso/blob/e110e12502b090e12b06c7615dd0a96a14a92585/brokerlso/qmfv2.py#L179-L194", "partition": "test"}
{"repo": "not-na/peng3d", "path": "peng3d/actor/player.py", "func_name": "FourDirectionalMoveController.get_motion_vector", "original_string": "def get_motion_vector(self):\n        \"\"\"\n        Returns the movement vector according to held buttons and the rotation.\n        \n        :return: 3-Tuple of ``(dx,dy,dz)``\n        :rtype: tuple\n        \"\"\"\n        if any(self.move):\n            x, y = self.actor._rot\n            strafe = math.degrees(math.atan2(*self.move))\n            y_angle = math.radians(y)\n            x_angle = math.radians(x + strafe)\n            dy = 0.0\n            dx = math.cos(x_angle)\n            dz = math.sin(x_angle)\n        else:\n            dy = 0.0\n            dx = 0.0\n            dz = 0.0\n        return (dx, dy, dz)", "language": "python", "code": "def get_motion_vector(self):\n        \"\"\"\n        Returns the movement vector according to held buttons and the rotation.\n        \n        :return: 3-Tuple of ``(dx,dy,dz)``\n        :rtype: tuple\n        \"\"\"\n        if any(self.move):\n            x, y = self.actor._rot\n            strafe = math.degrees(math.atan2(*self.move))\n            y_angle = math.radians(y)\n            x_angle = math.radians(x + strafe)\n            dy = 0.0\n            dx = math.cos(x_angle)\n            dz = math.sin(x_angle)\n        else:\n            dy = 0.0\n            dx = 0.0\n            dz = 0.0\n        return (dx, dy, dz)", "code_tokens": ["def", "get_motion_vector", "(", "self", ")", ":", "if", "any", "(", "self", ".", "move", ")", ":", "x", ",", "y", "=", "self", ".", "actor", ".", "_rot", "strafe", "=", "math", ".", "degrees", "(", "math", ".", "atan2", "(", "*", "self", ".", "move", ")", ")", "y_angle", "=", "math", ".", "radians", "(", "y", ")", "x_angle", "=", "math", ".", "radians", "(", "x", "+", "strafe", ")", "dy", "=", "0.0", "dx", "=", "math", ".", "cos", "(", "x_angle", ")", "dz", "=", "math", ".", "sin", "(", "x_angle", ")", "else", ":", "dy", "=", "0.0", "dx", "=", "0.0", "dz", "=", "0.0", "return", "(", "dx", ",", "dy", ",", "dz", ")"], "docstring": "Returns the movement vector according to held buttons and the rotation.\n        \n        :return: 3-Tuple of ``(dx,dy,dz)``\n        :rtype: tuple", "docstring_tokens": ["Returns", "the", "movement", "vector", "according", "to", "held", "buttons", "and", "the", "rotation", ".", ":", "return", ":", "3", "-", "Tuple", "of", "(", "dx", "dy", "dz", ")", ":", "rtype", ":", "tuple"], "sha": "1151be665b26cc8a479f6307086ba919e4d32d85", "url": "https://github.com/not-na/peng3d/blob/1151be665b26cc8a479f6307086ba919e4d32d85/peng3d/actor/player.py#L85-L104", "partition": "test"}
{"repo": "capitalone/giraffez", "path": "giraffez/load.py", "func_name": "TeradataBulkLoad.from_file", "original_string": "def from_file(self, filename, table=None, delimiter='|', null='NULL',\n            panic=True, quotechar='\"', parse_dates=False):\n        \"\"\"\n        Load from a file into the target table, handling each step of the\n        load process.\n\n        Can load from text files, and properly formatted giraffez archive\n        files. In both cases, if Gzip compression is detected the file will be\n        decompressed while reading and handled appropriately. The encoding is\n        determined automatically by the contents of the file.\n\n        It is not necessary to set the columns in use prior to loading from a file.\n        In the case of a text file, the header is used to determine column names\n        and their order. Valid delimiters include '|', ',', and '\\\\t' (tab). When\n        loading an archive file, the column information is decoded alongside the data.\n\n        :param str filename: The location of the file to be loaded\n        :param str table: The name of the target table, if it was not specified\n            to the constructor for the isntance\n        :param str null: The string that indicates a null value in the rows being\n            inserted from a file. Defaults to 'NULL'\n        :param str delimiter: When loading a file, indicates that fields are\n            separated by this delimiter. Defaults to :code:`None`, which causes the\n            delimiter to be determined from the header of the file. In most\n            cases, this behavior is sufficient\n        :param str quotechar: The character used to quote fields containing special characters,\n            like the delimiter.\n        :param bool panic: If :code:`True`, when an error is encountered it will be\n            raised. Otherwise, the error will be logged and :code:`self.error_count`\n            is incremented.\n        :return: The output of the call to\n            :meth:`~giraffez.load.TeradataBulkLoad.finish`\n        :raises `giraffez.errors.GiraffeError`: if table was not set and :code:`table`\n            is :code:`None`, or if a Teradata error ocurred while retrieving table info.\n        :raises `giraffez.errors.GiraffeEncodeError`: if :code:`panic` is :code:`True` and there\n            are format errors in the row values.\n        \"\"\"\n        if not self.table:\n            if not table:\n                raise GiraffeError(\"Table must be set or specified to load a file.\")\n            self.table = table\n        if not isinstance(null, basestring):\n            raise GiraffeError(\"Expected 'null' to be str, received {}\".format(type(null)))\n        with Reader(filename, delimiter=delimiter, quotechar=quotechar) as f:\n            if not isinstance(f.delimiter, basestring):\n                raise GiraffeError(\"Expected 'delimiter' to be str, received {}\".format(type(delimiter)))\n            self.columns = f.header\n            if isinstance(f, ArchiveFileReader):\n                self.mload.set_encoding(ROW_ENCODING_RAW)\n                self.preprocessor = lambda s: s\n            if parse_dates:\n                self.preprocessor = DateHandler(self.columns)\n            self._initiate()\n            self.mload.set_null(null)\n            self.mload.set_delimiter(delimiter)\n            i = 0\n            for i, line in enumerate(f, 1):\n                self.put(line, panic=panic)\n                if i % self.checkpoint_interval == 1:\n                    log.info(\"\\rBulkLoad\", \"Processed {} rows\".format(i), console=True)\n                    checkpoint_status = self.checkpoint()\n                    self.exit_code = self._exit_code()\n                    if self.exit_code != 0:\n                        return self.exit_code\n            log.info(\"\\rBulkLoad\", \"Processed {} rows\".format(i))\n            return self.finish()", "language": "python", "code": "def from_file(self, filename, table=None, delimiter='|', null='NULL',\n            panic=True, quotechar='\"', parse_dates=False):\n        \"\"\"\n        Load from a file into the target table, handling each step of the\n        load process.\n\n        Can load from text files, and properly formatted giraffez archive\n        files. In both cases, if Gzip compression is detected the file will be\n        decompressed while reading and handled appropriately. The encoding is\n        determined automatically by the contents of the file.\n\n        It is not necessary to set the columns in use prior to loading from a file.\n        In the case of a text file, the header is used to determine column names\n        and their order. Valid delimiters include '|', ',', and '\\\\t' (tab). When\n        loading an archive file, the column information is decoded alongside the data.\n\n        :param str filename: The location of the file to be loaded\n        :param str table: The name of the target table, if it was not specified\n            to the constructor for the isntance\n        :param str null: The string that indicates a null value in the rows being\n            inserted from a file. Defaults to 'NULL'\n        :param str delimiter: When loading a file, indicates that fields are\n            separated by this delimiter. Defaults to :code:`None`, which causes the\n            delimiter to be determined from the header of the file. In most\n            cases, this behavior is sufficient\n        :param str quotechar: The character used to quote fields containing special characters,\n            like the delimiter.\n        :param bool panic: If :code:`True`, when an error is encountered it will be\n            raised. Otherwise, the error will be logged and :code:`self.error_count`\n            is incremented.\n        :return: The output of the call to\n            :meth:`~giraffez.load.TeradataBulkLoad.finish`\n        :raises `giraffez.errors.GiraffeError`: if table was not set and :code:`table`\n            is :code:`None`, or if a Teradata error ocurred while retrieving table info.\n        :raises `giraffez.errors.GiraffeEncodeError`: if :code:`panic` is :code:`True` and there\n            are format errors in the row values.\n        \"\"\"\n        if not self.table:\n            if not table:\n                raise GiraffeError(\"Table must be set or specified to load a file.\")\n            self.table = table\n        if not isinstance(null, basestring):\n            raise GiraffeError(\"Expected 'null' to be str, received {}\".format(type(null)))\n        with Reader(filename, delimiter=delimiter, quotechar=quotechar) as f:\n            if not isinstance(f.delimiter, basestring):\n                raise GiraffeError(\"Expected 'delimiter' to be str, received {}\".format(type(delimiter)))\n            self.columns = f.header\n            if isinstance(f, ArchiveFileReader):\n                self.mload.set_encoding(ROW_ENCODING_RAW)\n                self.preprocessor = lambda s: s\n            if parse_dates:\n                self.preprocessor = DateHandler(self.columns)\n            self._initiate()\n            self.mload.set_null(null)\n            self.mload.set_delimiter(delimiter)\n            i = 0\n            for i, line in enumerate(f, 1):\n                self.put(line, panic=panic)\n                if i % self.checkpoint_interval == 1:\n                    log.info(\"\\rBulkLoad\", \"Processed {} rows\".format(i), console=True)\n                    checkpoint_status = self.checkpoint()\n                    self.exit_code = self._exit_code()\n                    if self.exit_code != 0:\n                        return self.exit_code\n            log.info(\"\\rBulkLoad\", \"Processed {} rows\".format(i))\n            return self.finish()", "code_tokens": ["def", "from_file", "(", "self", ",", "filename", ",", "table", "=", "None", ",", "delimiter", "=", "'|'", ",", "null", "=", "'NULL'", ",", "panic", "=", "True", ",", "quotechar", "=", "'\"'", ",", "parse_dates", "=", "False", ")", ":", "if", "not", "self", ".", "table", ":", "if", "not", "table", ":", "raise", "GiraffeError", "(", "\"Table must be set or specified to load a file.\"", ")", "self", ".", "table", "=", "table", "if", "not", "isinstance", "(", "null", ",", "basestring", ")", ":", "raise", "GiraffeError", "(", "\"Expected 'null' to be str, received {}\"", ".", "format", "(", "type", "(", "null", ")", ")", ")", "with", "Reader", "(", "filename", ",", "delimiter", "=", "delimiter", ",", "quotechar", "=", "quotechar", ")", "as", "f", ":", "if", "not", "isinstance", "(", "f", ".", "delimiter", ",", "basestring", ")", ":", "raise", "GiraffeError", "(", "\"Expected 'delimiter' to be str, received {}\"", ".", "format", "(", "type", "(", "delimiter", ")", ")", ")", "self", ".", "columns", "=", "f", ".", "header", "if", "isinstance", "(", "f", ",", "ArchiveFileReader", ")", ":", "self", ".", "mload", ".", "set_encoding", "(", "ROW_ENCODING_RAW", ")", "self", ".", "preprocessor", "=", "lambda", "s", ":", "s", "if", "parse_dates", ":", "self", ".", "preprocessor", "=", "DateHandler", "(", "self", ".", "columns", ")", "self", ".", "_initiate", "(", ")", "self", ".", "mload", ".", "set_null", "(", "null", ")", "self", ".", "mload", ".", "set_delimiter", "(", "delimiter", ")", "i", "=", "0", "for", "i", ",", "line", "in", "enumerate", "(", "f", ",", "1", ")", ":", "self", ".", "put", "(", "line", ",", "panic", "=", "panic", ")", "if", "i", "%", "self", ".", "checkpoint_interval", "==", "1", ":", "log", ".", "info", "(", "\"\\rBulkLoad\"", ",", "\"Processed {} rows\"", ".", "format", "(", "i", ")", ",", "console", "=", "True", ")", "checkpoint_status", "=", "self", ".", "checkpoint", "(", ")", "self", ".", "exit_code", "=", "self", ".", "_exit_code", "(", ")", "if", "self", ".", "exit_code", "!=", "0", ":", "return", "self", ".", "exit_code", "log", ".", "info", "(", "\"\\rBulkLoad\"", ",", "\"Processed {} rows\"", ".", "format", "(", "i", ")", ")", "return", "self", ".", "finish", "(", ")"], "docstring": "Load from a file into the target table, handling each step of the\n        load process.\n\n        Can load from text files, and properly formatted giraffez archive\n        files. In both cases, if Gzip compression is detected the file will be\n        decompressed while reading and handled appropriately. The encoding is\n        determined automatically by the contents of the file.\n\n        It is not necessary to set the columns in use prior to loading from a file.\n        In the case of a text file, the header is used to determine column names\n        and their order. Valid delimiters include '|', ',', and '\\\\t' (tab). When\n        loading an archive file, the column information is decoded alongside the data.\n\n        :param str filename: The location of the file to be loaded\n        :param str table: The name of the target table, if it was not specified\n            to the constructor for the isntance\n        :param str null: The string that indicates a null value in the rows being\n            inserted from a file. Defaults to 'NULL'\n        :param str delimiter: When loading a file, indicates that fields are\n            separated by this delimiter. Defaults to :code:`None`, which causes the\n            delimiter to be determined from the header of the file. In most\n            cases, this behavior is sufficient\n        :param str quotechar: The character used to quote fields containing special characters,\n            like the delimiter.\n        :param bool panic: If :code:`True`, when an error is encountered it will be\n            raised. Otherwise, the error will be logged and :code:`self.error_count`\n            is incremented.\n        :return: The output of the call to\n            :meth:`~giraffez.load.TeradataBulkLoad.finish`\n        :raises `giraffez.errors.GiraffeError`: if table was not set and :code:`table`\n            is :code:`None`, or if a Teradata error ocurred while retrieving table info.\n        :raises `giraffez.errors.GiraffeEncodeError`: if :code:`panic` is :code:`True` and there\n            are format errors in the row values.", "docstring_tokens": ["Load", "from", "a", "file", "into", "the", "target", "table", "handling", "each", "step", "of", "the", "load", "process", "."], "sha": "6b4d27eb1a1eaf188c6885c7364ef27e92b1b957", "url": "https://github.com/capitalone/giraffez/blob/6b4d27eb1a1eaf188c6885c7364ef27e92b1b957/giraffez/load.py#L202-L267", "partition": "test"}
{"repo": "4degrees/riffle", "path": "source/riffle/model.py", "func_name": "Filesystem.fetchMore", "original_string": "def fetchMore(self, index):\n        '''Fetch additional data under *index*.'''\n        if not index.isValid():\n            item = self.root\n        else:\n            item = index.internalPointer()\n\n        if item.canFetchMore():\n            startIndex = len(item.children)\n            additionalChildren = item.fetchChildren()\n            endIndex = startIndex + len(additionalChildren) - 1\n            if endIndex >= startIndex:\n                self.beginInsertRows(index, startIndex, endIndex)\n                for newChild in additionalChildren:\n                    item.addChild(newChild)\n                self.endInsertRows()", "language": "python", "code": "def fetchMore(self, index):\n        '''Fetch additional data under *index*.'''\n        if not index.isValid():\n            item = self.root\n        else:\n            item = index.internalPointer()\n\n        if item.canFetchMore():\n            startIndex = len(item.children)\n            additionalChildren = item.fetchChildren()\n            endIndex = startIndex + len(additionalChildren) - 1\n            if endIndex >= startIndex:\n                self.beginInsertRows(index, startIndex, endIndex)\n                for newChild in additionalChildren:\n                    item.addChild(newChild)\n                self.endInsertRows()", "code_tokens": ["def", "fetchMore", "(", "self", ",", "index", ")", ":", "if", "not", "index", ".", "isValid", "(", ")", ":", "item", "=", "self", ".", "root", "else", ":", "item", "=", "index", ".", "internalPointer", "(", ")", "if", "item", ".", "canFetchMore", "(", ")", ":", "startIndex", "=", "len", "(", "item", ".", "children", ")", "additionalChildren", "=", "item", ".", "fetchChildren", "(", ")", "endIndex", "=", "startIndex", "+", "len", "(", "additionalChildren", ")", "-", "1", "if", "endIndex", ">=", "startIndex", ":", "self", ".", "beginInsertRows", "(", "index", ",", "startIndex", ",", "endIndex", ")", "for", "newChild", "in", "additionalChildren", ":", "item", ".", "addChild", "(", "newChild", ")", "self", ".", "endInsertRows", "(", ")"], "docstring": "Fetch additional data under *index*.", "docstring_tokens": ["Fetch", "additional", "data", "under", "*", "index", "*", "."], "sha": "e5a0d908df8c93ff1ee7abdda8875fd1667df53d", "url": "https://github.com/4degrees/riffle/blob/e5a0d908df8c93ff1ee7abdda8875fd1667df53d/source/riffle/model.py#L474-L489", "partition": "test"}
{"repo": "dreidev/Suggestions", "path": "suggestions/views.py", "func_name": "update_suggestions_dictionary", "original_string": "def update_suggestions_dictionary(request, object):\n    \"\"\"\n    Updates the suggestions' dictionary for an object upon visiting its page\n    \"\"\"\n    if request.user.is_authenticated():\n        user = request.user\n        content_type = ContentType.objects.get_for_model(type(object))\n        try:\n            # Check if the user has visited this page before\n            ObjectView.objects.get(\n                user=user, object_id=object.id, content_type=content_type)\n        except:\n            ObjectView.objects.create(user=user, content_object=object)\n        # Get a list of all the objects a user has visited\n        viewed = ObjectView.objects.filter(user=user)\n\n    else:\n        update_dict_for_guests(request, object, content_type)\n        return\n\n    if viewed:\n        for obj in viewed:\n            if content_type == obj.content_type:\n                if not exists_in_dictionary(request, object,\n                                            content_type,\n                                            obj, True):\n                    # Create an entry if it's non existent\n                    if object.id != obj.object_id:\n                        ObjectViewDictionary.objects.create(\n                            current_object=object,\n                            visited_before_object=obj.content_object)\n                        if not exists_in_dictionary(request, obj,\n                                                    obj.content_type,\n                                                    object, False):\n                            ObjectViewDictionary.objects.create(\n                                current_object=obj.content_object,\n                                visited_before_object=object)\n    return", "language": "python", "code": "def update_suggestions_dictionary(request, object):\n    \"\"\"\n    Updates the suggestions' dictionary for an object upon visiting its page\n    \"\"\"\n    if request.user.is_authenticated():\n        user = request.user\n        content_type = ContentType.objects.get_for_model(type(object))\n        try:\n            # Check if the user has visited this page before\n            ObjectView.objects.get(\n                user=user, object_id=object.id, content_type=content_type)\n        except:\n            ObjectView.objects.create(user=user, content_object=object)\n        # Get a list of all the objects a user has visited\n        viewed = ObjectView.objects.filter(user=user)\n\n    else:\n        update_dict_for_guests(request, object, content_type)\n        return\n\n    if viewed:\n        for obj in viewed:\n            if content_type == obj.content_type:\n                if not exists_in_dictionary(request, object,\n                                            content_type,\n                                            obj, True):\n                    # Create an entry if it's non existent\n                    if object.id != obj.object_id:\n                        ObjectViewDictionary.objects.create(\n                            current_object=object,\n                            visited_before_object=obj.content_object)\n                        if not exists_in_dictionary(request, obj,\n                                                    obj.content_type,\n                                                    object, False):\n                            ObjectViewDictionary.objects.create(\n                                current_object=obj.content_object,\n                                visited_before_object=object)\n    return", "code_tokens": ["def", "update_suggestions_dictionary", "(", "request", ",", "object", ")", ":", "if", "request", ".", "user", ".", "is_authenticated", "(", ")", ":", "user", "=", "request", ".", "user", "content_type", "=", "ContentType", ".", "objects", ".", "get_for_model", "(", "type", "(", "object", ")", ")", "try", ":", "# Check if the user has visited this page before", "ObjectView", ".", "objects", ".", "get", "(", "user", "=", "user", ",", "object_id", "=", "object", ".", "id", ",", "content_type", "=", "content_type", ")", "except", ":", "ObjectView", ".", "objects", ".", "create", "(", "user", "=", "user", ",", "content_object", "=", "object", ")", "# Get a list of all the objects a user has visited", "viewed", "=", "ObjectView", ".", "objects", ".", "filter", "(", "user", "=", "user", ")", "else", ":", "update_dict_for_guests", "(", "request", ",", "object", ",", "content_type", ")", "return", "if", "viewed", ":", "for", "obj", "in", "viewed", ":", "if", "content_type", "==", "obj", ".", "content_type", ":", "if", "not", "exists_in_dictionary", "(", "request", ",", "object", ",", "content_type", ",", "obj", ",", "True", ")", ":", "# Create an entry if it's non existent", "if", "object", ".", "id", "!=", "obj", ".", "object_id", ":", "ObjectViewDictionary", ".", "objects", ".", "create", "(", "current_object", "=", "object", ",", "visited_before_object", "=", "obj", ".", "content_object", ")", "if", "not", "exists_in_dictionary", "(", "request", ",", "obj", ",", "obj", ".", "content_type", ",", "object", ",", "False", ")", ":", "ObjectViewDictionary", ".", "objects", ".", "create", "(", "current_object", "=", "obj", ".", "content_object", ",", "visited_before_object", "=", "object", ")", "return"], "docstring": "Updates the suggestions' dictionary for an object upon visiting its page", "docstring_tokens": ["Updates", "the", "suggestions", "dictionary", "for", "an", "object", "upon", "visiting", "its", "page"], "sha": "f04c181dc815d32c35b44c6e1c91521e88a9dd6c", "url": "https://github.com/dreidev/Suggestions/blob/f04c181dc815d32c35b44c6e1c91521e88a9dd6c/suggestions/views.py#L5-L42", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/wheel/install.py", "func_name": "parse_version", "original_string": "def parse_version(version):\n    \"\"\"Use parse_version from pkg_resources or distutils as available.\"\"\"\n    global parse_version\n    try:\n        from pkg_resources import parse_version\n    except ImportError:\n        from distutils.version import LooseVersion as parse_version\n    return parse_version(version)", "language": "python", "code": "def parse_version(version):\n    \"\"\"Use parse_version from pkg_resources or distutils as available.\"\"\"\n    global parse_version\n    try:\n        from pkg_resources import parse_version\n    except ImportError:\n        from distutils.version import LooseVersion as parse_version\n    return parse_version(version)", "code_tokens": ["def", "parse_version", "(", "version", ")", ":", "global", "parse_version", "try", ":", "from", "pkg_resources", "import", "parse_version", "except", "ImportError", ":", "from", "distutils", ".", "version", "import", "LooseVersion", "as", "parse_version", "return", "parse_version", "(", "version", ")"], "docstring": "Use parse_version from pkg_resources or distutils as available.", "docstring_tokens": ["Use", "parse_version", "from", "pkg_resources", "or", "distutils", "as", "available", "."], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/wheel/install.py#L42-L49", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/sts/internal/util.py", "func_name": "sum_mvns", "original_string": "def sum_mvns(distributions):\n  \"\"\"Attempt to sum MultivariateNormal distributions.\n\n  The sum of (multivariate) normal random variables is itself (multivariate)\n  normal, with mean given by the sum of means and (co)variance given by the\n  sum of (co)variances. This method exploits this fact to compute the\n  sum of a list of `tfd.MultivariateNormalDiag` objects.\n\n  It may in the future be extended to support summation of other forms of\n  (Multivariate)Normal distributions.\n\n  Args:\n    distributions: Python `iterable` of `tfd.MultivariateNormalDiag`\n      distribution instances. These must all have the same event\n      shape, and broadcast to a consistent batch shape.\n\n  Returns:\n    sum_distribution: A `tfd.MultivariateNormalDiag` instance with mean\n      equal to the sum of input means and covariance equal to the sum of\n      input covariances.\n  \"\"\"\n\n  graph_parents = [tensor for distribution in distributions\n                   for tensor in distribution._graph_parents]  # pylint: disable=protected-access\n  with tf.compat.v1.name_scope('sum_mvns', values=graph_parents):\n    if all([isinstance(mvn, tfd.MultivariateNormalDiag)\n            for mvn in distributions]):\n      return tfd.MultivariateNormalDiag(\n          loc=sum([mvn.mean() for mvn in distributions]),\n          scale_diag=tf.sqrt(sum([\n              mvn.scale.diag**2 for mvn in distributions])))\n    else:\n      raise NotImplementedError(\n          'Sums of distributions other than MultivariateNormalDiag are not '\n          'currently implemented. (given: {})'.format(distributions))", "language": "python", "code": "def sum_mvns(distributions):\n  \"\"\"Attempt to sum MultivariateNormal distributions.\n\n  The sum of (multivariate) normal random variables is itself (multivariate)\n  normal, with mean given by the sum of means and (co)variance given by the\n  sum of (co)variances. This method exploits this fact to compute the\n  sum of a list of `tfd.MultivariateNormalDiag` objects.\n\n  It may in the future be extended to support summation of other forms of\n  (Multivariate)Normal distributions.\n\n  Args:\n    distributions: Python `iterable` of `tfd.MultivariateNormalDiag`\n      distribution instances. These must all have the same event\n      shape, and broadcast to a consistent batch shape.\n\n  Returns:\n    sum_distribution: A `tfd.MultivariateNormalDiag` instance with mean\n      equal to the sum of input means and covariance equal to the sum of\n      input covariances.\n  \"\"\"\n\n  graph_parents = [tensor for distribution in distributions\n                   for tensor in distribution._graph_parents]  # pylint: disable=protected-access\n  with tf.compat.v1.name_scope('sum_mvns', values=graph_parents):\n    if all([isinstance(mvn, tfd.MultivariateNormalDiag)\n            for mvn in distributions]):\n      return tfd.MultivariateNormalDiag(\n          loc=sum([mvn.mean() for mvn in distributions]),\n          scale_diag=tf.sqrt(sum([\n              mvn.scale.diag**2 for mvn in distributions])))\n    else:\n      raise NotImplementedError(\n          'Sums of distributions other than MultivariateNormalDiag are not '\n          'currently implemented. (given: {})'.format(distributions))", "code_tokens": ["def", "sum_mvns", "(", "distributions", ")", ":", "graph_parents", "=", "[", "tensor", "for", "distribution", "in", "distributions", "for", "tensor", "in", "distribution", ".", "_graph_parents", "]", "# pylint: disable=protected-access", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "'sum_mvns'", ",", "values", "=", "graph_parents", ")", ":", "if", "all", "(", "[", "isinstance", "(", "mvn", ",", "tfd", ".", "MultivariateNormalDiag", ")", "for", "mvn", "in", "distributions", "]", ")", ":", "return", "tfd", ".", "MultivariateNormalDiag", "(", "loc", "=", "sum", "(", "[", "mvn", ".", "mean", "(", ")", "for", "mvn", "in", "distributions", "]", ")", ",", "scale_diag", "=", "tf", ".", "sqrt", "(", "sum", "(", "[", "mvn", ".", "scale", ".", "diag", "**", "2", "for", "mvn", "in", "distributions", "]", ")", ")", ")", "else", ":", "raise", "NotImplementedError", "(", "'Sums of distributions other than MultivariateNormalDiag are not '", "'currently implemented. (given: {})'", ".", "format", "(", "distributions", ")", ")"], "docstring": "Attempt to sum MultivariateNormal distributions.\n\n  The sum of (multivariate) normal random variables is itself (multivariate)\n  normal, with mean given by the sum of means and (co)variance given by the\n  sum of (co)variances. This method exploits this fact to compute the\n  sum of a list of `tfd.MultivariateNormalDiag` objects.\n\n  It may in the future be extended to support summation of other forms of\n  (Multivariate)Normal distributions.\n\n  Args:\n    distributions: Python `iterable` of `tfd.MultivariateNormalDiag`\n      distribution instances. These must all have the same event\n      shape, and broadcast to a consistent batch shape.\n\n  Returns:\n    sum_distribution: A `tfd.MultivariateNormalDiag` instance with mean\n      equal to the sum of input means and covariance equal to the sum of\n      input covariances.", "docstring_tokens": ["Attempt", "to", "sum", "MultivariateNormal", "distributions", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/sts/internal/util.py#L164-L198", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/extensions/standard/crz.py", "func_name": "crz", "original_string": "def crz(self, theta, ctl, tgt):\n    \"\"\"Apply crz from ctl to tgt with angle theta.\"\"\"\n    return self.append(CrzGate(theta), [ctl, tgt], [])", "language": "python", "code": "def crz(self, theta, ctl, tgt):\n    \"\"\"Apply crz from ctl to tgt with angle theta.\"\"\"\n    return self.append(CrzGate(theta), [ctl, tgt], [])", "code_tokens": ["def", "crz", "(", "self", ",", "theta", ",", "ctl", ",", "tgt", ")", ":", "return", "self", ".", "append", "(", "CrzGate", "(", "theta", ")", ",", "[", "ctl", ",", "tgt", "]", ",", "[", "]", ")"], "docstring": "Apply crz from ctl to tgt with angle theta.", "docstring_tokens": ["Apply", "crz", "from", "ctl", "to", "tgt", "with", "angle", "theta", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/extensions/standard/crz.py#L53-L55", "partition": "test"}
{"repo": "xtuml/pyxtuml", "path": "bridgepoint/gen_xsd_schema.py", "func_name": "build_core_type", "original_string": "def build_core_type(s_cdt):\n    '''\n    Build an xsd simpleType out of a S_CDT.\n    '''\n    s_dt = nav_one(s_cdt).S_DT[17]()\n    \n    if s_dt.name == 'void':\n        type_name = None\n    \n    elif s_dt.name == 'boolean':\n        type_name = 'xs:boolean'\n    \n    elif s_dt.name == 'integer':\n        type_name = 'xs:integer'\n    \n    elif s_dt.name == 'real':\n        type_name = 'xs:decimal'\n    \n    elif s_dt.name == 'string':\n        type_name = 'xs:string'\n    \n    elif s_dt.name == 'unique_id':\n        type_name = 'xs:integer'\n    \n    else:\n        type_name = None\n    \n    if type_name:\n        mapped_type = ET.Element('xs:simpleType', name=s_dt.name)\n        ET.SubElement(mapped_type, 'xs:restriction', base=type_name)\n        return mapped_type", "language": "python", "code": "def build_core_type(s_cdt):\n    '''\n    Build an xsd simpleType out of a S_CDT.\n    '''\n    s_dt = nav_one(s_cdt).S_DT[17]()\n    \n    if s_dt.name == 'void':\n        type_name = None\n    \n    elif s_dt.name == 'boolean':\n        type_name = 'xs:boolean'\n    \n    elif s_dt.name == 'integer':\n        type_name = 'xs:integer'\n    \n    elif s_dt.name == 'real':\n        type_name = 'xs:decimal'\n    \n    elif s_dt.name == 'string':\n        type_name = 'xs:string'\n    \n    elif s_dt.name == 'unique_id':\n        type_name = 'xs:integer'\n    \n    else:\n        type_name = None\n    \n    if type_name:\n        mapped_type = ET.Element('xs:simpleType', name=s_dt.name)\n        ET.SubElement(mapped_type, 'xs:restriction', base=type_name)\n        return mapped_type", "code_tokens": ["def", "build_core_type", "(", "s_cdt", ")", ":", "s_dt", "=", "nav_one", "(", "s_cdt", ")", ".", "S_DT", "[", "17", "]", "(", ")", "if", "s_dt", ".", "name", "==", "'void'", ":", "type_name", "=", "None", "elif", "s_dt", ".", "name", "==", "'boolean'", ":", "type_name", "=", "'xs:boolean'", "elif", "s_dt", ".", "name", "==", "'integer'", ":", "type_name", "=", "'xs:integer'", "elif", "s_dt", ".", "name", "==", "'real'", ":", "type_name", "=", "'xs:decimal'", "elif", "s_dt", ".", "name", "==", "'string'", ":", "type_name", "=", "'xs:string'", "elif", "s_dt", ".", "name", "==", "'unique_id'", ":", "type_name", "=", "'xs:integer'", "else", ":", "type_name", "=", "None", "if", "type_name", ":", "mapped_type", "=", "ET", ".", "Element", "(", "'xs:simpleType'", ",", "name", "=", "s_dt", ".", "name", ")", "ET", ".", "SubElement", "(", "mapped_type", ",", "'xs:restriction'", ",", "base", "=", "type_name", ")", "return", "mapped_type"], "docstring": "Build an xsd simpleType out of a S_CDT.", "docstring_tokens": ["Build", "an", "xsd", "simpleType", "out", "of", "a", "S_CDT", "."], "sha": "7dd9343b9a0191d1db1887ab9288d0a026608d9a", "url": "https://github.com/xtuml/pyxtuml/blob/7dd9343b9a0191d1db1887ab9288d0a026608d9a/bridgepoint/gen_xsd_schema.py#L75-L105", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/util.py", "func_name": "_filter_deprecation_warnings", "original_string": "def _filter_deprecation_warnings():\n    \"\"\"Apply filters to deprecation warnings.\n\n    Force the `DeprecationWarning` warnings to be displayed for the qiskit\n    module, overriding the system configuration as they are ignored by default\n    [1] for end-users. Additionally, silence the `ChangedInMarshmallow3Warning`\n    messages.\n\n    TODO: on Python 3.7, this might not be needed due to PEP-0565 [2].\n\n    [1] https://docs.python.org/3/library/warnings.html#default-warning-filters\n    [2] https://www.python.org/dev/peps/pep-0565/\n    \"\"\"\n    deprecation_filter = ('always', None, DeprecationWarning,\n                          re.compile(r'^qiskit\\.*', re.UNICODE), 0)\n\n    # Instead of using warnings.simple_filter() directly, the internal\n    # _add_filter() function is used for being able to match against the\n    # module.\n    try:\n        warnings._add_filter(*deprecation_filter, append=False)\n    except AttributeError:\n        # ._add_filter is internal and not available in some Python versions.\n        pass\n\n    # Add a filter for ignoring ChangedInMarshmallow3Warning, as we depend on\n    # marhsmallow 2 explicitly. 2.17.0 introduced new deprecation warnings that\n    # are useful for eventually migrating, but too verbose for our purposes.\n    warnings.simplefilter('ignore', category=ChangedInMarshmallow3Warning)", "language": "python", "code": "def _filter_deprecation_warnings():\n    \"\"\"Apply filters to deprecation warnings.\n\n    Force the `DeprecationWarning` warnings to be displayed for the qiskit\n    module, overriding the system configuration as they are ignored by default\n    [1] for end-users. Additionally, silence the `ChangedInMarshmallow3Warning`\n    messages.\n\n    TODO: on Python 3.7, this might not be needed due to PEP-0565 [2].\n\n    [1] https://docs.python.org/3/library/warnings.html#default-warning-filters\n    [2] https://www.python.org/dev/peps/pep-0565/\n    \"\"\"\n    deprecation_filter = ('always', None, DeprecationWarning,\n                          re.compile(r'^qiskit\\.*', re.UNICODE), 0)\n\n    # Instead of using warnings.simple_filter() directly, the internal\n    # _add_filter() function is used for being able to match against the\n    # module.\n    try:\n        warnings._add_filter(*deprecation_filter, append=False)\n    except AttributeError:\n        # ._add_filter is internal and not available in some Python versions.\n        pass\n\n    # Add a filter for ignoring ChangedInMarshmallow3Warning, as we depend on\n    # marhsmallow 2 explicitly. 2.17.0 introduced new deprecation warnings that\n    # are useful for eventually migrating, but too verbose for our purposes.\n    warnings.simplefilter('ignore', category=ChangedInMarshmallow3Warning)", "code_tokens": ["def", "_filter_deprecation_warnings", "(", ")", ":", "deprecation_filter", "=", "(", "'always'", ",", "None", ",", "DeprecationWarning", ",", "re", ".", "compile", "(", "r'^qiskit\\.*'", ",", "re", ".", "UNICODE", ")", ",", "0", ")", "# Instead of using warnings.simple_filter() directly, the internal", "# _add_filter() function is used for being able to match against the", "# module.", "try", ":", "warnings", ".", "_add_filter", "(", "*", "deprecation_filter", ",", "append", "=", "False", ")", "except", "AttributeError", ":", "# ._add_filter is internal and not available in some Python versions.", "pass", "# Add a filter for ignoring ChangedInMarshmallow3Warning, as we depend on", "# marhsmallow 2 explicitly. 2.17.0 introduced new deprecation warnings that", "# are useful for eventually migrating, but too verbose for our purposes.", "warnings", ".", "simplefilter", "(", "'ignore'", ",", "category", "=", "ChangedInMarshmallow3Warning", ")"], "docstring": "Apply filters to deprecation warnings.\n\n    Force the `DeprecationWarning` warnings to be displayed for the qiskit\n    module, overriding the system configuration as they are ignored by default\n    [1] for end-users. Additionally, silence the `ChangedInMarshmallow3Warning`\n    messages.\n\n    TODO: on Python 3.7, this might not be needed due to PEP-0565 [2].\n\n    [1] https://docs.python.org/3/library/warnings.html#default-warning-filters\n    [2] https://www.python.org/dev/peps/pep-0565/", "docstring_tokens": ["Apply", "filters", "to", "deprecation", "warnings", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/util.py#L28-L56", "partition": "test"}
{"repo": "aholkner/bacon", "path": "native/Vendor/FreeType/src/tools/docmaker/utils.py", "func_name": "make_file_list", "original_string": "def  make_file_list( args = None ):\n    \"\"\"builds a list of input files from command-line arguments\"\"\"\n    file_list = []\n    # sys.stderr.write( repr( sys.argv[1 :] ) + '\\n' )\n\n    if not args:\n        args = sys.argv[1 :]\n\n    for pathname in args:\n        if string.find( pathname, '*' ) >= 0:\n            newpath = glob.glob( pathname )\n            newpath.sort()  # sort files -- this is important because\n                            # of the order of files\n        else:\n            newpath = [pathname]\n\n        file_list.extend( newpath )\n\n    if len( file_list ) == 0:\n        file_list = None\n    else:\n        # now filter the file list to remove non-existing ones\n        file_list = filter( file_exists, file_list )\n\n    return file_list", "language": "python", "code": "def  make_file_list( args = None ):\n    \"\"\"builds a list of input files from command-line arguments\"\"\"\n    file_list = []\n    # sys.stderr.write( repr( sys.argv[1 :] ) + '\\n' )\n\n    if not args:\n        args = sys.argv[1 :]\n\n    for pathname in args:\n        if string.find( pathname, '*' ) >= 0:\n            newpath = glob.glob( pathname )\n            newpath.sort()  # sort files -- this is important because\n                            # of the order of files\n        else:\n            newpath = [pathname]\n\n        file_list.extend( newpath )\n\n    if len( file_list ) == 0:\n        file_list = None\n    else:\n        # now filter the file list to remove non-existing ones\n        file_list = filter( file_exists, file_list )\n\n    return file_list", "code_tokens": ["def", "make_file_list", "(", "args", "=", "None", ")", ":", "file_list", "=", "[", "]", "# sys.stderr.write( repr( sys.argv[1 :] ) + '\\n' )", "if", "not", "args", ":", "args", "=", "sys", ".", "argv", "[", "1", ":", "]", "for", "pathname", "in", "args", ":", "if", "string", ".", "find", "(", "pathname", ",", "'*'", ")", ">=", "0", ":", "newpath", "=", "glob", ".", "glob", "(", "pathname", ")", "newpath", ".", "sort", "(", ")", "# sort files -- this is important because", "# of the order of files", "else", ":", "newpath", "=", "[", "pathname", "]", "file_list", ".", "extend", "(", "newpath", ")", "if", "len", "(", "file_list", ")", "==", "0", ":", "file_list", "=", "None", "else", ":", "# now filter the file list to remove non-existing ones", "file_list", "=", "filter", "(", "file_exists", ",", "file_list", ")", "return", "file_list"], "docstring": "builds a list of input files from command-line arguments", "docstring_tokens": ["builds", "a", "list", "of", "input", "files", "from", "command", "-", "line", "arguments"], "sha": "edf3810dcb211942d392a8637945871399b0650d", "url": "https://github.com/aholkner/bacon/blob/edf3810dcb211942d392a8637945871399b0650d/native/Vendor/FreeType/src/tools/docmaker/utils.py#L106-L130", "partition": "test"}
{"repo": "jaraco/jaraco.itertools", "path": "jaraco/itertools.py", "func_name": "nwise", "original_string": "def nwise(iter, n):\n\t\"\"\"\n\tLike pairwise, except returns n-tuples of adjacent items.\n\ts -> (s0,s1,...,sn), (s1,s2,...,s(n+1)), ...\n\t\"\"\"\n\titerset = [iter]\n\twhile len(iterset) < n:\n\t\titerset[-1:] = itertools.tee(iterset[-1])\n\t\tnext(iterset[-1], None)\n\treturn six.moves.zip(*iterset)", "language": "python", "code": "def nwise(iter, n):\n\t\"\"\"\n\tLike pairwise, except returns n-tuples of adjacent items.\n\ts -> (s0,s1,...,sn), (s1,s2,...,s(n+1)), ...\n\t\"\"\"\n\titerset = [iter]\n\twhile len(iterset) < n:\n\t\titerset[-1:] = itertools.tee(iterset[-1])\n\t\tnext(iterset[-1], None)\n\treturn six.moves.zip(*iterset)", "code_tokens": ["def", "nwise", "(", "iter", ",", "n", ")", ":", "iterset", "=", "[", "iter", "]", "while", "len", "(", "iterset", ")", "<", "n", ":", "iterset", "[", "-", "1", ":", "]", "=", "itertools", ".", "tee", "(", "iterset", "[", "-", "1", "]", ")", "next", "(", "iterset", "[", "-", "1", "]", ",", "None", ")", "return", "six", ".", "moves", ".", "zip", "(", "*", "iterset", ")"], "docstring": "Like pairwise, except returns n-tuples of adjacent items.\n\ts -> (s0,s1,...,sn), (s1,s2,...,s(n+1)), ...", "docstring_tokens": ["Like", "pairwise", "except", "returns", "n", "-", "tuples", "of", "adjacent", "items", ".", "s", "-", ">", "(", "s0", "s1", "...", "sn", ")", "(", "s1", "s2", "...", "s", "(", "n", "+", "1", "))", "..."], "sha": "0dc47c8924fa3d9ab676c3a6e195f03f728b72c6", "url": "https://github.com/jaraco/jaraco.itertools/blob/0dc47c8924fa3d9ab676c3a6e195f03f728b72c6/jaraco/itertools.py#L803-L812", "partition": "test"}
{"repo": "coleifer/irc", "path": "irc.py", "func_name": "IRCConnection.handle_ping", "original_string": "def handle_ping(self, payload):\n        \"\"\"\\\n        Respond to periodic PING messages from server\n        \"\"\"\n        self.logger.info('server ping: %s' % payload)\n        self.send('PONG %s' % payload, True)", "language": "python", "code": "def handle_ping(self, payload):\n        \"\"\"\\\n        Respond to periodic PING messages from server\n        \"\"\"\n        self.logger.info('server ping: %s' % payload)\n        self.send('PONG %s' % payload, True)", "code_tokens": ["def", "handle_ping", "(", "self", ",", "payload", ")", ":", "self", ".", "logger", ".", "info", "(", "'server ping: %s'", "%", "payload", ")", "self", ".", "send", "(", "'PONG %s'", "%", "payload", ",", "True", ")"], "docstring": "\\\n        Respond to periodic PING messages from server", "docstring_tokens": ["\\", "Respond", "to", "periodic", "PING", "messages", "from", "server"], "sha": "f9d2bd6369aafe6cb0916c9406270ca8ecea2080", "url": "https://github.com/coleifer/irc/blob/f9d2bd6369aafe6cb0916c9406270ca8ecea2080/irc.py#L192-L197", "partition": "test"}
{"repo": "xtuml/pyxtuml", "path": "xtuml/meta.py", "func_name": "MetaModel.new", "original_string": "def new(self, kind, *args, **kwargs):\n        '''\n        Create and return a new instance in the metamodel of some *kind*.\n        \n        Optionally, initial attribute values may be assigned to the new instance\n        by passing them as positional or keyword arguments. Positional arguments\n        are assigned in the order in which they appear in the metaclass.\n        '''\n        metaclass = self.find_metaclass(kind)\n        return metaclass.new(*args, **kwargs)", "language": "python", "code": "def new(self, kind, *args, **kwargs):\n        '''\n        Create and return a new instance in the metamodel of some *kind*.\n        \n        Optionally, initial attribute values may be assigned to the new instance\n        by passing them as positional or keyword arguments. Positional arguments\n        are assigned in the order in which they appear in the metaclass.\n        '''\n        metaclass = self.find_metaclass(kind)\n        return metaclass.new(*args, **kwargs)", "code_tokens": ["def", "new", "(", "self", ",", "kind", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "metaclass", "=", "self", ".", "find_metaclass", "(", "kind", ")", "return", "metaclass", ".", "new", "(", "*", "args", ",", "*", "*", "kwargs", ")"], "docstring": "Create and return a new instance in the metamodel of some *kind*.\n        \n        Optionally, initial attribute values may be assigned to the new instance\n        by passing them as positional or keyword arguments. Positional arguments\n        are assigned in the order in which they appear in the metaclass.", "docstring_tokens": ["Create", "and", "return", "a", "new", "instance", "in", "the", "metamodel", "of", "some", "*", "kind", "*", ".", "Optionally", "initial", "attribute", "values", "may", "be", "assigned", "to", "the", "new", "instance", "by", "passing", "them", "as", "positional", "or", "keyword", "arguments", ".", "Positional", "arguments", "are", "assigned", "in", "the", "order", "in", "which", "they", "appear", "in", "the", "metaclass", "."], "sha": "7dd9343b9a0191d1db1887ab9288d0a026608d9a", "url": "https://github.com/xtuml/pyxtuml/blob/7dd9343b9a0191d1db1887ab9288d0a026608d9a/xtuml/meta.py#L1188-L1197", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval", "path": "perceval/backends/core/stackexchange.py", "func_name": "StackExchange.parse_questions", "original_string": "def parse_questions(raw_page):\n        \"\"\"Parse a StackExchange API raw response.\n\n        The method parses the API response retrieving the\n        questions from the received items\n\n        :param items: items from where to parse the questions\n\n        :returns: a generator of questions\n        \"\"\"\n        raw_questions = json.loads(raw_page)\n        questions = raw_questions['items']\n        for question in questions:\n            yield question", "language": "python", "code": "def parse_questions(raw_page):\n        \"\"\"Parse a StackExchange API raw response.\n\n        The method parses the API response retrieving the\n        questions from the received items\n\n        :param items: items from where to parse the questions\n\n        :returns: a generator of questions\n        \"\"\"\n        raw_questions = json.loads(raw_page)\n        questions = raw_questions['items']\n        for question in questions:\n            yield question", "code_tokens": ["def", "parse_questions", "(", "raw_page", ")", ":", "raw_questions", "=", "json", ".", "loads", "(", "raw_page", ")", "questions", "=", "raw_questions", "[", "'items'", "]", "for", "question", "in", "questions", ":", "yield", "question"], "docstring": "Parse a StackExchange API raw response.\n\n        The method parses the API response retrieving the\n        questions from the received items\n\n        :param items: items from where to parse the questions\n\n        :returns: a generator of questions", "docstring_tokens": ["Parse", "a", "StackExchange", "API", "raw", "response", "."], "sha": "41c908605e88b7ebc3a536c643fa0f212eaf9e0e", "url": "https://github.com/chaoss/grimoirelab-perceval/blob/41c908605e88b7ebc3a536c643fa0f212eaf9e0e/perceval/backends/core/stackexchange.py#L160-L173", "partition": "test"}
{"repo": "CiscoDevNet/webexteamssdk", "path": "webexteamssdk/restsession.py", "func_name": "RestSession.wait_on_rate_limit", "original_string": "def wait_on_rate_limit(self, value):\n        \"\"\"Enable or disable automatic rate-limit handling.\"\"\"\n        check_type(value, bool, may_be_none=False)\n        self._wait_on_rate_limit = value", "language": "python", "code": "def wait_on_rate_limit(self, value):\n        \"\"\"Enable or disable automatic rate-limit handling.\"\"\"\n        check_type(value, bool, may_be_none=False)\n        self._wait_on_rate_limit = value", "code_tokens": ["def", "wait_on_rate_limit", "(", "self", ",", "value", ")", ":", "check_type", "(", "value", ",", "bool", ",", "may_be_none", "=", "False", ")", "self", ".", "_wait_on_rate_limit", "=", "value"], "docstring": "Enable or disable automatic rate-limit handling.", "docstring_tokens": ["Enable", "or", "disable", "automatic", "rate", "-", "limit", "handling", "."], "sha": "6fc2cc3557e080ba4b2a380664cb2a0532ae45cd", "url": "https://github.com/CiscoDevNet/webexteamssdk/blob/6fc2cc3557e080ba4b2a380664cb2a0532ae45cd/webexteamssdk/restsession.py#L174-L177", "partition": "test"}
{"repo": "katerina7479/pypdflite", "path": "pypdflite/pdfdocument.py", "func_name": "PDFDocument.add_text", "original_string": "def add_text(self, text, cursor=None, justification=None):\r\n        \"\"\" Input text, short or long. Writes in order, within the defined page boundaries. Sequential add_text commands will print without\r\n            additional whitespace. \"\"\"\r\n        if cursor is None:\r\n            cursor = self.page.cursor\r\n\r\n        text = re.sub(\"\\s\\s+\" , \" \", text)\r\n\r\n        if justification is None:\r\n            justification = self.justification\r\n\r\n        if '\\n' in text:\r\n            text_list = text.split('\\n')\r\n            for text in text_list:\r\n                PDFText(self.session, self.page, text, self.font, self.text_color, cursor, justification, self.double_spacing)\r\n                self.add_newline()\r\n        else:\r\n            PDFText(self.session, self.page, text, self.font, self.text_color, cursor, justification, self.double_spacing)", "language": "python", "code": "def add_text(self, text, cursor=None, justification=None):\r\n        \"\"\" Input text, short or long. Writes in order, within the defined page boundaries. Sequential add_text commands will print without\r\n            additional whitespace. \"\"\"\r\n        if cursor is None:\r\n            cursor = self.page.cursor\r\n\r\n        text = re.sub(\"\\s\\s+\" , \" \", text)\r\n\r\n        if justification is None:\r\n            justification = self.justification\r\n\r\n        if '\\n' in text:\r\n            text_list = text.split('\\n')\r\n            for text in text_list:\r\n                PDFText(self.session, self.page, text, self.font, self.text_color, cursor, justification, self.double_spacing)\r\n                self.add_newline()\r\n        else:\r\n            PDFText(self.session, self.page, text, self.font, self.text_color, cursor, justification, self.double_spacing)", "code_tokens": ["def", "add_text", "(", "self", ",", "text", ",", "cursor", "=", "None", ",", "justification", "=", "None", ")", ":", "if", "cursor", "is", "None", ":", "cursor", "=", "self", ".", "page", ".", "cursor", "text", "=", "re", ".", "sub", "(", "\"\\s\\s+\"", ",", "\" \"", ",", "text", ")", "if", "justification", "is", "None", ":", "justification", "=", "self", ".", "justification", "if", "'\\n'", "in", "text", ":", "text_list", "=", "text", ".", "split", "(", "'\\n'", ")", "for", "text", "in", "text_list", ":", "PDFText", "(", "self", ".", "session", ",", "self", ".", "page", ",", "text", ",", "self", ".", "font", ",", "self", ".", "text_color", ",", "cursor", ",", "justification", ",", "self", ".", "double_spacing", ")", "self", ".", "add_newline", "(", ")", "else", ":", "PDFText", "(", "self", ".", "session", ",", "self", ".", "page", ",", "text", ",", "self", ".", "font", ",", "self", ".", "text_color", ",", "cursor", ",", "justification", ",", "self", ".", "double_spacing", ")"], "docstring": "Input text, short or long. Writes in order, within the defined page boundaries. Sequential add_text commands will print without\r\n            additional whitespace.", "docstring_tokens": ["Input", "text", "short", "or", "long", ".", "Writes", "in", "order", "within", "the", "defined", "page", "boundaries", ".", "Sequential", "add_text", "commands", "will", "print", "without", "additional", "whitespace", "."], "sha": "ac2501f30d6619eae9dea5644717575ca9263d0a", "url": "https://github.com/katerina7479/pypdflite/blob/ac2501f30d6619eae9dea5644717575ca9263d0a/pypdflite/pdfdocument.py#L238-L255", "partition": "test"}
{"repo": "google/pybadges", "path": "pybadges/precalculate_text.py", "func_name": "write_json", "original_string": "def write_json(f: TextIO, deja_vu_sans_path: str,\n               measurer: text_measurer.TextMeasurer,\n               encodings: Iterable[str]) -> None:\n    \"\"\"Write the data required by PrecalculatedTextMeasurer to a stream.\"\"\"\n    supported_characters = list(\n        generate_supported_characters(deja_vu_sans_path))\n    kerning_characters = ''.join(\n        generate_encodeable_characters(supported_characters, encodings))\n    char_to_length = calculate_character_to_length_mapping(measurer,\n                                                           supported_characters)\n    pair_to_kerning = calculate_pair_to_kern_mapping(measurer, char_to_length,\n                                                     kerning_characters)\n    json.dump(\n        {'mean-character-length': statistics.mean(char_to_length.values()),\n         'character-lengths': char_to_length,\n         'kerning-characters': kerning_characters,\n         'kerning-pairs': pair_to_kerning},\n        f, sort_keys=True, indent=1)", "language": "python", "code": "def write_json(f: TextIO, deja_vu_sans_path: str,\n               measurer: text_measurer.TextMeasurer,\n               encodings: Iterable[str]) -> None:\n    \"\"\"Write the data required by PrecalculatedTextMeasurer to a stream.\"\"\"\n    supported_characters = list(\n        generate_supported_characters(deja_vu_sans_path))\n    kerning_characters = ''.join(\n        generate_encodeable_characters(supported_characters, encodings))\n    char_to_length = calculate_character_to_length_mapping(measurer,\n                                                           supported_characters)\n    pair_to_kerning = calculate_pair_to_kern_mapping(measurer, char_to_length,\n                                                     kerning_characters)\n    json.dump(\n        {'mean-character-length': statistics.mean(char_to_length.values()),\n         'character-lengths': char_to_length,\n         'kerning-characters': kerning_characters,\n         'kerning-pairs': pair_to_kerning},\n        f, sort_keys=True, indent=1)", "code_tokens": ["def", "write_json", "(", "f", ":", "TextIO", ",", "deja_vu_sans_path", ":", "str", ",", "measurer", ":", "text_measurer", ".", "TextMeasurer", ",", "encodings", ":", "Iterable", "[", "str", "]", ")", "->", "None", ":", "supported_characters", "=", "list", "(", "generate_supported_characters", "(", "deja_vu_sans_path", ")", ")", "kerning_characters", "=", "''", ".", "join", "(", "generate_encodeable_characters", "(", "supported_characters", ",", "encodings", ")", ")", "char_to_length", "=", "calculate_character_to_length_mapping", "(", "measurer", ",", "supported_characters", ")", "pair_to_kerning", "=", "calculate_pair_to_kern_mapping", "(", "measurer", ",", "char_to_length", ",", "kerning_characters", ")", "json", ".", "dump", "(", "{", "'mean-character-length'", ":", "statistics", ".", "mean", "(", "char_to_length", ".", "values", "(", ")", ")", ",", "'character-lengths'", ":", "char_to_length", ",", "'kerning-characters'", ":", "kerning_characters", ",", "'kerning-pairs'", ":", "pair_to_kerning", "}", ",", "f", ",", "sort_keys", "=", "True", ",", "indent", "=", "1", ")"], "docstring": "Write the data required by PrecalculatedTextMeasurer to a stream.", "docstring_tokens": ["Write", "the", "data", "required", "by", "PrecalculatedTextMeasurer", "to", "a", "stream", "."], "sha": "d42c8080adb21b81123ac9540c53127ed2fa1edc", "url": "https://github.com/google/pybadges/blob/d42c8080adb21b81123ac9540c53127ed2fa1edc/pybadges/precalculate_text.py#L142-L159", "partition": "test"}
{"repo": "treycucco/pyebnf", "path": "pyebnf/compiler.py", "func_name": "Compiler._node_to_asn", "original_string": "def _node_to_asn(self, node):\n    \"\"\"Convert a parse tree node into an absract syntax tree node.\"\"\"\n    if node.is_type(TokenType.identifier):\n      return Identifier(node.svalue)\n\n    elif node.is_type(TokenType.terminal):\n      return Terminal(node.svalue)\n\n    elif node.is_type(TokenType.option_group):\n      expr = node.children[0]\n      return OptionGroup(self._expression_to_asn(expr))\n\n    elif node.is_type(TokenType.repetition_group):\n      expr = node.children[0]\n      return RepetitionGroup(self._expression_to_asn(expr))\n\n    elif node.is_type(TokenType.grouping_group):\n      expr = node.children[0]\n      return GroupingGroup(self._expression_to_asn(expr))\n\n    elif node.is_type(TokenType.special_handling):\n      ident = node.children[0]\n      return SpecialHandling(ident)\n\n    elif node.is_type(TokenType.number):\n      return Number(node.svalue)\n\n    elif node.is_type((TokenType.operator, TokenType.op_mult, TokenType.op_add)):\n      return OperatorNode(OPERATOR_INDEX[node.svalue], node.position)\n\n    else:\n      raise Exception(\"Unhandled parse tree node: {0}\".format(node))", "language": "python", "code": "def _node_to_asn(self, node):\n    \"\"\"Convert a parse tree node into an absract syntax tree node.\"\"\"\n    if node.is_type(TokenType.identifier):\n      return Identifier(node.svalue)\n\n    elif node.is_type(TokenType.terminal):\n      return Terminal(node.svalue)\n\n    elif node.is_type(TokenType.option_group):\n      expr = node.children[0]\n      return OptionGroup(self._expression_to_asn(expr))\n\n    elif node.is_type(TokenType.repetition_group):\n      expr = node.children[0]\n      return RepetitionGroup(self._expression_to_asn(expr))\n\n    elif node.is_type(TokenType.grouping_group):\n      expr = node.children[0]\n      return GroupingGroup(self._expression_to_asn(expr))\n\n    elif node.is_type(TokenType.special_handling):\n      ident = node.children[0]\n      return SpecialHandling(ident)\n\n    elif node.is_type(TokenType.number):\n      return Number(node.svalue)\n\n    elif node.is_type((TokenType.operator, TokenType.op_mult, TokenType.op_add)):\n      return OperatorNode(OPERATOR_INDEX[node.svalue], node.position)\n\n    else:\n      raise Exception(\"Unhandled parse tree node: {0}\".format(node))", "code_tokens": ["def", "_node_to_asn", "(", "self", ",", "node", ")", ":", "if", "node", ".", "is_type", "(", "TokenType", ".", "identifier", ")", ":", "return", "Identifier", "(", "node", ".", "svalue", ")", "elif", "node", ".", "is_type", "(", "TokenType", ".", "terminal", ")", ":", "return", "Terminal", "(", "node", ".", "svalue", ")", "elif", "node", ".", "is_type", "(", "TokenType", ".", "option_group", ")", ":", "expr", "=", "node", ".", "children", "[", "0", "]", "return", "OptionGroup", "(", "self", ".", "_expression_to_asn", "(", "expr", ")", ")", "elif", "node", ".", "is_type", "(", "TokenType", ".", "repetition_group", ")", ":", "expr", "=", "node", ".", "children", "[", "0", "]", "return", "RepetitionGroup", "(", "self", ".", "_expression_to_asn", "(", "expr", ")", ")", "elif", "node", ".", "is_type", "(", "TokenType", ".", "grouping_group", ")", ":", "expr", "=", "node", ".", "children", "[", "0", "]", "return", "GroupingGroup", "(", "self", ".", "_expression_to_asn", "(", "expr", ")", ")", "elif", "node", ".", "is_type", "(", "TokenType", ".", "special_handling", ")", ":", "ident", "=", "node", ".", "children", "[", "0", "]", "return", "SpecialHandling", "(", "ident", ")", "elif", "node", ".", "is_type", "(", "TokenType", ".", "number", ")", ":", "return", "Number", "(", "node", ".", "svalue", ")", "elif", "node", ".", "is_type", "(", "(", "TokenType", ".", "operator", ",", "TokenType", ".", "op_mult", ",", "TokenType", ".", "op_add", ")", ")", ":", "return", "OperatorNode", "(", "OPERATOR_INDEX", "[", "node", ".", "svalue", "]", ",", "node", ".", "position", ")", "else", ":", "raise", "Exception", "(", "\"Unhandled parse tree node: {0}\"", ".", "format", "(", "node", ")", ")"], "docstring": "Convert a parse tree node into an absract syntax tree node.", "docstring_tokens": ["Convert", "a", "parse", "tree", "node", "into", "an", "absract", "syntax", "tree", "node", "."], "sha": "3634ddabbe5d73508bcc20f4a591f86a46634e1d", "url": "https://github.com/treycucco/pyebnf/blob/3634ddabbe5d73508bcc20f4a591f86a46634e1d/pyebnf/compiler.py#L239-L270", "partition": "test"}
{"repo": "solvebio/solvebio-python", "path": "solvebio/cli/ipython.py", "func_name": "launch_ipython_5_shell", "original_string": "def launch_ipython_5_shell(args):\n    \"\"\"Open the SolveBio shell (IPython wrapper) with IPython 5+\"\"\"\n    import IPython  # noqa\n    from traitlets.config import Config\n\n    c = Config()\n    path = os.path.dirname(os.path.abspath(__file__))\n\n    try:\n        # see if we're already inside IPython\n        get_ipython  # pylint: disable=undefined-variable\n        _print(\"WARNING: Running IPython within IPython.\")\n    except NameError:\n        c.InteractiveShell.banner1 = 'SolveBio Python shell started.\\n'\n\n    c.InteractiveShellApp.exec_files = ['{}/ipython_init.py'.format(path)]\n    IPython.start_ipython(argv=[], config=c)", "language": "python", "code": "def launch_ipython_5_shell(args):\n    \"\"\"Open the SolveBio shell (IPython wrapper) with IPython 5+\"\"\"\n    import IPython  # noqa\n    from traitlets.config import Config\n\n    c = Config()\n    path = os.path.dirname(os.path.abspath(__file__))\n\n    try:\n        # see if we're already inside IPython\n        get_ipython  # pylint: disable=undefined-variable\n        _print(\"WARNING: Running IPython within IPython.\")\n    except NameError:\n        c.InteractiveShell.banner1 = 'SolveBio Python shell started.\\n'\n\n    c.InteractiveShellApp.exec_files = ['{}/ipython_init.py'.format(path)]\n    IPython.start_ipython(argv=[], config=c)", "code_tokens": ["def", "launch_ipython_5_shell", "(", "args", ")", ":", "import", "IPython", "# noqa", "from", "traitlets", ".", "config", "import", "Config", "c", "=", "Config", "(", ")", "path", "=", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "abspath", "(", "__file__", ")", ")", "try", ":", "# see if we're already inside IPython", "get_ipython", "# pylint: disable=undefined-variable", "_print", "(", "\"WARNING: Running IPython within IPython.\"", ")", "except", "NameError", ":", "c", ".", "InteractiveShell", ".", "banner1", "=", "'SolveBio Python shell started.\\n'", "c", ".", "InteractiveShellApp", ".", "exec_files", "=", "[", "'{}/ipython_init.py'", ".", "format", "(", "path", ")", "]", "IPython", ".", "start_ipython", "(", "argv", "=", "[", "]", ",", "config", "=", "c", ")"], "docstring": "Open the SolveBio shell (IPython wrapper) with IPython 5+", "docstring_tokens": ["Open", "the", "SolveBio", "shell", "(", "IPython", "wrapper", ")", "with", "IPython", "5", "+"], "sha": "b29614643043afd19c1d8074e8f25c6700d51a73", "url": "https://github.com/solvebio/solvebio-python/blob/b29614643043afd19c1d8074e8f25c6700d51a73/solvebio/cli/ipython.py#L32-L48", "partition": "test"}
{"repo": "Clinical-Genomics/scout", "path": "scout/adapter/mongo/index.py", "func_name": "IndexHandler.indexes", "original_string": "def indexes(self, collection=None):\n        \"\"\"Return a list with the current indexes\n        \n        Skip the mandatory _id_ indexes\n        \n        Args:\n            collection(str)\n\n        Returns:\n            indexes(list)\n        \"\"\"\n        \n        indexes = []\n\n        for collection_name in self.collections():\n            if collection and collection != collection_name:\n                continue\n            for index_name in self.db[collection_name].index_information():\n                if index_name != '_id_':\n                    indexes.append(index_name)\n        return indexes", "language": "python", "code": "def indexes(self, collection=None):\n        \"\"\"Return a list with the current indexes\n        \n        Skip the mandatory _id_ indexes\n        \n        Args:\n            collection(str)\n\n        Returns:\n            indexes(list)\n        \"\"\"\n        \n        indexes = []\n\n        for collection_name in self.collections():\n            if collection and collection != collection_name:\n                continue\n            for index_name in self.db[collection_name].index_information():\n                if index_name != '_id_':\n                    indexes.append(index_name)\n        return indexes", "code_tokens": ["def", "indexes", "(", "self", ",", "collection", "=", "None", ")", ":", "indexes", "=", "[", "]", "for", "collection_name", "in", "self", ".", "collections", "(", ")", ":", "if", "collection", "and", "collection", "!=", "collection_name", ":", "continue", "for", "index_name", "in", "self", ".", "db", "[", "collection_name", "]", ".", "index_information", "(", ")", ":", "if", "index_name", "!=", "'_id_'", ":", "indexes", ".", "append", "(", "index_name", ")", "return", "indexes"], "docstring": "Return a list with the current indexes\n        \n        Skip the mandatory _id_ indexes\n        \n        Args:\n            collection(str)\n\n        Returns:\n            indexes(list)", "docstring_tokens": ["Return", "a", "list", "with", "the", "current", "indexes", "Skip", "the", "mandatory", "_id_", "indexes", "Args", ":", "collection", "(", "str", ")"], "sha": "90a551e2e1653a319e654c2405c2866f93d0ebb9", "url": "https://github.com/Clinical-Genomics/scout/blob/90a551e2e1653a319e654c2405c2866f93d0ebb9/scout/adapter/mongo/index.py#L10-L30", "partition": "test"}
{"repo": "rwl/godot", "path": "godot/graph.py", "func_name": "Graph.redraw_canvas", "original_string": "def redraw_canvas(self):\n        \"\"\" Parses the Xdot attributes of all graph components and adds\n            the components to a new canvas.\n        \"\"\"\n        from xdot_parser import XdotAttrParser\n\n        xdot_parser = XdotAttrParser()\n        canvas = self._component_default()\n\n        for node in self.nodes:\n            components = xdot_parser.parse_xdot_data( node._draw_ )\n            canvas.add( *components )\n\n            components = xdot_parser.parse_xdot_data( node._ldraw_ )\n            canvas.add( *components )\n\n        for edge in self.edges:\n            components = xdot_parser.parse_xdot_data( edge._draw_ )\n            canvas.add( *components )\n            components = xdot_parser.parse_xdot_data( edge._ldraw_ )\n            canvas.add( *components )\n            components = xdot_parser.parse_xdot_data( edge._hdraw_ )\n            canvas.add( *components )\n            components = xdot_parser.parse_xdot_data( edge._tdraw_ )\n            canvas.add( *components )\n            components = xdot_parser.parse_xdot_data( edge._hldraw_ )\n            canvas.add( *components )\n            components = xdot_parser.parse_xdot_data( edge._tldraw_ )\n            canvas.add( *components )\n\n        self.component = canvas\n        self.vp.request_redraw()", "language": "python", "code": "def redraw_canvas(self):\n        \"\"\" Parses the Xdot attributes of all graph components and adds\n            the components to a new canvas.\n        \"\"\"\n        from xdot_parser import XdotAttrParser\n\n        xdot_parser = XdotAttrParser()\n        canvas = self._component_default()\n\n        for node in self.nodes:\n            components = xdot_parser.parse_xdot_data( node._draw_ )\n            canvas.add( *components )\n\n            components = xdot_parser.parse_xdot_data( node._ldraw_ )\n            canvas.add( *components )\n\n        for edge in self.edges:\n            components = xdot_parser.parse_xdot_data( edge._draw_ )\n            canvas.add( *components )\n            components = xdot_parser.parse_xdot_data( edge._ldraw_ )\n            canvas.add( *components )\n            components = xdot_parser.parse_xdot_data( edge._hdraw_ )\n            canvas.add( *components )\n            components = xdot_parser.parse_xdot_data( edge._tdraw_ )\n            canvas.add( *components )\n            components = xdot_parser.parse_xdot_data( edge._hldraw_ )\n            canvas.add( *components )\n            components = xdot_parser.parse_xdot_data( edge._tldraw_ )\n            canvas.add( *components )\n\n        self.component = canvas\n        self.vp.request_redraw()", "code_tokens": ["def", "redraw_canvas", "(", "self", ")", ":", "from", "xdot_parser", "import", "XdotAttrParser", "xdot_parser", "=", "XdotAttrParser", "(", ")", "canvas", "=", "self", ".", "_component_default", "(", ")", "for", "node", "in", "self", ".", "nodes", ":", "components", "=", "xdot_parser", ".", "parse_xdot_data", "(", "node", ".", "_draw_", ")", "canvas", ".", "add", "(", "*", "components", ")", "components", "=", "xdot_parser", ".", "parse_xdot_data", "(", "node", ".", "_ldraw_", ")", "canvas", ".", "add", "(", "*", "components", ")", "for", "edge", "in", "self", ".", "edges", ":", "components", "=", "xdot_parser", ".", "parse_xdot_data", "(", "edge", ".", "_draw_", ")", "canvas", ".", "add", "(", "*", "components", ")", "components", "=", "xdot_parser", ".", "parse_xdot_data", "(", "edge", ".", "_ldraw_", ")", "canvas", ".", "add", "(", "*", "components", ")", "components", "=", "xdot_parser", ".", "parse_xdot_data", "(", "edge", ".", "_hdraw_", ")", "canvas", ".", "add", "(", "*", "components", ")", "components", "=", "xdot_parser", ".", "parse_xdot_data", "(", "edge", ".", "_tdraw_", ")", "canvas", ".", "add", "(", "*", "components", ")", "components", "=", "xdot_parser", ".", "parse_xdot_data", "(", "edge", ".", "_hldraw_", ")", "canvas", ".", "add", "(", "*", "components", ")", "components", "=", "xdot_parser", ".", "parse_xdot_data", "(", "edge", ".", "_tldraw_", ")", "canvas", ".", "add", "(", "*", "components", ")", "self", ".", "component", "=", "canvas", "self", ".", "vp", ".", "request_redraw", "(", ")"], "docstring": "Parses the Xdot attributes of all graph components and adds\n            the components to a new canvas.", "docstring_tokens": ["Parses", "the", "Xdot", "attributes", "of", "all", "graph", "components", "and", "adds", "the", "components", "to", "a", "new", "canvas", "."], "sha": "013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f", "url": "https://github.com/rwl/godot/blob/013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f/godot/graph.py#L907-L938", "partition": "test"}
{"repo": "rwl/godot", "path": "godot/ui/graph_view_model.py", "func_name": "GraphViewModel._parse_dot_code_fired", "original_string": "def _parse_dot_code_fired(self):\n        \"\"\" Parses the dot_code string and replaces the existing model.\n        \"\"\"\n        parser = GodotDataParser()\n        graph  = parser.parse_dot_data(self.dot_code)\n        if graph is not None:\n            self.model = graph", "language": "python", "code": "def _parse_dot_code_fired(self):\n        \"\"\" Parses the dot_code string and replaces the existing model.\n        \"\"\"\n        parser = GodotDataParser()\n        graph  = parser.parse_dot_data(self.dot_code)\n        if graph is not None:\n            self.model = graph", "code_tokens": ["def", "_parse_dot_code_fired", "(", "self", ")", ":", "parser", "=", "GodotDataParser", "(", ")", "graph", "=", "parser", ".", "parse_dot_data", "(", "self", ".", "dot_code", ")", "if", "graph", "is", "not", "None", ":", "self", ".", "model", "=", "graph"], "docstring": "Parses the dot_code string and replaces the existing model.", "docstring_tokens": ["Parses", "the", "dot_code", "string", "and", "replaces", "the", "existing", "model", "."], "sha": "013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f", "url": "https://github.com/rwl/godot/blob/013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f/godot/ui/graph_view_model.py#L180-L186", "partition": "test"}
{"repo": "zomux/deepy", "path": "deepy/layers/layer.py", "func_name": "NeuralLayer.compute", "original_string": "def compute(self, *inputs, **kwargs):\n        \"\"\"\n        Compute based on NeuralVariable.\n        :type inputs:  list of NeuralVariable\n        :return: NeuralVariable\n        \"\"\"\n        from deepy.core.neural_var import NeuralVariable\n        from deepy.core.graph import graph\n        if type(inputs[0]) != NeuralVariable:\n            raise SystemError(\"The input of `compute` must be NeuralVar\")\n\n        dims = [t.dim() for t in inputs]\n        if len(inputs) == 1:\n            self.init(input_dim=dims[0])\n        else:\n            self.init(input_dims=dims)\n        # Check block\n        if self.parameters and not self._linked_block:\n            self.belongs_to(graph.default_block())\n        # convert kwargs\n        train_kwargs, _, _ = convert_to_theano_var(kwargs)\n\n        output = self.compute_tensor(*[t.tensor for t in inputs], **train_kwargs)\n\n        if type(output) != list and type(output) != tuple:\n            return NeuralVariable(output, dim=self.output_dim)\n        else:\n            return [NeuralVariable(*item) for item in zip(output, self.output_dims)]", "language": "python", "code": "def compute(self, *inputs, **kwargs):\n        \"\"\"\n        Compute based on NeuralVariable.\n        :type inputs:  list of NeuralVariable\n        :return: NeuralVariable\n        \"\"\"\n        from deepy.core.neural_var import NeuralVariable\n        from deepy.core.graph import graph\n        if type(inputs[0]) != NeuralVariable:\n            raise SystemError(\"The input of `compute` must be NeuralVar\")\n\n        dims = [t.dim() for t in inputs]\n        if len(inputs) == 1:\n            self.init(input_dim=dims[0])\n        else:\n            self.init(input_dims=dims)\n        # Check block\n        if self.parameters and not self._linked_block:\n            self.belongs_to(graph.default_block())\n        # convert kwargs\n        train_kwargs, _, _ = convert_to_theano_var(kwargs)\n\n        output = self.compute_tensor(*[t.tensor for t in inputs], **train_kwargs)\n\n        if type(output) != list and type(output) != tuple:\n            return NeuralVariable(output, dim=self.output_dim)\n        else:\n            return [NeuralVariable(*item) for item in zip(output, self.output_dims)]", "code_tokens": ["def", "compute", "(", "self", ",", "*", "inputs", ",", "*", "*", "kwargs", ")", ":", "from", "deepy", ".", "core", ".", "neural_var", "import", "NeuralVariable", "from", "deepy", ".", "core", ".", "graph", "import", "graph", "if", "type", "(", "inputs", "[", "0", "]", ")", "!=", "NeuralVariable", ":", "raise", "SystemError", "(", "\"The input of `compute` must be NeuralVar\"", ")", "dims", "=", "[", "t", ".", "dim", "(", ")", "for", "t", "in", "inputs", "]", "if", "len", "(", "inputs", ")", "==", "1", ":", "self", ".", "init", "(", "input_dim", "=", "dims", "[", "0", "]", ")", "else", ":", "self", ".", "init", "(", "input_dims", "=", "dims", ")", "# Check block", "if", "self", ".", "parameters", "and", "not", "self", ".", "_linked_block", ":", "self", ".", "belongs_to", "(", "graph", ".", "default_block", "(", ")", ")", "# convert kwargs", "train_kwargs", ",", "_", ",", "_", "=", "convert_to_theano_var", "(", "kwargs", ")", "output", "=", "self", ".", "compute_tensor", "(", "*", "[", "t", ".", "tensor", "for", "t", "in", "inputs", "]", ",", "*", "*", "train_kwargs", ")", "if", "type", "(", "output", ")", "!=", "list", "and", "type", "(", "output", ")", "!=", "tuple", ":", "return", "NeuralVariable", "(", "output", ",", "dim", "=", "self", ".", "output_dim", ")", "else", ":", "return", "[", "NeuralVariable", "(", "*", "item", ")", "for", "item", "in", "zip", "(", "output", ",", "self", ".", "output_dims", ")", "]"], "docstring": "Compute based on NeuralVariable.\n        :type inputs:  list of NeuralVariable\n        :return: NeuralVariable", "docstring_tokens": ["Compute", "based", "on", "NeuralVariable", ".", ":", "type", "inputs", ":", "list", "of", "NeuralVariable", ":", "return", ":", "NeuralVariable"], "sha": "090fbad22a08a809b12951cd0d4984f5bd432698", "url": "https://github.com/zomux/deepy/blob/090fbad22a08a809b12951cd0d4984f5bd432698/deepy/layers/layer.py#L70-L97", "partition": "test"}
{"repo": "sbneto/s3conf", "path": "s3conf/client.py", "func_name": "exec_command", "original_string": "def exec_command(ctx, section, command, map_files):\n    \"\"\"\n    Sets the process environemnt and executes the [COMMAND] in the same context. Does not modify the current shell\n    environment.\n\n    If the [COMMAND] has option-like arguments, use the standard POSIX pattern \"--\" to separate options\n    from arguments. Considering our configuration in the \"dev\" section, we could write:\n\n    s3conf -v info exec dev -- ping -v google.com\n    \"\"\"\n    try:\n        logger.debug('Running exec command')\n        existing_sections = config.ConfigFileResolver(config.LOCAL_CONFIG_FILE).sections()\n        command = ' '.join(command)\n        if section not in existing_sections:\n            command = '{} {}'.format(section, command) if command else section\n            section = None\n\n        if not command:\n            logger.warning('No command detected.')\n            click.echo(exec_command.get_help(ctx))\n            return\n\n        settings = config.Settings(section=section)\n        storage = STORAGES['s3'](settings=settings)\n        conf = s3conf.S3Conf(storage=storage, settings=settings)\n\n        env_vars = conf.get_envfile().as_dict()\n        if env_vars.get('S3CONF_MAP') and map_files:\n            conf.download_mapping(env_vars.get('S3CONF_MAP'))\n\n        current_env = os.environ.copy()\n        current_env.update(env_vars)\n        logger.debug('Executing command \"%s\"', command)\n        subprocess.run(shlex.split(command), env=current_env, check=True)\n    except exceptions.EnvfilePathNotDefinedError:\n        raise exceptions.EnvfilePathNotDefinedUsageError()", "language": "python", "code": "def exec_command(ctx, section, command, map_files):\n    \"\"\"\n    Sets the process environemnt and executes the [COMMAND] in the same context. Does not modify the current shell\n    environment.\n\n    If the [COMMAND] has option-like arguments, use the standard POSIX pattern \"--\" to separate options\n    from arguments. Considering our configuration in the \"dev\" section, we could write:\n\n    s3conf -v info exec dev -- ping -v google.com\n    \"\"\"\n    try:\n        logger.debug('Running exec command')\n        existing_sections = config.ConfigFileResolver(config.LOCAL_CONFIG_FILE).sections()\n        command = ' '.join(command)\n        if section not in existing_sections:\n            command = '{} {}'.format(section, command) if command else section\n            section = None\n\n        if not command:\n            logger.warning('No command detected.')\n            click.echo(exec_command.get_help(ctx))\n            return\n\n        settings = config.Settings(section=section)\n        storage = STORAGES['s3'](settings=settings)\n        conf = s3conf.S3Conf(storage=storage, settings=settings)\n\n        env_vars = conf.get_envfile().as_dict()\n        if env_vars.get('S3CONF_MAP') and map_files:\n            conf.download_mapping(env_vars.get('S3CONF_MAP'))\n\n        current_env = os.environ.copy()\n        current_env.update(env_vars)\n        logger.debug('Executing command \"%s\"', command)\n        subprocess.run(shlex.split(command), env=current_env, check=True)\n    except exceptions.EnvfilePathNotDefinedError:\n        raise exceptions.EnvfilePathNotDefinedUsageError()", "code_tokens": ["def", "exec_command", "(", "ctx", ",", "section", ",", "command", ",", "map_files", ")", ":", "try", ":", "logger", ".", "debug", "(", "'Running exec command'", ")", "existing_sections", "=", "config", ".", "ConfigFileResolver", "(", "config", ".", "LOCAL_CONFIG_FILE", ")", ".", "sections", "(", ")", "command", "=", "' '", ".", "join", "(", "command", ")", "if", "section", "not", "in", "existing_sections", ":", "command", "=", "'{} {}'", ".", "format", "(", "section", ",", "command", ")", "if", "command", "else", "section", "section", "=", "None", "if", "not", "command", ":", "logger", ".", "warning", "(", "'No command detected.'", ")", "click", ".", "echo", "(", "exec_command", ".", "get_help", "(", "ctx", ")", ")", "return", "settings", "=", "config", ".", "Settings", "(", "section", "=", "section", ")", "storage", "=", "STORAGES", "[", "'s3'", "]", "(", "settings", "=", "settings", ")", "conf", "=", "s3conf", ".", "S3Conf", "(", "storage", "=", "storage", ",", "settings", "=", "settings", ")", "env_vars", "=", "conf", ".", "get_envfile", "(", ")", ".", "as_dict", "(", ")", "if", "env_vars", ".", "get", "(", "'S3CONF_MAP'", ")", "and", "map_files", ":", "conf", ".", "download_mapping", "(", "env_vars", ".", "get", "(", "'S3CONF_MAP'", ")", ")", "current_env", "=", "os", ".", "environ", ".", "copy", "(", ")", "current_env", ".", "update", "(", "env_vars", ")", "logger", ".", "debug", "(", "'Executing command \"%s\"'", ",", "command", ")", "subprocess", ".", "run", "(", "shlex", ".", "split", "(", "command", ")", ",", "env", "=", "current_env", ",", "check", "=", "True", ")", "except", "exceptions", ".", "EnvfilePathNotDefinedError", ":", "raise", "exceptions", ".", "EnvfilePathNotDefinedUsageError", "(", ")"], "docstring": "Sets the process environemnt and executes the [COMMAND] in the same context. Does not modify the current shell\n    environment.\n\n    If the [COMMAND] has option-like arguments, use the standard POSIX pattern \"--\" to separate options\n    from arguments. Considering our configuration in the \"dev\" section, we could write:\n\n    s3conf -v info exec dev -- ping -v google.com", "docstring_tokens": ["Sets", "the", "process", "environemnt", "and", "executes", "the", "[", "COMMAND", "]", "in", "the", "same", "context", ".", "Does", "not", "modify", "the", "current", "shell", "environment", "."], "sha": "92fd2973beccc85bb21d3157ff227929e62ed695", "url": "https://github.com/sbneto/s3conf/blob/92fd2973beccc85bb21d3157ff227929e62ed695/s3conf/client.py#L135-L171", "partition": "test"}
