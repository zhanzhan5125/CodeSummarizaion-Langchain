
===============================[ 1 ]===============================
def _restrict_along_direction(value_and_gradients_function,
                              position,
                              direction):
  def _restricted_func(t):
    t = _broadcast(t, position)
    pt = position + tf.expand_dims(t, axis=-1) * direction
    objective_value, gradient = value_and_gradients_function(pt)
    return ValueAndGradient(
        x=t,
        f=objective_value,
        df=tf.reduce_sum(input_tensor=gradient * direction, axis=-1),
        full_gradient=gradient)
  return _restricted_func
*** Original Comment ***: Evaluates a function and its gradient at a point along a specified direction from a given position, returning both the function value and the directional derivative.
*** Original Thought ***: 
*** Validation Score ***: 4.828503499364793
*** Validation Thought ***: Comment accurately describes code function. It's concise, correct, and no redundant information is present.


===============================[ 2 ]===============================
def _find_namespaces_from_child(parent, child, namespaces):
        for cur_child in parent.childNodes:
            if cur_child is child:
                return True
            if _MinidomXmlToObject._find_namespaces_from_child(cur_child, child, namespaces):
                for key in cur_child.attributes.keys():
                    if key.startswith('xmlns:') or key == 'xmlns':
                        namespaces[key] = cur_child.attributes[key]
                break
        return False
*** Original Comment ***: Checks if the 'child' node is a direct or indirect child of 'parent', and if so, collects XML namespace attributes from the 'child' node's ancestors into the 'namespaces' dictionary.
*** Original Thought ***: 
*** Validation Score ***: 4.028255959685176
*** Validation Thought ***: Read code, identifies recursive search and namespace collection. Comment correct, concise, but omits break condition detail. Almost perfect, minor detail missing.


===============================[ 3 ]===============================
def CovInv(self):
        self.recurse(full_matrix=True)
        return self.tree.root.cinv
*** Original Comment ***: # Recursively updates the full matrix and returns the inverse covariance matrix from the root of the tree.
*** Original Thought ***: 
*** Validation Score ***: 4.909212478845032
*** Validation Thought ***: Code defines CovInv function. Comment matches function's actions. Correct and concise. No redundant info.


===============================[ 4 ]===============================
def _getadvanced(self, index):
        index = [asarray(i) for i in index]
        shape = index[0].shape
        if not all([i.shape == shape for i in index]):
            raise ValueError("shape mismatch: indexing arrays could not be broadcast "
                             "together with shapes " + ("%s " * self.ndim)
                             % tuple([i.shape for i in index]))
        index = tuple([listify(i, d) for (i, d) in zip(index, self.shape)])
        key_tuples = list(zip(*index[0:self.split]))
        value_tuples = list(zip(*index[self.split:]))
        d = {}
        for k, g in groupby(zip(value_tuples, key_tuples), lambda x: x[1]):
            d[k] = map(lambda x: x[0], list(g))
        def key_check(key):
            return key in key_tuples
        def key_func(key):
            return unravel_index(key, shape)
        filtered = self._rdd.filter(lambda kv: key_check(kv[0]))
        if len(value_tuples) > 0:
            flattened = filtered.flatMap(lambda kv: [(kv[0], kv[1][i]) for i in d[kv[0]]])
        else:
            flattened = filtered
        indexed = flattened.zipWithIndex()
        rdd = indexed.map(lambda kkv: (key_func(kkv[1]), kkv[0][1]))
        split = len(shape)
        return rdd, shape, split
*** Original Comment ***: This function processes a multi-dimensional index to filter and transform elements in a distributed dataset, ensuring the indices have matching shapes and applying a custom mapping and filtering logic based on the provided index.
*** Original Thought ***: 
*** Validation Score ***: 3.9947414911145036
*** Validation Thought ***: Comment summarizes code's purpose. Correctly mentions filtering, transforming, shape checking. Could be more concise, but no major errors.


===============================[ 5 ]===============================
def _satisfies_wolfe(val_0,
                     val_c,
                     f_lim,
                     sufficient_decrease_param,
                     curvature_param):
  exact_wolfe_suff_dec = (sufficient_decrease_param * val_0.df >=
                          (val_c.f - val_0.f) / val_c.x)
  wolfe_curvature = val_c.df >= curvature_param * val_0.df
  exact_wolfe = exact_wolfe_suff_dec & wolfe_curvature
  approx_wolfe_applies = val_c.f <= f_lim
  approx_wolfe_suff_dec = ((2 * sufficient_decrease_param - 1) * val_0.df
                           >= val_c.df)
  approx_wolfe = approx_wolfe_applies & approx_wolfe_suff_dec & wolfe_curvature
  is_satisfied = exact_wolfe | approx_wolfe
  return is_satisfied
*** Original Comment ***: Checks if the Wolfe conditions, both exact and approximate, are satisfied for a given set of values and parameters.
*** Original Thought ***: 
*** Validation Score ***: 4.690017229564661
*** Validation Thought ***: Comment correctly summarizes the function's purpose. It's concise, accurate, and no redundant information.


===============================[ 6 ]===============================
def get_misses(self):
        return [self.stats[cache_level]['MISS_count']/self.first_dim_factor
                for cache_level in range(len(self.machine['memory hierarchy']))]
*** Original Comment ***: Returns a list of miss ratios for each cache level in the machine's memory hierarchy, adjusted by the `first_dim_factor`.
*** Original Thought ***: 
*** Validation Score ***: 4.861311148966586
*** Validation Thought ***: Comment matches code function, correctly and concisely describes the return value and adjustment by 'first_dim_factor'.


===============================[ 7 ]===============================
def call(self, inputs):
    net = self.encoder_net(tf.cast(inputs, tf.float32))
    return ed.MultivariateNormalDiag(
        loc=net[..., :self.latent_size],
        scale_diag=tf.nn.softplus(net[..., self.latent_size:]),
        name="latent_code_posterior")
*** Original Comment ***: This function casts the input to float32, passes it through an encoder network, and returns a multivariate normal distribution with learned mean and diagonal covariance as the latent code posterior.
*** Original Thought ***: 
*** Validation Score ***: 4.987246900364398
*** Validation Thought ***: Comment matches code function, is correct and concise. Explains casting, encoder, and output distribution clearly.


===============================[ 8 ]===============================
def from_yaml(cls, defaults, **kwargs):
        if "token" not in defaults:
            kwargs["token"] = None
        defaults = copy.deepcopy(defaults)
        return cls(
            defaults=defaults,
            token=kwargs.pop("token"),
            directory=kwargs.pop("directory"),
            **kwargs
        )
*** Original Comment ***: Initializes a class instance with default settings from YAML, ensuring a token is set (even if None), and handling additional configuration parameters.
*** Original Thought ***: 
*** Validation Score ***: 4.098061324130144
*** Validation Thought ***: Read code. Comment matches functionality. Explains defaults, token, and extra params. Slightly verbose but clear. Minor details omitted, not affecting comprehension.


===============================[ 9 ]===============================
def _connect_ssh(entry, username, idfile, tunnel=None):
    if entry.hostname != "" and entry.hostname is not None:
        _host = entry.hostname
    elif entry.public_ip != "" and entry.public_ip is not None:
        _host = entry.public_ip
    elif entry.private_ip != "" and entry.private_ip is not None:
        if tunnel is None:
            raise ValueError("Entry does not have a hostname or public IP. "
                             "You can connect via private IP if you use a "
                             "tunnel.")
        _host = entry.private_ip
    else:
        raise ValueError("No hostname, public IP or private IP information "
                         "found on host entry. I don't know how to connect.")
    command = _build_ssh_command(_host, username, idfile, None, tunnel)
    print('Connecting to %s...' % cyan(entry.display()))
    print('SSH command: %s' % green(command))
    proc = subprocess.Popen(command, shell=True)
    return proc.wait()
*** Original Comment ***: Establishes an SSH connection to a given host using hostname, public IP, or private IP through a tunnel if necessary, and raises exceptions if connection details are insufficient.
*** Original Thought ***: 
*** Validation Score ***: 4.2514074301531055
*** Validation Thought ***: Comment summarizes code well, includes error handling. Minor details omitted, e.g., command building/printing, but core function described. Concise and correct.


===============================[ 10 ]===============================
def _fitch_intersect(self, arrays):
        def pairwise_intersect(arr1, arr2):
            s2 = set(arr2)
            b3 = [val for val in arr1 if val in s2]
            return b3
        arrays = list(arrays) 
        N = len(arrays)
        while N > 1:
            arr1 = arrays.pop()
            arr2 = arrays.pop()
            arr = pairwise_intersect(arr1, arr2)
            arrays.append(arr)
            N = len(arrays)
        return arrays[0]
*** Original Comment ***: This function performs a pairwise intersection on a list of arrays, reducing them to a single array containing only the elements common to all arrays.
*** Original Thought ***: 
*** Validation Score ***: 4.676175370723426
*** Validation Thought ***: Read code, identified pairwise intersection of arrays. Comment concise, correct, explains main function. No redundant info. Score 5.


===============================[ 11 ]===============================
def ancestry(self, context):
        log.debug("get ancestry %s", context)
        if context is None:
            return
        if hasattr(context, 'im_class'):
            context = context.im_class
        elif hasattr(context, '__self__'):
            context = context.__self__.__class__
        if hasattr(context, '__module__'):
            ancestors = context.__module__.split('.')
        elif hasattr(context, '__name__'):
            ancestors = context.__name__.split('.')[:-1]
        else:
            raise TypeError("%s has no ancestors?" % context)
        while ancestors:
            log.debug(" %s ancestors %s", context, ancestors)
            yield resolve_name('.'.join(ancestors))
            ancestors.pop()
*** Original Comment ***: # This function retrieves and yields the ancestry of a given context, handling different types of context objects and logging the process.
*** Original Thought ***: 
*** Validation Score ***: 4.819468394987949
*** Validation Thought ***: Comment summarizes function well, mentioning retrieval, handling types, and logging. No redundant info. Correct and concise.


===============================[ 12 ]===============================
def get_item_abspath(self, identifier):
        admin_metadata = self.get_admin_metadata()
        uuid = admin_metadata["uuid"]
        dataset_cache_abspath = os.path.join(self._s3_cache_abspath, uuid)
        mkdir_parents(dataset_cache_abspath)
        bucket_fpath = self.data_key_prefix + identifier
        obj = self.s3resource.Object(self.bucket, bucket_fpath)
        relpath = obj.get()['Metadata']['handle']
        _, ext = os.path.splitext(relpath)
        local_item_abspath = os.path.join(
            dataset_cache_abspath,
            identifier + ext
        )
        if not os.path.isfile(local_item_abspath):
            tmp_local_item_abspath = local_item_abspath + ".tmp"
            self.s3resource.Bucket(self.bucket).download_file(
                bucket_fpath,
                tmp_local_item_abspath
            )
            os.rename(tmp_local_item_abspath, local_item_abspath)
        return local_item_abspath
*** Original Comment ***: Returns the absolute path of a local cache file corresponding to the given identifier, downloading it from S3 if not already cached.
*** Original Thought ***: 
*** Validation Score ***: 4.275936000674272
*** Validation Thought ***: Code retrieves/creates file path, downloads if needed. Comment summarizes well but omits mkdir_parents and temp file details.


===============================[ 13 ]===============================
def _main(self, client, copy_source, bucket, key, extra_args, callbacks,
              size):
        client.copy_object(
            CopySource=copy_source, Bucket=bucket, Key=key, **extra_args)
        for callback in callbacks:
            callback(bytes_transferred=size)
*** Original Comment ***: Executes an S3 copy operation with additional arguments and invokes callbacks with the size of the transferred data.
*** Original Thought ***: 
*** Validation Score ***: 4.125704858113605
*** Validation Thought ***: Comment summarizes code function well, but doesn't mention 'client' or 'copy_source' specifics. Still, it's clear and concise.


===============================[ 14 ]===============================
def set_option(self, key, value):
        if key == "rtmpdump":
            key = "rtmp-rtmpdump"
        elif key == "rtmpdump-proxy":
            key = "rtmp-proxy"
        elif key == "errorlog":
            key = "subprocess-errorlog"
        elif key == "errorlog-path":
            key = "subprocess-errorlog-path"
        if key == "http-proxy":
            self.http.proxies["http"] = update_scheme("http://", value)
        elif key == "https-proxy":
            self.http.proxies["https"] = update_scheme("https://", value)
        elif key == "http-cookies":
            if isinstance(value, dict):
                self.http.cookies.update(value)
            else:
                self.http.parse_cookies(value)
        elif key == "http-headers":
            if isinstance(value, dict):
                self.http.headers.update(value)
            else:
                self.http.parse_headers(value)
        elif key == "http-query-params":
            if isinstance(value, dict):
                self.http.params.update(value)
            else:
                self.http.parse_query_params(value)
        elif key == "http-trust-env":
            self.http.trust_env = value
        elif key == "http-ssl-verify":
            self.http.verify = value
        elif key == "http-disable-dh":
            if value:
                requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += ':!DH'
                try:
                    requests.packages.urllib3.contrib.pyopenssl.DEFAULT_SSL_CIPHER_LIST =                        requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS.encode("ascii")
                except AttributeError:
                    pass
        elif key == "http-ssl-cert":
            self.http.cert = value
        elif key == "http-timeout":
            self.http.timeout = value
        else:
            self.options.set(key, value)
*** Original Comment ***: Sets the specified option by updating the appropriate configuration attribute or dictionary within the object, with special handling for proxy, cookies, headers, query params, and SSL settings.
*** Original Thought ***: 
*** Validation Score ***: 4.078254957895923
*** Validation Thought ***: Comment summarizes code's function. Correct and concise, but lacks detail on key renaming. Not a '5' due to this omission.


===============================[ 15 ]===============================
def paid_invoices_by_date(request, form):
    products = form.cleaned_data["product"]
    categories = form.cleaned_data["category"]
    invoices = commerce.Invoice.objects.filter(
        (
            Q(lineitem__product__in=products) |
            Q(lineitem__product__category__in=categories)
        ),
        status=commerce.Invoice.STATUS_PAID,
    )
    payments = commerce.PaymentBase.objects.all()
    payments = payments.filter(
        invoice__in=invoices,
    )
    payments = payments.order_by("invoice")
    invoice_max_time = payments.values("invoice").annotate(
        max_time=Max("time")
    )
    zero_value_invoices = invoices.filter(value=0)
    times = itertools.chain(
        (line["max_time"] for line in invoice_max_time),
        (invoice.issue_time for invoice in zero_value_invoices),
    )
    by_date = collections.defaultdict(int)
    for time in times:
        date = datetime.datetime(
            year=time.year, month=time.month, day=time.day
        )
        by_date[date] += 1
    data = [(date_, count) for date_, count in sorted(by_date.items())]
    data = [(date_.strftime("%Y-%m-%d"), count) for date_, count in data]
    return ListReport(
        "Paid Invoices By Date",
        ["date", "count"],
        data,
    )
*** Original Comment ***: # Retrieves and aggregates paid invoices and zero-value invoices by date, returning a report with the count of such invoices for each date.
*** Original Thought ***: 
*** Validation Score ***: 4.359973680641606
*** Validation Thought ***: Comment summarizes code's function well. Minor details omitted, but main purpose clear. Concise and correct, hence score of 4.


===============================[ 16 ]===============================
def transcript_str(transcript_obj, gene_name=None):
    if transcript_obj.get('exon'):
        gene_part, part_count_raw = 'exon', transcript_obj['exon']
    elif transcript_obj.get('intron'):
        gene_part, part_count_raw = 'intron', transcript_obj['intron']
    else:
        gene_part, part_count_raw = 'intergenic', '0'
    part_count = part_count_raw.rpartition('/')[0]
    change_str = "{}:{}{}:{}:{}".format(
        transcript_obj.get('refseq_id', ''),
        gene_part,
        part_count,
        transcript_obj.get('coding_sequence_name', 'NA'),
        transcript_obj.get('protein_sequence_name', 'NA'),
    )
    if gene_name:
        change_str = "{}:".format(gene_name) + change_str
    return change_str
*** Original Comment ***: # Generates a string representation of a transcript object, including gene part, count, and sequence names, with an optional gene name prefix.
*** Original Thought ***: 
*** Validation Score ***: 4.789007411067948
*** Validation Thought ***: Comment summarizes code function accurately, concisely, and without redundancy. It matches the code's logic and outputs.


===============================[ 17 ]===============================
def compute(self, tdb, tdb2, derivative=True):
        scalar = not getattr(tdb, 'shape', 0) and not getattr(tdb2, 'shape', 0)
        if scalar:
            tdb = array((tdb,))
        data = self._data
        if data is None:
            self._data = data = self._load()
        initial_epoch, interval_length, coefficients = data
        component_count, n, coefficient_count = coefficients.shape
        index, offset = divmod((tdb - initial_epoch) + tdb2, interval_length)
        index = index.astype(int)
        if (index < 0).any() or (index > n).any():
            final_epoch = initial_epoch + interval_length * n
            raise ValueError('segment only covers dates %.1f through %.1f'
                            % (initial_epoch, final_epoch))
        omegas = (index == n)
        index[omegas] -= 1
        offset[omegas] += interval_length
        coefficients = coefficients[:,index]
        T = empty((coefficient_count, len(index)))
        T[0] = 1.0
        T[1] = t1 = 2.0 * offset / interval_length - 1.0
        twot1 = t1 + t1
        for i in range(2, coefficient_count):
            T[i] = twot1 * T[i-1] - T[i-2]
        components = (T.T * coefficients).sum(axis=2)
        if scalar:
            components = components[:,0]
        if not derivative:
            return components
        dT = empty_like(T)
        dT[0] = 0.0
        dT[1] = 1.0
        if coefficient_count > 2:
            dT[2] = twot1 + twot1
            for i in range(3, coefficient_count):
                dT[i] = twot1 * dT[i-1] - dT[i-2] + T[i-1] + T[i-1]
        dT *= 2.0
        dT /= interval_length
        rates = (dT.T * coefficients).sum(axis=2)
        if scalar:
            rates = rates[:,0]
        return components, rates
*** Original Comment ***: Calculates and optionally returns the derivative of a time-dependent function using Chebyshev polynomials over a specified interval.
*** Original Thought ***: 
*** Validation Score ***: 4.0188679285679685
*** Validation Thought ***: Comment summarizes code's function. Mentions Chebyshev polynomials, derivatives, and intervals. Omits error handling and scalar handling details. Mostly comprehensive.


===============================[ 18 ]===============================
def expand_and_standardize_dataset(response_index, response_header, data_set, col_vals, headers, standardizers, feats_to_ignore, columns_to_expand, outcome_trans_dict):
  modified_set = []
  for row_index, row in enumerate(data_set):
    new_row = []
    for col_index, val in enumerate(row):
      header = headers[col_index]
      if col_index == response_index:
        new_outcome = outcome_trans_dict[val]
        new_row.append(new_outcome)
      elif header in feats_to_ignore:
        pass
      elif header in columns_to_expand:
        for poss_val in col_vals[header]:
          if val == poss_val:
            new_cat_val = 1.0
          else:
            new_cat_val = -1.0
          new_row.append(new_cat_val)
      else:
        new_cont_val = float((val - standardizers[header]['mean']) / standardizers[header]['std_dev'])
        new_row.append(new_cont_val)
    modified_set.append(new_row)
  expanded_headers = []
  for header in headers:
    if header in feats_to_ignore:
      pass
    elif (header in columns_to_expand) and (header is not response_header):
      for poss_val in col_vals[header]:
        new_header = '{}_{}'.format(header,poss_val)
        expanded_headers.append(new_header)
    else:
      expanded_headers.append(header)
  return modified_set, expanded_headers
*** Original Comment ***: This function preprocesses a dataset by transforming the outcome variable, ignoring specified features, expanding categorical variables, and standardizing continuous variables.
*** Original Thought ***: 
*** Validation Score ***: 4.983021536642061
*** Validation Thought ***: Comment accurately summarizes key functions: transforming, ignoring, expanding, standardizing. No redundant info. Correct and concise.


===============================[ 19 ]===============================
def _get_convert_to_tensor_fn(identifier):
  if identifier is None:
    return None
  if isinstance(identifier, six.string_types):
    identifier = str(identifier)
    return _deserialize(identifier)
  if isinstance(identifier, dict):
    return _deserialize(identifier)
  if isinstance(identifier, property):
    identifier = identifier.fget
  if callable(identifier):
    return identifier
  raise ValueError('Could not interpret '
                   'convert-to-tensor function identifier:', identifier)
*** Original Comment ***: Returns a tensor conversion function based on the provided identifier, which can be a string, dictionary, property, or callable.
*** Original Thought ***: 
*** Validation Score ***: 4.052662012504052
*** Validation Thought ***: Comment summarizes code purpose. Includes types of identifiers. Omits error handling. Not redundant. Correct and concise.


===============================[ 20 ]===============================
def plot_rb_data(xdata, ydatas, yavg, yerr, fit, survival_prob, ax=None,
                 show_plt=True):
    if not HAS_MATPLOTLIB:
        raise ImportError('The function plot_rb_data needs matplotlib. '
                          'Run "pip install matplotlib" before.')
    if ax is None:
        plt.figure()
        ax = plt.gca()
    for ydata in ydatas:
        ax.plot(xdata, ydata, color='gray', linestyle='none', marker='x')
    ax.errorbar(xdata, yavg, yerr=yerr, color='r', linestyle='--', linewidth=3)
    ax.plot(xdata, survival_prob(xdata, *fit), color='blue', linestyle='-', linewidth=2)
    ax.tick_params(labelsize=14)
    ax.set_xlabel('Clifford Length', fontsize=16)
    ax.set_ylabel('Z', fontsize=16)
    ax.grid(True)
    if show_plt:
        plt.show()
*** Original Comment ***: This function plots randomized benchmarking data with individual sequences in gray, the average sequence fidelity in red with error bars, and the fitted survival probability curve in blue, with the option to display the plot immediately.
*** Original Thought ***: 
*** Validation Score ***: 4.817062158005371
*** Validation Thought ***: Comment matches code functionality, mentions key elements (colors, error bars, fit), and is concise. No redundant info.


===============================[ 21 ]===============================
def ns_bindings(
    ns_name: str,
    module: types.ModuleType = None,
    ns_var_name: str = NS_VAR_NAME,
    ns_var_ns: str = NS_VAR_NS,
):
    symbol = sym.Symbol(ns_name)
    ns = Namespace.get_or_create(symbol, module=module)
    ns_var_sym = sym.Symbol(ns_var_name, ns=ns_var_ns)
    ns_var = Maybe(Var.find(ns_var_sym)).or_else_raise(
        lambda: RuntimeException(
            f"Dynamic Var {sym.Symbol(ns_var_name, ns=ns_var_ns)} not bound!"
        )
    )
    try:
        logger.debug(f"Binding {ns_var_sym} to {ns}")
        ns_var.push_bindings(ns)
        yield ns_var.value
    finally:
        ns_var.pop_bindings()
        logger.debug(f"Reset bindings for {ns_var_sym} to {ns_var.value}")
*** Original Comment ***: Creates a context manager for temporarily binding a namespace variable to a given namespace, yielding its value, and ensuring bindings are reset afterwards.
*** Original Thought ***: 
*** Validation Score ***: 4.927005024335628
*** Validation Thought ***: Comment accurately describes the context manager's purpose and behavior in concise terms. No redundant information.


===============================[ 22 ]===============================
def f_remove_child(self, name, recursive=False, predicate=None):
        if name not in self._children:
            raise ValueError('Your group `%s` does not contain the child `%s`.' %
                             (self.v_full_name, name))
        else:
            child = self._children[name]
            if (name not in self._links and
                    not child.v_is_leaf and
                    child.f_has_children() and
                    not recursive):
                raise TypeError('Cannot remove child. It is a group with children. Use'
                                ' f_remove with ``recursive = True``')
            else:
                self._nn_interface._remove_subtree(self, name, predicate)
*** Original Comment ***: Removes a child node from the current group, with options for recursive removal and custom predicates, raising errors if conditions are not met.
*** Original Thought ***: 
*** Validation Score ***: 4.200519641685965
*** Validation Thought ***: Comment summarizes function well, includes error conditions. Lacks detail on predicate's role. Concise, no redundancy. Almost perfect.


===============================[ 23 ]===============================
def setZeroResettableKWH(self, password="00000000"):
        result = False
        self.setContext("setZeroResettableKWH")
        try:
            if not self.requestA():
                self.writeCmdMsg("Bad read CRC on setting")
            else:
                if not self.serialCmdPwdAuth(password):
                    self.writeCmdMsg("Password failure")
                else:
                    req_str = "0157310230304433282903"
                    req_str += self.calc_crc16(req_str[2:].decode("hex"))
                    self.m_serial_port.write(req_str.decode("hex"))
                    if self.m_serial_port.getResponse(self.getContext()).encode("hex") == "06":
                        self.writeCmdMsg("Success: 06 returned.")
                        result = True
            self.serialPostEnd()
        except:
            ekm_log(traceback.format_exc(sys.exc_info()))
        self.setContext("")
        return result
*** Original Comment ***: Sets the kWh meter to zero if the correct password is provided and logs the result; returns True on success, False otherwise.
*** Original Thought ***: 
*** Validation Score ***: 4.093380347275682
*** Validation Thought ***: Code resets kWh meter with password. Comment matches but lacks try-except detail. Concise, correct, but not fully comprehensive.


===============================[ 24 ]===============================
def get_environ_vars(self):
        for key, val in os.environ.items():
            if _environ_prefix_re.search(key):
                yield (_environ_prefix_re.sub("", key).lower(), val)
*** Original Comment ***: # This function yields environment variable names (stripped of a specific prefix and converted to lowercase) and their values if they match a predefined regular expression pattern.
*** Original Thought ***: 
*** Validation Score ***: 4.8983441697634875
*** Validation Thought ***: Comment matches code function and is concise, clear, and correct. No redundant information.


===============================[ 25 ]===============================
def create_cname_record(self, name, values, ttl=60, weight=None, region=None,
                           set_identifier=None):
        self._halt_if_already_deleted()
        values = locals()
        del values['self']
        return self._add_record(CNAMEResourceRecordSet, **values)
*** Original Comment ***: Creates a CNAME record with the specified parameters, ensuring the operation is not performed on a deleted entity.
*** Original Thought ***: 
*** Validation Score ***: 4.224928096219101
*** Validation Thought ***: Code defines function to create CNAME record. Comment summarizes well, but doesn't mention 'values' local variable assignment.


===============================[ 26 ]===============================
def _wakeup(self):
        log.info("send: WAKEUP")
        for i in xrange(3):
            self.port.write('\n')  
            ack = self.port.read(len(self.WAKE_ACK))  
            log_raw('read', ack)
            if ack == self.WAKE_ACK:
                return
        raise NoDeviceException('Can not access weather station')
*** Original Comment ***: Initializes communication with the weather station by sending a wakeup signal and waits for an acknowledgment; raises an exception if no acknowledgment is received after three attempts.
*** Original Thought ***: 
*** Validation Score ***: 4.946025497983165
*** Validation Thought ***: Comment matches code function, is correct, concise, and lacks redundant information.


===============================[ 27 ]===============================
def _srvc_load_several_items(self, iterable, *args, **kwargs):
        for input_tuple in iterable:
            msg = input_tuple[0]
            item = input_tuple[1]
            if len(input_tuple) > 2:
                args = input_tuple[2]
            if len(input_tuple) > 3:
                kwargs = input_tuple[3]
            if len(input_tuple) > 4:
                raise RuntimeError('You shall not pass!')
            self.load(msg, item, *args, **kwargs)
*** Original Comment ***: # Loads multiple items by iterating over an iterable, each element of which can contain a message, an item, and optional args and kwargs; raises an error if an element contains more than four items.
*** Original Thought ***: 
*** Validation Score ***: 4.402151560010842
*** Validation Thought ***: Comment matches code function, explains error case. Slightly verbose but clear. Missing explanation for args, kwargs reassignment.


===============================[ 28 ]===============================
def estimate_beats(self):
        if self._audio_percussive is None:
            self._audio_harmonic, self._audio_percussive = self.compute_HPSS()
        tempo, frames = librosa.beat.beat_track(
            y=self._audio_percussive, sr=self.sr,
            hop_length=self.hop_length)
        times = librosa.frames_to_time(frames, sr=self.sr,
                                       hop_length=self.hop_length)
        if len(times) > 0 and times[0] == 0:
            times = times[1:]
            frames = frames[1:]
        return times, frames
*** Original Comment ***: Estimates the beat times and frame indices of percussive elements in an audio signal using harmonic-percussive source separation and Librosa's beat tracking.
*** Original Thought ***: 
*** Validation Score ***: 4.559338553796469
*** Validation Thought ***: Read code, identified beat estimation using HPSS and Librosa. Comment concise, correct, summarizes code purpose. No redundancy.


===============================[ 29 ]===============================
def updateLogicalInterface(self, logicalInterfaceId, name, schemaId, description=None):
        req = ApiClient.oneLogicalInterfaceUrl % (self.host, "/draft", logicalInterfaceId)
        body = {"name" : name, "schemaId" : schemaId, "id" : logicalInterfaceId}
        if description:
            body["description"] = description
        resp = requests.put(req, auth=self.credentials, headers={"Content-Type":"application/json"},
                            data=json.dumps(body),  verify=self.verify)
        if resp.status_code == 200:
            self.logger.debug("Logical interface updated")
        else:
            raise ibmiotf.APIException(resp.status_code, "HTTP error updating logical interface", resp)
        return resp.json()
*** Original Comment ***: Updates the specified logical interface with new name, schemaId, and optionally description, raising an exception for HTTP errors.
*** Original Thought ***: 
*** Validation Score ***: 4.311598980467031
*** Validation Thought ***: Comment summarizes function well, includes optional description. Misses 'draft' context and logging detail. Otherwise, clear and concise.


===============================[ 30 ]===============================
def add_path(path, config=None):
    log.debug('Add path %s' % path)    
    if not path:
        return []
    added = []
    parent = os.path.dirname(path)
    if (parent
        and os.path.exists(os.path.join(path, '__init__.py'))):
        added.extend(add_path(parent, config))
    elif not path in sys.path:
        log.debug("insert %s into sys.path", path)
        sys.path.insert(0, path)
        added.append(path)
    if config and config.srcDirs:
        for dirname in config.srcDirs:
            dirpath = os.path.join(path, dirname)
            if os.path.isdir(dirpath):
                sys.path.insert(0, dirpath)
                added.append(dirpath)
    return added
*** Original Comment ***: Adds the specified `path` to `sys.path` if it contains an `__init__.py` or is not already in `sys.path`, and also adds subdirectories specified in `config.srcDirs`.
*** Original Thought ***: 
*** Validation Score ***: 3.998453401685053
*** Validation Thought ***: Comment summarizes key points. Misses logging details but they're not essential. Concise and correct. Minor details omitted.


===============================[ 31 ]===============================
def _convert_endpoint(endpoint):
    pb_endpoint = zipkin_pb2.Endpoint()
    if endpoint.service_name:
        pb_endpoint.service_name = endpoint.service_name
    if endpoint.port and endpoint.port != 0:
        pb_endpoint.port = endpoint.port
    if endpoint.ipv4:
        pb_endpoint.ipv4 = socket.inet_pton(socket.AF_INET, endpoint.ipv4)
    if endpoint.ipv6:
        pb_endpoint.ipv6 = socket.inet_pton(socket.AF_INET6, endpoint.ipv6)
    return pb_endpoint
*** Original Comment ***: Converts a given endpoint object to a protobuf Endpoint format, populating service name, port, and IP address fields if available.
*** Original Thought ***: 
*** Validation Score ***: 4.641684814284887
*** Validation Thought ***: Read code, identifies conversion function. Comment matches code's function, is correct and concise. No redundant info, easy to understand.


===============================[ 32 ]===============================
def register_blueprints(app):
    app.register_blueprint(public.public_bp)
    app.register_blueprint(genes.genes_bp)
    app.register_blueprint(cases.cases_bp)
    app.register_blueprint(login.login_bp)
    app.register_blueprint(variants.variants_bp)
    app.register_blueprint(panels.panels_bp)
    app.register_blueprint(dashboard.dashboard_bp)
    app.register_blueprint(api.api_bp)
    app.register_blueprint(alignviewers.alignviewers_bp)
    app.register_blueprint(phenotypes.hpo_bp)
    app.register_blueprint(institutes.overview)
*** Original Comment ***: Registers multiple Flask blueprints to the application for different components such as public views, genes, cases, login, variants, panels, dashboard, API, alignment viewers, phenotypes, and institutes overview.
*** Original Thought ***: 
*** Validation Score ***: 4.996281633577851
*** Validation Thought ***: Comment accurately summarizes code function. It's concise, correct, and includes all key points without redundancy.


===============================[ 33 ]===============================
def peek(self, n):
        if self._len < n:
            raise ValueError("Not enough bytes in the queue")
        def part_generator(n):
            remaining = n
            for part in self._parts:
                if len(part) <= remaining:
                    yield part
                    remaining -= len(part)
                else:
                    yield part[:remaining]
                    remaining = 0 
                if remaining == 0:
                    break
        return "".join(part_generator(n))
*** Original Comment ***: Returns the first `n` bytes from the queue without removing them, raising a `ValueError` if there are not enough bytes available.
*** Original Thought ***: 
*** Validation Score ***: 4.971720023041736
*** Validation Thought ***: Comment matches code function, is correct, concise, and no redundant information.


===============================[ 34 ]===============================
def get_memory_info(self):
        rss, vms = _psutil_bsd.get_process_memory_info(self.pid)[:2]
        return nt_meminfo(rss, vms)
*** Original Comment ***: Returns a named tuple containing the Resident Set Size (RSS) and Virtual Memory Size (VMS) of the process identified by `self.pid`.
*** Original Thought ***: 
*** Validation Score ***: 4.974612216295484
*** Validation Thought ***: Comment matches code function, correctly and concisely describes what the code does. No redundant information.


===============================[ 35 ]===============================
def spawn(self, ident, func, *args, **kwargs):
        if self._pool:
            self._pool.spawn_n(self._spawner, ident, func, *args, **kwargs)
            sleep()
        else:
            self._spawner(ident, func, *args, **kwargs)
*** Original Comment ***: Initializes a new task in the pool if available, or directly runs the task with the provided identifier and function, along with any arguments and keyword arguments.
*** Original Thought ***: 
*** Validation Score ***: 4.310379186084385
*** Validation Thought ***: Comment summarizes code well, explaining conditional task handling. Minor lack of detail on 'sleep()' function. Otherwise, clear and concise.


===============================[ 36 ]===============================
def dist_in_usersite(dist):
    norm_path = normalize_path(dist_location(dist))
    return norm_path.startswith(normalize_path(user_site))
*** Original Comment ***: # Check if the given distribution is located within the user site directory by normalizing the paths and comparing them.
*** Original Thought ***: 
*** Validation Score ***: 4.827323969696626
*** Validation Thought ***: Code checks if distribution is in user site. Comment matches functionality. Correct and concise. No redundant info.


===============================[ 37 ]===============================
def parse_gene(gene_info):
    gene = {}
    identifier = None
    hgnc_id = None
    try:
        if 'hgnc_id' in gene_info:
            hgnc_id = int(gene_info['hgnc_id'])
        elif 'hgnc_idnumber' in gene_info:
            hgnc_id = int(gene_info['hgnc_idnumber'])
        elif 'hgncid' in gene_info:
            hgnc_id = int(gene_info['hgncid'])
    except ValueError as e:
        raise SyntaxError("Invalid hgnc id: {0}".format(hgnc_id))
    gene['hgnc_id'] = hgnc_id
    identifier = hgnc_id
    hgnc_symbol = None
    if 'hgnc_symbol' in gene_info:
        hgnc_symbol = gene_info['hgnc_symbol']
    elif 'hgncsymbol' in gene_info:
        hgnc_symbol = gene_info['hgncsymbol']
    elif 'symbol' in gene_info:
        hgnc_symbol = gene_info['symbol']
    gene['hgnc_symbol'] = hgnc_symbol
    if not identifier:
        if hgnc_symbol:
            identifier = hgnc_symbol
        else:
            raise SyntaxError("No gene identifier could be found")
    gene['identifier'] = identifier
    transcripts = ""
    if 'disease_associated_transcripts' in gene_info:
        transcripts = gene_info['disease_associated_transcripts']
    elif 'disease_associated_transcript' in gene_info:
        transcripts = gene_info['disease_associated_transcript']
    elif 'transcripts' in gene_info:
        transcripts = gene_info['transcripts']
    gene['transcripts'] = [
            transcript.strip() for transcript in
            transcripts.split(',') if transcript
        ]
    models = ""
    if 'genetic_disease_models' in gene_info:
        models = gene_info['genetic_disease_models']
    elif 'genetic_disease_model' in gene_info:
        models = gene_info['genetic_disease_model']
    elif 'inheritance_models' in gene_info:
        models = gene_info['inheritance_models']
    elif 'genetic_inheritance_models' in gene_info:
        models = gene_info['genetic_inheritance_models']
    gene['inheritance_models'] = [
        model.strip() for model in models.split(',')
        if model.strip() in VALID_MODELS
    ]
    gene['mosaicism'] = True if gene_info.get('mosaicism') else False
    gene['reduced_penetrance'] = True if gene_info.get('reduced_penetrance') else False
    gene['database_entry_version'] = gene_info.get('database_entry_version')
    return gene
*** Original Comment ***: Parses gene information from a dictionary, extracting and validating HGNC ID, symbol, associated transcripts, disease models, and additional attributes, raising a SyntaxError for invalid or missing identifiers.
*** Original Thought ***: 
*** Validation Score ***: 4.560923167597228
*** Validation Thought ***: Comment summarizes code function accurately, concisely, and without redundancy. It covers all key points.


===============================[ 38 ]===============================
def quadrature_scheme_softmaxnormal_quantiles(
    normal_loc, normal_scale, quadrature_size,
    validate_args=False, name=None):
  with tf.name_scope(name or "softmax_normal_grid_and_probs"):
    normal_loc = tf.convert_to_tensor(value=normal_loc, name="normal_loc")
    dt = dtype_util.base_dtype(normal_loc.dtype)
    normal_scale = tf.convert_to_tensor(
        value=normal_scale, dtype=dt, name="normal_scale")
    normal_scale = maybe_check_quadrature_param(
        normal_scale, "normal_scale", validate_args)
    dist = normal.Normal(loc=normal_loc, scale=normal_scale)
    def _get_batch_ndims():
      ndims = tensorshape_util.rank(dist.batch_shape)
      if ndims is None:
        ndims = tf.shape(input=dist.batch_shape_tensor())[0]
      return ndims
    batch_ndims = _get_batch_ndims()
    def _get_final_shape(qs):
      bs = tensorshape_util.with_rank_at_least(dist.batch_shape, 1)
      num_components = tf.compat.dimension_value(bs[-1])
      if num_components is not None:
        num_components += 1
      tail = tf.TensorShape([num_components, qs])
      return bs[:-1].concatenate(tail)
    def _compute_quantiles():
      zero = tf.zeros([], dtype=dist.dtype)
      edges = tf.linspace(zero, 1., quadrature_size + 3)[1:-1]
      edges = tf.reshape(
          edges,
          shape=tf.concat(
              [[-1], tf.ones([batch_ndims], dtype=tf.int32)], axis=0))
      quantiles = dist.quantile(edges)
      quantiles = softmax_centered_bijector.SoftmaxCentered().forward(quantiles)
      perm = tf.concat([tf.range(1, 1 + batch_ndims), [0]], axis=0)
      quantiles = tf.transpose(a=quantiles, perm=perm)
      tensorshape_util.set_shape(
          quantiles, _get_final_shape(quadrature_size + 1))
      return quantiles
    quantiles = _compute_quantiles()
    grid = (quantiles[..., :-1] + quantiles[..., 1:]) / 2.
    tensorshape_util.set_shape(grid, _get_final_shape(quadrature_size))
    probs = tf.fill(
        dims=[quadrature_size], value=1. / tf.cast(quadrature_size, dist.dtype))
    return grid, probs
*** Original Comment ***: Generates a grid of quantiles and corresponding probabilities for a softmax-transformed normal distribution using a specified quadrature scheme size.
*** Original Thought ***: 
*** Validation Score ***: 4.359723766938494
*** Validation Thought ***: Comment summarizes code's purpose. Minor details omitted, but main function clear. Concise, correct, but lacks detail on quantile calculation method.


===============================[ 39 ]===============================
def _block_tuple(iterator, dtypes, bsize=-1):
    i = 0
    blocked_tuple = None
    for tuple_i in iterator:
        if blocked_tuple is None:
            blocked_tuple = tuple([] for _ in range(len(tuple_i)))
        if (bsize > 0) and (i >= bsize):
            yield tuple(_pack_accumulated(x, dtype)
                        for x, dtype in zip(blocked_tuple, dtypes))
            blocked_tuple = tuple([] for _ in range(len(tuple_i)))
            i = 0
        for x_j, x in zip(tuple_i, blocked_tuple):
            x.append(x_j)
        i += 1
    if i > 0:
        yield tuple(_pack_accumulated(x, dtype)
                    for x, dtype in zip(blocked_tuple, dtypes))
*** Original Comment ***: Generates and yields blocks of tuples from an iterator, with optional type conversion and block size control.
*** Original Thought ***: 
*** Validation Score ***: 4.3448532770315
*** Validation Thought ***: Comment summarizes code purpose. Includes type conversion, block size. Lacks detail on block initialization/reset. Mostly clear and concise.


===============================[ 40 ]===============================
def make_time_tree(self, time_marginal=False, clock_rate=None, **kwargs):
        self.logger("ClockTree: Maximum likelihood tree optimization with temporal constraints",1)
        self.init_date_constraints(clock_rate=clock_rate, **kwargs)
        if time_marginal:
            self._ml_t_marginal(assign_dates = time_marginal=="assign")
        else:
            self._ml_t_joint()
        self.convert_dates()
*** Original Comment ***: Initializes and optimizes a maximum likelihood phylogenetic tree with temporal constraints, optionally using marginal or joint likelihood methods based on the `time_marginal` parameter.
*** Original Thought ***: 
*** Validation Score ***: 4.1977394510186565
*** Validation Thought ***: Comment summarizes code function well, mentions temporal constraints and likelihood methods. Slightly lacks detail on date conversion, but remains concise and correct.


===============================[ 41 ]===============================
def _run_sql(self, sql, params, raw=True, output=False):
        toget = 'source_raw' if raw else 'source'
        sqlfrom = "history"
        if output:
            sqlfrom = "history LEFT JOIN output_history USING (session, line)"
            toget = "history.%s, output_history.output" % toget
        cur = self.db.execute("SELECT session, line, %s FROM %s " %                                (toget, sqlfrom) + sql, params)
        if output:    
            return ((ses, lin, (inp, out)) for ses, lin, inp, out in cur)
        return cur
*** Original Comment ***: Executes a SQL query with optional parameters, joining with output history if required, and returns a cursor or generator with the results.
*** Original Thought ***: 
*** Validation Score ***: 4.049766594981469
*** Validation Thought ***: Comment summarizes key points: execution, optional params, join condition, return type. Minor details omitted, but main functionality clear.


===============================[ 42 ]===============================
def read_image(filepath):
  im_bytes = tf.io.read_file(filepath)
  im = tf.image.decode_image(im_bytes, channels=CHANNELS)
  im = tf.image.convert_image_dtype(im, tf.float32)
  return im
*** Original Comment ***: Reads an image from the specified file path, decodes it with a predefined number of channels, converts its data type to float32, and returns the resulting tensor.
*** Original Thought ***: 
*** Validation Score ***: 4.977642667513242
*** Validation Thought ***: Comment matches code function, mentions decoding, data type conversion, and tensor return. Correct and concise.


===============================[ 43 ]===============================
def highlight(string, keywords, cls_name='highlighted'):
    if not keywords:
        return string
    if not string:
        return ''
    include, exclude = get_text_tokenizer(keywords)
    highlighted = highlight_text(include, string, cls_name)
    return highlighted
*** Original Comment ***: Adds HTML class-based highlighting to keywords found within a given string, returning the modified string.
*** Original Thought ***: 
*** Validation Score ***: 4.014028746386461
*** Validation Thought ***: Code adds HTML class to keywords in string. Comment concise, correct, but doesn't mention handling empty inputs. Almost perfect.


===============================[ 44 ]===============================
def expand_to_vector(x, tensor_name=None, op_name=None, validate_args=False):
  with tf.name_scope(op_name or "expand_to_vector"):
    x = tf.convert_to_tensor(value=x, name="x")
    ndims = tensorshape_util.rank(x.shape)
    if ndims is None:
      if validate_args:
        x = with_dependencies([
            assert_util.assert_rank_at_most(
                x, 1, message="Input is neither scalar nor vector.")
        ], x)
      ndims = tf.rank(x)
      expanded_shape = pick_vector(
          tf.equal(ndims, 0), np.array([1], dtype=np.int32), tf.shape(input=x))
      return tf.reshape(x, expanded_shape)
    elif ndims == 0:
      x_const = tf.get_static_value(x)
      if x_const is not None:
        return tf.convert_to_tensor(
            value=dtype_util.as_numpy_dtype(x.dtype)([x_const]),
            name=tensor_name)
      else:
        return tf.reshape(x, [1])
    elif ndims != 1:
      raise ValueError("Input is neither scalar nor vector.")
    return x
*** Original Comment ***: Expands a scalar or vector `x` into a 1-D tensor, with optional argument validation and custom naming within TensorFlow's graph context.
*** Original Thought ***: 
*** Validation Score ***: 4.4465644581161285
*** Validation Thought ***: Comment summarizes code's purpose. Includes validation and naming. Misses error handling for non-scalar/vector inputs. Concise, no redundancy.


===============================[ 45 ]===============================
def godot_options(self, info):
        if info.initialized:
            self.edit_traits( parent = info.ui.control,
                              kind   = "livemodal",
                              view   = "options_view" )
*** Original Comment ***: Initializes a modal dialog with specific traits if the `info` object is already initialized.
*** Original Thought ***: 
*** Validation Score ***: 3.9468524134287812
*** Validation Thought ***: Comment summarizes code well, but 'godot_options' purpose isn't clear. No logical errors, slightly lacks conciseness due to 'specific traits' vagueness.


===============================[ 46 ]===============================
def _get_rule_transform(self, rule):
    rd = self._find_directive(lambda d: d.name == "rule" and d.args.get("name") == rule.name)
    if rd:
      args = rd.args
    else:
      args = {}
    transform = args.get("transform", "retype")
    if transform == "retype":
      new_name = args.get("to_type", "TokenType.{0}".format(rule.name))
      return ".retyped({0})".format(new_name)
    elif transform == "compress":
      new_name = args.get("to_type", "TokenType.{0}".format(rule.name))
      if new_name == "identity":
        return ".compressed()"
      else:
        return ".compressed({0})".format(new_name)
    elif transform == "identity":
      return ""
*** Original Comment ***: Returns a string representing the transformation directive for a given rule, based on the rule's name and associated arguments.
*** Original Thought ***: 
*** Validation Score ***: 4.101255897388038
*** Validation Thought ***: Comment summarizes function well, but doesn't mention default 'retype' behavior or empty string for 'identity'.


===============================[ 47 ]===============================
def draft_pick(self):
        doc = self.get_main_doc()
        try:
            p_tags = doc('div#meta p')
            draft_p_tag = next(p for p in p_tags.items() if p.text().lower().startswith('draft'))
            draft_pick = int(re.search(r'(\d+)\w{,3}\s+?overall', draft_p_tag.text()).group(1))
            return draft_pick
        except Exception as e:
            return None
*** Original Comment ***: Extracts and returns the draft pick number from a 'div#meta p' element that starts with 'draft' in the main document, or returns None if an error occurs.
*** Original Thought ***: 
*** Validation Score ***: 4.879151573370423
*** Validation Thought ***: Read code, identifies draft pick extraction with error handling. Comment matches functionality and is concise. No redundant info.


===============================[ 48 ]===============================
def picard_mark_duplicates(job, bam, bai, validation_stringency='LENIENT'):
    work_dir = job.fileStore.getLocalTempDir()
    job.fileStore.readGlobalFile(bam, os.path.join(work_dir, 'sorted.bam'))
    job.fileStore.readGlobalFile(bai, os.path.join(work_dir, 'sorted.bai'))
    command = ['MarkDuplicates',
               'INPUT=sorted.bam',
               'OUTPUT=mkdups.bam',
               'METRICS_FILE=metrics.txt',
               'ASSUME_SORTED=true',
               'CREATE_INDEX=true',
               'VALIDATION_STRINGENCY=%s' % validation_stringency.upper()]
    docker_parameters = ['--rm',
                         '--log-driver', 'none',
                         '-e', 'JAVA_OPTIONS=-Djava.io.tmpdir=/data/ -Xmx{}'.format(job.memory),
                         '-v', '{}:/data'.format(work_dir)]
    start_time = time.time()
    dockerCall(job=job, workDir=work_dir,
               parameters=command,
               tool='quay.io/ucsc_cgl/picardtools:1.95--dd5ac549b95eb3e5d166a5e310417ef13651994e',
               dockerParameters=docker_parameters)
    end_time = time.time()
    _log_runtime(job, start_time, end_time, "Picard MarkDuplicates")
    bam = job.fileStore.writeGlobalFile(os.path.join(work_dir, 'mkdups.bam'))
    bai = job.fileStore.writeGlobalFile(os.path.join(work_dir, 'mkdups.bai'))
    return bam, bai
*** Original Comment ***: This function `picard_mark_duplicates` identifies and marks duplicate reads in a BAM file using Picard's MarkDuplicates tool, with configurable validation stringency, within a job file system environment.
*** Original Thought ***: 
*** Validation Score ***: 4.609865978682142
*** Validation Thought ***: Comment summarizes function well, mentions key points. Misses minor details like output files, but these are not critical for understanding.


===============================[ 49 ]===============================
def patch_transport_fake_push_producer(transport):
    patch_if_missing(transport, 'pauseProducing', lambda: None)
    patch_if_missing(transport, 'resumeProducing', lambda: None)
    patch_if_missing(transport, 'stopProducing', transport.loseConnection)
*** Original Comment ***: Adds no-op implementations for `pauseProducing` and `resumeProducing`, and maps `stopProducing` to `loseConnection` if they are not already implemented in the `transport` object.
*** Original Thought ***: 
*** Validation Score ***: 4.8706741269144675
*** Validation Thought ***: Code adds default methods to transport. Comment correctly and concisely describes code behavior. No redundant info.


===============================[ 50 ]===============================
def format_function(
    func_body,
    func_type=None,
    indent=2,
    format_locals=True,
):
    if func_type is None:
        yield 'func'
    else:
        param_section = ' (param {})'.format(' '.join(
            map(format_lang_type, func_type.param_types)
        )) if func_type.param_types else ''
        result_section = ' (result {})'.format(
            format_lang_type(func_type.return_type)
        ) if func_type.return_type else ''
        yield 'func' + param_section + result_section
    if format_locals and func_body.locals:
        yield '(locals {})'.format(' '.join(itertools.chain.from_iterable(
            itertools.repeat(format_lang_type(x.type), x.count)
            for x in func_body.locals
        )))
    level = 1
    for cur_insn in decode_bytecode(func_body.code):
        if cur_insn.op.flags & INSN_LEAVE_BLOCK:
            level -= 1
        yield ' ' * (level * indent) + format_instruction(cur_insn)
        if cur_insn.op.flags & INSN_ENTER_BLOCK:
            level += 1
*** Original Comment ***: Generates a formatted string representation of a function's signature and body, with optional indentation and local variable formatting.
*** Original Thought ***: 
*** Validation Score ***: 4.162831216915877
*** Validation Thought ***: Comment summarizes code well, includes key points, but lacks detail on bytecode decoding. Not fully comprehensive, but concise.


===============================[ 51 ]===============================
def get_description(self):
        def split_header(s, get_header=True):
            s = s.lstrip().rstrip()
            parts = s.splitlines()
            if parts[0].startswith('#'):
                if get_header:
                    header = re.sub('#+\s*', '', parts.pop(0))
                    if not parts:
                        return header, ''
                else:
                    header = ''
                rest = '\n'.join(parts).lstrip().split('\n\n')
                desc = rest[0].replace('\n', ' ')
                return header, desc
            else:
                if get_header:
                    if parts[0].startswith(('=', '-')):
                        parts = parts[1:]
                    header = parts.pop(0)
                    if parts and parts[0].startswith(('=', '-')):
                        parts.pop(0)
                    if not parts:
                        return header, ''
                else:
                    header = ''
                rest = '\n'.join(parts).lstrip().split('\n\n')
                desc = rest[0].replace('\n', ' ')
                return header, desc
        first_cell = self.nb['cells'][0]
        if not first_cell['cell_type'] == 'markdown':
            return '', ''
        header, desc = split_header(first_cell['source'])
        if not desc and len(self.nb['cells']) > 1:
            second_cell = self.nb['cells'][1]
            if second_cell['cell_type'] == 'markdown':
                _, desc = split_header(second_cell['source'], False)
        return header, desc
*** Original Comment ***: Extracts and returns the header and description from the first markdown cell of a Jupyter notebook, handling optional formatting.
*** Original Thought ***: 
*** Validation Score ***: 4.161618159472374
*** Validation Thought ***: Comment summarizes function well, but doesn't mention handling of second cell if first lacks description.


===============================[ 52 ]===============================
def has_comment(src):
    readline = StringIO(src).readline
    toktypes = set()
    try:
        for t in tokenize.generate_tokens(readline):
            toktypes.add(t[0])
    except tokenize.TokenError:
        pass
    return(tokenize.COMMENT in toktypes)
*** Original Comment ***: Checks if the provided source code string contains any comments by tokenizing the input and searching for comment tokens.
*** Original Thought ***: 
*** Validation Score ***: 4.90212480967509
*** Validation Thought ***: Comment matches code function, correctly and concisely describes the process of checking for comments.


===============================[ 53 ]===============================
def _topic(self, topic):
        if self.int_id >= 0:
            base = "engine.%i" % self.int_id
        else:
            base = "kernel.%s" % self.ident
        return py3compat.cast_bytes("%s.%s" % (base, topic))
*** Original Comment ***: This function constructs and returns a byte-encoded string that uniquely identifies a topic based on the internal ID or identifier of an engine or kernel instance.
*** Original Thought ***: 
*** Validation Score ***: 4.696656968316834
*** Validation Thought ***: Comment accurately describes function's purpose and behavior. It's correct and concise, no redundant info.


===============================[ 54 ]===============================
def rdiscover_modules(directory):
    found = list()
    if os.path.isdir(directory):
        for entry in os.listdir(directory):
            next_dir = os.path.join(directory, entry)
            if os.path.isfile(os.path.join(next_dir, MODULE_INIT_FILE)):
                modules = _search_for_modules(next_dir, True, entry)
                found.extend(modules)
    return found
*** Original Comment ***: Scans the specified directory for subdirectories containing a module initializer file and returns a list of discovered modules.
*** Original Thought ***: 
*** Validation Score ***: 4.40098512413133
*** Validation Thought ***: Code scans directories for modules. Comment concise, correct, but doesn't mention MODULE_INIT_FILE explicitly. Minor detail omitted.


===============================[ 55 ]===============================
def _event_filter_console_keypress(self, event):
        intercepted = False
        cursor = self._control.textCursor()
        position = cursor.position()
        key = event.key()
        ctrl_down = self._control_key_down(event.modifiers())
        alt_down = event.modifiers() & QtCore.Qt.AltModifier
        shift_down = event.modifiers() & QtCore.Qt.ShiftModifier
        if event.matches(QtGui.QKeySequence.Copy):
            self.copy()
            intercepted = True
        elif event.matches(QtGui.QKeySequence.Cut):
            self.cut()
            intercepted = True
        elif event.matches(QtGui.QKeySequence.Paste):
            self.paste()
            intercepted = True
        elif key in (QtCore.Qt.Key_Return, QtCore.Qt.Key_Enter):
            intercepted = True
            self._cancel_completion()
            if self._in_buffer(position):
                if self._reading:
                    self._append_plain_text('\n')
                    self._reading = False
                    if self._reading_callback:
                        self._reading_callback()
                elif not self._executing:
                    cursor.movePosition(QtGui.QTextCursor.End,
                                        QtGui.QTextCursor.KeepAnchor)
                    at_end = len(cursor.selectedText().strip()) == 0
                    single_line = (self._get_end_cursor().blockNumber() ==
                                   self._get_prompt_cursor().blockNumber())
                    if (at_end or shift_down or single_line) and not ctrl_down:
                        self.execute(interactive = not shift_down)
                    else:
                        cursor.beginEditBlock()
                        cursor.setPosition(position)
                        cursor.insertText('\n')
                        self._insert_continuation_prompt(cursor)
                        cursor.endEditBlock()
                        self._control.moveCursor(QtGui.QTextCursor.End)
                        self._control.setTextCursor(cursor)
        elif ctrl_down:
            if key == QtCore.Qt.Key_G:
                self._keyboard_quit()
                intercepted = True
            elif key == QtCore.Qt.Key_K:
                if self._in_buffer(position):
                    cursor.clearSelection()
                    cursor.movePosition(QtGui.QTextCursor.EndOfLine,
                                        QtGui.QTextCursor.KeepAnchor)
                    if not cursor.hasSelection():
                        cursor.movePosition(QtGui.QTextCursor.NextBlock,
                                            QtGui.QTextCursor.KeepAnchor)
                        cursor.movePosition(QtGui.QTextCursor.Right,
                                            QtGui.QTextCursor.KeepAnchor,
                                            len(self._continuation_prompt))
                    self._kill_ring.kill_cursor(cursor)
                    self._set_cursor(cursor)
                intercepted = True
            elif key == QtCore.Qt.Key_L:
                self.prompt_to_top()
                intercepted = True
            elif key == QtCore.Qt.Key_O:
                if self._page_control and self._page_control.isVisible():
                    self._page_control.setFocus()
                intercepted = True
            elif key == QtCore.Qt.Key_U:
                if self._in_buffer(position):
                    cursor.clearSelection()
                    start_line = cursor.blockNumber()
                    if start_line == self._get_prompt_cursor().blockNumber():
                        offset = len(self._prompt)
                    else:
                        offset = len(self._continuation_prompt)
                    cursor.movePosition(QtGui.QTextCursor.StartOfBlock,
                                        QtGui.QTextCursor.KeepAnchor)
                    cursor.movePosition(QtGui.QTextCursor.Right,
                                        QtGui.QTextCursor.KeepAnchor, offset)
                    self._kill_ring.kill_cursor(cursor)
                    self._set_cursor(cursor)
                intercepted = True
            elif key == QtCore.Qt.Key_Y:
                self._keep_cursor_in_buffer()
                self._kill_ring.yank()
                intercepted = True
            elif key in (QtCore.Qt.Key_Backspace, QtCore.Qt.Key_Delete):
                if key == QtCore.Qt.Key_Backspace:
                    cursor = self._get_word_start_cursor(position)
                else: 
                    cursor = self._get_word_end_cursor(position)
                cursor.setPosition(position, QtGui.QTextCursor.KeepAnchor)
                self._kill_ring.kill_cursor(cursor)
                intercepted = True
            elif key == QtCore.Qt.Key_D:
                if len(self.input_buffer) == 0:
                    self.exit_requested.emit(self)
                else:
                    new_event = QtGui.QKeyEvent(QtCore.QEvent.KeyPress,
                                                QtCore.Qt.Key_Delete,
                                                QtCore.Qt.NoModifier)
                    QtGui.qApp.sendEvent(self._control, new_event)
                    intercepted = True
        elif alt_down:
            if key == QtCore.Qt.Key_B:
                self._set_cursor(self._get_word_start_cursor(position))
                intercepted = True
            elif key == QtCore.Qt.Key_F:
                self._set_cursor(self._get_word_end_cursor(position))
                intercepted = True
            elif key == QtCore.Qt.Key_Y:
                self._kill_ring.rotate()
                intercepted = True
            elif key == QtCore.Qt.Key_Backspace:
                cursor = self._get_word_start_cursor(position)
                cursor.setPosition(position, QtGui.QTextCursor.KeepAnchor)
                self._kill_ring.kill_cursor(cursor)
                intercepted = True
            elif key == QtCore.Qt.Key_D:
                cursor = self._get_word_end_cursor(position)
                cursor.setPosition(position, QtGui.QTextCursor.KeepAnchor)
                self._kill_ring.kill_cursor(cursor)
                intercepted = True
            elif key == QtCore.Qt.Key_Delete:
                intercepted = True
            elif key == QtCore.Qt.Key_Greater:
                self._control.moveCursor(QtGui.QTextCursor.End)
                intercepted = True
            elif key == QtCore.Qt.Key_Less:
                self._control.setTextCursor(self._get_prompt_cursor())
                intercepted = True
        else:
            if shift_down:
                anchormode = QtGui.QTextCursor.KeepAnchor
            else:
                anchormode = QtGui.QTextCursor.MoveAnchor
            if key == QtCore.Qt.Key_Escape:
                self._keyboard_quit()
                intercepted = True
            elif key == QtCore.Qt.Key_Up:
                if self._reading or not self._up_pressed(shift_down):
                    intercepted = True
                else:
                    prompt_line = self._get_prompt_cursor().blockNumber()
                    intercepted = cursor.blockNumber() <= prompt_line
            elif key == QtCore.Qt.Key_Down:
                if self._reading or not self._down_pressed(shift_down):
                    intercepted = True
                else:
                    end_line = self._get_end_cursor().blockNumber()
                    intercepted = cursor.blockNumber() == end_line
            elif key == QtCore.Qt.Key_Tab:
                if not self._reading:
                    if self._tab_pressed():
                        cursor.insertText(' '*4)
                    intercepted = True
            elif key == QtCore.Qt.Key_Left:
                line, col = cursor.blockNumber(), cursor.columnNumber()
                if line > self._get_prompt_cursor().blockNumber() and                        col == len(self._continuation_prompt):
                    self._control.moveCursor(QtGui.QTextCursor.PreviousBlock,
                                             mode=anchormode)
                    self._control.moveCursor(QtGui.QTextCursor.EndOfBlock,
                                             mode=anchormode)
                    intercepted = True
                else:
                    intercepted = not self._in_buffer(position - 1)
            elif key == QtCore.Qt.Key_Right:
                original_block_number = cursor.blockNumber()
                cursor.movePosition(QtGui.QTextCursor.Right,
                                mode=anchormode)
                if cursor.blockNumber() != original_block_number:
                    cursor.movePosition(QtGui.QTextCursor.Right,
                                        n=len(self._continuation_prompt),
                                        mode=anchormode)
                self._set_cursor(cursor)
                intercepted = True
            elif key == QtCore.Qt.Key_Home:
                start_line = cursor.blockNumber()
                if start_line == self._get_prompt_cursor().blockNumber():
                    start_pos = self._prompt_pos
                else:
                    cursor.movePosition(QtGui.QTextCursor.StartOfBlock,
                                        QtGui.QTextCursor.KeepAnchor)
                    start_pos = cursor.position()
                    start_pos += len(self._continuation_prompt)
                    cursor.setPosition(position)
                if shift_down and self._in_buffer(position):
                    cursor.setPosition(start_pos, QtGui.QTextCursor.KeepAnchor)
                else:
                    cursor.setPosition(start_pos)
                self._set_cursor(cursor)
                intercepted = True
            elif key == QtCore.Qt.Key_Backspace:
                line, col = cursor.blockNumber(), cursor.columnNumber()
                if not self._reading and                        col == len(self._continuation_prompt) and                        line > self._get_prompt_cursor().blockNumber():
                    cursor.beginEditBlock()
                    cursor.movePosition(QtGui.QTextCursor.StartOfBlock,
                                        QtGui.QTextCursor.KeepAnchor)
                    cursor.removeSelectedText()
                    cursor.deletePreviousChar()
                    cursor.endEditBlock()
                    intercepted = True
                else:
                    anchor = cursor.anchor()
                    if anchor == position:
                        intercepted = not self._in_buffer(position - 1)
                    else:
                        intercepted = not self._in_buffer(min(anchor, position))
            elif key == QtCore.Qt.Key_Delete:
                if not self._reading and self._in_buffer(position) and                        cursor.atBlockEnd() and not cursor.hasSelection():
                    cursor.movePosition(QtGui.QTextCursor.NextBlock,
                                        QtGui.QTextCursor.KeepAnchor)
                    cursor.movePosition(QtGui.QTextCursor.Right,
                                        QtGui.QTextCursor.KeepAnchor,
                                        len(self._continuation_prompt))
                    cursor.removeSelectedText()
                    intercepted = True
                else:
                    anchor = cursor.anchor()
                    intercepted = (not self._in_buffer(anchor) or
                                   not self._in_buffer(position))
        if not (self._control_key_down(event.modifiers(), include_command=True)
                or key in (QtCore.Qt.Key_PageUp, QtCore.Qt.Key_PageDown)
                or (self._executing and not self._reading)):
            self._keep_cursor_in_buffer()
        return intercepted
*** Original Comment ***: Handles key press events in a console-like interface, managing text editing, command execution, and custom keybindings while ensuring cursor stays within editable text buffer.
*** Original Thought ***: 
*** Validation Score ***: 4.31929830844393
*** Validation Thought ***: Comment summarizes key functions: text editing, command execution, keybindings. Minor details omitted, but overall comprehensive.


===============================[ 56 ]===============================
def new_code_cell(code=None, prompt_number=None):
    cell = NotebookNode()
    cell.cell_type = u'code'
    if code is not None:
        cell.code = unicode(code)
    if prompt_number is not None:
        cell.prompt_number = int(prompt_number)
    return cell
*** Original Comment ***: Creates a new code cell with optional code content and prompt number for a Jupyter Notebook.
*** Original Thought ***: 
*** Validation Score ***: 4.930261650254541
*** Validation Thought ***: Comment matches code function. Includes optional parameters. Correct and concise. No redundant info.


===============================[ 57 ]===============================
def _fill_levenshtein_table(self, first, second, update_func, add_pred, clear_pred,
                               threshold=None):
        m, n = len(first), len(second)
        if threshold is None:
            threshold = 0.0
            for a, b in zip(first, second):
                threshold += self.get_operation_cost(a, b)
            if m > n:
                for a in first[n: ]:
                    threshold += self.get_operation_cost(a, '')
            elif m < n:
                for b in second[m: ]:
                    threshold += self.get_operation_cost('', b)
            threshold *= 2
        costs = np.zeros(shape=(m + 1, n + 1), dtype=np.float64)
        costs[:] = np.inf
        backtraces = [None] * (m + 1)
        for i in range(m + 1):
            backtraces[i] = [[] for j in range(n + 1)]
        costs[0][0] = 0.0
        for i in range(m + 1):
            for i_right in range(i, min(i + self.max_up_length, m) + 1):
                up = first[i: i_right]
                max_low_length = self.max_low_lengths_by_up.get(up, -1)
                if max_low_length == -1: 
                    continue
                up_costs = self.operation_costs[up]
                for j in range(n + 1):
                    if costs[i][j] > threshold:
                        continue
                    if len(backtraces[i][j]) == 0 and i + j > 0:
                        continue 
                    for j_right in range((j if i_right > i else j + 1),
                                         min(j + max_low_length, n) + 1):
                        low = second[j: j_right]
                        curr_cost = up_costs.get(low, np.inf)
                        old_cost = costs[i_right][j_right]
                        new_cost = costs[i][j] + curr_cost
                        if new_cost > threshold:
                            continue
                        if add_pred(new_cost, old_cost):
                            if clear_pred(new_cost, old_cost):
                                backtraces[i_right][j_right] = []
                            costs[i_right][j_right] = update_func(new_cost, old_cost)
                            backtraces[i_right][j_right].append((i, j))
        return costs, backtraces
*** Original Comment ***: Initializes and populates the Levenshtein distance matrix with custom update, addition, and clearing predicates, and applies a threshold to optimize performance.
*** Original Thought ***: 
*** Validation Score ***: 4.017766084507811
*** Validation Thought ***: Comment summarizes the function well, mentioning key aspects. Slightly lacks detail on matrix population specifics but is concise and correct.


===============================[ 58 ]===============================
def roc_auc_score(y_true: Union[List[List[float]], List[List[int]], np.ndarray],
                  y_pred: Union[List[List[float]], List[List[int]], np.ndarray]) -> float:
    try:
        return sklearn.metrics.roc_auc_score(np.squeeze(np.array(y_true)),
                                             np.squeeze(np.array(y_pred)), average="macro")
    except ValueError:
        return 0.
*** Original Comment ***: Calculates the ROC AUC score from true binary labels and prediction scores, returning 0 on ValueError.
*** Original Thought ***: 
*** Validation Score ***: 4.651519454735477
*** Validation Thought ***: Comment matches code function and is concise, clear, and correct. No redundant information.


===============================[ 59 ]===============================
def crop_image(img, start_y, start_x, h, w):
    return img[start_y:start_y + h, start_x:start_x + w, :].copy()
*** Original Comment ***: Crops the image `img` to a rectangle of height `h` and width `w` starting from (start_y, start_x) and returns a new image.
*** Original Thought ***: 
*** Validation Score ***: 4.998928908080215
*** Validation Thought ***: Code crops image. Comment matches functionality. Correct and concise. No redundant info. Easy to understand.


===============================[ 60 ]===============================
def advance_robots(self):
        self = lens.robots.Each().call_step_towards(self.player)(self)
        self = lens.crashes.call_union(duplicates(self.robots))(self)
        self = lens.robots.modify(lambda r: list(set(r) - self.crashes))(self)
        return self
*** Original Comment ***: Updates the positions of robots, handles collisions by merging duplicates, and removes crashed robots from the active list.
*** Original Thought ***: 
*** Validation Score ***: 4.561596386817315
*** Validation Thought ***: Comment accurately describes code function in a concise manner without redundancy or error.


===============================[ 61 ]===============================
def select_features(cls, features_id, file_struct, annot_beats, framesync):
        if not annot_beats and framesync:
            feat_type = FeatureTypes.framesync
        elif annot_beats and not framesync:
            feat_type = FeatureTypes.ann_beatsync
        elif not annot_beats and not framesync:
            feat_type = FeatureTypes.est_beatsync
        else:
            raise FeatureTypeNotFound("Type of features not valid.")
        if features_id not in features_registry.keys():
            raise FeaturesNotFound(
                "The features '%s' are invalid (valid features are %s)"
                % (features_id, features_registry.keys()))
        return features_registry[features_id](file_struct, feat_type)
*** Original Comment ***: This function selects and returns a feature extraction method based on the synchronization type (frame or beat) and whether the beats are annotated or estimated, raising exceptions for invalid feature types or IDs.
*** Original Thought ***: 
*** Validation Score ***: 4.778384450787003
*** Validation Thought ***: Comment matches code logic, explains conditions, and exceptions. Correct and concise. No redundant info.


===============================[ 62 ]===============================
def small_parts(script, ratio=0.2, non_closed_only=False):
    filter_xml = ''.join([
        '  <filter name="Small component selection">\n',
        '    <Param name="NbFaceRatio" ',
        'value="{}" '.format(ratio),
        'description="Small component ratio" ',
        'type="RichFloat" ',
        '/>\n',
        '    <Param name="NonClosedOnly" ',
        'value="{}" '.format(str(non_closed_only).lower()),
        'description="Select only non closed components" ',
        'type="RichBool" ',
        '/>\n',
        '  </filter>\n'])
    util.write_filter(script, filter_xml)
    return None
*** Original Comment ***: # Generates and writes an XML filter for selecting small components in a script, with options for ratio and non-closed components.
*** Original Thought ***: 
*** Validation Score ***: 4.803026322999667
*** Validation Thought ***: Comment matches code function, includes key parameters ratio and non_closed_only, and is concise. No redundant info.


===============================[ 63 ]===============================
def check_standard_dir(module_path):
    if 'site-packages' in module_path:
        return True
    for stdlib_path in _STDLIB_PATHS:
        if fnmatch.fnmatchcase(module_path, stdlib_path + '*'):
            return True
    return False
*** Original Comment ***: Checks if the given `module_path` is within the 'site-packages' directory or matches any of the standard library paths.
*** Original Thought ***: 
*** Validation Score ***: 4.971545903172279
*** Validation Thought ***: Comment matches code function. Correct and concise. No redundant info. Explains 'site-packages' and 'standard library paths' check.


===============================[ 64 ]===============================
def _search_archives(self, origin, backend_name, category, archived_after):
        for archive_path in self._search_files():
            try:
                archive = Archive(archive_path)
            except ArchiveError:
                continue
            match = archive.origin == origin and                archive.backend_name == backend_name and                archive.category == category and                archive.created_on >= archived_after
            if not match:
                continue
            yield archive_path, archive.created_on
*** Original Comment ***: Searches through archived files and yields the path and creation date of archives matching the specified origin, backend name, category, and creation date threshold.
*** Original Thought ***: 
*** Validation Score ***: 4.695550997747787
*** Validation Thought ***: Read code, identifies archive search with conditions. Comment matches functionality, is correct and concise. No redundant info.


===============================[ 65 ]===============================
def table_exists(self, table_name, db='default'):
        try:
            self.get_table(table_name, db)
            return True
        except Exception:
            return False
*** Original Comment ***: Checks if the specified table exists in the database by attempting to retrieve it and returns a boolean result.
*** Original Thought ***: 
*** Validation Score ***: 4.991486838287752
*** Validation Thought ***: Code checks table existence. Comment matches functionality. Correct and concise. No redundant info.


===============================[ 66 ]===============================
def delete_vacation(self, index, vacation):
        body = {"selection": {
                    "selectionType": "thermostats",
                    "selectionMatch": self.thermostats[index]['identifier']},
                "functions": [{"type": "deleteVacation", "params": {
                    "name": vacation
                }}]}
        log_msg_action = "delete a vacation"
        return self.make_request(body, log_msg_action)
*** Original Comment ***: Deletes a vacation with the specified name from the thermostat at the given index by sending a request to the thermostat's API.
*** Original Thought ***: 
*** Validation Score ***: 4.887921418107094
*** Validation Thought ***: Comment matches code function, is correct and concise. Explains main action without unnecessary details.


===============================[ 67 ]===============================
def get_transfers(self, start=0, stop=None, inclusion_states=False):
        return extended.GetTransfersCommand(self.adapter)(
            seed=self.seed,
            start=start,
            stop=stop,
            inclusionStates=inclusion_states,
        )
*** Original Comment ***: Retrieves a list of transfer transactions for a given seed, optionally filtered by start and stop indexes and including the state of each transfer.
*** Original Thought ***: 
*** Validation Score ***: 4.90029482969103
*** Validation Thought ***: Comment matches code function, includes optional parameters, and is concise. No redundant info, easy to understand.


===============================[ 68 ]===============================
def write(self, output_buffer, kmip_version=enums.KMIPVersion.KMIP_1_0):
        local_buffer = utils.BytearrayStream()
        if self._operations:
            for operation in self._operations:
                operation.write(local_buffer, kmip_version=kmip_version)
        if self._object_types:
            for object_type in self._object_types:
                object_type.write(local_buffer, kmip_version=kmip_version)
        if self._vendor_identification:
            self._vendor_identification.write(
                local_buffer,
                kmip_version=kmip_version
            )
        if self._server_information:
            self._server_information.write(
                local_buffer,
                kmip_version=kmip_version
            )
        if self._application_namespaces:
            for application_namespace in self._application_namespaces:
                application_namespace.write(
                    local_buffer,
                    kmip_version=kmip_version
                )
        if kmip_version >= enums.KMIPVersion.KMIP_1_1:
            if self._extension_information:
                for extension_information in self._extension_information:
                    extension_information.write(
                        local_buffer,
                        kmip_version=kmip_version
                    )
        if kmip_version >= enums.KMIPVersion.KMIP_1_2:
            if self._attestation_types:
                for attestation_type in self._attestation_types:
                    attestation_type.write(
                        local_buffer,
                        kmip_version=kmip_version
                    )
        if kmip_version >= enums.KMIPVersion.KMIP_1_3:
            if self._rng_parameters:
                for rng_parameters in self._rng_parameters:
                    rng_parameters.write(
                        local_buffer,
                        kmip_version=kmip_version
                    )
            if self._profile_information:
                for profile_information in self._profile_information:
                    profile_information.write(
                        local_buffer,
                        kmip_version=kmip_version
                    )
            if self._validation_information:
                for validation_information in self._validation_information:
                    validation_information.write(
                        local_buffer,
                        kmip_version=kmip_version
                    )
            if self._capability_information:
                for capability_information in self._capability_information:
                    capability_information.write(
                        local_buffer,
                        kmip_version=kmip_version
                    )
            if self._client_registration_methods:
                for client_reg_method in self._client_registration_methods:
                    client_reg_method.write(
                        local_buffer,
                        kmip_version=kmip_version
                    )
        if kmip_version >= enums.KMIPVersion.KMIP_2_0:
            if self._defaults_information:
                self._defaults_information.write(
                    local_buffer,
                    kmip_version=kmip_version
                )
            if self._storage_protection_masks:
                for storage_protection_mask in self._storage_protection_masks:
                    storage_protection_mask.write(
                        local_buffer,
                        kmip_version=kmip_version
                    )
        self.length = local_buffer.length()
        super(QueryResponsePayload, self).write(
            output_buffer,
            kmip_version=kmip_version
        )
        output_buffer.write(local_buffer.buffer)
*** Original Comment ***: Writes the QueryResponsePayload content to an output buffer, handling various KMIP versions and including conditional serialization of different payload components.
*** Original Thought ***: 
*** Validation Score ***: 4.357422730494203
*** Validation Thought ***: Comment summarizes code's function. Includes KMIP version handling and conditional serialization. Lacks detail on buffer usage but remains concise and correct.


===============================[ 69 ]===============================
def insert_bytes(fobj, size, offset, BUFFER_SIZE=2**16):
    assert 0 < size
    assert 0 <= offset
    locked = False
    fobj.seek(0, 2)
    filesize = fobj.tell()
    movesize = filesize - offset
    fobj.write(b'\x00' * size)
    fobj.flush()
    try:
        try:
            import mmap
            file_map = mmap.mmap(fobj.fileno(), filesize + size)
            try:
                file_map.move(offset + size, offset, movesize)
            finally:
                file_map.close()
        except (ValueError, EnvironmentError, ImportError):
            locked = lock(fobj)
            fobj.truncate(filesize)
            fobj.seek(0, 2)
            padsize = size
            while padsize:
                addsize = min(BUFFER_SIZE, padsize)
                fobj.write(b"\x00" * addsize)
                padsize -= addsize
            fobj.seek(filesize, 0)
            while movesize:
                thismove = min(BUFFER_SIZE, movesize)
                fobj.seek(-thismove, 1)
                nextpos = fobj.tell()
                data = fobj.read(thismove)
                fobj.seek(-thismove + size, 1)
                fobj.write(data)
                fobj.seek(nextpos)
                movesize -= thismove
            fobj.flush()
    finally:
        if locked:
            unlock(fobj)
*** Original Comment ***: Inserts a specified number of zero bytes into a file at a given offset, using memory mapping if available, or a manual byte-shifting method as a fallback.
*** Original Thought ***: 
*** Validation Score ***: 4.825579211819349
*** Validation Thought ***: Comment accurately summarizes code function. Correct, concise, no redundancy. Explains main function and fallback method.


===============================[ 70 ]===============================
def prepare_pids(self):
        self.pids = []
        for fetcher in self.pid_fetchers:
            val = fetcher(None, self.revisions[-1][1])
            if val:
                self.pids.append(val)
*** Original Comment ***: Initializes or clears the `self.pids` list and populates it with process IDs (PIDs) retrieved from each `pid_fetcher` for the most recent revision.
*** Original Thought ***: 
*** Validation Score ***: 4.373549888150052
*** Validation Thought ***: Comment is clear, concise, and correct. Minor detail missing about 'val' check, but not critical for understanding.


===============================[ 71 ]===============================
def as_dagster_type(
    existing_type,
    name=None,
    description=None,
    input_schema=None,
    output_schema=None,
    serialization_strategy=None,
    storage_plugins=None,
):
    check.type_param(existing_type, 'existing_type')
    check.opt_str_param(name, 'name')
    check.opt_str_param(description, 'description')
    check.opt_inst_param(input_schema, 'input_schema', InputSchema)
    check.opt_inst_param(output_schema, 'output_schema', OutputSchema)
    check.opt_inst_param(serialization_strategy, 'serialization_strategy', SerializationStrategy)
    storage_plugins = check.opt_dict_param(storage_plugins, 'storage_plugins')
    if serialization_strategy is None:
        serialization_strategy = PickleSerializationStrategy()
    name = existing_type.__name__ if name is None else name
    return _decorate_as_dagster_type(
        existing_type,
        key=name,
        name=name,
        description=description,
        input_schema=input_schema,
        output_schema=output_schema,
        serialization_strategy=serialization_strategy,
        storage_plugins=storage_plugins,
    )
*** Original Comment ***: Creates a Dagster type by decorating an existing Python type with optional metadata and serialization strategies.
*** Original Thought ***: 
*** Validation Score ***: 4.281519510229196
*** Validation Thought ***: Comment summarizes the function well, but doesn't mention storage_plugins. Still, it's clear and concise.


===============================[ 72 ]===============================
def evaluate_marker(cls, text, extra=None):
        return cls.interpret(parser.expr(text).totuple(1)[1])
*** Original Comment ***: # This function evaluates a given expression `text` by parsing it and interpreting the result using the class's `interpret` method.
*** Original Thought ***: 
*** Validation Score ***: 4.865097089480491
*** Validation Thought ***: Comment matches code function, is correct, concise, and lacks redundant information.


===============================[ 73 ]===============================
def url(self):
        if self._url is not None:
            url = self._url
        else:
            url = getattr(self.nb.metadata, 'url', None)
        if url is not None:
            return nbviewer_link(url)
*** Original Comment ***: # Returns a nbviewer formatted link to the notebook if a URL is set in the metadata or instance variable.
*** Original Thought ***: 
*** Validation Score ***: 4.3146295405191895
*** Validation Thought ***: Code checks for URL in two places. Comment summarizes well but doesn't mention fallback to metadata. Slightly lacks detail.


===============================[ 74 ]===============================
def save_graph_only_from_checkpoint(input_checkpoint, output_file_path, output_node_names, as_text=False):
    check_input_checkpoint(input_checkpoint)
    output_node_names = output_node_names_string_as_list(output_node_names)
    with tf.Session() as sess:
        restore_from_checkpoint(sess, input_checkpoint)
        save_graph_only(sess, output_file_path, output_node_names, as_text=as_text)
*** Original Comment ***: # This function saves the graph structure from a TensorFlow checkpoint to a specified file, with options for text format and specific node names.
*** Original Thought ***: 
*** Validation Score ***: 4.982833938516791
*** Validation Thought ***: Comment matches code function, mentions key aspects (checkpoint, file, text format, node names). Correct and concise.


===============================[ 75 ]===============================
def sanger_ordered(self, institute_id=None, user_id=None):
        query = {'$match': {
                '$and': [
                    {'verb': 'sanger'},
                ],
            }}
        if institute_id:
            query['$match']['$and'].append({'institute': institute_id})
        if user_id:
            query['$match']['$and'].append({'user_id': user_id})
        results = self.event_collection.aggregate([
            query,
            {'$group': {
                '_id': "$case",
                'vars': {'$addToSet' : '$variant_id'}
            }}
        ])
        sanger_ordered =  [item for item in results]
        return sanger_ordered
*** Original Comment ***: Constructs a MongoDB aggregation query to retrieve Sanger sequencing events filtered by institute and/or user, grouped by case with a set of unique variant IDs.
*** Original Thought ***: 
*** Validation Score ***: 4.806819267468945
*** Validation Thought ***: Code creates a query for Sanger events, filters by institute/user, groups by case. Comment is accurate, concise, and complete.


===============================[ 76 ]===============================
def copy_notebook(self, notebook_id):
        last_mod, nb = self.get_notebook_object(notebook_id)
        name = nb.metadata.name + '-Copy'
        path, name = self.increment_filename(name)
        nb.metadata.name = name
        notebook_id = self.new_notebook_id(name)
        self.save_notebook_object(notebook_id, nb)
        return notebook_id
*** Original Comment ***: Creates a copy of the specified notebook by appending '-Copy' to its name, ensuring a unique filename, and saving the new notebook with an updated ID.
*** Original Thought ***: 
*** Validation Score ***: 4.937012399907581
*** Validation Thought ***: Read code, understood it copies a notebook. Comment concise, correct, explains main function. No redundant info.


===============================[ 77 ]===============================
def debug_storage(storage, base_info=False, chars=True, runs=False):
    import codecs
    import locale
    import sys
    if six.PY2:
        stderr = codecs.getwriter(locale.getpreferredencoding())(sys.stderr)
    else:
        stderr = sys.stderr
    caller = inspect.stack()[1][3]
    stderr.write('in %s\n' % caller)
    if base_info:
        stderr.write(u'  base level  : %d\n' % storage['base_level'])
        stderr.write(u'  base dir    : %s\n' % storage['base_dir'])
    if runs:
        stderr.write(u'  runs        : %s\n' % list(storage['runs']))
    if chars:
        output = u'  Chars       : '
        for _ch in storage['chars']:
            if _ch != '\n':
                output += _ch['ch']
            else:
                output += 'C'
        stderr.write(output + u'\n')
        output = u'  Res. levels : %s\n' % u''.join(
            [six.text_type(_ch['level']) for _ch in storage['chars']])
        stderr.write(output)
        _types = [_ch['type'].ljust(3) for _ch in storage['chars']]
        for i in range(3):
            if i:
                output = u'                %s\n'
            else:
                output = u'  Res. types  : %s\n'
            stderr.write(output % u''.join([_t[i] for _t in _types]))
*** Original Comment ***: This function `debug_storage` writes debugging information about a storage object to standard error, with options to include base info, character data, and run data, and has special handling for Python 2 encoding issues.
*** Original Thought ***: 
*** Validation Score ***: 4.221386084405017
*** Validation Thought ***: Comment summarizes function well, mentions options and Python 2 handling. Slightly lacks detail on 'special handling' specifics.


===============================[ 78 ]===============================
def get(self, alert_type, alert_args=None):
        alert_args = alert_args or {}
        alerts = self.list()
        return [
            alert
            for alert
            in alerts
            if alert.get('type') == alert_type
            and dict_is_subset(alert_args, alert.get('args'))
        ]
*** Original Comment ***: Returns a list of alerts of a specific type where the optional `alert_args` are a subset of the alert's arguments.
*** Original Thought ***: 
*** Validation Score ***: 4.8671756092611265
*** Validation Thought ***: Comment matches code function, is correct, concise, and no redundant info. Explains optional args. Full score.


===============================[ 79 ]===============================
def way(self, w):
        if w.id not in self.way_ids:
            return
        way_points = []
        for n in w.nodes:
            try:
                way_points.append(Point(n.location.lon, n.location.lat))
            except o.InvalidLocationError:
                logging.debug('InvalidLocationError at way %s node %s', w.id, n.ref)
        self.ways[w.id] = Way(w.id, way_points)
*** Original Comment ***: Checks if the way `w` is in `self.way_ids`, and if so, iterates through its nodes to create `Way` objects with valid locations, logging any invalid locations encountered.
*** Original Thought ***: 
*** Validation Score ***: 4.421648144804923
*** Validation Thought ***: Comment summarizes code well, mentioning key actions and error handling. Minor detail: doesn't specify storing in 'self.ways'.


===============================[ 80 ]===============================
def _make_reversed_wildcards(self, old_length=-1):
        if len(self._reversed_wildcards) > 0:
            start = old_length
        else:
            start = -1
        for wildcards, func in self._wildcard_functions.items():
            for irun in range(start, len(self)):
                translated_name = func(irun)
                if not translated_name in self._reversed_wildcards:
                    self._reversed_wildcards[translated_name] = ([], wildcards)
                self._reversed_wildcards[translated_name][0].append(irun)
*** Original Comment ***: Initializes or updates the `_reversed_wildcards` dictionary with indices and wildcard patterns based on the current length of the object and the provided `old_length`.
*** Original Thought ***: 
*** Validation Score ***: 4.0662810233474636
*** Validation Thought ***: Comment summarizes code well, but doesn't mention conditional start assignment. Otherwise, clear and concise. No redundant info.


===============================[ 81 ]===============================
def reject_command(self, id, **params):
        return self.api.post(self.subpath('/commands/%s/reject' % id), data=params)
*** Original Comment ***: Sends a POST request to the API to reject a command with the specified ID, passing additional parameters if provided.
*** Original Thought ***: 
*** Validation Score ***: 4.973359950855604
*** Validation Thought ***: Comment matches code function, is correct and concise. Explains main action, parameters, and purpose without redundancy.


===============================[ 82 ]===============================
def sanitize_block(self, block):
        embed_type = block.get('type', None)
        data = block.get('data', {})
        serializer = self.serializers.get(embed_type, None)
        if serializer is None:
            return block
        block['data'] = serializer.to_internal_value(data)
        return block
*** Original Comment ***: Ensures the data within a block is properly sanitized by applying a type-specific serializer, if available, before returning the block.
*** Original Thought ***: 
*** Validation Score ***: 4.814554717353391
*** Validation Thought ***: Comment matches code function, mentions type-specific serializer, and is concise. No redundant info, easy to understand.


===============================[ 83 ]===============================
def handle_stop(self, signame, set_stop):
        if set_stop:
            self.sigs[signame].b_stop       = True
            self.sigs[signame].print_method = self.dbgr.intf[-1].msg
            self.sigs[signame].pass_along   = False
        else:
            self.sigs[signame].b_stop       = False
            pass
        return set_stop
*** Original Comment ***: Sets the stop flag for a signal and updates its print method and pass-along behavior based on the `set_stop` parameter, then returns the `set_stop` value.
*** Original Thought ***: 
*** Validation Score ***: 4.779263883772243
*** Validation Thought ***: Comment matches code function, is correct and concise. Explains conditional behavior and return value. No redundant info.


===============================[ 84 ]===============================
def vectorize_target(self, size):
        if self._train_set:
            self._train_set = self._vectorize_set(self._train_set, size)
        if self._valid_set:
            self._valid_set = self._vectorize_set(self._valid_set, size)
        if self._test_set:
            self._test_set = self._vectorize_set(self._test_set, size)
*** Original Comment ***: # Converts the target variables in the train, validation, and test sets to a vectorized form of the specified size.
*** Original Thought ***: 
*** Validation Score ***: 4.9713602213090855
*** Validation Thought ***: Comment accurately describes code function without unnecessary detail. It's correct and concise.


===============================[ 85 ]===============================
def _map_w_to_data(self):
        self._Wmapped_index = vq(self.data, self.W)
        self.Wmapped = np.zeros(self.W.shape)
        for i, s in enumerate(self._Wmapped_index):
            self.Wmapped[:,i] = self.data[:,s]
*** Original Comment ***: Assigns each vector in `self.data` to the closest vector in `self.W` and creates a new array `self.Wmapped` with the mapped vectors.
*** Original Thought ***: 
*** Validation Score ***: 4.3225040037997875
*** Validation Thought ***: Comment summarizes code well, explaining main function. Minor details omitted, but overall clear and concise. No logical errors.


===============================[ 86 ]===============================
def __run(self, ctx):
        try:
            data = yield from decode(ctx)
        except ParseError:
            return JError().parse()
        except InvalidRequest:
            return JError().request()
        except InternalError:
            return JError().internal()
        try:
            i_app = getattr(self, data['method'])
            i_app = asyncio.coroutine(i_app)
        except Exception:
            return JError(data).method()
        try:
            resp = yield from i_app(self, ctx, data)
        except InvalidParams:
            return JError(data).params()
        except InternalError:
            return JError(data).internal()
        return JResponse(jsonrpc={
            "id": data['id'], "result": resp
            })
*** Original Comment ***: Executes a JSON-RPC method by decoding the context, handling various exceptions, and returning either a JSON-RPC response or an error object.
*** Original Thought ***: 
*** Validation Score ***: 4.251847855681982
*** Validation Thought ***: Code handles exceptions, executes method. Comment summarizes well but lacks detail on async nature. Still, clear and concise.


===============================[ 87 ]===============================
def classe(self, name):
        for klass in self.classes():
            if klass.node.name == name:
                return klass
        raise KeyError(name)
*** Original Comment ***: Searches for a class with the specified name in the collection and returns it if found, otherwise raises a KeyError.
*** Original Thought ***: 
*** Validation Score ***: 4.9770836080751195
*** Validation Thought ***: Code checks classes for name match. Comment accurately describes function. Concise, no redundancy. Correctness and conciseness met.


===============================[ 88 ]===============================
def delete(self, id, coordinates):
        p_mins, p_maxs = self.get_coordinate_pointers(coordinates)
        core.rt.Index_DeleteData(
            self.handle, id, p_mins, p_maxs, self.properties.dimension)
*** Original Comment ***: Deletes the data with the specified `id` within the given `coordinates` from the index using the `Index_DeleteData` method.
*** Original Thought ***: 
*** Validation Score ***: 4.52108868006937
*** Validation Thought ***: Comment matches code function. Explains parameters and method used. Could mention 'coordinate pointers' for full clarity.


===============================[ 89 ]===============================
def trace(self, app):
        if app not in self._trace_cache:
            process = subprocess.Popen(
                "trace-deps.js {}".format(app), shell=True,
                stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                env=self.env, universal_newlines=True, cwd=self._package_json_dir
            )
            out, err = process.communicate()
            if err:
                raise TraceError(err)
            self._trace_cache[app] = json.loads(out)
        return self._trace_cache[app]
*** Original Comment ***: Executes a subprocess to trace app dependencies, caching the results, and raises `TraceError` on failure.
*** Original Thought ***: 
*** Validation Score ***: 4.627508312654909
*** Validation Thought ***: Comment matches code function, is correct, concise, and has no redundant info.


===============================[ 90 ]===============================
def exists(self, file_ref):
        if file_ref not in FILE_REFS:
            raise ValueError("Unknown file_ref: '%s'. Available: '%s'." % (file_ref, list(sorted(FILE_REFS._fields))))
        return os.path.isfile(self._path(file_ref))
*** Original Comment ***: Checks if the given `file_ref` exists in the predefined `FILE_REFS` and returns True if the corresponding file path exists on the filesystem, otherwise raises a `ValueError`.
*** Original Thought ***: 
*** Validation Score ***: 4.859172043261602
*** Validation Thought ***: Comment matches code function. Correct and concise. No redundant info. Explains exception and return value.


===============================[ 91 ]===============================
def getoutput(cmd):
    with AvoidUNCPath() as path:
        if path is not None:
            cmd = '"pushd %s &&"%s' % (path, cmd)
        out = process_handler(cmd, lambda p: p.communicate()[0], STDOUT)
    if out is None:
        out = b''
    return py3compat.bytes_to_str(out)
*** Original Comment ***: Executes a command in a non-UNC path context, returning its output as a string, with fallback to an empty string if no output is generated.
*** Original Thought ***: 
*** Validation Score ***: 4.493568588250121
*** Validation Thought ***: Comment summarizes code well, mentioning non-UNC context and output handling. Minor details omitted, but main function clear. Not a '5' due to slight lack of detail.


===============================[ 92 ]===============================
def get_safe_contract(w3: Web3, address=None):
    return w3.eth.contract(address,
                           abi=GNOSIS_SAFE_INTERFACE['abi'],
                           bytecode=GNOSIS_SAFE_INTERFACE['bytecode'])
*** Original Comment ***: Returns a contract object for interacting with a Gnosis Safe at a given address using the provided Web3 instance.
*** Original Thought ***: 
*** Validation Score ***: 4.976342063431922
*** Validation Thought ***: Comment matches code function, correctly and concisely describes its purpose without redundancy. It's clear and accurate.


===============================[ 93 ]===============================
def headers_present(self, headers):
        headers = {name: re.compile('(.*)') for name in headers}
        self.add_matcher(matcher('HeadersMatcher', headers))
*** Original Comment ***: Checks if specified headers are present and adds a corresponding matcher to validate them using regular expressions.
*** Original Thought ***: 
*** Validation Score ***: 4.1737883292849425
*** Validation Thought ***: Code creates regex matchers for headers. Comment explains well but doesn't mention regex is for any value.


===============================[ 94 ]===============================
def is_method_call(func, types=(), methods=()):
    return (
        isinstance(func, astroid.BoundMethod)
        and isinstance(func.bound, astroid.Instance)
        and (func.bound.name in types if types else True)
        and (func.name in methods if methods else True)
    )
*** Original Comment ***: Checks if `func` is a bound method of an instance of any type in `types` (if provided) and if the method name is in `methods` (if provided).
*** Original Thought ***: 
*** Validation Score ***: 4.93863332902896
*** Validation Thought ***: Comment matches code functionality and is concise. No redundant information. Correctly summarizes the code's purpose.


===============================[ 95 ]===============================
def EntityLock(entity, acquire_timeout, lock_timeout):
    return Lock(entity._connection, entity._pk, acquire_timeout, lock_timeout)
===============================[ 95 ]===============================
def EntityLock(entity, acquire_timeout, lock_timeout):
    return Lock(entity._connection, entity._pk, acquire_timeout, lock_timeout)
===============================[ 95 ]===============================
def EntityLock(entity, acquire_timeout, lock_timeout):
    return Lock(entity._connection, entity._pk, acquire_timeout, lock_timeout)
*** Original Comment ***: Acquires a lock on the given entity within the specified acquire timeout and holds it for the lock timeout duration.
*** Original Thought ***: 
*** Validation Score ***: 4.146860169763286
*** Validation Thought ***: Code defines EntityLock. Comment explains purpose and timeouts. Minor details missing, but main function clear. Not perfect, but quite comprehensive.


===============================[ 96 ]===============================
def _uniform_unit_norm(dimension, shape, dtype, seed):
  raw = normal.Normal(
      loc=dtype_util.as_numpy_dtype(dtype)(0),
      scale=dtype_util.as_numpy_dtype(dtype)(1)).sample(
          tf.concat([shape, [dimension]], axis=0), seed=seed())
  unit_norm = raw / tf.norm(tensor=raw, ord=2, axis=-1)[..., tf.newaxis]
  return unit_norm
*** Original Comment ***: Generates a tensor of the given shape with vectors on the last axis having a uniform unit norm, using the specified data type and random seed.
*** Original Thought ***: 
*** Validation Score ***: 4.76270203081705
*** Validation Thought ***: Comment accurately describes function's purpose, parameters, and output. It's concise, correct, and clear.


===============================[ 97 ]===============================
def is_from_fallback_block(node: astroid.node_classes.NodeNG) -> bool:
    context = find_try_except_wrapper_node(node)
    if not context:
        return False
    if isinstance(context, astroid.ExceptHandler):
        other_body = context.parent.body
        handlers = context.parent.handlers
    else:
        other_body = itertools.chain.from_iterable(
            handler.body for handler in context.handlers
        )
        handlers = context.handlers
    has_fallback_imports = any(
        isinstance(import_node, (astroid.ImportFrom, astroid.Import))
        for import_node in other_body
    )
    ignores_import_error = _except_handlers_ignores_exception(handlers, ImportError)
    return ignores_import_error or has_fallback_imports
*** Original Comment ***: Checks if the given AST node is within a try-except block that handles ImportError or contains fallback imports.
*** Original Thought ***: 
*** Validation Score ***: 4.586927086535975
*** Validation Thought ***: Comment matches code function. Correctly summarizes logic. Concise, no redundancy. Meets comprehensiveness criteria.


===============================[ 98 ]===============================
def before_app_request(self, f):
        self.record_once(lambda s: s.app.before_request_funcs
            .setdefault(None, []).append(f))
        return f
*** Original Comment ***: Registers a function to be run before each request in a Flask application.
*** Original Thought ***: 
*** Validation Score ***: 4.938208730721841
*** Validation Thought ***: Code registers function to run before requests. Comment is correct, concise, and clear. No redundant information.


===============================[ 99 ]===============================
def fetch_ensembl_exons(build='37'):
    LOG.info("Fetching ensembl exons build %s ...", build)
    if build == '37':
        url = 'http://grch37.ensembl.org'
    else:
        url = 'http://www.ensembl.org'
    dataset_name = 'hsapiens_gene_ensembl'
    dataset = pybiomart.Dataset(name=dataset_name, host=url)
    attributes = [
        'chromosome_name',
        'ensembl_gene_id',
        'ensembl_transcript_id',
        'ensembl_exon_id',
        'exon_chrom_start',
        'exon_chrom_end',
        '5_utr_start',
        '5_utr_end',
        '3_utr_start',
        '3_utr_end',
        'strand',
        'rank'
    ]
    filters = {
        'chromosome_name': CHROMOSOMES,
    }
    result = dataset.query(
        attributes = attributes,
        filters = filters
    )
    return result
*** Original Comment ***: Retrieves exon data from the Ensembl database for either the GRCh37 or the current human genome build using pybiomart.
*** Original Thought ***: 
*** Validation Score ***: 4.815513345021241
*** Validation Thought ***: Code fetches exon data from Ensembl. Comment accurately and concisely summarizes the function. No redundant info.


===============================[ 100 ]===============================
def cudnn_bi_gru(units,
                 n_hidden,
                 seq_lengths=None,
                 n_layers=1,
                 trainable_initial_states=False,
                 name='cudnn_bi_gru',
                 reuse=False):
    with tf.variable_scope(name, reuse=reuse):
        if seq_lengths is None:
            seq_lengths = tf.ones([tf.shape(units)[0]], dtype=tf.int32) * tf.shape(units)[1]
        with tf.variable_scope('Forward'):
            h_fw, h_last_fw = cudnn_gru_wrapper(units,
                                                n_hidden,
                                                n_layers=n_layers,
                                                trainable_initial_states=trainable_initial_states,
                                                seq_lengths=seq_lengths,
                                                reuse=reuse)
        with tf.variable_scope('Backward'):
            reversed_units = tf.reverse_sequence(units, seq_lengths=seq_lengths, seq_dim=1, batch_dim=0)
            h_bw, h_last_bw = cudnn_gru_wrapper(reversed_units,
                                                n_hidden,
                                                n_layers=n_layers,
                                                trainable_initial_states=trainable_initial_states,
                                                seq_lengths=seq_lengths,
                                                reuse=reuse)
            h_bw = tf.reverse_sequence(h_bw, seq_lengths=seq_lengths, seq_dim=1, batch_dim=0)
    return (h_fw, h_bw), (h_last_fw, h_last_bw)
*** Original Comment ***: Initializes a bidirectional CuDNN GRU layer with specified number of units, hidden state size, sequence lengths, layer count, and options for trainable initial states and reuse.
*** Original Thought ***: 
*** Validation Score ***: 4.119321347628862
*** Validation Thought ***: Comment summarizes code's function. Includes parameters and options. Lacks detail on variable scopes and reverse operations, but remains concise and correct.


===============================[ 101 ]===============================
def xml_report(self, morfs=None, outfile=None, ignore_errors=None,
                    omit=None, include=None):
        self._harvest_data()
        self.config.from_args(
            ignore_errors=ignore_errors, omit=omit, include=include,
            xml_output=outfile,
            )
        file_to_close = None
        delete_file = False
        if self.config.xml_output:
            if self.config.xml_output == '-':
                outfile = sys.stdout
            else:
                outfile = open(self.config.xml_output, "w")
                file_to_close = outfile
        try:
            try:
                reporter = XmlReporter(self, self.config)
                return reporter.report(morfs, outfile=outfile)
            except CoverageException:
                delete_file = True
                raise
        finally:
            if file_to_close:
                file_to_close.close()
                if delete_file:
                    file_be_gone(self.config.xml_output)
*** Original Comment ***: Generates an XML report for the given modules or files, handling output streams and exceptions during report creation.
*** Original Thought ***: 
*** Validation Score ***: 4.12259402433538
*** Validation Thought ***: Comment summarizes code's purpose. Mentions output handling and exceptions. Doesn't detail inner workings like config settings. Concise, slightly lacks detail.


===============================[ 102 ]===============================
def p_create_class_event_statement(self, p):
        p[0] = CreateClassEventNode(variable_name=p[4],
                                    event_specification=p[6],
                                    key_letter=p[8])
*** Original Comment ***: Creates an instance of `CreateClassEventNode` with specified variable name, event specification, and key letter from the parsed elements.
*** Original Thought ***: 
*** Validation Score ***: 4.980437032912822
*** Validation Thought ***: Comment matches code function, is correct and concise. No redundant info.


===============================[ 103 ]===============================
def processStream(self):
        print('addr  hex{:{}s}binary context explanation'.format(
            '', self.width-10))
        print('Stream header'.center(60, '-'))
        self.windowSize = self.verboseRead(WindowSizeAlphabet())
        print('Metablock header'.center(60, '='))
        self.ISLAST = False
        self.output = bytearray()
        while not self.ISLAST:
            self.ISLAST = self.verboseRead(
                BoolCode('LAST', description="Last block"))
            if self.ISLAST:
                if self.verboseRead(
                    BoolCode('EMPTY', description="Empty block")): break
            if self.metablockLength(): continue
            if not self.ISLAST and self.uncompressed(): continue
            print('Block type descriptors'.center(60, '-'))
            self.numberOfBlockTypes = {}
            self.currentBlockCounts = {}
            self.blockTypeCodes = {}
            self.blockCountCodes = {}
            for blockType in (L,I,D): self.blockType(blockType)
            print('Distance code parameters'.center(60, '-'))
            self.NPOSTFIX, self.NDIRECT = self.verboseRead(DistanceParamAlphabet())
            self.readLiteralContextModes()
            print('Context maps'.center(60, '-'))
            self.cmaps = {}
            numberOfTrees = {I: self.numberOfBlockTypes[I]}
            for blockType in (L,D):
                numberOfTrees[blockType] = self.contextMap(blockType)
            print('Prefix code lists'.center(60, '-'))
            self.prefixCodes = {}
            for blockType in (L,I,D):
                self.readPrefixArray(blockType, numberOfTrees[blockType])
            self.metablock()
*** Original Comment ***: # Processes a stream by reading headers, block types, and context maps, and then decompressing the metablock.
*** Original Thought ***: 
*** Validation Score ***: 4.0336577430961436
*** Validation Thought ***: Comment summarizes key actions. Minor details omitted, but main functionality captured. Concise and correct, slightly less detailed.


===============================[ 104 ]===============================
def of(jvalue, bigdl_type="float"):
        def get_py_name(jclass_name):
            if jclass_name == "StaticGraph" or jclass_name == "DynamicGraph":
                return "Model"
            elif jclass_name == "Input":
                return "Layer"
            else:
                return jclass_name
        jname = callBigDlFunc(bigdl_type,
                                      "getRealClassNameOfJValue",
                                      jvalue)
        jpackage_name = ".".join(jname.split(".")[:-1])
        pclass_name = get_py_name(jname.split(".")[-1])
        if "com.intel.analytics.bigdl.nn.keras.Model" == jname or                        "com.intel.analytics.bigdl.nn.keras.Sequential" == jname:
            base_module = importlib.import_module('bigdl.nn.keras.topology')
        elif "com.intel.analytics.bigdl.nn.keras" == jpackage_name:
            base_module = importlib.import_module('bigdl.nn.keras.layer')
        else:
            base_module = importlib.import_module('bigdl.nn.layer')
        realClassName = "Layer" 
        if pclass_name in dir(base_module):
            realClassName = pclass_name
        module = getattr(base_module, realClassName)
        jvalue_creator = getattr(module, "from_jvalue")
        model = jvalue_creator(jvalue, bigdl_type)
        return model
*** Original Comment ***: This function resolves a Java value to its corresponding Python class within the BigDL library, dynamically importing the required module and creating an instance of the model or layer.
*** Original Thought ***: 
*** Validation Score ***: 4.112670913139489
*** Validation Thought ***: Comment summarizes code's purpose. Minor details omitted, but main functionality captured. Concise and correct, but could mention class renaming.


===============================[ 105 ]===============================
def expand(self, other):
        if len(other) == 2:
            other += other
        mid = len(other) // 2
        self.ll = map(min, self.ll, other[:mid])
        self.ur = map(max, self.ur, other[mid:])
*** Original Comment ***: # This function expands the bounding area represented by self to include the area represented by 'other', assuming 'other' is a sequence with length 2 or more.
*** Original Thought ***: 
*** Validation Score ***: 3.6158022125791565
*** Validation Thought ***: Comment explains function purpose. Assumes knowledge of 'bounding area'. Lacks detail on 'other' length effect. Mostly clear.


===============================[ 106 ]===============================
def dataReceived(self, data):
        self._unprocessed_data.enqueue(data)
        while True:
            if len(self._unprocessed_data) < self._header.size:
                return 
            hdr_data = self._unprocessed_data.peek(self._header.size)
            packet_length, typekey = self._header.unpack(hdr_data)
            total_length = self._header.size + packet_length
            if len(self._unprocessed_data) < total_length:
                return 
            self._unprocessed_data.drop(self._header.size)
            packet = self._unprocessed_data.dequeue(packet_length)
            self._start_receive = None
            typename = self._type_register.get(typekey, None)
            if typename is None:
                self.on_unregistered_type(typekey, packet)
            else:
                self.packet_received(typename, packet)
*** Original Comment ***: Handles incoming data by queuing it, parsing packets when enough data is available, and dispatching them to appropriate handlers based on their type.
*** Original Thought ***: 
*** Validation Score ***: 4.265167235100035
*** Validation Thought ***: Comment summarizes key functions: queuing, parsing, dispatching. Minor details omitted, but main purpose clear. Concise and correct.


===============================[ 107 ]===============================
def __parse_hits(self, hit_raw):
        bs_result = bs4.BeautifulSoup(hit_raw, 'html.parser')
        hit_string = bs_result.find("div", id="resultStats").text
        hit_string = hit_string.replace(',', u'')
        hit_string = hit_string.replace('.', u'')
        fetched_on = datetime_utcnow().timestamp()
        id_args = self.keywords[:]
        id_args.append(str(fetched_on))
        hits_json = {
            'fetched_on': fetched_on,
            'id': uuid(*id_args),
            'keywords': self.keywords,
            'type': 'googleSearchHits'
        }
        if not hit_string:
            logger.warning("No hits for %s", self.keywords)
            hits_json['hits'] = 0
            return hits_json
        str_hits = re.search(r'\d+', hit_string).group(0)
        hits = int(str_hits)
        hits_json['hits'] = hits
        return hits_json
*** Original Comment ***: Parses the number of search hits from raw HTML, logs a warning if none are found, and returns a JSON object with the parsed data and metadata.
*** Original Thought ***: 
*** Validation Score ***: 4.445184639447814
*** Validation Thought ***: Comment summarizes parsing hits and logging. Doesn't mention datetime or uuid details. Concise, but omits some metadata handling.


===============================[ 108 ]===============================
def _formatters_default(self):
        formatter_classes = [
            PlainTextFormatter,
            HTMLFormatter,
            SVGFormatter,
            PNGFormatter,
            JPEGFormatter,
            LatexFormatter,
            JSONFormatter,
            JavascriptFormatter
        ]
        d = {}
        for cls in formatter_classes:
            f = cls(config=self.config)
            d[f.format_type] = f
        return d
*** Original Comment ***: Initializes and maps formatter instances to their respective format types in a dictionary.
*** Original Thought ***: 
*** Validation Score ***: 4.951946751104937
*** Validation Thought ***: Comment accurately summarizes code function. It's concise, correct, and no redundant information is present.


===============================[ 109 ]===============================
def _format(self, date_to_convert=None):
        if not date_to_convert:  
            date_to_convert = self.expiration_date
        regex_dates = {
            "1": r"([0-9]{2})-([a-z]{3})-([0-9]{4})",
            "2": r"([0-9]{2})\.([0-9]{2})\.([0-9]{4})$",
            "3": r"([0-3][0-9])\/(0[1-9]|1[012])\/([0-9]{4})",
            "4": r"([0-9]{4})-([0-9]{2})-([0-9]{2})$",
            "5": r"([0-9]{4})\.([0-9]{2})\.([0-9]{2})$",
            "6": r"([0-9]{4})\/([0-9]{2})\/([0-9]{2})$",
            "7": r"([0-9]{4})\.([0-9]{2})\.([0-9]{2})\s[0-9]{2}:[0-9]{2}:[0-9]{2}",
            "8": r"([0-9]{4})([0-9]{2})([0-9]{2})\s[0-9]{2}:[0-9]{2}:[0-9]{2}",
            "9": r"([0-9]{4})-([0-9]{2})-([0-9]{2})\s[0-9]{2}:[0-9]{2}:[0-9]{2}",
            "10": r"([0-9]{2})\.([0-9]{2})\.([0-9]{4})\s[0-9]{2}:[0-9]{2}:[0-9]{2}",
            "11": r"([0-9]{2})-([A-Z]{1}[a-z]{2})-([0-9]{4})\s[0-9]{2}:[0-9]{2}:[0-9]{2}\s[A-Z]{1}.*",  
            "12": r"([0-9]{4})\/([0-9]{2})\/([0-9]{2})\s[0-9]{2}:[0-9]{2}:[0-9]{2}\s\(.*\)",
            "13": r"([0-9]{4})\/([0-9]{2})\/([0-9]{2})\s[0-9]{2}:[0-9]{2}:[0-9]{2}$",
            "14": r"[a-zA-Z]{3}\s([a-zA-Z]{3})\s([0-9]{2})\s[0-9]{2}:[0-9]{2}:[0-9]{2}\s[A-Z]{3}\s([0-9]{4})",  
            "15": r"[a-zA-Z]{3}\s([a-zA-Z]{3})\s([0-9]{2})\s([0-9]{4})",
            "16": r"([0-9]{4})-([0-9]{2})-([0-9]{2})T[0-9]{2}:[0-9]{2}:[0-9]{2}$",
            "17": r"([0-9]{4})-([0-9]{2})-([0-9]{2})T[0-9]{2}:[0-9]{2}:[0-9]{2}[A-Z].*",
            "18": r"([0-9]{4})-([0-9]{2})-([0-9]{2})T[0-9]{2}:[0-9]{2}:[0-9]{2}[+-][0-9]{4}",
            "19": r"([0-9]{4})-([0-9]{2})-([0-9]{2})T[0-9]{2}:[0-9]{2}:[0-9]{2}\.[0-9].*[+-][0-9]{2}:[0-9]{2}",  
            "20": r"([0-9]{4})-([0-9]{2})-([0-9]{2})T[0-9]{2}:[0-9]{2}:[0-9]{2}\.[0-9]{6}$",
            "21": r"([0-9]{4})-([0-9]{2})-([0-9]{2})T[0-9]{2}:[0-9]{2}:[0-9]{2}\.[0-9].*[A-Z]",
            "22": r"([0-9]{2})-([0-9]{2})-([0-9]{4})",
            "23": r"([0-9]{4})\.\s([0-9]{2})\.\s([0-9]{2})\.",
            "24": r"([0-9]{4})-([0-9]{2})-([0-9]{2})T[0-9]{2}:[0-9]{2}:[0-9]{2}[+-][0-9]{2}:[0-9]{2}",  
            "25": r"(?=[0-9]{8})(?=([0-9]{4})([0-9]{2})([0-9]{2}))",
            "26": r"([0-9]{2})-([A-Z]{1}[a-z]{2})-([0-9]{4})$",
            "27": r"([0-9]{2})\.([0-9]{1})\.([0-9]{4})",
            "28": r"([0-9]{1,2})\s([A-Z]{1}[a-z]{2})\s([0-9]{4})",
            "29": r"([0-9]{2})-([A-Z]{1}[a-z]*)-([0-9]{4})",
            "30": r"([0-9]{4})-([A-Z]{1}[a-z]{2})-([0-9]{2})\.",
            "31": r"[a-zA-Z]{3}\s([a-zA-Z]{3})\s([0-9]{1,2})\s[0-9]{2}:[0-9]{2}:[0-9]{2}\s([0-9]{4})",  
            "32": r"()[a-zA-Z]{3}\s([a-zA-Z]{3})\s([0-9]{4})\s[0-9]{2}:[0-9]{2}:[0-9]{2}",
            "33": r"([A-Z]{1}[a-z]*)\s([0-9]{1,2})\s([0-9]{4})",
            "34": r"([0-9]{1,2})\.([0-9]{1,2})\.([0-9]{4})",
            "35": r"([0-9]{4})([0-9]{2})([0-9]{2})[0-9]+",
            "36": r"(0[1-9]|1[012])\/([0-3][0-9])\/([0-9]{4})",
            "37": r"([A-Z]{1}[a-z].*)\s\s([0-9]{1,2})\s([0-9]{4})",
            "38": r"([0-9]{1,})[a-z]{1,}\s([A-Z].*)\s(2[0-9]{3})",
        }
        for regx in regex_dates:
            matched_result = Regex(
                date_to_convert, regex_dates[regx], return_data=True, rematch=True
            ).match()
            if matched_result:
                date = self._cases_management(regx, matched_result)
                if date:
                    return "-".join(date)
        return ""
*** Original Comment ***: # Formats a given date string to a standard format using predefined regex patterns, defaulting to the object's expiration date if no date is provided.
*** Original Thought ***: 
*** Validation Score ***: 4.883253680588219
*** Validation Thought ***: Comment accurately describes function's purpose, default behavior, and use of regex. It's concise and correct.


===============================[ 110 ]===============================
def eval_file(filename: str, ctx: compiler.CompilerContext, module: types.ModuleType):
    last = None
    for form in reader.read_file(filename, resolver=runtime.resolve_alias):
        last = compiler.compile_and_exec_form(form, ctx, module)
    return last
*** Original Comment ***: Evaluates each form in the specified file within the given compiler context and module, returning the result of the last form evaluated.
*** Original Thought ***: 
*** Validation Score ***: 4.907686667111316
*** Validation Thought ***: Comment matches code function. Describes loop, context, module use, and return value. Correct and concise.


===============================[ 111 ]===============================
def get_manifests(self, repo_name, digest=None):
    if not hasattr(self, 'manifests'):
        self.manifests = {}
    schemaVersions = ['v1', 'v2', 'config']
    for schemaVersion in schemaVersions:
        manifest = self._get_manifest(repo_name, digest, schemaVersion)
        if manifest is not None:
            if schemaVersion == "v2" and "config" in manifest:
                bot.debug('Attempting to get config as blob in verison 2 manifest')
                url = self._get_layerLink(repo_name, manifest['config']['digest'])        
                headers = {'Accept': manifest['config']['mediaType']}
                self.manifests['config'] = self._get(url, headers=headers)
            self.manifests[schemaVersion] = manifest
    return self.manifests
*** Original Comment ***: Retrieves and caches manifest data for specified repository names and schema versions, with optional digest, handling v2 schema config as a special case.
*** Original Thought ***: 
*** Validation Score ***: 4.257797316335224
*** Validation Thought ***: Comment summarizes code purpose well, mentions caching, schema versions, and special handling for v2. Slightly lacks detail on 'config' handling.


===============================[ 112 ]===============================
async def resume(self, *, device: Optional[SomeDevice] = None):
        await self._user.http.play_playback(None, device_id=str(device))
*** Original Comment ***: Initiates playback on the specified device, or the default device if none is provided, in an asynchronous manner.
*** Original Thought ***: 
*** Validation Score ***: 4.503746510365142
*** Validation Thought ***: Code resumes playback on a device. Comment is clear, concise, and correct. Minor detail about 'None' is implicit.


===============================[ 113 ]===============================
def disable_timestamp(method):
    @wraps(method)
    def wrapper(*args, **kwargs):
        result = None
        with correct_date():
            result = method(*args, **kwargs)
        return result
    return wrapper
*** Original Comment ***: Adds a decorator to temporarily disable timestamp validation within the `correct_date` context manager while executing the wrapped method.
*** Original Thought ***: 
*** Validation Score ***: 4.473121656038867
*** Validation Thought ***: Comment accurately describes the decorator's purpose and use of context manager. It's concise and correct.


===============================[ 114 ]===============================
def _get_access_token():
    access_token = os.environ.get(ACCESS_TOKEN_ENVIRONMENT_VARIABLE)
    if access_token:
        return access_token
    else:
        for access_token_variable in LEGACY_ACCESS_TOKEN_ENVIRONMENT_VARIABLES:
            access_token = os.environ.get(access_token_variable)
            if access_token:
                env_var_deprecation_warning = PendingDeprecationWarning(
                    "Use of the `{legacy}` environment variable will be "
                    "deprecated in the future.  Please update your "
                    "environment(s) to use the new `{new}` environment "
                    "variable.".format(
                        legacy=access_token,
                        new=ACCESS_TOKEN_ENVIRONMENT_VARIABLE,
                    )
                )
                warnings.warn(env_var_deprecation_warning)
                return access_token
*** Original Comment ***: Retrieves the current access token from environment variables, with a deprecation warning for legacy variables.
*** Original Thought ***: 
*** Validation Score ***: 4.480264617402972
*** Validation Thought ***: Comment summarizes code function accurately and concisely, including deprecation warning. No redundant information.


===============================[ 115 ]===============================
def sanitize_for_archive(url, headers, payload):
        url = re.sub('bot.*/', 'botXXXXX/', url)
        return url, headers, payload
*** Original Comment ***: Removes specific bot-related patterns from the URL for privacy before archiving, while leaving headers and payload unchanged.
*** Original Thought ***: 
*** Validation Score ***: 4.581820997598923
*** Validation Thought ***: Code sanitizes URLs by replacing bot patterns. Comment accurately and concisely describes the code's function. No redundant information.


===============================[ 116 ]===============================
def remove_subscriber(self, ws):
        if ws in self.subscribers:
            self.subscribers.remove(ws)
        for name in self.available_events:
            self.remove_event_subscriber(name, ws)
*** Original Comment ***: Removes a WebSocket connection from the subscribers list and unsubscribes it from all available events.
*** Original Thought ***: 
*** Validation Score ***: 4.974710595863254
*** Validation Thought ***: Comment matches code function. Correct and concise. No redundant info. Explains main action clearly.


===============================[ 117 ]===============================
def backward_smoothing_update(filtered_mean,
                              filtered_cov,
                              predicted_mean,
                              predicted_cov,
                              next_posterior_mean,
                              next_posterior_cov,
                              transition_matrix):
  tmp_gain_cov = transition_matrix.matmul(filtered_cov)
  predicted_cov_chol = tf.linalg.cholesky(predicted_cov)
  gain_transpose = tf.linalg.cholesky_solve(predicted_cov_chol, tmp_gain_cov)
  posterior_mean = (filtered_mean +
                    tf.linalg.matmul(gain_transpose,
                                     next_posterior_mean - predicted_mean,
                                     adjoint_a=True))
  posterior_cov = (
      filtered_cov +
      tf.linalg.matmul(gain_transpose,
                       tf.linalg.matmul(
                           next_posterior_cov - predicted_cov, gain_transpose),
                       adjoint_a=True))
  return (posterior_mean, posterior_cov)
*** Original Comment ***: Performs a backward smoothing update step in a Kalman filter by adjusting the mean and covariance estimates based on the next time step's posterior estimates and the transition dynamics.
*** Original Thought ***: 
*** Validation Score ***: 4.868948249150076
*** Validation Thought ***: Comment accurately summarizes code function. Correct, concise, no redundant info. Matches code's Kalman filter update step.


===============================[ 118 ]===============================
def plot(self, timestep="AUTO", metric="AUTO", server=False, **kwargs):
        assert_is_type(metric, "AUTO", "logloss", "auc", "classification_error", "rmse")
        if self._model_json["algo"] in ("deeplearning", "deepwater", "xgboost", "drf", "gbm"):
            if metric == "AUTO":
                metric = "logloss"
        self._plot(timestep=timestep, metric=metric, server=server)
*** Original Comment ***: # Plots the specified metric over time for certain model types, defaulting to logloss if the metric is set to AUTO.
*** Original Thought ***: 
*** Validation Score ***: 4.4556388207783355
*** Validation Thought ***: Comment summarizes function well, mentions default behavior. Doesn't mention 'timestep' or 'server' args, but these are less critical. Mostly comprehensive.


===============================[ 119 ]===============================
def run(self):
        backend_args = vars(self.parsed_args)
        category = backend_args.pop('category', None)
        filter_classified = backend_args.pop('filter_classified', False)
        archived_since = backend_args.pop('archived_since', None)
        if self.archive_manager and self.parsed_args.fetch_archive:
            items = fetch_from_archive(self.BACKEND, backend_args,
                                       self.archive_manager,
                                       category,
                                       archived_since)
        else:
            items = fetch(self.BACKEND, backend_args, category,
                          filter_classified=filter_classified,
                          manager=self.archive_manager)
        try:
            for item in items:
                if self.json_line:
                    obj = json.dumps(item, separators=(',', ':'), sort_keys=True)
                else:
                    obj = json.dumps(item, indent=4, sort_keys=True)
                self.outfile.write(obj)
                self.outfile.write('\n')
        except IOError as e:
            raise RuntimeError(str(e))
        except Exception as e:
            raise RuntimeError(str(e))
*** Original Comment ***: Extracts and processes items from an archive or directly from the backend based on the provided arguments, then outputs them in JSON format.
*** Original Thought ***: 
*** Validation Score ***: 4.3025666889660235
*** Validation Thought ***: Comment summarizes main function well. Minor details missing, but overall clear and concise. No logical errors detected.


===============================[ 120 ]===============================
def formatday(self, day, weekday):
        self.wkday_not_today = '<td class="%s"><div class="td-inner">' % (
            self.cssclasses[weekday])
        self.wkday_today = (
            '<td class="%s calendar-today"><div class="td-inner">' % (
                self.cssclasses[weekday])
        )
        if URLS_NAMESPACE:
            url_name = '%s:day_list' % (URLS_NAMESPACE)
        else:
            url_name = 'day_list'
        self.day_url = reverse(url_name, args=(self.yr, self.mo, day))
        self.day = day
        self.anch = '<a href="%s">%d</a>' % (
            self.day_url, day
        )
        self.end = '</div></td>'
*** Original Comment ***: Initializes HTML table data for a calendar day with optional URL linking, handling both regular and today's date styling based on the provided weekday.
*** Original Thought ***: 
*** Validation Score ***: 4.02249735510217
*** Validation Thought ***: Code initializes HTML for calendar day. Comment summarizes well, but doesn't mention URLS_NAMESPACE condition. Slightly lacks detail.


===============================[ 121 ]===============================
def from_json_compatible(schema, dct):
    kwargs = {}
    for key in dct:
        field_type = schema._fields.get(key)
        if field_type is None:
            warnings.warn("Unexpected field encountered in line for record %s: %r" % (schema.__name__, key))
            continue
        kwargs[key] = field_type.avro_load(dct[key])
    return schema(**kwargs)
*** Original Comment ***: Converts a dictionary to a schema object, warning for unexpected fields and using field-specific deserialization.
*** Original Thought ***: 
*** Validation Score ***: 4.445250492463635
*** Validation Thought ***: Comment summarizes code well, mentions warnings, deserialization. Misses loop detail. Concise, no redundancy. Not perfect, but quite comprehensive.


===============================[ 122 ]===============================
def create_py(self, nb, force=False):
        if list(map(int, re.findall('\d+', nbconvert.__version__))) >= [4, 2]:
            py_file = os.path.basename(self.py_file)
        else:
            py_file = self.py_file
        try:
            level = logger.logger.level
        except AttributeError:
            level = logger.level
        spr.call(['jupyter', 'nbconvert', '--to=python',
                  '--output=' + py_file, '--log-level=%s' % level,
                  self.outfile])
        with open(self.py_file) as f:
            py_content = f.read()
        py_content = re.sub('^\s*get_ipython\(\).magic.*', '# \g<0>',
                            py_content, flags=re.MULTILINE)
        with open(self.py_file, 'w') as f:
            f.write(py_content)
*** Original Comment ***: This function checks the `nbconvert` version and sets the Python file name accordingly, then converts a Jupyter notebook to a Python file, cleans up magic commands, and writes the cleaned content back to the file.
*** Original Thought ***: 
*** Validation Score ***: 4.207738552942388
*** Validation Thought ***: Code converts notebook to Python file, cleans magic commands. Comment matches functionality but omits logging level detail.


===============================[ 123 ]===============================
def write_top(outpath, molecules, title):
    topmolecules = []
    for i in molecules:
        if i[0].endswith('.o'):
            topmolecules.append(tuple([i[0][:-2]]+list(i[1:])))
        else:
            topmolecules.append(i)
    if outpath:
        with open(outpath, "w") as top:
            print('#include "martini.itp"\n', file=top)
            print('[ system ]', file=top)
            print('; name', file=top)
            print(title, file=top)
            print('\n', file=top)
            print('[ molecules ]', file=top)
            print('; name  number', file=top)
            print("\n".join("%-10s %7d"%i for i in topmolecules), file=top)
    else:
        added_molecules = (molecule for molecule in topmolecules
                           if molecule[0] != 'Protein')
        print("\n".join("%-10s %7d"%i for i in added_molecules), file=sys.stderr)
*** Original Comment ***: Generates a GROMACS topology file for the given molecules, optionally filtering out '.o' extensions and excluding 'Protein' entries when not writing to a file.
*** Original Thought ***: 
*** Validation Score ***: 4.018371973672223
*** Validation Thought ***: Comment summarizes code well, mentions key points, but doesn't specify 'Protein' exclusion is for stderr output. Slightly lacks detail.


===============================[ 124 ]===============================
def augment_detections(hyper_params, feature, label):
    if hyper_params.problem.get("augmentation", None) is None:
        return feature, label
    img_h, img_w, img_c = feature["image"].shape
    augmented_feature = {}
    augmented_label = {}
    augmented_feature["image"] = feature["image"].copy()
    if "depth" in feature:
        augmented_feature["depth"] = feature["depth"].copy()
    if "calibration" in feature:
        augmented_feature["calibration"] = feature["calibration"]
    augmented_feature["hflipped"] = np.array([0], dtype=np.uint8)
    augmented_feature["crop_offset"] = np.array([0, 0], dtype=np.int8)
    for k in label.keys():
        augmented_label[k] = [detection.copy() for detection in label[k]]
    if hyper_params.problem.augmentation.get("enable_horizontal_flip", False):
        if random.random() < 0.5:
            img_h, img_w, img_c = augmented_feature["image"].shape
            augmented_feature["image"] = np.fliplr(augmented_feature["image"])
            if "depth" in feature:
                augmented_feature["depth"] = np.fliplr(augmented_feature["depth"])
            augmented_feature["hflipped"][0] = 1
            hflip_detections(augmented_label, img_w)
    if hyper_params.problem.augmentation.get("enable_micro_translation", False):
        img_h, img_w, img_c = augmented_feature["image"].shape
        dx = int(random.random() * 3)
        dy = int(random.random() * 3)
        augmented_feature["image"] = crop_image(augmented_feature["image"], dy, dx, img_h - dy, img_w - dx)
        if "depth" in feature:
            augmented_feature["depth"] = crop_image(augmented_feature["depth"], dy, dx, img_h - dy, img_w - dx)
        augmented_feature["crop_offset"][0] += dy
        augmented_feature["crop_offset"][1] += dx
        move_detections(augmented_label, -dy, -dx)
    if hyper_params.problem.augmentation.get("random_crop", None) is not None:
        img_h, img_w, img_c = augmented_feature["image"].shape
        target_w = hyper_params.problem.augmentation.random_crop.shape.width
        target_h = hyper_params.problem.augmentation.random_crop.shape.height
        delta_x = max(int(math.ceil((target_w + 1 - img_w) / 2)), 0)
        delta_y = max(int(math.ceil((target_h + 1 - img_h) / 2)), 0)
        move_detections(augmented_label, delta_y, delta_x)
        augmented_feature["image"] = cv2.copyMakeBorder(augmented_feature["image"],
                                                        delta_y, delta_y, delta_x, delta_x,
                                                        cv2.BORDER_CONSTANT)
        img_h, img_w, img_c = augmented_feature["image"].shape
        start_x = 0
        start_y = 0
        if len(augmented_label["detections_2d"]) != 0:
            idx = random.randint(0, len(augmented_label["detections_2d"]) - 1)
            detection = augmented_label["detections_2d"][idx]
            start_x = int(detection.cx - random.random() * (target_w - 20) / 2.0 - 10)
            start_y = int(detection.cy - random.random() * (target_h - 20) / 2.0 - 10)
        else:
            start_x = int(img_w * random.random())
            start_y = int(img_h * random.random())
        if start_x < 0:
            start_x = 0
        if start_y < 0:
            start_y = 0
        if start_x >= img_w - target_w:
            start_x = img_w - target_w - 1
        if start_y >= img_h - target_h:
            start_y = img_h - target_h - 1
        augmented_feature["image"] = crop_image(augmented_feature["image"], start_y, start_x, target_h, target_w)
        if "depth" in feature:
            augmented_feature["depth"] = crop_image(augmented_feature["depth"], start_y, start_x, target_h, target_w)
        augmented_feature["crop_offset"][0] += start_y
        augmented_feature["crop_offset"][1] += start_x
        move_detections(augmented_label, -start_y, -start_x)
    if hyper_params.problem.augmentation.get("enable_texture_augmentation", False):
        if random.random() < 0.5:
            augmented_feature["image"] = full_texture_augmentation(augmented_feature["image"])
    return augmented_feature, augmented_label
*** Original Comment ***: Applies specified augmentation techniques to the input feature and label data based on the provided hyperparameters for data augmentation.
*** Original Thought ***: 
*** Validation Score ***: 4.292546822225391
*** Validation Thought ***: Comment summarizes code purpose. Lacks detail on specific augmentations but is correct and concise.


===============================[ 125 ]===============================
def _remove_overlaps(segmentation_mask, fronts):
    fidxs, sidxs = np.where((segmentation_mask != fronts) & (segmentation_mask != 0) & (fronts != 0))
    fronts[fidxs, sidxs] = 0
*** Original Comment ***: Removes overlapping areas in `fronts` where `segmentation_mask` and `fronts` both have non-zero values, setting the overlaps in `fronts` to zero.
*** Original Thought ***: 
*** Validation Score ***: 4.974252266467767
*** Validation Thought ***: Comment matches code function, is correct and concise. Explains non-zero overlap removal clearly without extra info.


===============================[ 126 ]===============================
def skip_module(*modules):
    modules = (modules and isinstance(modules[0], list)) and              modules[0] or modules
    for module in modules:
        if not module in SKIPPED_MODULES:
            SKIPPED_MODULES.append(module)
    traceback.extract_tb = _new_extract_tb
*** Original Comment ***: # Adds the specified modules to the SKIPPED_MODULES list if they are not already present and overrides traceback.extract_tb with a new function.
*** Original Thought ***: 
*** Validation Score ***: 4.321548732784304
*** Validation Thought ***: Comment explains main actions. Minor detail on 'new function' is vague, but overall clear and concise.


===============================[ 127 ]===============================
def get_config(self):
    return {
        'initializers': [
            tf.compat.v2.initializers.serialize(
                tf.keras.initializers.get(init))
            for init in self.initializers
        ],
        'sizes': self.sizes,
        'validate_args': self.validate_args,
    }
*** Original Comment ***: Returns a dictionary containing serialized initializers, layer sizes, and a flag indicating whether to validate arguments for the configuration of the current object.
*** Original Thought ***: 
*** Validation Score ***: 4.930445775592734
*** Validation Thought ***: Comment matches code function, includes all elements returned, and is concise. No redundant info.


===============================[ 128 ]===============================
def set_query_parameter(url, param_name, param_value):
    scheme, netloc, path, query_string, fragment = urlsplit(url)
    query_params = parse_qs(query_string)
    query_params[param_name] = [param_value]
    new_query_string = urlencode(query_params, doseq=True)
    return urlunsplit((scheme, netloc, path, new_query_string, fragment))
*** Original Comment ***: # Updates the query parameter 'param_name' with 'param_value' in the given 'url' and returns the modified URL.
*** Original Thought ***: 
*** Validation Score ***: 4.991264885854691
*** Validation Thought ***: Read code, understood function. Comment matches function, is correct and concise. No redundant info. Meets high comprehensiveness.


===============================[ 129 ]===============================
def alphavsks(self,autozoom=True,**kwargs):
        pylab.plot(self._alpha_values, self._xmin_kstest, '.')
        pylab.errorbar(self._alpha, self._ks, xerr=self._alphaerr, fmt='+')
        ax=pylab.gca()
        if autozoom:
            ax.set_ylim(0.8*(self._ks),3*(self._ks))
            ax.set_xlim((self._alpha)-5*self._alphaerr,(self._alpha)+5*self._alphaerr)
        ax.set_ylabel("KS statistic")
        ax.set_xlabel(r'$\alpha$')
        pylab.draw()
        return ax
*** Original Comment ***: Plots the KS statistic against alpha values with optional auto-zoom functionality and error bars, returning the axis object.
*** Original Thought ***: 
*** Validation Score ***: 4.836401461718707
*** Validation Thought ***: Comment matches code function, mentions plot, auto-zoom, error bars, axis object return. Correct and concise.


===============================[ 130 ]===============================
def build_tr(tr, meta_data, row_spans):
    tr_el = etree.Element('tr')
    w_namespace = get_namespace(tr, 'w')
    visited_nodes = []
    for el in tr:
        if el in visited_nodes:
            continue
        visited_nodes.append(el)
        if el.tag == '%stc' % w_namespace:
            v_merge = get_v_merge(el)
            if (
                    v_merge is not None and
                    v_merge.get('%sval' % w_namespace) != 'restart'):
                continue
            texts = []
            for td_content in el:
                if td_content in visited_nodes:
                    continue
                if is_li(td_content, meta_data):
                    li_nodes = get_single_list_nodes_data(
                        td_content,
                        meta_data,
                    )
                    list_el, list_visited_nodes = build_list(
                        li_nodes,
                        meta_data,
                    )
                    visited_nodes.extend(list_visited_nodes)
                    texts.append(etree.tostring(list_el))
                elif td_content.tag == '%stbl' % w_namespace:
                    table_el, table_visited_nodes = build_table(
                        td_content,
                        meta_data,
                    )
                    visited_nodes.extend(table_visited_nodes)
                    texts.append(etree.tostring(table_el))
                elif td_content.tag == '%stcPr' % w_namespace:
                    visited_nodes.append(td_content)
                    continue
                else:
                    text = get_element_content(
                        td_content,
                        meta_data,
                        is_td=True,
                    )
                    texts.append(text)
            data = '<br />'.join(t for t in texts if t is not None)
            td_el = etree.XML('<td>%s</td>' % data)
            colspan = get_grid_span(el)
            if colspan > 1:
                td_el.set('colspan', '%d' % colspan)
            v_merge = get_v_merge(el)
            if (
                    v_merge is not None and
                    v_merge.get('%sval' % w_namespace) == 'restart'):
                rowspan = next(row_spans)
                td_el.set('rowspan', '%d' % rowspan)
            tr_el.append(td_el)
    return tr_el
*** Original Comment ***: This function constructs an HTML table row (`<tr>`) element from a given WordprocessingML (XML) table row (`tr`), handling merged cells and nested lists and tables.
*** Original Thought ***: 
*** Validation Score ***: 4.5134704674883475
*** Validation Thought ***: Read code, identifies HTML row creation. Comment matches functionality, mentions merging and nesting. Slightly lacks detail on text handling.


===============================[ 131 ]===============================
def put_container(self, container, headers=None, query=None, cdn=False,
                      body=None):
        path = self._container_path(container)
        return self.request(
            'PUT', path, body or '', headers, query=query, cdn=cdn)
*** Original Comment ***: Adds or updates a container by sending a PUT request to the specified container path with optional headers, query parameters, and body content.
*** Original Thought ***: 
*** Validation Score ***: 4.82197416386547
*** Validation Thought ***: Comment matches code function, is correct, concise, and includes optional parameters. No redundant info.


===============================[ 132 ]===============================
def assign_moving_mean_variance(
    mean_var, variance_var, value, decay, name=None):
  with tf.compat.v1.name_scope(name, "assign_moving_mean_variance",
                               [variance_var, mean_var, value, decay]):
    with tf.compat.v1.colocate_with(variance_var):
      with tf.compat.v1.colocate_with(mean_var):
        base_dtype = mean_var.dtype.base_dtype
        if not base_dtype.is_floating:
          raise TypeError(
              "mean_var.base_dtype({}) does not have float type "
              "`dtype`.".format(base_dtype.name))
        if base_dtype != variance_var.dtype.base_dtype:
          raise TypeError(
              "mean_var.base_dtype({}) != variance_var.base_dtype({})".format(
                  base_dtype.name,
                  variance_var.dtype.base_dtype.name))
        value = tf.convert_to_tensor(
            value=value, dtype=base_dtype, name="value")
        decay = tf.convert_to_tensor(
            value=decay, dtype=base_dtype, name="decay")
        delta = value - mean_var
        with tf.control_dependencies([delta]):
          mean_var = mean_var.assign_add((1. - decay) * delta)
          variance_var = variance_var.assign_sub(
              (1. - decay) * (variance_var - decay * tf.square(delta)))
        return mean_var, variance_var
*** Original Comment ***: Updates the moving mean and variance variables with new values using exponential decay.
*** Original Thought ***: 
*** Validation Score ***: 4.446114944694136
*** Validation Thought ***: Code updates moving mean/variance. Comment concise, correct. Misses error checks, but main function captured. Not perfect, but clear.


===============================[ 133 ]===============================
def _resolve_option(self, options, option_name, section_name):
        if getattr(options, option_name, None) is not None:
            return
        if option_name.startswith(section_name + '_'):
            environ_name = option_name.upper()
            conf_name = option_name[len(section_name) + 1:]
        else:
            environ_name = (section_name + '_' + option_name).upper()
            conf_name = option_name
        setattr(
            options, option_name,
            os.environ.get(
                environ_name,
                (self.context.conf.get(section_name, {})).get(conf_name)))
*** Original Comment ***: Checks if an option is already set; if not, resolves the option by checking environment variables and configuration, then sets it on the options object.
*** Original Thought ***: 
*** Validation Score ***: 4.314840447876187
*** Validation Thought ***: Read code, identifies option resolution logic. Comment summarizes well, slightly verbose but correct. Missing default value handling detail. Score: 4.


===============================[ 134 ]===============================
def _dot_product(self, imgs_to_decode):
        return np.dot(imgs_to_decode.T, self.feature_images).T
*** Original Comment ***: Calculates the dot product between the transposed matrix of images to decode and the feature images, then transposes the result.
*** Original Thought ***: 
*** Validation Score ***: 4.956838318272439
*** Validation Thought ***: Comment matches code function and is concise. No redundant information, correctly describes the code's main action.


===============================[ 135 ]===============================
def provider_operations_metadata(self):
        api_version = self._get_api_version('provider_operations_metadata')
        if api_version == '2015-07-01':
            from .v2015_07_01.operations import ProviderOperationsMetadataOperations as OperationClass
        elif api_version == '2018-01-01-preview':
            from .v2018_01_01_preview.operations import ProviderOperationsMetadataOperations as OperationClass
        else:
            raise NotImplementedError("APIVersion {} is not available".format(api_version))
        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))
*** Original Comment ***: # Retrieves and initializes the appropriate ProviderOperationsMetadataOperations class based on the API version.
*** Original Thought ***: 
*** Validation Score ***: 4.864100759967496
*** Validation Thought ***: Comment matches code function. Correctly summarizes key action. No redundant info. Clear and concise. Comprehensiveness criteria met.


===============================[ 136 ]===============================
def resource_groups(self):
        api_version = self._get_api_version('resource_groups')
        if api_version == '2016-02-01':
            from .v2016_02_01.operations import ResourceGroupsOperations as OperationClass
        elif api_version == '2016-09-01':
            from .v2016_09_01.operations import ResourceGroupsOperations as OperationClass
        elif api_version == '2017-05-10':
            from .v2017_05_10.operations import ResourceGroupsOperations as OperationClass
        elif api_version == '2018-02-01':
            from .v2018_02_01.operations import ResourceGroupsOperations as OperationClass
        elif api_version == '2018-05-01':
            from .v2018_05_01.operations import ResourceGroupsOperations as OperationClass
        else:
            raise NotImplementedError("APIVersion {} is not available".format(api_version))
        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))
*** Original Comment ***: Selects and returns the appropriate ResourceGroupsOperations class based on the API version.
*** Original Thought ***: 
*** Validation Score ***: 4.927456654185824
*** Validation Thought ***: Comment correctly summarizes code function. It's concise, no redundant information. Clearly states main action of selecting ResourceGroupsOperations class based on API version.


===============================[ 137 ]===============================
def _decode(self, data, decode_content, flush_decoder):
        try:
            if decode_content and self._decoder:
                data = self._decoder.decompress(data)
        except (IOError, zlib.error) as e:
            content_encoding = self.headers.get('content-encoding', '').lower()
            raise DecodeError(
                "Received response with content-encoding: %s, but "
                "failed to decode it." % content_encoding, e)
        if flush_decoder and decode_content and self._decoder:
            buf = self._decoder.decompress(binary_type())
            data += buf + self._decoder.flush()
        return data
*** Original Comment ***: Decompresses encoded data using the set decoder, handling potential decompression errors and flushing the decoder if required.
*** Original Thought ***: 
*** Validation Score ***: 4.2673115811402305
*** Validation Thought ***: Comment summarizes key actions: decompression, error handling, flushing. Minor details omitted, but main function clear. Concise and correct.


===============================[ 138 ]===============================
def get_tweets(user, pages=25):
    url = f'https://twitter.com/i/profiles/show/{user}/timeline/tweets?include_available_features=1&include_entities=1&include_new_items_bar=true'
    headers = {
        'Accept': 'application/json, text/javascript, */*; q=0.01',
        'Referer': f'https://twitter.com/{user}',
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/603.3.8 (KHTML, like Gecko) Version/10.1.2 Safari/603.3.8',
        'X-Twitter-Active-User': 'yes',
        'X-Requested-With': 'XMLHttpRequest',
        'Accept-Language': 'en-US'
    }
    def gen_tweets(pages):
        r = session.get(url, headers=headers)
        while pages > 0:
            try:
                html = HTML(html=r.json()['items_html'],
                            url='bunk', default_encoding='utf-8')
            except KeyError:
                raise ValueError(
                    f'Oops! Either "{user}" does not exist or is private.')
            comma = ","
            dot = "."
            tweets = []
            for tweet in html.find('html > .stream-item'):
                try:
                    text = tweet.find('.tweet-text')[0].full_text
                except IndexError:  
                    continue
                tweet_id = tweet.find('.js-permalink')[0].attrs['data-conversation-id']
                time = datetime.fromtimestamp(int(tweet.find('._timestamp')[0].attrs['data-time-ms']) / 1000.0)
                interactions = [
                    x.text
                    for x in tweet.find('.ProfileTweet-actionCount')
                ]
                replies = int(
                    interactions[0].split(' ')[0].replace(comma, '').replace(dot, '')
                    or interactions[3]
                )
                retweets = int(
                    interactions[1].split(' ')[0].replace(comma, '').replace(dot, '')
                    or interactions[4]
                    or interactions[5]
                )
                likes = int(
                    interactions[2].split(' ')[0].replace(comma, '').replace(dot, '')
                    or interactions[6]
                    or interactions[7]
                )
                hashtags = [
                    hashtag_node.full_text
                    for hashtag_node in tweet.find('.twitter-hashtag')
                ]
                urls = [
                    url_node.attrs['data-expanded-url']
                    for url_node in tweet.find('a.twitter-timeline-link:not(.u-hidden)')
                ]
                photos = [
                    photo_node.attrs['data-image-url']
                    for photo_node in tweet.find('.AdaptiveMedia-photoContainer')
                ]
                videos = []
                video_nodes = tweet.find(".PlayableMedia-player")
                for node in video_nodes:
                    styles = node.attrs['style'].split()
                    for style in styles:
                        if style.startswith('background'):
                            tmp = style.split('/')[-1]
                            video_id = tmp[:tmp.index('.jpg')]
                            videos.append({'id': video_id})
                tweets.append({
                    'tweetId': tweet_id,
                    'time': time,
                    'text': text,
                    'replies': replies,
                    'retweets': retweets,
                    'likes': likes,
                    'entries': {
                        'hashtags': hashtags, 'urls': urls,
                        'photos': photos, 'videos': videos
                    }
                })
            last_tweet = html.find('.stream-item')[-1].attrs['data-item-id']
            for tweet in tweets:
                if tweet:
                    tweet['text'] = re.sub('http', ' http', tweet['text'], 1)
                    yield tweet
            r = session.get(url, params={'max_position': last_tweet}, headers=headers)
            pages += -1
    yield from gen_tweets(pages)
*** Original Comment ***: Fetches and yields tweets from a specified user's timeline on Twitter, handling pagination and extracting tweet details such as text, interactions, and media.
*** Original Thought ***: 
*** Validation Score ***: 4.503131952409327
*** Validation Thought ***: Comment summarizes main function well, includes key elements like pagination, tweet details. Slightly lacks clarity on error handling.


===============================[ 139 ]===============================
def __info_yenczlib_gen(self):
        escape = 0
        dcrc32 = 0
        inflate = zlib.decompressobj(-15)
        header = next(self.__info_plain_gen())
        if not header.startswith("=ybegin"):
            raise NNTPDataError("Bad yEnc header")
        buf, trailer = fifo.Fifo(), ""
        for line in self.__info_plain_gen():
            if line.startswith("=yend"):
                trailer = line
                continue
            data, escape, dcrc32 = yenc.decode(line, escape, dcrc32)
            try:
                data = inflate.decompress(data)
            except zlib.error:
                raise NNTPDataError("Decompression failed")
            if not data:
                continue
            buf.write(data)
            for l in buf:
                yield l
        if not trailer:
            raise NNTPDataError("Missing yEnc trailer")
        ecrc32 = yenc.crc32(trailer)
        if ecrc32 is None:
            raise NNTPDataError("Bad yEnc trailer")
        if ecrc32 != dcrc32 & 0xffffffff:
            raise NNTPDataError("Bad yEnc CRC")
*** Original Comment ***: Generates and yields decompressed data from yEnc-encoded input, validating headers, trailers, and CRC32 checksums.
*** Original Thought ***: 
*** Validation Score ***: 4.877947876271995
*** Validation Thought ***: Comment summarizes code function accurately, concisely, and without redundancy. It covers key points: decompression, validation, yEnc.


===============================[ 140 ]===============================
def _load_attr(name: str, ctx: ast.AST = ast.Load()) -> ast.Attribute:
    attrs = name.split(".")
    def attr_node(node, idx):
        if idx >= len(attrs):
            node.ctx = ctx
            return node
        return attr_node(
            ast.Attribute(value=node, attr=attrs[idx], ctx=ast.Load()), idx + 1
        )
    return attr_node(ast.Name(id=attrs[0], ctx=ast.Load()), 1)
*** Original Comment ***: Generates an AST (Abstract Syntax Tree) `Attribute` node from a dotted attribute string, applying the specified context to the last attribute.
*** Original Thought ***: 
*** Validation Score ***: 4.886409401739566
*** Validation Thought ***: Comment matches code function. Correctly summarizes without redundancy. Clear and concise. No logical errors.


===============================[ 141 ]===============================
def open_in_browser(doc, encoding=None):
    import os
    import webbrowser
    import tempfile
    if not isinstance(doc, etree._ElementTree):
        doc = etree.ElementTree(doc)
    handle, fn = tempfile.mkstemp(suffix='.html')
    f = os.fdopen(handle, 'wb')
    try:
        doc.write(f, method="html", encoding=encoding or doc.docinfo.encoding or "UTF-8")
    finally:
        f.close()
    url = 'file://' + fn.replace(os.path.sep, '/')
    print(url)
    webbrowser.open(url)
*** Original Comment ***: Opens an XML/HTML document in the default web browser after writing it to a temporary file.
*** Original Thought ***: 
*** Validation Score ***: 4.150027459872864
*** Validation Thought ***: Code opens a document in a browser. Comment summarizes well but doesn't mention XML to ElementTree conversion.


===============================[ 142 ]===============================
def check_gpu_existence():
    global _gpu_available
    if _gpu_available is None:
        sess_config = tf.ConfigProto()
        sess_config.gpu_options.allow_growth = True
        try:
            with tf.Session(config=sess_config):
                device_list = device_lib.list_local_devices()
                _gpu_available = any(device.device_type == 'GPU' for device in device_list)
        except AttributeError as e:
            log.warning(f'Got an AttributeError `{e}`, assuming documentation building')
            _gpu_available = False
    return _gpu_available
*** Original Comment ***: Checks if a GPU is available for TensorFlow, setting a global flag accordingly, and handles potential attribute errors by assuming no GPU is available.
*** Original Thought ***: 
*** Validation Score ***: 4.601557603810862
*** Validation Thought ***: Comment summarizes function's purpose, handling of errors, and global flag update. Correct and concise, no redundancy.


===============================[ 143 ]===============================
def __fetch_crate_data(self, crate_id):
        raw_crate = self.client.crate(crate_id)
        crate = json.loads(raw_crate)
        return crate['crate']
*** Original Comment ***: Retrieves and parses the JSON data for a specific crate identified by `crate_id` from a client service.
*** Original Thought ***: 
*** Validation Score ***: 4.749378932649763
*** Validation Thought ***: Comment matches code function. It's correct, concise, and no redundant info. Explains the method's purpose clearly.


===============================[ 144 ]===============================
def verify(self, key):
        answer = _lib.NETSCAPE_SPKI_verify(self._spki, key._pkey)
        if answer <= 0:
            _raise_current_error()
        return True
*** Original Comment ***: Checks if the provided key correctly verifies the SPKI (Subject Public Key Info) structure, raising an error if verification fails, and returns True otherwise.
*** Original Thought ***: 
*** Validation Score ***: 4.732269232194593
*** Validation Thought ***: Comment accurately describes function's purpose, error handling, and return value. It's concise and correct.


===============================[ 145 ]===============================
def current_lr(self):
        if self.optimizer is None:
            raise RuntimeError(
                'lr is not applicable because optimizer does not exist.')
        return [group['lr'] for group in self.optimizer.param_groups]
*** Original Comment ***: Returns a list of current learning rates from the optimizer's parameter groups if the optimizer exists, otherwise raises a RuntimeError indicating the optimizer is not set.
*** Original Thought ***: 
*** Validation Score ***: 4.994943074400609
*** Validation Thought ***: Comment matches code function and is concise. No redundant information, explains error case. Correct and concise.


===============================[ 146 ]===============================
def delete_team(self, name):
        res = self.get_team(name)
        if res[0] == False:
            return res
        t = res[1]
        res = requests.delete(self.url + '/api/teams/' + str(t['id']), headers=self.hdrs, verify=self.ssl_verify)
        if not self._checkResponse(res):
            return [False, self.lasterr]
        return [True, None]
*** Original Comment ***: Deletes the team with the specified name after retrieving it, returning the result of the deletion attempt.
*** Original Thought ***: 
*** Validation Score ***: 4.196314697839336
*** Validation Thought ***: Code deletes a team by name. Comment summarizes well, but doesn't mention error handling. Slightly less than perfect.


===============================[ 147 ]===============================
def __download_price(self, symbol: str, currency: str, agent: str):
        from finance_quote_python import Quote
        assert isinstance(symbol, str)
        assert isinstance(currency, str)
        assert isinstance(agent, str)
        if not symbol:
            return None
        dl = Quote()
        dl.logger = self.logger
        dl.set_source(agent)
        dl.set_currency(currency)
        result = dl.fetch(agent, [symbol])
        if not result:
            raise ValueError(f"Did not receive a response for {symbol}.")
        price = result[0]
        if not price:
            raise ValueError(f"Price not downloaded/parsed for {symbol}.")
        else:
            self.add_price(price)
        return price
*** Original Comment ***: Fetches and adds the current price of the specified symbol using the given currency and agent, raising an error if the price cannot be retrieved or parsed.
*** Original Thought ***: 
*** Validation Score ***: 4.26408215207578
*** Validation Thought ***: Comment summarizes key actions (fetch, add, error handling). Minor details omitted (assertions, None return). Mostly correct and concise.


===============================[ 148 ]===============================
def purge_queue(self, name):
        content = {"_object_id": {"_object_name": "org.apache.qpid.broker:queue:{0}".format(name)},
                   "_method_name": "purge",
                   "_arguments": {"type": "queue",
                                  "name": name,
                                  "filter": dict()}}
        logger.debug("Message content -> {0}".format(content))
        return content, self.method_properties
*** Original Comment ***: # Sends a request to purge all messages from the specified queue in the Apache Qpid broker.
*** Original Thought ***: 
*** Validation Score ***: 4.703840401027454
*** Validation Thought ***: Comment accurately describes code's function. It's concise, correct, and no redundant info. Matches code's purpose.


===============================[ 149 ]===============================
def get_motion_vector(self):
        if any(self.move):
            x, y = self.actor._rot
            strafe = math.degrees(math.atan2(*self.move))
            y_angle = math.radians(y)
            x_angle = math.radians(x + strafe)
            dy = 0.0
            dx = math.cos(x_angle)
            dz = math.sin(x_angle)
        else:
            dy = 0.0
            dx = 0.0
            dz = 0.0
        return (dx, dy, dz)
*** Original Comment ***: Calculates and returns the motion vector for an actor based on its current rotation and movement direction.
*** Original Thought ***: 
*** Validation Score ***: 4.516186294528372
*** Validation Thought ***: Code calculates motion vector. Comment concise, correct, but assumes understanding of 'motion vector'.


===============================[ 150 ]===============================
def from_file(self, filename, table=None, delimiter='|', null='NULL',
            panic=True, quotechar='"', parse_dates=False):
        if not self.table:
            if not table:
                raise GiraffeError("Table must be set or specified to load a file.")
            self.table = table
        if not isinstance(null, basestring):
            raise GiraffeError("Expected 'null' to be str, received {}".format(type(null)))
        with Reader(filename, delimiter=delimiter, quotechar=quotechar) as f:
            if not isinstance(f.delimiter, basestring):
                raise GiraffeError("Expected 'delimiter' to be str, received {}".format(type(delimiter)))
            self.columns = f.header
            if isinstance(f, ArchiveFileReader):
                self.mload.set_encoding(ROW_ENCODING_RAW)
                self.preprocessor = lambda s: s
            if parse_dates:
                self.preprocessor = DateHandler(self.columns)
            self._initiate()
            self.mload.set_null(null)
            self.mload.set_delimiter(delimiter)
            i = 0
            for i, line in enumerate(f, 1):
                self.put(line, panic=panic)
                if i % self.checkpoint_interval == 1:
                    log.info("\rBulkLoad", "Processed {} rows".format(i), console=True)
                    checkpoint_status = self.checkpoint()
                    self.exit_code = self._exit_code()
                    if self.exit_code != 0:
                        return self.exit_code
            log.info("\rBulkLoad", "Processed {} rows".format(i))
            return self.finish()
*** Original Comment ***: Loads data from a file into a table, handling various data types and delimiters, with error checking and optional date parsing.
*** Original Thought ***: 
*** Validation Score ***: 4.041135384655945
*** Validation Thought ***: Comment summarizes key points, mentions error checking, data types, delimiters, and date parsing. Slightly lacks detail on error types.


===============================[ 151 ]===============================
def _retrieve(self):
        if self._authorization() and "whois_db" not in PyFunceble.INTERN:
            if PyFunceble.path.isfile(self.whois_db_path):
                PyFunceble.INTERN["whois_db"] = Dict().from_json(
                    File(self.whois_db_path).read()
                )
            else:
                PyFunceble.INTERN["whois_db"] = {}
*** Original Comment ***: Checks for authorization, then loads or initializes the WHOIS database from a JSON file if not already loaded.
*** Original Thought ***: 
*** Validation Score ***: 4.141894607865312
*** Validation Thought ***: Comment summarizes code well, but doesn't mention handling of missing file. Slightly lacks completeness.


===============================[ 152 ]===============================
def fetchMore(self, index):
        if not index.isValid():
            item = self.root
        else:
            item = index.internalPointer()
        if item.canFetchMore():
            startIndex = len(item.children)
            additionalChildren = item.fetchChildren()
            endIndex = startIndex + len(additionalChildren) - 1
            if endIndex >= startIndex:
                self.beginInsertRows(index, startIndex, endIndex)
                for newChild in additionalChildren:
                    item.addChild(newChild)
                self.endInsertRows()
*** Original Comment ***: Fetches and inserts additional children into the item at the given index if more can be fetched, updating the model accordingly.
*** Original Thought ***: 
*** Validation Score ***: 4.553429035482816
*** Validation Thought ***: Comment summarizes code function accurately and concisely. No redundant information, meets criteria fully.


===============================[ 153 ]===============================
def _very_fast_search(self, node, key, max_depth, with_links, crun):
        if key in self._links_count:
            return
        parent_full_name = node.v_full_name
        starting_depth = node.v_depth
        candidate_dict = self._get_candidate_dict(key, crun)
        if with_links:
            upper_bound = 1
        else:
            upper_bound = FAST_UPPER_BOUND
        if len(candidate_dict) > upper_bound:
            raise pex.TooManyGroupsError('Too many nodes')
        result_node = None
        for goal_name in candidate_dict:
            if goal_name.startswith(parent_full_name):
                candidate = candidate_dict[goal_name]
                if candidate.v_depth - starting_depth <= max_depth:
                    if result_node is not None:
                        raise pex.NotUniqueNodeError('Node `%s` has been found more than once, '
                                                     'full name of first occurrence is `%s` and of'
                                                     'second `%s`'
                                                     % (key, goal_name, result_node.v_full_name))
                    result_node = candidate
        if result_node is not None:
            return result_node, result_node.v_depth
*** Original Comment ***: Searches for a node with the specified key within a maximum depth, optionally considering links, and handles multiple or excessive matches.
*** Original Thought ***: 
*** Validation Score ***: 4.021661664985555
*** Validation Thought ***: Read code, identified search function with conditions. Comment concise, correct, but lacks detail on error handling specifics.


===============================[ 154 ]===============================
def _pool_single_run(kwargs):
    wrap_mode = kwargs['wrap_mode']
    traj = kwargs['traj']
    traj.v_storage_service = _pool_single_run.storage_service
    if wrap_mode == pypetconstants.WRAP_MODE_LOCAL:
        traj.v_storage_service.free_references()
    return _sigint_handling_single_run(kwargs)
*** Original Comment ***: Executes a single run within a multiprocessing pool, handling signal interrupts and managing trajectory storage based on the specified wrapping mode.
*** Original Thought ***: 
*** Validation Score ***: 4.025666040050644
*** Validation Thought ***: Comment summarizes code purpose and signal handling. Minor details omitted, but main function clear. Concise and correct.


===============================[ 155 ]===============================
def update_suggestions_dictionary(request, object):
    if request.user.is_authenticated():
        user = request.user
        content_type = ContentType.objects.get_for_model(type(object))
        try:
            ObjectView.objects.get(
                user=user, object_id=object.id, content_type=content_type)
        except:
            ObjectView.objects.create(user=user, content_object=object)
        viewed = ObjectView.objects.filter(user=user)
    else:
        update_dict_for_guests(request, object, content_type)
        return
    if viewed:
        for obj in viewed:
            if content_type == obj.content_type:
                if not exists_in_dictionary(request, object,
                                            content_type,
                                            obj, True):
                    if object.id != obj.object_id:
                        ObjectViewDictionary.objects.create(
                            current_object=object,
                            visited_before_object=obj.content_object)
                        if not exists_in_dictionary(request, obj,
                                                    obj.content_type,
                                                    object, False):
                            ObjectViewDictionary.objects.create(
                                current_object=obj.content_object,
                                visited_before_object=object)
    return
*** Original Comment ***: # Tracks and updates the user's viewed objects for personalized suggestions.
*** Original Thought ***: 
*** Validation Score ***: 3.72847854304546
*** Validation Thought ***: Comment summarizes code purpose. Minor details omitted, but main functionality (tracking/updating views) is clear. Concise, no redundancy.


===============================[ 156 ]===============================
def _set_top_cursor(self, cursor):
        scrollbar = self._control.verticalScrollBar()
        scrollbar.setValue(scrollbar.maximum())
        original_cursor = self._control.textCursor()
        self._control.setTextCursor(cursor)
        self._control.ensureCursorVisible()
        self._control.setTextCursor(original_cursor)
*** Original Comment ***: Sets the text cursor to the specified position, ensures it's visible, and then restores the original cursor position without changing the scrollbar's position.
*** Original Thought ***: 
*** Validation Score ***: 4.2560326370459975
*** Validation Thought ***: Comment describes main actions. Minor detail about scrollbar's maximum value setting is omitted. Otherwise, accurate and concise.


===============================[ 157 ]===============================
def parse_version(version):
    global parse_version
    try:
        from pkg_resources import parse_version
    except ImportError:
        from distutils.version import LooseVersion as parse_version
    return parse_version(version)
*** Original Comment ***: # Attempts to import `parse_version` from `pkg_resources`, falls back to `LooseVersion` from `distutils.version` if unavailable, and returns the parsed version object.
*** Original Thought ***: 
*** Validation Score ***: 4.856443298727443
*** Validation Thought ***: Comment matches code function, is correct, concise, and lacks redundant information.


===============================[ 158 ]===============================
def sum_mvns(distributions):
  graph_parents = [tensor for distribution in distributions
                   for tensor in distribution._graph_parents]  
  with tf.compat.v1.name_scope('sum_mvns', values=graph_parents):
    if all([isinstance(mvn, tfd.MultivariateNormalDiag)
            for mvn in distributions]):
      return tfd.MultivariateNormalDiag(
          loc=sum([mvn.mean() for mvn in distributions]),
          scale_diag=tf.sqrt(sum([
              mvn.scale.diag**2 for mvn in distributions])))
    else:
      raise NotImplementedError(
          'Sums of distributions other than MultivariateNormalDiag are not '
          'currently implemented. (given: {})'.format(distributions))
*** Original Comment ***: # Sums a list of MultivariateNormalDiag distributions, raising NotImplementedError for other types.
*** Original Thought ***: 
*** Validation Score ***: 4.466047477618429
*** Validation Thought ***: Comment summarizes main function. Correct and concise, but doesn't mention graph_parents or name_scope details.


===============================[ 159 ]===============================
def crz(self, theta, ctl, tgt):
    return self.append(CrzGate(theta), [ctl, tgt], [])
*** Original Comment ***: Adds a controlled-Z rotation gate with an angle `theta` to the quantum circuit, with `ctl` as the control qubit and `tgt` as the target qubit.
*** Original Thought ***: 
*** Validation Score ***: 4.988553427722655
*** Validation Thought ***: Comment matches code function and is concise. No redundant info. Correctly summarizes the code's purpose.


===============================[ 160 ]===============================
def parse(cls, parser, token):
        bits, as_var = parse_as_var(parser, token)
        tag_name, args, kwargs = parse_token_kwargs(parser, bits, ('template',) + cls.allowed_kwargs, compile_args=cls.compile_args, compile_kwargs=cls.compile_kwargs)
        cls.validate_args(tag_name, *args)
        return cls(tag_name, as_var, *args, **kwargs)
*** Original Comment ***: Initializes a template tag by parsing the token using the given parser and returns an instance of the class with the parsed arguments and keyword arguments.
*** Original Thought ***: 
*** Validation Score ***: 4.142786503223986
*** Validation Thought ***: Comment summarizes function well, but doesn't mention validation step. Otherwise, clear and concise. Missing detail lowers score slightly.


===============================[ 161 ]===============================
def build_core_type(s_cdt):
    s_dt = nav_one(s_cdt).S_DT[17]()
    if s_dt.name == 'void':
        type_name = None
    elif s_dt.name == 'boolean':
        type_name = 'xs:boolean'
    elif s_dt.name == 'integer':
        type_name = 'xs:integer'
    elif s_dt.name == 'real':
        type_name = 'xs:decimal'
    elif s_dt.name == 'string':
        type_name = 'xs:string'
    elif s_dt.name == 'unique_id':
        type_name = 'xs:integer'
    else:
        type_name = None
    if type_name:
        mapped_type = ET.Element('xs:simpleType', name=s_dt.name)
        ET.SubElement(mapped_type, 'xs:restriction', base=type_name)
        return mapped_type
*** Original Comment ***: # This function maps a custom data type to an XML schema simple type, returning an XML element representing the mapped type if applicable.
*** Original Thought ***: 
*** Validation Score ***: 4.524218304421593
*** Validation Thought ***: Comment accurately summarizes the code's function of mapping data types to XML schema. It's correct and concise.


===============================[ 162 ]===============================
def _filter_deprecation_warnings():
    deprecation_filter = ('always', None, DeprecationWarning,
                          re.compile(r'^qiskit\.*', re.UNICODE), 0)
    try:
        warnings._add_filter(*deprecation_filter, append=False)
    except AttributeError:
        pass
    warnings.simplefilter('ignore', category=ChangedInMarshmallow3Warning)
*** Original Comment ***: Sets up filters to always display deprecation warnings originating from the 'qiskit' package and ignore warnings categorized as 'ChangedInMarshmallow3Warning'.
*** Original Thought ***: 
*** Validation Score ***: 4.656902133849169
*** Validation Thought ***: Comment matches code function. Correctly summarizes actions without extra details. Concise and accurate.


===============================[ 163 ]===============================
def  make_file_list( args = None ):
    file_list = []
    if not args:
        args = sys.argv[1 :]
    for pathname in args:
        if string.find( pathname, '*' ) >= 0:
            newpath = glob.glob( pathname )
            newpath.sort()  
        else:
            newpath = [pathname]
        file_list.extend( newpath )
    if len( file_list ) == 0:
        file_list = None
    else:
        file_list = filter( file_exists, file_list )
    return file_list
*** Original Comment ***: # This function constructs a list of file paths from command-line arguments or a provided list, expanding wildcards and filtering out non-existent files.
*** Original Thought ***: 
*** Validation Score ***: 4.9794802456888245
*** Validation Thought ***: Comment matches code function, explains wildcard expansion and filtering, is concise and correct.


===============================[ 164 ]===============================
def nwise(iter, n):
 iterset = [iter]
 while len(iterset) < n:
		iterset[-1:] = itertools.tee(iterset[-1])
  next(iterset[-1], None)
 return six.moves.zip(*iterset)
*** Original Comment ***: Generates overlapping n-tuples from an input iterable.
*** Original Thought ***: 
*** Validation Score ***: 4.044071925747102
*** Validation Thought ***: Code creates n-tuples from iterable. Comment concise, correct. Misses 'overlapping' detail. Almost perfect, slight room for improvement.


===============================[ 165 ]===============================
def handle_ping(self, payload):
        self.logger.info('server ping: %s' % payload)
        self.send('PONG %s' % payload, True)
*** Original Comment ***: Responds to a ping request by logging the payload and sending back a 'PONG' response with the payload.
*** Original Thought ***: 
*** Validation Score ***: 4.99056306165636
*** Validation Thought ***: Comment matches code function. It's correct, concise, and no redundant info. Explains logging and response clearly.


===============================[ 166 ]===============================
def set_completer_frame(self, frame=None):
        if frame:
            self.Completer.namespace = frame.f_locals
            self.Completer.global_namespace = frame.f_globals
        else:
            self.Completer.namespace = self.user_ns
            self.Completer.global_namespace = self.user_global_ns
*** Original Comment ***: Sets the autocompleter's local and global namespaces to the given frame's if provided, or to the user's namespaces otherwise.
*** Original Thought ***: 
*** Validation Score ***: 4.996625410785249
*** Validation Thought ***: Comment accurately describes code function. It's concise, correct, and no redundant information.


===============================[ 167 ]===============================
def new(self, kind, *args, **kwargs):
        metaclass = self.find_metaclass(kind)
        return metaclass.new(*args, **kwargs)
*** Original Comment ***: Creates a new instance of the specified `kind` using the associated metaclass and additional arguments and keyword arguments provided.
*** Original Thought ***: 
*** Validation Score ***: 4.931524656307273
*** Validation Thought ***: Code defines 'new' method. Comment matches functionality, is correct and concise. No redundant info.


===============================[ 168 ]===============================
def parse_questions(raw_page):
        raw_questions = json.loads(raw_page)
        questions = raw_questions['items']
        for question in questions:
            yield question
*** Original Comment ***: Extracts and yields individual questions from a JSON-formatted string containing a collection of questions under the 'items' key.
*** Original Thought ***: 
*** Validation Score ***: 4.9749788322951325
*** Validation Thought ***: Code parses JSON, extracts 'items'. Comment matches functionality, is correct and concise. No redundant info.


===============================[ 169 ]===============================
def delayed(f):
    def wrapped(*args, **kwargs):
        key_promise = list([(key, promisify(value)) for key, value in kwargs.items()])
        arg_promises = list([promisify(value) for value in args])
        kwarg_promises = list([promise for key, promise in key_promise])
        promises = arg_promises + kwarg_promises
        for promise in promises:
            def echo_error(exc, promise=promise):
                print("error with ", promise, "exception is", exc)
            def echo(value, promise=promise):
                print("done with ", repr(promise), "value is", value)
        allarguments = aplus.listPromise(*promises)
        def call(_):
            kwargs_real = {key: promise.get() for key, promise in key_promise}
            args_real = list([promise.get() for promise in arg_promises])
            return f(*args_real, **kwargs_real)
        def error(exc):
            print("error", exc)
            raise exc
        return allarguments.then(call, error)
    return wrapped
*** Original Comment ***: Wraps a function `f` to handle asynchronous arguments using promises, providing error handling and result logging for each promise.
*** Original Thought ***: 
*** Validation Score ***: 4.290249778944495
*** Validation Thought ***: Comment summarizes async handling and logging. Misses details on how arguments are processed but captures essence. Slightly lacks conciseness.


===============================[ 170 ]===============================
def filter(func):
    def expand_kv(kv):
        return func(*kv)
    def filter_values(value):
        cls = type(value)
        if isinstance(value, dict):
            return cls(_filter(expand_kv, value.items()))
        else:
            return cls(_filter(func, value))
    return transform(filter_values)
*** Original Comment ***: The `filter` function decorates a given function `func` to filter key-value pairs in dictionaries or elements in iterables based on `func`'s criteria, applying the filtering through a transformation process.
*** Original Thought ***: 
*** Validation Score ***: 3.9405703577173186
*** Validation Thought ***: Comment summarizes code's purpose well, but lacks detail on transformation process specifics. Still, it's concise and mostly accurate.


===============================[ 171 ]===============================
def wait_on_rate_limit(self, value):
        check_type(value, bool, may_be_none=False)
        self._wait_on_rate_limit = value
*** Original Comment ***: Sets the `_wait_on_rate_limit` attribute to the specified boolean value, ensuring the input is of type bool and not None.
*** Original Thought ***: 
*** Validation Score ***: 4.740910612444593
*** Validation Thought ***: Comment matches code function. Explains setting attribute with type check. Correct and concise. No redundant info.


===============================[ 172 ]===============================
def add_text(self, text, cursor=None, justification=None):
        if cursor is None:
            cursor = self.page.cursor
        text = re.sub("\s\s+" , " ", text)
        if justification is None:
            justification = self.justification
        if '\n' in text:
            text_list = text.split('\n')
            for text in text_list:
                PDFText(self.session, self.page, text, self.font, self.text_color, cursor, justification, self.double_spacing)
                self.add_newline()
        else:
            PDFText(self.session, self.page, text, self.font, self.text_color, cursor, justification, self.double_spacing)
*** Original Comment ***: Adds the provided text to a PDF page, handling cursor positioning, text justification, and newline characters.
*** Original Thought ***: 
*** Validation Score ***: 4.779944628967267
*** Validation Thought ***: Comment is concise, accurate, and covers key points: cursor positioning, justification, handling newlines. No redundant info.


===============================[ 173 ]===============================
def write_json(f: TextIO, deja_vu_sans_path: str,
               measurer: text_measurer.TextMeasurer,
               encodings: Iterable[str]) -> None:
    supported_characters = list(
        generate_supported_characters(deja_vu_sans_path))
    kerning_characters = ''.join(
        generate_encodeable_characters(supported_characters, encodings))
    char_to_length = calculate_character_to_length_mapping(measurer,
                                                           supported_characters)
    pair_to_kerning = calculate_pair_to_kern_mapping(measurer, char_to_length,
                                                     kerning_characters)
    json.dump(
        {'mean-character-length': statistics.mean(char_to_length.values()),
         'character-lengths': char_to_length,
         'kerning-characters': kerning_characters,
         'kerning-pairs': pair_to_kerning},
        f, sort_keys=True, indent=1)
*** Original Comment ***: Writes a JSON file containing mean character length, individual character lengths, kerning characters, and kerning pairs, based on the specified font and encodings.
*** Original Thought ***: 
*** Validation Score ***: 4.9282426807834545
*** Validation Thought ***: Comment accurately summarizes code's function. Correct, concise, no redundant info. Matches code's output.


===============================[ 174 ]===============================
def login_decorator(func):
    def wrap(provider, *args, **kwargs):
        error = None
        result = authomatic.core.LoginResult(provider)
        try:
            func(provider, *args, **kwargs)
        except Exception as e:  
            if provider.settings.report_errors:
                error = e
                if not isinstance(error, CancellationError):
                    provider._log(
                        logging.ERROR,
                        u'Reported suppressed exception: {0}!'.format(
                            repr(error)),
                        exc_info=1)
            else:
                if provider.settings.debug:
                    provider.write(
                        _error_traceback_html(
                            sys.exc_info(),
                            traceback.format_exc()))
                raise
        if provider.user or error:
            result = authomatic.core.LoginResult(provider)
            result.error = error
            if isinstance(provider.session, authomatic.core.Session):
                provider.session.delete()
            provider._log(logging.INFO, u'Procedure finished.')
            if provider.callback:
                provider.callback(result)
            return result
        else:
            provider.save_session()
    return wrap
*** Original Comment ***: Adds error handling and logging to a login process, wrapping the original function with additional functionality for reporting and debugging.
*** Original Thought ***: 
*** Validation Score ***: 4.046436532300205
*** Validation Thought ***: Comment summarizes error handling, logging, wrapping. Misses session handling details. Correct, concise, but not fully comprehensive.


===============================[ 175 ]===============================
def _node_to_asn(self, node):
    if node.is_type(TokenType.identifier):
      return Identifier(node.svalue)
    elif node.is_type(TokenType.terminal):
      return Terminal(node.svalue)
    elif node.is_type(TokenType.option_group):
      expr = node.children[0]
      return OptionGroup(self._expression_to_asn(expr))
    elif node.is_type(TokenType.repetition_group):
      expr = node.children[0]
      return RepetitionGroup(self._expression_to_asn(expr))
    elif node.is_type(TokenType.grouping_group):
      expr = node.children[0]
      return GroupingGroup(self._expression_to_asn(expr))
    elif node.is_type(TokenType.special_handling):
      ident = node.children[0]
      return SpecialHandling(ident)
    elif node.is_type(TokenType.number):
      return Number(node.svalue)
    elif node.is_type((TokenType.operator, TokenType.op_mult, TokenType.op_add)):
      return OperatorNode(OPERATOR_INDEX[node.svalue], node.position)
    else:
      raise Exception("Unhandled parse tree node: {0}".format(node))
*** Original Comment ***: Converts a parse tree node into an Abstract Syntax Notation (ASN) object based on the node's type.
*** Original Thought ***: 
*** Validation Score ***: 4.6044504527582575
*** Validation Thought ***: Comment summarizes code function accurately and concisely. No redundant information. Correctly reflects code's purpose.


===============================[ 176 ]===============================
def get_handler(self, args):
    if len(args) == 2:
      args += ['.']
    self.validate('cmd|s3|local', args)
    source = args[1]
    target = args[2]
    self.s3handler().get_files(source, target)
*** Original Comment ***: # This function extends the argument list if necessary, validates the command type, and initiates the transfer of files from the source to the target using the S3 handler.
*** Original Thought ***: 
*** Validation Score ***: 4.589635254754422
*** Validation Thought ***: Comment matches code function, explains argument extension, validation, and file transfer initiation. Correct and concise.


===============================[ 177 ]===============================
def launch_ipython_5_shell(args):
    import IPython  
    from traitlets.config import Config
    c = Config()
    path = os.path.dirname(os.path.abspath(__file__))
    try:
        get_ipython  
        _print("WARNING: Running IPython within IPython.")
    except NameError:
        c.InteractiveShell.banner1 = 'SolveBio Python shell started.\n'
    c.InteractiveShellApp.exec_files = ['{}/ipython_init.py'.format(path)]
    IPython.start_ipython(argv=[], config=c)
*** Original Comment ***: Initializes and starts an embedded IPython shell with a custom banner and initialization file for the SolveBio Python environment.
*** Original Thought ***: 
*** Validation Score ***: 4.07635243505242
*** Validation Thought ***: Read code, identifies IPython shell launch with config. Comment concise, correct, but lacks try-except detail. Scored 4 for slight omission.


===============================[ 178 ]===============================
def _wrap_command(cmds, cls, strict=True):
    class WrappedCommand(cls):
        def run(self):
            if not getattr(self, 'uninstall', None):
                try:
                    [self.run_command(cmd) for cmd in cmds]
                except Exception:
                    if strict:
                        raise
                    else:
                        pass
            update_package_data(self.distribution)
            result = cls.run(self)
            return result
    return WrappedCommand
*** Original Comment ***: Initializes a subclass of `cls` that wraps the execution of a list of commands `cmds`, with an option to enforce strict error handling.
*** Original Thought ***: 
*** Validation Score ***: 4.0367232121401795
*** Validation Thought ***: Comment summarizes code purpose well. Minor details missing, like update_package_data, but main functionality covered. Concise and correct.


===============================[ 179 ]===============================
def indexes(self, collection=None):
        indexes = []
        for collection_name in self.collections():
            if collection and collection != collection_name:
                continue
            for index_name in self.db[collection_name].index_information():
                if index_name != '_id_':
                    indexes.append(index_name)
        return indexes
*** Original Comment ***: Returns a list of index names for the specified collection or all collections if no specific collection is provided, excluding the default '_id_' index.
*** Original Thought ***: 
*** Validation Score ***: 4.853834590891218
*** Validation Thought ***: Comment accurately describes code function. Excludes '_id_', mentions all collections. Correct and concise. No redundant info.


===============================[ 180 ]===============================
def redraw_canvas(self):
        from xdot_parser import XdotAttrParser
        xdot_parser = XdotAttrParser()
        canvas = self._component_default()
        for node in self.nodes:
            components = xdot_parser.parse_xdot_data( node._draw_ )
            canvas.add( *components )
            components = xdot_parser.parse_xdot_data( node._ldraw_ )
            canvas.add( *components )
        for edge in self.edges:
            components = xdot_parser.parse_xdot_data( edge._draw_ )
            canvas.add( *components )
            components = xdot_parser.parse_xdot_data( edge._ldraw_ )
            canvas.add( *components )
            components = xdot_parser.parse_xdot_data( edge._hdraw_ )
            canvas.add( *components )
            components = xdot_parser.parse_xdot_data( edge._tdraw_ )
            canvas.add( *components )
            components = xdot_parser.parse_xdot_data( edge._hldraw_ )
            canvas.add( *components )
            components = xdot_parser.parse_xdot_data( edge._tldraw_ )
            canvas.add( *components )
        self.component = canvas
        self.vp.request_redraw()
*** Original Comment ***: Recreates the canvas by parsing and adding xdot attributes for all nodes and edges, then requests a redraw of the visual presentation.
*** Original Thought ***: 
*** Validation Score ***: 4.476970646557119
*** Validation Thought ***: Read code, identifies parsing and adding xdot data for nodes/edges. Comment concise, correct, summarizes code purpose. Score 5.


===============================[ 181 ]===============================
def _declare_special(self, name, sep, klass):
        if name in self._special:
            special = self._special[name]
            if not isinstance(special, klass) or sep != special._sep:
                raise ValueError('variable %s already declared as %s '
                                 'with separator "%s"' %
                                 (name, special.__class__.__name__,
                                  special._sep))
        else:
            self._special[name] = klass(self, name, sep)
*** Original Comment ***: Checks if a special variable `name` is already declared with the same class `klass` and separator `sep`, and if not, declares it; raises ValueError if a conflict is detected.
*** Original Thought ***: 
*** Validation Score ***: 4.712252469808294
*** Validation Thought ***: Comment matches code function, is correct, concise, and lacks redundant information.


===============================[ 182 ]===============================
def _parse_dot_code_fired(self):
        parser = GodotDataParser()
        graph  = parser.parse_dot_data(self.dot_code)
        if graph is not None:
            self.model = graph
*** Original Comment ***: Initializes a parser for Godot data, parses the `dot_code` attribute, and sets the `model` attribute to the resulting graph if parsing is successful.
*** Original Thought ***: 
*** Validation Score ***: 4.905987762713455
*** Validation Thought ***: Comment matches code function, is correct, concise, and lacks redundant information.


===============================[ 183 ]===============================
def index_scan_ids(self, fname, val):
        disj = []
        for fname2 in self.indexes[fname]['feature_names']:
            disj.append({'term': {fname_to_idx_name(fname2): val}})
        query = {
            'constant_score': {
                'filter': {'or': disj},
            },
        }
        hits = scan(self.conn, index=self.index, doc_type=self.type, query={
            '_source': False,
            'query': query,
        })
        for hit in hits:
            yield did(hit['_id'])
*** Original Comment ***: # This function generates a generator that yields document IDs from a scan search in Elasticsearch where any of the indexed features match the given value.
*** Original Thought ***: 
*** Validation Score ***: 4.225630039199564
*** Validation Thought ***: Comment summarizes code's function well. Minor lack of detail on 'scan' specifics, but overall clear and concise. No logical errors.


===============================[ 184 ]===============================
def query(self, query):
        if query is None:
            return
        if log.level >= VERBOSE:
            self.options("query", query, 6)
        else:
            self.options("query", truncate(query), 6)
        statements = parse_statement(remove_curly_quotes(query))
        if not statements:
            raise GiraffeError("Unable to parse SQL statement")
        if len(statements) > 1:
            show_warning(("MORE THAN ONE STATEMENT RECEIVED, EXPORT OPERATIONS ALLOW ONE "
                "STATEMENT - ONLY THE FIRST STATEMENT WILL BE USED."), RuntimeWarning)
        statement = statements[0]
        log.debug("Debug[2]", "Statement (sanitized): {!r}".format(statement))
        if not (statement.startswith("select ") or statement.startswith("sel ")):
            statement = "select * from {}".format(statement)
        if statement == self.query:
            return
        else:
            self._query = statement
        self.initiated = False
        try:
            self.export.set_query(statement)
        except InvalidCredentialsError as error:
            if self.protect:
                Config.lock_connection(self.config, self.dsn, self.key_file)
            raise error
*** Original Comment ***: Executes a given SQL query with verbose logging, input sanitization, and single-statement enforcement, handling errors and updating the export query if valid.
*** Original Thought ***: 
*** Validation Score ***: 4.005830552972467
*** Validation Thought ***: Comment summarizes key points: logging, sanitization, enforcement. Misses details on error handling specifics. Concise and correct.


===============================[ 185 ]===============================
def ndtri(p, name="ndtri"):
  with tf.name_scope(name):
    p = tf.convert_to_tensor(value=p, name="p")
    if dtype_util.as_numpy_dtype(p.dtype) not in [np.float32, np.float64]:
      raise TypeError(
          "p.dtype=%s is not handled, see docstring for supported types."
          % p.dtype)
    return _ndtri(p)
*** Original Comment ***: Calculates the inverse of the cumulative distribution function of the standard normal distribution for a given probability `p`, ensuring the input tensor `p` is of a supported float type.
*** Original Thought ***: 
*** Validation Score ***: 4.647416895991498
*** Validation Thought ***: Comment matches code function, checks input type, and is concise. No redundant info.


===============================[ 186 ]===============================
def compute(self, *inputs, **kwargs):
        from deepy.core.neural_var import NeuralVariable
        from deepy.core.graph import graph
        if type(inputs[0]) != NeuralVariable:
            raise SystemError("The input of `compute` must be NeuralVar")
        dims = [t.dim() for t in inputs]
        if len(inputs) == 1:
            self.init(input_dim=dims[0])
        else:
            self.init(input_dims=dims)
        if self.parameters and not self._linked_block:
            self.belongs_to(graph.default_block())
        train_kwargs, _, _ = convert_to_theano_var(kwargs)
        output = self.compute_tensor(*[t.tensor for t in inputs], **train_kwargs)
        if type(output) != list and type(output) != tuple:
            return NeuralVariable(output, dim=self.output_dim)
        else:
            return [NeuralVariable(*item) for item in zip(output, self.output_dims)]
*** Original Comment ***: # This function initializes and computes the output of a neural network layer, ensuring inputs are NeuralVariables and handling parameter linking within a computation graph.
*** Original Thought ***: 
*** Validation Score ***: 4.09159020550626
*** Validation Thought ***: Comment summarizes initialization, computation, input checks, and graph linking. Minor details omitted, but main functionality covered. Concise and correct.


===============================[ 187 ]===============================
def exec_command(ctx, section, command, map_files):
    try:
        logger.debug('Running exec command')
        existing_sections = config.ConfigFileResolver(config.LOCAL_CONFIG_FILE).sections()
        command = ' '.join(command)
        if section not in existing_sections:
            command = '{} {}'.format(section, command) if command else section
            section = None
        if not command:
            logger.warning('No command detected.')
            click.echo(exec_command.get_help(ctx))
            return
        settings = config.Settings(section=section)
        storage = STORAGES['s3'](settings=settings)
        conf = s3conf.S3Conf(storage=storage, settings=settings)
        env_vars = conf.get_envfile().as_dict()
        if env_vars.get('S3CONF_MAP') and map_files:
            conf.download_mapping(env_vars.get('S3CONF_MAP'))
        current_env = os.environ.copy()
        current_env.update(env_vars)
        logger.debug('Executing command "%s"', command)
        subprocess.run(shlex.split(command), env=current_env, check=True)
    except exceptions.EnvfilePathNotDefinedError:
        raise exceptions.EnvfilePathNotDefinedUsageError()
*** Original Comment ***: Executes a specified command within a given configuration section, handling S3 mappings and environment setup.
*** Original Thought ***: 
*** Validation Score ***: 4.044360182962972
*** Validation Thought ***: Comment summarizes code's function. Mentions S3, env setup. Omits try-except block, but main functionality covered. Concise, no redundancy.


===============================[ 188 ]===============================
def cgetter(self, fcget: typing.Optional[typing.Callable[[typing.Any], typing.Any]]) -> "AdvancedProperty":
        self.__fcget = fcget
        return self
*** Original Comment ***: Sets a custom getter function for the `AdvancedProperty` and returns the instance for method chaining.
*** Original Thought ***: 
*** Validation Score ***: 4.954178722040068
*** Validation Thought ***: Code defines a method to set a getter. Comment accurately and concisely describes the method's purpose and behavior.


