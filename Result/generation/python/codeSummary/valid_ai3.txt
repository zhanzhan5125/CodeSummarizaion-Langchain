===============================[ 1 ]===============================
def _restrict_along_direction(value_and_gradients_function,
                              position,
                              direction):
  def _restricted_func(t):
    t = _broadcast(t, position)
    pt = position + tf.expand_dims(t, axis=-1) * direction
    objective_value, gradient = value_and_gradients_function(pt)
    return ValueAndGradient(
        x=t,
        f=objective_value,
        df=tf.reduce_sum(input_tensor=gradient * direction, axis=-1),
        full_gradient=gradient)
  return _restricted_func
*** Original Comment ***: Creates a restricted function that evaluates the objective and its gradient along a specific direction.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'that evaluates' to streamline the comment: 'Creates a restricted function to evaluate the objective and its gradient along a specific direction.'
----------------Refine 1----------------
*** Refine comment ***: Creates a restricted function to evaluate the objective and its gradient along a specific direction.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'to evaluate' as 'Creates a restricted function for the objective and its gradient along a specific direction.'
*** Final Score ***: 4
*** Final Comment ***: Creates a restricted function to evaluate the objective and its gradient along a specific direction.
===============================[ 1 ]===============================
def _restrict_along_direction(value_and_gradients_function,
                              position,
                              direction):
  def _restricted_func(t):
    t = _broadcast(t, position)
    pt = position + tf.expand_dims(t, axis=-1) * direction
    objective_value, gradient = value_and_gradients_function(pt)
    return ValueAndGradient(
        x=t,
        f=objective_value,
        df=tf.reduce_sum(input_tensor=gradient * direction, axis=-1),
        full_gradient=gradient)
  return _restricted_func
*** Original Comment ***: Defines a function that restricts another function along a given direction, returning the value and projected gradient at that point.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'at that point' as it is implied by 'returning the value and projected gradient'.
----------------Refine 1----------------
*** Refine comment ***: Defines a function that restricts another function along a given direction, returning the value and projected gradient.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'another' as it is implied by context.
*** Final Score ***: 4
*** Final Comment ***: Defines a function that restricts another function along a given direction, returning the value and projected gradient.


===============================[ 2 ]===============================
def _find_namespaces_from_child(parent, child, namespaces):
        for cur_child in parent.childNodes:
            if cur_child is child:
                return True
            if _MinidomXmlToObject._find_namespaces_from_child(cur_child, child, namespaces):
                for key in cur_child.attributes.keys():
                    if key.startswith('xmlns:') or key == 'xmlns':
                        namespaces[key] = cur_child.attributes[key]
                break
        return False
*** Original Comment ***: Recursively search for namespaces starting from a child node and collect them into the provided namespaces dictionary.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'starting from a child node and' as it is implied by 'recursively search'.
----------------Refine 1----------------
*** Refine comment ***: Recursively search for namespaces and collect them into the provided namespaces dictionary.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'and collect them' as 'search for namespaces' implies collection.
*** Final Score ***: 4
*** Final Comment ***: Recursively search for namespaces and collect them into the provided namespaces dictionary.


===============================[ 3 ]===============================
def CovInv(self):
        self.recurse(full_matrix=True)
        return self.tree.root.cinv
*** Original Comment ***: Calculates the inverse of the covariance matrix for the root of the tree.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Calculates the inverse of the covariance matrix for the root of the tree.


===============================[ 4 ]===============================
def _satisfies_wolfe(val_0,
                     val_c,
                     f_lim,
                     sufficient_decrease_param,
                     curvature_param):
  exact_wolfe_suff_dec = (sufficient_decrease_param * val_0.df >=
                          (val_c.f - val_0.f) / val_c.x)
  wolfe_curvature = val_c.df >= curvature_param * val_0.df
  exact_wolfe = exact_wolfe_suff_dec & wolfe_curvature
  approx_wolfe_applies = val_c.f <= f_lim
  approx_wolfe_suff_dec = ((2 * sufficient_decrease_param - 1) * val_0.df
                           >= val_c.df)
  approx_wolfe = approx_wolfe_applies & approx_wolfe_suff_dec & wolfe_curvature
  is_satisfied = exact_wolfe | approx_wolfe
  return is_satisfied
*** Original Comment ***: Determine if the Wolfe conditions are satisfied for line search
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Determine if the Wolfe conditions are satisfied for line search


===============================[ 5 ]===============================
def get_misses(self):
        return [self.stats[cache_level]['MISS_count']/self.first_dim_factor
                for cache_level in range(len(self.machine['memory hierarchy']))]
*** Original Comment ***: Return a list of miss counts for each cache level, adjusted by the first dimension factor.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'for each cache level,' as it's implied by the list comprehension.
----------------Refine 1----------------
*** Refine comment ***: Return a list of miss counts, adjusted by the first dimension factor.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'counts,' as 'misses' implies a count.
*** Final Score ***: 4
*** Final Comment ***: Return a list of miss counts, adjusted by the first dimension factor.


===============================[ 6 ]===============================
def call(self, inputs):
    net = self.encoder_net(tf.cast(inputs, tf.float32))
    return ed.MultivariateNormalDiag(
        loc=net[..., :self.latent_size],
        scale_diag=tf.nn.softplus(net[..., self.latent_size:]),
        name="latent_code_posterior")
*** Original Comment ***: The 'call' method of a class defines a custom layer or model that applies an encoder network to the inputs and returns a multivariate normal distribution with diagonal covariance, parameterized by the encoder's output.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'of a class' as it's implied by the context and 'or model' as it's not necessary to mention both layer and model. Simplify to: 'The call method applies an encoder network to inputs, returning a multivariate normal distribution parameterized by the encoder's output.'
----------------Refine 1----------------
*** Refine comment ***: The call method applies an encoder network to inputs, returning a multivariate normal distribution parameterized by the encoder's output.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'applies an' and 'to inputs', resulting in: 'The call method uses the encoder network, returning a multivariate normal distribution parameterized by the encoder's output.'
*** Final Score ***: 4
*** Final Comment ***: The call method applies an encoder network to inputs, returning a multivariate normal distribution parameterized by the encoder's output.


===============================[ 7 ]===============================
def ancestry(self, context):
        log.debug("get ancestry %s", context)
        if context is None:
            return
        if hasattr(context, 'im_class'):
            context = context.im_class
        elif hasattr(context, '__self__'):
            context = context.__self__.__class__
        if hasattr(context, '__module__'):
            ancestors = context.__module__.split('.')
        elif hasattr(context, '__name__'):
            ancestors = context.__name__.split('.')[:-1]
        else:
            raise TypeError("%s has no ancestors?" % context)
        while ancestors:
            log.debug(" %s ancestors %s", context, ancestors)
            yield resolve_name('.'.join(ancestors))
            ancestors.pop()
*** Original Comment ***: Generates a sequence of module names that represent the ancestry of the given context.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'that represent' as it is implied by 'ancestry'.
----------------Refine 1----------------
*** Refine comment ***: Generates a sequence of module names representing the ancestry of the given context.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'representing' as it is implied by 'ancestry'.
*** Final Score ***: 4
*** Final Comment ***: Generates a sequence of module names representing the ancestry of the given context.


===============================[ 8 ]===============================
def get_item_abspath(self, identifier):
        admin_metadata = self.get_admin_metadata()
        uuid = admin_metadata["uuid"]
        dataset_cache_abspath = os.path.join(self._s3_cache_abspath, uuid)
        mkdir_parents(dataset_cache_abspath)
        bucket_fpath = self.data_key_prefix + identifier
        obj = self.s3resource.Object(self.bucket, bucket_fpath)
        relpath = obj.get()['Metadata']['handle']
        _, ext = os.path.splitext(relpath)
        local_item_abspath = os.path.join(
            dataset_cache_abspath,
            identifier + ext
        )
        if not os.path.isfile(local_item_abspath):
            tmp_local_item_abspath = local_item_abspath + ".tmp"
            self.s3resource.Bucket(self.bucket).download_file(
                bucket_fpath,
                tmp_local_item_abspath
            )
            os.rename(tmp_local_item_abspath, local_item_abspath)
        return local_item_abspath
*** Original Comment ***: Retrieves the absolute path of an item by its identifier, downloading it from S3 if it doesn't exist locally.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'by its identifier,' as it is implied by 'get_item'.
----------------Refine 1----------------
*** Refine comment ***: Retrieves the absolute path of an item, downloading it from S3 if it doesn't exist locally.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Retrieves the absolute path of an item, downloading it from S3 if it doesn't exist locally.


===============================[ 9 ]===============================
def set_option(self, key, value):
        if key == "rtmpdump":
            key = "rtmp-rtmpdump"
        elif key == "rtmpdump-proxy":
            key = "rtmp-proxy"
        elif key == "errorlog":
            key = "subprocess-errorlog"
        elif key == "errorlog-path":
            key = "subprocess-errorlog-path"
        if key == "http-proxy":
            self.http.proxies["http"] = update_scheme("http://", value)
        elif key == "https-proxy":
            self.http.proxies["https"] = update_scheme("https://", value)
        elif key == "http-cookies":
            if isinstance(value, dict):
                self.http.cookies.update(value)
            else:
                self.http.parse_cookies(value)
        elif key == "http-headers":
            if isinstance(value, dict):
                self.http.headers.update(value)
            else:
                self.http.parse_headers(value)
        elif key == "http-query-params":
            if isinstance(value, dict):
                self.http.params.update(value)
            else:
                self.http.parse_query_params(value)
        elif key == "http-trust-env":
            self.http.trust_env = value
        elif key == "http-ssl-verify":
            self.http.verify = value
        elif key == "http-disable-dh":
            if value:
                requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += ':!DH'
                try:
                    requests.packages.urllib3.contrib.pyopenssl.DEFAULT_SSL_CIPHER_LIST =                        requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS.encode("ascii")
                except AttributeError:
                    pass
        elif key == "http-ssl-cert":
            self.http.cert = value
        elif key == "http-timeout":
            self.http.timeout = value
        else:
            self.options.set(key, value)
*** Original Comment ***: Configures various options for HTTP requests, including proxy settings, cookies, headers, query parameters, SSL verification, and timeout, with special handling for certain option keys.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'with special handling for certain option keys' as it is implied by the context of the code.
----------------Refine 1----------------
*** Refine comment ***: Configures various options for HTTP requests, including proxy settings, cookies, headers, query parameters, SSL verification, and timeout.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'various' as it is implied by the list of specific options.
*** Final Score ***: 4
*** Final Comment ***: Configures various options for HTTP requests, including proxy settings, cookies, headers, query parameters, SSL verification, and timeout.


===============================[ 10 ]===============================
def paid_invoices_by_date(request, form):
    products = form.cleaned_data["product"]
    categories = form.cleaned_data["category"]
    invoices = commerce.Invoice.objects.filter(
        (
            Q(lineitem__product__in=products) |
            Q(lineitem__product__category__in=categories)
        ),
        status=commerce.Invoice.STATUS_PAID,
    )
    payments = commerce.PaymentBase.objects.all()
    payments = payments.filter(
        invoice__in=invoices,
    )
    payments = payments.order_by("invoice")
    invoice_max_time = payments.values("invoice").annotate(
        max_time=Max("time")
    )
    zero_value_invoices = invoices.filter(value=0)
    times = itertools.chain(
        (line["max_time"] for line in invoice_max_time),
        (invoice.issue_time for invoice in zero_value_invoices),
    )
    by_date = collections.defaultdict(int)
    for time in times:
        date = datetime.datetime(
            year=time.year, month=time.month, day=time.day
        )
        by_date[date] += 1
    data = [(date_, count) for date_, count in sorted(by_date.items())]
    data = [(date_.strftime("%Y-%m-%d"), count) for date_, count in data]
    return ListReport(
        "Paid Invoices By Date",
        ["date", "count"],
        data,
    )
*** Original Comment ***: The function paid_invoices_by_date retrieves and aggregates paid invoices by date, considering specified products and categories, and returns a report.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by stating 'Aggregates paid invoices by date for given products and categories, returning a report.'
----------------Refine 1----------------
*** Refine comment ***: Aggregates paid invoices by date for given products and categories, returning a report.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'for given products and categories,' as the context implies filtering by parameters.
*** Final Score ***: 4
*** Final Comment ***: Aggregates paid invoices by date for given products and categories, returning a report.


===============================[ 11 ]===============================
def expand_and_standardize_dataset(response_index, response_header, data_set, col_vals, headers, standardizers, feats_to_ignore, columns_to_expand, outcome_trans_dict):
  modified_set = []
  for row_index, row in enumerate(data_set):
    new_row = []
    for col_index, val in enumerate(row):
      header = headers[col_index]
      if col_index == response_index:
        new_outcome = outcome_trans_dict[val]
        new_row.append(new_outcome)
      elif header in feats_to_ignore:
        pass
      elif header in columns_to_expand:
        for poss_val in col_vals[header]:
          if val == poss_val:
            new_cat_val = 1.0
          else:
            new_cat_val = -1.0
          new_row.append(new_cat_val)
      else:
        new_cont_val = float((val - standardizers[header]['mean']) / standardizers[header]['std_dev'])
        new_row.append(new_cont_val)
    modified_set.append(new_row)
  expanded_headers = []
  for header in headers:
    if header in feats_to_ignore:
      pass
    elif (header in columns_to_expand) and (header is not response_header):
      for poss_val in col_vals[header]:
        new_header = '{}_{}'.format(header,poss_val)
        expanded_headers.append(new_header)
    else:
      expanded_headers.append(header)
  return modified_set, expanded_headers
*** Original Comment ***: Expands categorical features into binary features, standardizes continuous features, and transforms the outcome variable based on a given dictionary, returning the modified dataset and updated headers.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed; the comment is already concise and to the point.
*** Final Score ***: 5
*** Final Comment ***: Expands categorical features into binary features, standardizes continuous features, and transforms the outcome variable based on a given dictionary, returning the modified dataset and updated headers.


===============================[ 12 ]===============================
def _get_convert_to_tensor_fn(identifier):
  if identifier is None:
    return None
  if isinstance(identifier, six.string_types):
    identifier = str(identifier)
    return _deserialize(identifier)
  if isinstance(identifier, dict):
    return _deserialize(identifier)
  if isinstance(identifier, property):
    identifier = identifier.fget
  if callable(identifier):
    return identifier
  raise ValueError('Could not interpret '
                   'convert-to-tensor function identifier:', identifier)
*** Original Comment ***: Determines the appropriate conversion function for a given identifier, which can be a string, dictionary, property, or callable.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'appropriate' as it is implied by the context.
----------------Refine 1----------------
*** Refine comment ***: Determines the conversion function for a given identifier, which can be a string, dictionary, property, or callable.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'for a given identifier,' as it is implied by the context.
*** Final Score ***: 4
*** Final Comment ***: Determines the conversion function for a given identifier, which can be a string, dictionary, property, or callable.


===============================[ 13 ]===============================
def plot_rb_data(xdata, ydatas, yavg, yerr, fit, survival_prob, ax=None,
                 show_plt=True):
    if not HAS_MATPLOTLIB:
        raise ImportError('The function plot_rb_data needs matplotlib. '
                          'Run "pip install matplotlib" before.')
    if ax is None:
        plt.figure()
        ax = plt.gca()
    for ydata in ydatas:
        ax.plot(xdata, ydata, color='gray', linestyle='none', marker='x')
    ax.errorbar(xdata, yavg, yerr=yerr, color='r', linestyle='--', linewidth=3)
    ax.plot(xdata, survival_prob(xdata, *fit), color='blue', linestyle='-', linewidth=2)
    ax.tick_params(labelsize=14)
    ax.set_xlabel('Clifford Length', fontsize=16)
    ax.set_ylabel('Z', fontsize=16)
    ax.grid(True)
    if show_plt:
        plt.show()
*** Original Comment ***: Plots randomized benchmarking data including individual sequences as gray x's, average sequence fidelity with error bars in red, and the fitted survival probability curve in blue.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing color descriptions as they are evident from the code, e.g., 'Plots randomized benchmarking data with individual sequences, average sequence fidelity with error bars, and the fitted survival probability curve.'
----------------Refine 1----------------
*** Refine comment ***: Plots randomized benchmarking data with individual sequences, average sequence fidelity with error bars, and the fitted survival probability curve.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'with individual sequences, average sequence fidelity with error bars, and' as these details are implied by the context of plotting data.
*** Final Score ***: 4
*** Final Comment ***: Plots randomized benchmarking data with individual sequences, average sequence fidelity with error bars, and the fitted survival probability curve.


===============================[ 14 ]===============================
def get_environ_vars(self):
        for key, val in os.environ.items():
            if _environ_prefix_re.search(key):
                yield (_environ_prefix_re.sub("", key).lower(), val)
*** Original Comment ***: Retrieve environment variables with a specific prefix and yield them as lowercase keys without the prefix.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'with a specific prefix' as 'Retrieve' implies specificity.
----------------Refine 1----------------
*** Refine comment ***: Retrieve and yield environment variables, transforming keys to lowercase and stripping the prefix.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Retrieve and yield environment variables, transforming keys to lowercase and stripping the prefix.


===============================[ 15 ]===============================
def create_cname_record(self, name, values, ttl=60, weight=None, region=None,
                           set_identifier=None):
        self._halt_if_already_deleted()
        values = locals()
        del values['self']
        return self._add_record(CNAMEResourceRecordSet, **values)
*** Original Comment ***: Creates a CNAME record with the specified parameters and adds it to the DNS record set.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Creates a CNAME record with the specified parameters and adds it to the DNS record set.


===============================[ 16 ]===============================
def _wakeup(self):
        log.info("send: WAKEUP")
        for i in xrange(3):
            self.port.write('\n')  
            ack = self.port.read(len(self.WAKE_ACK))  
            log_raw('read', ack)
            if ack == self.WAKE_ACK:
                return
        raise NoDeviceException('Can not access weather station')
*** Original Comment ***: Attempts to wake up the connected weather station by sending newline characters and checking for an acknowledgment. If the expected acknowledgment is not received after three attempts, a NoDeviceException is raised.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'by sending newline characters and' as it is implied by 'wake up' and 'checking for an acknowledgment' as it is implied by 'If the expected acknowledgment is not received'.
----------------Refine 1----------------
*** Refine comment ***: Attempts to wake up the connected weather station. If the expected acknowledgment is not received after three attempts, a NoDeviceException is raised.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'If the expected acknowledgment is not received after three attempts,' as this is implied by the code structure.
*** Final Score ***: 4
*** Final Comment ***: Attempts to wake up the connected weather station. If the expected acknowledgment is not received after three attempts, a NoDeviceException is raised.


===============================[ 17 ]===============================
def _srvc_load_several_items(self, iterable, *args, **kwargs):
        for input_tuple in iterable:
            msg = input_tuple[0]
            item = input_tuple[1]
            if len(input_tuple) > 2:
                args = input_tuple[2]
            if len(input_tuple) > 3:
                kwargs = input_tuple[3]
            if len(input_tuple) > 4:
                raise RuntimeError('You shall not pass!')
            self.load(msg, item, *args, **kwargs)
*** Original Comment ***: Processes a tuple containing a message and an item, and optional args and kwargs, to load them using the load method. Raises an error if the tuple contains more than four elements.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by stating 'Loads message-item pairs from tuples with optional arguments, raising an error for tuples exceeding four elements.'
----------------Refine 1----------------
*** Refine comment ***: Loads message-item pairs from tuples with optional arguments, raising an error for tuples exceeding four elements.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'with optional arguments,' as it is implied by the use of '*args, **kwargs' in the code.
*** Final Score ***: 4
*** Final Comment ***: Loads message-item pairs from tuples with optional arguments, raising an error for tuples exceeding four elements.


===============================[ 18 ]===============================
def estimate_beats(self):
        if self._audio_percussive is None:
            self._audio_harmonic, self._audio_percussive = self.compute_HPSS()
        tempo, frames = librosa.beat.beat_track(
            y=self._audio_percussive, sr=self.sr,
            hop_length=self.hop_length)
        times = librosa.frames_to_time(frames, sr=self.sr,
                                       hop_length=self.hop_length)
        if len(times) > 0 and times[0] == 0:
            times = times[1:]
            frames = frames[1:]
        return times, frames
*** Original Comment ***: Estimate the beats of the audio track by separating harmonic and percussive components and then tracking the beats.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'of the audio track' as it is implied by the context.
----------------Refine 1----------------
*** Refine comment ***: Estimate beats by separating harmonic and percussive components and then tracking the beats.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'by separating harmonic and percussive components and' as it details the method rather than the core responsibility.
*** Final Score ***: 4
*** Final Comment ***: Estimate beats by separating harmonic and percussive components and then tracking the beats.


===============================[ 19 ]===============================
def updateLogicalInterface(self, logicalInterfaceId, name, schemaId, description=None):
        req = ApiClient.oneLogicalInterfaceUrl % (self.host, "/draft", logicalInterfaceId)
        body = {"name" : name, "schemaId" : schemaId, "id" : logicalInterfaceId}
        if description:
            body["description"] = description
        resp = requests.put(req, auth=self.credentials, headers={"Content-Type":"application/json"},
                            data=json.dumps(body),  verify=self.verify)
        if resp.status_code == 200:
            self.logger.debug("Logical interface updated")
        else:
            raise ibmiotf.APIException(resp.status_code, "HTTP error updating logical interface", resp)
        return resp.json()
*** Original Comment ***: Updates a logical interface with the given parameters and handles the response.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'with the given parameters and' as it is implied by the context.
----------------Refine 1----------------
*** Refine comment ***: Updates a logical interface and handles the response.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'and handles the response' as it is implied by the action of updating.
*** Final Score ***: 4
*** Final Comment ***: Updates a logical interface and handles the response.


===============================[ 20 ]===============================
def add_path(path, config=None):
    log.debug('Add path %s' % path)    
    if not path:
        return []
    added = []
    parent = os.path.dirname(path)
    if (parent
        and os.path.exists(os.path.join(path, '__init__.py'))):
        added.extend(add_path(parent, config))
    elif not path in sys.path:
        log.debug("insert %s into sys.path", path)
        sys.path.insert(0, path)
        added.append(path)
    if config and config.srcDirs:
        for dirname in config.srcDirs:
            dirpath = os.path.join(path, dirname)
            if os.path.isdir(dirpath):
                sys.path.insert(0, dirpath)
                added.append(dirpath)
    return added
*** Original Comment ***: The function add_path adds the specified path to sys.path if it's not already present, and recursively adds the parent directory if it contains an __init__.py file. It also adds any specified source directories from the config to sys.path.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'specified' as it is implied and 'if it's not already present' as the code explains this condition. Simplify to 'The function add_path adds a path to sys.path, recursively includes the parent directory with __init__.py, and appends config source directories.'
----------------Refine 1----------------
*** Refine comment ***: The function add_path adds a path to sys.path, recursively includes the parent directory with __init__.py, and appends config source directories.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'recursively includes the parent directory with __init__.py, and' as it is implied by 'adds a path'.
*** Final Score ***: 4
*** Final Comment ***: The function add_path adds a path to sys.path, recursively includes the parent directory with __init__.py, and appends config source directories.


===============================[ 21 ]===============================
def _convert_endpoint(endpoint):
    pb_endpoint = zipkin_pb2.Endpoint()
    if endpoint.service_name:
        pb_endpoint.service_name = endpoint.service_name
    if endpoint.port and endpoint.port != 0:
        pb_endpoint.port = endpoint.port
    if endpoint.ipv4:
        pb_endpoint.ipv4 = socket.inet_pton(socket.AF_INET, endpoint.ipv4)
    if endpoint.ipv6:
        pb_endpoint.ipv6 = socket.inet_pton(socket.AF_INET6, endpoint.ipv6)
    return pb_endpoint
*** Original Comment ***: Converts an endpoint object to a protobuf Endpoint message.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Converts an endpoint object to a protobuf Endpoint message.


===============================[ 22 ]===============================
def register_blueprints(app):
    app.register_blueprint(public.public_bp)
    app.register_blueprint(genes.genes_bp)
    app.register_blueprint(cases.cases_bp)
    app.register_blueprint(login.login_bp)
    app.register_blueprint(variants.variants_bp)
    app.register_blueprint(panels.panels_bp)
    app.register_blueprint(dashboard.dashboard_bp)
    app.register_blueprint(api.api_bp)
    app.register_blueprint(alignviewers.alignviewers_bp)
    app.register_blueprint(phenotypes.hpo_bp)
    app.register_blueprint(institutes.overview)
*** Original Comment ***: Registers multiple blueprints to the Flask application for different components like public, genes, cases, login, etc.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'for different components like' and simply stating 'Registers blueprints for public, genes, cases, login, etc., to the Flask application.'
----------------Refine 1----------------
*** Refine comment ***: Registers blueprints for public, genes, cases, login, etc., to the Flask application.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'etc.,' as the code snippet already implies multiple blueprints are being registered.
*** Final Score ***: 4
*** Final Comment ***: Registers blueprints for public, genes, cases, login, etc., to the Flask application.


===============================[ 23 ]===============================
def peek(self, n):
        if self._len < n:
            raise ValueError("Not enough bytes in the queue")
        def part_generator(n):
            remaining = n
            for part in self._parts:
                if len(part) <= remaining:
                    yield part
                    remaining -= len(part)
                else:
                    yield part[:remaining]
                    remaining = 0 
                if remaining == 0:
                    break
        return "".join(part_generator(n))
*** Original Comment ***: Peeks at the first n bytes in the queue without removing them.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Peeks at the first n bytes in the queue without removing them.


===============================[ 24 ]===============================
def get_memory_info(self):
        rss, vms = _psutil_bsd.get_process_memory_info(self.pid)[:2]
        return nt_meminfo(rss, vms)
*** Original Comment ***: Retrieves memory information for a process, returning resident set size and virtual memory size.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Retrieves memory information for a process, returning resident set size and virtual memory size.


===============================[ 25 ]===============================
def dist_in_usersite(dist):
    norm_path = normalize_path(dist_location(dist))
    return norm_path.startswith(normalize_path(user_site))
*** Original Comment ***: Determine if the distribution is installed in the user site directory
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Determine if the distribution is installed in the user site directory


===============================[ 26 ]===============================
def quadrature_scheme_softmaxnormal_quantiles(
    normal_loc, normal_scale, quadrature_size,
    validate_args=False, name=None):
  with tf.name_scope(name or "softmax_normal_grid_and_probs"):
    normal_loc = tf.convert_to_tensor(value=normal_loc, name="normal_loc")
    dt = dtype_util.base_dtype(normal_loc.dtype)
    normal_scale = tf.convert_to_tensor(
        value=normal_scale, dtype=dt, name="normal_scale")
    normal_scale = maybe_check_quadrature_param(
        normal_scale, "normal_scale", validate_args)
    dist = normal.Normal(loc=normal_loc, scale=normal_scale)
    def _get_batch_ndims():
      ndims = tensorshape_util.rank(dist.batch_shape)
      if ndims is None:
        ndims = tf.shape(input=dist.batch_shape_tensor())[0]
      return ndims
    batch_ndims = _get_batch_ndims()
    def _get_final_shape(qs):
      bs = tensorshape_util.with_rank_at_least(dist.batch_shape, 1)
      num_components = tf.compat.dimension_value(bs[-1])
      if num_components is not None:
        num_components += 1
      tail = tf.TensorShape([num_components, qs])
      return bs[:-1].concatenate(tail)
    def _compute_quantiles():
      zero = tf.zeros([], dtype=dist.dtype)
      edges = tf.linspace(zero, 1., quadrature_size + 3)[1:-1]
      edges = tf.reshape(
          edges,
          shape=tf.concat(
              [[-1], tf.ones([batch_ndims], dtype=tf.int32)], axis=0))
      quantiles = dist.quantile(edges)
      quantiles = softmax_centered_bijector.SoftmaxCentered().forward(quantiles)
      perm = tf.concat([tf.range(1, 1 + batch_ndims), [0]], axis=0)
      quantiles = tf.transpose(a=quantiles, perm=perm)
      tensorshape_util.set_shape(
          quantiles, _get_final_shape(quadrature_size + 1))
      return quantiles
    quantiles = _compute_quantiles()
    grid = (quantiles[..., :-1] + quantiles[..., 1:]) / 2.
    tensorshape_util.set_shape(grid, _get_final_shape(quadrature_size))
    probs = tf.fill(
        dims=[quadrature_size], value=1. / tf.cast(quadrature_size, dist.dtype))
    return grid, probs
*** Original Comment ***: Creates a quadrature scheme based on quantiles of a softmax-normal distribution for numerical integration.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'based on' as it's implied by context: 'Creates a quadrature scheme using quantiles of a softmax-normal distribution for numerical integration.'
----------------Refine 1----------------
*** Refine comment ***: Creates a quadrature scheme using quantiles of a softmax-normal distribution for numerical integration.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'for numerical integration' as it is implied by 'quadrature scheme'.
*** Final Score ***: 4
*** Final Comment ***: Creates a quadrature scheme using quantiles of a softmax-normal distribution for numerical integration.


===============================[ 27 ]===============================
def _run_sql(self, sql, params, raw=True, output=False):
        toget = 'source_raw' if raw else 'source'
        sqlfrom = "history"
        if output:
            sqlfrom = "history LEFT JOIN output_history USING (session, line)"
            toget = "history.%s, output_history.output" % toget
        cur = self.db.execute("SELECT session, line, %s FROM %s " %                                (toget, sqlfrom) + sql, params)
        if output:    
            return ((ses, lin, (inp, out)) for ses, lin, inp, out in cur)
        return cur
*** Original Comment ***: Execute a SQL query with specified parameters and return the results.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Execute a SQL query with specified parameters and return the results.


===============================[ 28 ]===============================
def read_image(filepath):
  im_bytes = tf.io.read_file(filepath)
  im = tf.image.decode_image(im_bytes, channels=CHANNELS)
  im = tf.image.convert_image_dtype(im, tf.float32)
  return im
*** Original Comment ***: Reads an image from a file, decodes it, converts its data type to float32, and returns the image tensor.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'from a file' as 'read_image' implies reading from a file, and 'converts its data type' can be shortened to 'converts to'. Suggested comment: 'Reads, decodes, and converts an image to float32, then returns the tensor.'
----------------Refine 1----------------
*** Refine comment ***: Reads, decodes, and converts an image to float32, then returns the tensor.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'then returns the tensor' as 'returns' is implied by the function definition.
*** Final Score ***: 4
*** Final Comment ***: Reads, decodes, and converts an image to float32, then returns the tensor.


===============================[ 29 ]===============================
def highlight(string, keywords, cls_name='highlighted'):
    if not keywords:
        return string
    if not string:
        return ''
    include, exclude = get_text_tokenizer(keywords)
    highlighted = highlight_text(include, string, cls_name)
    return highlighted
===============================[ 29 ]===============================
def highlight(string, keywords, cls_name='highlighted'):
    if not keywords:
        return string
    if not string:
        return ''
    include, exclude = get_text_tokenizer(keywords)
    highlighted = highlight_text(include, string, cls_name)
    return highlighted
*** Original Comment ***: Apply syntax highlighting to the provided string using the specified keywords and CSS class name.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'the provided' and 'using the specified' as they are implied by the context. Suggested comment: 'Apply syntax highlighting to string with keywords and CSS class name.'
----------------Refine 1----------------
*** Refine comment ***: Apply syntax highlighting to string with keywords and CSS class name.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'with keywords and CSS class name' as it's implied by 'syntax highlighting' and 'string'.
*** Final Score ***: 4
*** Final Comment ***: Apply syntax highlighting to string with keywords and CSS class name.


===============================[ 30 ]===============================
def expand_to_vector(x, tensor_name=None, op_name=None, validate_args=False):
  with tf.name_scope(op_name or "expand_to_vector"):
    x = tf.convert_to_tensor(value=x, name="x")
    ndims = tensorshape_util.rank(x.shape)
    if ndims is None:
      if validate_args:
        x = with_dependencies([
            assert_util.assert_rank_at_most(
                x, 1, message="Input is neither scalar nor vector.")
        ], x)
      ndims = tf.rank(x)
      expanded_shape = pick_vector(
          tf.equal(ndims, 0), np.array([1], dtype=np.int32), tf.shape(input=x))
      return tf.reshape(x, expanded_shape)
    elif ndims == 0:
      x_const = tf.get_static_value(x)
      if x_const is not None:
        return tf.convert_to_tensor(
            value=dtype_util.as_numpy_dtype(x.dtype)([x_const]),
            name=tensor_name)
      else:
        return tf.reshape(x, [1])
    elif ndims != 1:
      raise ValueError("Input is neither scalar nor vector.")
    return x
*** Original Comment ***: Expands a scalar to a vector or ensures the input is a vector, with optional validation.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'or ensures the input is a vector,' as it is implied by 'expands a scalar to a vector.'
----------------Refine 1----------------
*** Refine comment ***: Expands a scalar to a vector, with optional validation.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'with optional validation' as it is implied by the parameter list.
*** Final Score ***: 4
*** Final Comment ***: Expands a scalar to a vector, with optional validation.


===============================[ 31 ]===============================
def godot_options(self, info):
        if info.initialized:
            self.edit_traits( parent = info.ui.control,
                              kind   = "livemodal",
                              view   = "options_view" )
*** Original Comment ***: Invoke the options view for an already initialized object within a modal dialog.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'for an already initialized object' as 'initialized' is implied by the context.
----------------Refine 1----------------
*** Refine comment ***: Invoke the options view within a modal dialog.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Invoke the options view within a modal dialog.


===============================[ 32 ]===============================
def _get_rule_transform(self, rule):
    rd = self._find_directive(lambda d: d.name == "rule" and d.args.get("name") == rule.name)
    if rd:
      args = rd.args
    else:
      args = {}
    transform = args.get("transform", "retype")
    if transform == "retype":
      new_name = args.get("to_type", "TokenType.{0}".format(rule.name))
      return ".retyped({0})".format(new_name)
    elif transform == "compress":
      new_name = args.get("to_type", "TokenType.{0}".format(rule.name))
      if new_name == "identity":
        return ".compressed()"
      else:
        return ".compressed({0})".format(new_name)
    elif transform == "identity":
      return ""
*** Original Comment ***: Retrieves the transformation for a rule based on directives, supporting retype, compress, and identity transformations.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'based on directives, supporting' as it is implied by the context.
----------------Refine 1----------------
*** Refine comment ***: Retrieves the transformation for a rule, supporting retype, compress, and identity transformations.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'supporting retype, compress, and identity transformations' as these details are implied by the context.
*** Final Score ***: 4
*** Final Comment ***: Retrieves the transformation for a rule, supporting retype, compress, and identity transformations.


===============================[ 33 ]===============================
def picard_mark_duplicates(job, bam, bai, validation_stringency='LENIENT'):
    work_dir = job.fileStore.getLocalTempDir()
    job.fileStore.readGlobalFile(bam, os.path.join(work_dir, 'sorted.bam'))
    job.fileStore.readGlobalFile(bai, os.path.join(work_dir, 'sorted.bai'))
    command = ['MarkDuplicates',
               'INPUT=sorted.bam',
               'OUTPUT=mkdups.bam',
               'METRICS_FILE=metrics.txt',
               'ASSUME_SORTED=true',
               'CREATE_INDEX=true',
               'VALIDATION_STRINGENCY=%s' % validation_stringency.upper()]
    docker_parameters = ['--rm',
                         '--log-driver', 'none',
                         '-e', 'JAVA_OPTIONS=-Djava.io.tmpdir=/data/ -Xmx{}'.format(job.memory),
                         '-v', '{}:/data'.format(work_dir)]
    start_time = time.time()
    dockerCall(job=job, workDir=work_dir,
               parameters=command,
               tool='quay.io/ucsc_cgl/picardtools:1.95--dd5ac549b95eb3e5d166a5e310417ef13651994e',
               dockerParameters=docker_parameters)
    end_time = time.time()
    _log_runtime(job, start_time, end_time, "Picard MarkDuplicates")
    bam = job.fileStore.writeGlobalFile(os.path.join(work_dir, 'mkdups.bam'))
    bai = job.fileStore.writeGlobalFile(os.path.join(work_dir, 'mkdups.bai'))
    return bam, bai
*** Original Comment ***: Use Picard's MarkDuplicates to mark duplicates in a BAM file within a Toil job environment.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'within a Toil job environment' as it is implied by the context of the function.
----------------Refine 1----------------
*** Refine comment ***: Use Picard's MarkDuplicates to mark duplicates in a BAM file.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Use Picard's MarkDuplicates to mark duplicates in a BAM file.


===============================[ 34 ]===============================
def patch_transport_fake_push_producer(transport):
    patch_if_missing(transport, 'pauseProducing', lambda: None)
    patch_if_missing(transport, 'resumeProducing', lambda: None)
    patch_if_missing(transport, 'stopProducing', transport.loseConnection)
*** Original Comment ***: Patches the given transport object to ensure it has pauseProducing, resumeProducing, and stopProducing methods.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Patches the given transport object to ensure it has pauseProducing, resumeProducing, and stopProducing methods.


===============================[ 35 ]===============================
def format_function(
    func_body,
    func_type=None,
    indent=2,
    format_locals=True,
):
    if func_type is None:
        yield 'func'
    else:
        param_section = ' (param {})'.format(' '.join(
            map(format_lang_type, func_type.param_types)
        )) if func_type.param_types else ''
        result_section = ' (result {})'.format(
            format_lang_type(func_type.return_type)
        ) if func_type.return_type else ''
        yield 'func' + param_section + result_section
    if format_locals and func_body.locals:
        yield '(locals {})'.format(' '.join(itertools.chain.from_iterable(
            itertools.repeat(format_lang_type(x.type), x.count)
            for x in func_body.locals
        )))
    level = 1
    for cur_insn in decode_bytecode(func_body.code):
        if cur_insn.op.flags & INSN_LEAVE_BLOCK:
            level -= 1
        yield ' ' * (level * indent) + format_instruction(cur_insn)
        if cur_insn.op.flags & INSN_ENTER_BLOCK:
            level += 1
*** Original Comment ***: Generates and yields formatted WebAssembly function definition lines.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Generates and yields formatted WebAssembly function definition lines.


===============================[ 36 ]===============================
def get_description(self):
        def split_header(s, get_header=True):
            s = s.lstrip().rstrip()
            parts = s.splitlines()
            if parts[0].startswith('#'):
                if get_header:
                    header = re.sub('#+\s*', '', parts.pop(0))
                    if not parts:
                        return header, ''
                else:
                    header = ''
                rest = '\n'.join(parts).lstrip().split('\n\n')
                desc = rest[0].replace('\n', ' ')
                return header, desc
            else:
                if get_header:
                    if parts[0].startswith(('=', '-')):
                        parts = parts[1:]
                    header = parts.pop(0)
                    if parts and parts[0].startswith(('=', '-')):
                        parts.pop(0)
                    if not parts:
                        return header, ''
                else:
                    header = ''
                rest = '\n'.join(parts).lstrip().split('\n\n')
                desc = rest[0].replace('\n', ' ')
                return header, desc
        first_cell = self.nb['cells'][0]
        if not first_cell['cell_type'] == 'markdown':
            return '', ''
        header, desc = split_header(first_cell['source'])
        if not desc and len(self.nb['cells']) > 1:
            second_cell = self.nb['cells'][1]
            if second_cell['cell_type'] == 'markdown':
                _, desc = split_header(second_cell['source'], False)
        return header, desc
*** Original Comment ***: Extracts the header and description from the first markdown cell of a Jupyter notebook
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed; the comment is already concise and to the point.
*** Final Score ***: 5
*** Final Comment ***: Extracts the header and description from the first markdown cell of a Jupyter notebook


===============================[ 37 ]===============================
def has_comment(src):
    readline = StringIO(src).readline
    toktypes = set()
    try:
        for t in tokenize.generate_tokens(readline):
            toktypes.add(t[0])
    except tokenize.TokenError:
        pass
    return(tokenize.COMMENT in toktypes)
*** Original Comment ***: Check if the given source code contains any comments.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Check if the given source code contains any comments.


===============================[ 38 ]===============================
def _topic(self, topic):
        if self.int_id >= 0:
            base = "engine.%i" % self.int_id
        else:
            base = "kernel.%s" % self.ident
        return py3compat.cast_bytes("%s.%s" % (base, topic))
*** Original Comment ***: Generate a topic string based on the internal ID or kernel identity.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'based on the internal ID or kernel identity' as it is implied by the context.
----------------Refine 1----------------
*** Refine comment ***: Generate a topic string.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Generate a topic string.


===============================[ 39 ]===============================
def rdiscover_modules(directory):
    found = list()
    if os.path.isdir(directory):
        for entry in os.listdir(directory):
            next_dir = os.path.join(directory, entry)
            if os.path.isfile(os.path.join(next_dir, MODULE_INIT_FILE)):
                modules = _search_for_modules(next_dir, True, entry)
                found.extend(modules)
    return found
*** Original Comment ***: Recursively discover module directories containing an __init__.py file
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Recursively discover module directories containing an __init__.py file


===============================[ 40 ]===============================
def _event_filter_console_keypress(self, event):
        intercepted = False
        cursor = self._control.textCursor()
        position = cursor.position()
        key = event.key()
        ctrl_down = self._control_key_down(event.modifiers())
        alt_down = event.modifiers() & QtCore.Qt.AltModifier
        shift_down = event.modifiers() & QtCore.Qt.ShiftModifier
        if event.matches(QtGui.QKeySequence.Copy):
            self.copy()
            intercepted = True
        elif event.matches(QtGui.QKeySequence.Cut):
            self.cut()
            intercepted = True
        elif event.matches(QtGui.QKeySequence.Paste):
            self.paste()
            intercepted = True
        elif key in (QtCore.Qt.Key_Return, QtCore.Qt.Key_Enter):
            intercepted = True
            self._cancel_completion()
            if self._in_buffer(position):
                if self._reading:
                    self._append_plain_text('\n')
                    self._reading = False
                    if self._reading_callback:
                        self._reading_callback()
                elif not self._executing:
                    cursor.movePosition(QtGui.QTextCursor.End,
                                        QtGui.QTextCursor.KeepAnchor)
                    at_end = len(cursor.selectedText().strip()) == 0
                    single_line = (self._get_end_cursor().blockNumber() ==
                                   self._get_prompt_cursor().blockNumber())
                    if (at_end or shift_down or single_line) and not ctrl_down:
                        self.execute(interactive = not shift_down)
                    else:
                        cursor.beginEditBlock()
                        cursor.setPosition(position)
                        cursor.insertText('\n')
                        self._insert_continuation_prompt(cursor)
                        cursor.endEditBlock()
                        self._control.moveCursor(QtGui.QTextCursor.End)
                        self._control.setTextCursor(cursor)
        elif ctrl_down:
            if key == QtCore.Qt.Key_G:
                self._keyboard_quit()
                intercepted = True
            elif key == QtCore.Qt.Key_K:
                if self._in_buffer(position):
                    cursor.clearSelection()
                    cursor.movePosition(QtGui.QTextCursor.EndOfLine,
                                        QtGui.QTextCursor.KeepAnchor)
                    if not cursor.hasSelection():
                        cursor.movePosition(QtGui.QTextCursor.NextBlock,
                                            QtGui.QTextCursor.KeepAnchor)
                        cursor.movePosition(QtGui.QTextCursor.Right,
                                            QtGui.QTextCursor.KeepAnchor,
                                            len(self._continuation_prompt))
                    self._kill_ring.kill_cursor(cursor)
                    self._set_cursor(cursor)
                intercepted = True
            elif key == QtCore.Qt.Key_L:
                self.prompt_to_top()
                intercepted = True
            elif key == QtCore.Qt.Key_O:
                if self._page_control and self._page_control.isVisible():
                    self._page_control.setFocus()
                intercepted = True
            elif key == QtCore.Qt.Key_U:
                if self._in_buffer(position):
                    cursor.clearSelection()
                    start_line = cursor.blockNumber()
                    if start_line == self._get_prompt_cursor().blockNumber():
                        offset = len(self._prompt)
                    else:
                        offset = len(self._continuation_prompt)
                    cursor.movePosition(QtGui.QTextCursor.StartOfBlock,
                                        QtGui.QTextCursor.KeepAnchor)
                    cursor.movePosition(QtGui.QTextCursor.Right,
                                        QtGui.QTextCursor.KeepAnchor, offset)
                    self._kill_ring.kill_cursor(cursor)
                    self._set_cursor(cursor)
                intercepted = True
            elif key == QtCore.Qt.Key_Y:
                self._keep_cursor_in_buffer()
                self._kill_ring.yank()
                intercepted = True
            elif key in (QtCore.Qt.Key_Backspace, QtCore.Qt.Key_Delete):
                if key == QtCore.Qt.Key_Backspace:
                    cursor = self._get_word_start_cursor(position)
                else: 
                    cursor = self._get_word_end_cursor(position)
                cursor.setPosition(position, QtGui.QTextCursor.KeepAnchor)
                self._kill_ring.kill_cursor(cursor)
                intercepted = True
            elif key == QtCore.Qt.Key_D:
                if len(self.input_buffer) == 0:
                    self.exit_requested.emit(self)
                else:
                    new_event = QtGui.QKeyEvent(QtCore.QEvent.KeyPress,
                                                QtCore.Qt.Key_Delete,
                                                QtCore.Qt.NoModifier)
                    QtGui.qApp.sendEvent(self._control, new_event)
                    intercepted = True
        elif alt_down:
            if key == QtCore.Qt.Key_B:
                self._set_cursor(self._get_word_start_cursor(position))
                intercepted = True
            elif key == QtCore.Qt.Key_F:
                self._set_cursor(self._get_word_end_cursor(position))
                intercepted = True
            elif key == QtCore.Qt.Key_Y:
                self._kill_ring.rotate()
                intercepted = True
            elif key == QtCore.Qt.Key_Backspace:
                cursor = self._get_word_start_cursor(position)
                cursor.setPosition(position, QtGui.QTextCursor.KeepAnchor)
                self._kill_ring.kill_cursor(cursor)
                intercepted = True
            elif key == QtCore.Qt.Key_D:
                cursor = self._get_word_end_cursor(position)
                cursor.setPosition(position, QtGui.QTextCursor.KeepAnchor)
                self._kill_ring.kill_cursor(cursor)
                intercepted = True
            elif key == QtCore.Qt.Key_Delete:
                intercepted = True
            elif key == QtCore.Qt.Key_Greater:
                self._control.moveCursor(QtGui.QTextCursor.End)
                intercepted = True
            elif key == QtCore.Qt.Key_Less:
                self._control.setTextCursor(self._get_prompt_cursor())
                intercepted = True
        else:
            if shift_down:
                anchormode = QtGui.QTextCursor.KeepAnchor
            else:
                anchormode = QtGui.QTextCursor.MoveAnchor
            if key == QtCore.Qt.Key_Escape:
                self._keyboard_quit()
                intercepted = True
            elif key == QtCore.Qt.Key_Up:
                if self._reading or not self._up_pressed(shift_down):
                    intercepted = True
                else:
                    prompt_line = self._get_prompt_cursor().blockNumber()
                    intercepted = cursor.blockNumber() <= prompt_line
            elif key == QtCore.Qt.Key_Down:
                if self._reading or not self._down_pressed(shift_down):
                    intercepted = True
                else:
                    end_line = self._get_end_cursor().blockNumber()
                    intercepted = cursor.blockNumber() == end_line
            elif key == QtCore.Qt.Key_Tab:
                if not self._reading:
                    if self._tab_pressed():
                        cursor.insertText(' '*4)
                    intercepted = True
            elif key == QtCore.Qt.Key_Left:
                line, col = cursor.blockNumber(), cursor.columnNumber()
                if line > self._get_prompt_cursor().blockNumber() and                        col == len(self._continuation_prompt):
                    self._control.moveCursor(QtGui.QTextCursor.PreviousBlock,
                                             mode=anchormode)
                    self._control.moveCursor(QtGui.QTextCursor.EndOfBlock,
                                             mode=anchormode)
                    intercepted = True
                else:
                    intercepted = not self._in_buffer(position - 1)
            elif key == QtCore.Qt.Key_Right:
                original_block_number = cursor.blockNumber()
                cursor.movePosition(QtGui.QTextCursor.Right,
                                mode=anchormode)
                if cursor.blockNumber() != original_block_number:
                    cursor.movePosition(QtGui.QTextCursor.Right,
                                        n=len(self._continuation_prompt),
                                        mode=anchormode)
                self._set_cursor(cursor)
                intercepted = True
            elif key == QtCore.Qt.Key_Home:
                start_line = cursor.blockNumber()
                if start_line == self._get_prompt_cursor().blockNumber():
                    start_pos = self._prompt_pos
                else:
                    cursor.movePosition(QtGui.QTextCursor.StartOfBlock,
                                        QtGui.QTextCursor.KeepAnchor)
                    start_pos = cursor.position()
                    start_pos += len(self._continuation_prompt)
                    cursor.setPosition(position)
                if shift_down and self._in_buffer(position):
                    cursor.setPosition(start_pos, QtGui.QTextCursor.KeepAnchor)
                else:
                    cursor.setPosition(start_pos)
                self._set_cursor(cursor)
                intercepted = True
            elif key == QtCore.Qt.Key_Backspace:
                line, col = cursor.blockNumber(), cursor.columnNumber()
                if not self._reading and                        col == len(self._continuation_prompt) and                        line > self._get_prompt_cursor().blockNumber():
                    cursor.beginEditBlock()
                    cursor.movePosition(QtGui.QTextCursor.StartOfBlock,
                                        QtGui.QTextCursor.KeepAnchor)
                    cursor.removeSelectedText()
                    cursor.deletePreviousChar()
                    cursor.endEditBlock()
                    intercepted = True
                else:
                    anchor = cursor.anchor()
                    if anchor == position:
                        intercepted = not self._in_buffer(position - 1)
                    else:
                        intercepted = not self._in_buffer(min(anchor, position))
            elif key == QtCore.Qt.Key_Delete:
                if not self._reading and self._in_buffer(position) and                        cursor.atBlockEnd() and not cursor.hasSelection():
                    cursor.movePosition(QtGui.QTextCursor.NextBlock,
                                        QtGui.QTextCursor.KeepAnchor)
                    cursor.movePosition(QtGui.QTextCursor.Right,
                                        QtGui.QTextCursor.KeepAnchor,
                                        len(self._continuation_prompt))
                    cursor.removeSelectedText()
                    intercepted = True
                else:
                    anchor = cursor.anchor()
                    intercepted = (not self._in_buffer(anchor) or
                                   not self._in_buffer(position))
        if not (self._control_key_down(event.modifiers(), include_command=True)
                or key in (QtCore.Qt.Key_PageUp, QtCore.Qt.Key_PageDown)
                or (self._executing and not self._reading)):
            self._keep_cursor_in_buffer()
        return intercepted
*** Original Comment ***: Implements a custom event filter for a widget, handling key press events for navigation and editing, and other events like mouse presses and releases.
*** Validation Score ***: 2
*** Validation Suggestion ***: Be more concise by stating 'Filters key press events for widget navigation and editing.'
----------------Refine 1----------------
*** Refine comment ***: Filters key press events for widget navigation and editing.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'for widget navigation and editing' as it is implied by 'Filters key press events'.
*** Final Score ***: 4
*** Final Comment ***: Filters key press events for widget navigation and editing.


===============================[ 41 ]===============================
def new_code_cell(code=None, prompt_number=None):
    cell = NotebookNode()
    cell.cell_type = u'code'
    if code is not None:
        cell.code = unicode(code)
    if prompt_number is not None:
        cell.prompt_number = int(prompt_number)
    return cell
*** Original Comment ***: Create a new code cell for a Jupyter notebook with optional code and prompt number
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'for a Jupyter notebook' as the context is implied by the function's purpose.
----------------Refine 1----------------
*** Refine comment ***: Create a new code cell with optional code and prompt number
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'new' as 'create' implies newness, and 'with optional code and prompt number' as 'optional' is implied by the context. Suggested comment: 'Create code cell with code and prompt number.'
*** Final Score ***: 4
*** Final Comment ***: Create a new code cell with optional code and prompt number


===============================[ 42 ]===============================
def roc_auc_score(y_true: Union[List[List[float]], List[List[int]], np.ndarray],
                  y_pred: Union[List[List[float]], List[List[int]], np.ndarray]) -> float:
    try:
        return sklearn.metrics.roc_auc_score(np.squeeze(np.array(y_true)),
                                             np.squeeze(np.array(y_pred)), average="macro")
    except ValueError:
        return 0.
*** Original Comment ***: Calculates the ROC AUC score using sklearn's roc_auc_score function, handling exceptions by returning 0.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'using sklearn's roc_auc_score function,' as the function name is self-explanatory.
----------------Refine 1----------------
*** Refine comment ***: Calculates the ROC AUC score, handling exceptions by returning 0.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Calculates the ROC AUC score, handling exceptions by returning 0.


===============================[ 43 ]===============================
def crop_image(img, start_y, start_x, h, w):
    return img[start_y:start_y + h, start_x:start_x + w, :].copy()
*** Original Comment ***: Crops the image starting from (start_x, start_y) to a window of size (h, w) and returns a copy of the cropped area.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'starting from (start_x, start_y) to a' as it is implied by 'crop'.
----------------Refine 1----------------
*** Refine comment ***: Crops the image to a window of size (h, w) and returns a copy of the cropped area.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Crops the image to a window of size (h, w) and returns a copy of the cropped area.


===============================[ 44 ]===============================
def advance_robots(self):
        self = lens.robots.Each().call_step_towards(self.player)(self)
        self = lens.crashes.call_union(duplicates(self.robots))(self)
        self = lens.robots.modify(lambda r: list(set(r) - self.crashes))(self)
        return self
*** Original Comment ***: The advance_robots method updates the state of robots by moving them towards the player, handling crashes, and removing crashed robots.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'by moving them towards the player, handling crashes, and removing crashed robots' as it details the method's internal actions rather than its core responsibility.
----------------Refine 1----------------
*** Refine comment ***: The advance_robots method updates the state of robots in the game.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'in the game' as it is implied by context.
*** Final Score ***: 4
*** Final Comment ***: The advance_robots method updates the state of robots in the game.


===============================[ 45 ]===============================
def check_standard_dir(module_path):
    if 'site-packages' in module_path:
        return True
    for stdlib_path in _STDLIB_PATHS:
        if fnmatch.fnmatchcase(module_path, stdlib_path + '*'):
            return True
    return False
*** Original Comment ***: Determine if the given module path is within the standard library or site-packages directory.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Determine if the given module path is within the standard library or site-packages directory.


===============================[ 46 ]===============================
def table_exists(self, table_name, db='default'):
        try:
            self.get_table(table_name, db)
            return True
        except Exception:
            return False
*** Original Comment ***: Determines if a specified table exists within a given database by attempting to retrieve it and returning a boolean result based on the success of the operation
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'by attempting to retrieve it and' as it is implied by the function's action.
----------------Refine 1----------------
*** Refine comment ***: Determines if a specified table exists within a given database, returning a boolean result based on the success of the operation
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'within a given database, returning a boolean result based on the success of the operation' as it is implied by the function's purpose and return type.
*** Final Score ***: 4
*** Final Comment ***: Determines if a specified table exists within a given database, returning a boolean result based on the success of the operation


===============================[ 47 ]===============================
def delete_vacation(self, index, vacation):
        body = {"selection": {
                    "selectionType": "thermostats",
                    "selectionMatch": self.thermostats[index]['identifier']},
                "functions": [{"type": "deleteVacation", "params": {
                    "name": vacation
                }}]}
        log_msg_action = "delete a vacation"
        return self.make_request(body, log_msg_action)
*** Original Comment ***: Deletes a vacation from the thermostat with the given index and vacation name
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'with the given index and vacation name' as it is implied by the context of the function parameters.
----------------Refine 1----------------
*** Refine comment ***: Deletes a vacation from the thermostat
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Deletes a vacation from the thermostat


===============================[ 48 ]===============================
def get_transfers(self, start=0, stop=None, inclusion_states=False):
        return extended.GetTransfersCommand(self.adapter)(
            seed=self.seed,
            start=start,
            stop=stop,
            inclusionStates=inclusion_states,
        )
*** Original Comment ***: The get_transfers method retrieves transfers for a given seed, with optional parameters for start, stop, and inclusion of states.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'for a given seed, with optional parameters for' as it is implied by the method signature.
----------------Refine 1----------------
*** Refine comment ***: Retrieves transfers, allowing specification of start, stop, and inclusion of states.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Retrieves transfers, allowing specification of start, stop, and inclusion of states.


===============================[ 49 ]===============================
def write(self, output_buffer, kmip_version=enums.KMIPVersion.KMIP_1_0):
        local_buffer = utils.BytearrayStream()
        if self._operations:
            for operation in self._operations:
                operation.write(local_buffer, kmip_version=kmip_version)
        if self._object_types:
            for object_type in self._object_types:
                object_type.write(local_buffer, kmip_version=kmip_version)
        if self._vendor_identification:
            self._vendor_identification.write(
                local_buffer,
                kmip_version=kmip_version
            )
        if self._server_information:
            self._server_information.write(
                local_buffer,
                kmip_version=kmip_version
            )
        if self._application_namespaces:
            for application_namespace in self._application_namespaces:
                application_namespace.write(
                    local_buffer,
                    kmip_version=kmip_version
                )
        if kmip_version >= enums.KMIPVersion.KMIP_1_1:
            if self._extension_information:
                for extension_information in self._extension_information:
                    extension_information.write(
                        local_buffer,
                        kmip_version=kmip_version
                    )
        if kmip_version >= enums.KMIPVersion.KMIP_1_2:
            if self._attestation_types:
                for attestation_type in self._attestation_types:
                    attestation_type.write(
                        local_buffer,
                        kmip_version=kmip_version
                    )
        if kmip_version >= enums.KMIPVersion.KMIP_1_3:
            if self._rng_parameters:
                for rng_parameters in self._rng_parameters:
                    rng_parameters.write(
                        local_buffer,
                        kmip_version=kmip_version
                    )
            if self._profile_information:
                for profile_information in self._profile_information:
                    profile_information.write(
                        local_buffer,
                        kmip_version=kmip_version
                    )
            if self._validation_information:
                for validation_information in self._validation_information:
                    validation_information.write(
                        local_buffer,
                        kmip_version=kmip_version
                    )
            if self._capability_information:
                for capability_information in self._capability_information:
                    capability_information.write(
                        local_buffer,
                        kmip_version=kmip_version
                    )
            if self._client_registration_methods:
                for client_reg_method in self._client_registration_methods:
                    client_reg_method.write(
                        local_buffer,
                        kmip_version=kmip_version
                    )
        if kmip_version >= enums.KMIPVersion.KMIP_2_0:
            if self._defaults_information:
                self._defaults_information.write(
                    local_buffer,
                    kmip_version=kmip_version
                )
            if self._storage_protection_masks:
                for storage_protection_mask in self._storage_protection_masks:
                    storage_protection_mask.write(
                        local_buffer,
                        kmip_version=kmip_version
                    )
        self.length = local_buffer.length()
        super(QueryResponsePayload, self).write(
            output_buffer,
            kmip_version=kmip_version
        )
        output_buffer.write(local_buffer.buffer)
*** Original Comment ***: Serialize the QueryResponsePayload object into bytes and write it to the output buffer, considering the KMIP version for conditional serialization of attributes.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by stating 'Serialize QueryResponsePayload to bytes with KMIP version-specific attributes and write to output buffer.'
----------------Refine 1----------------
*** Refine comment ***: Serialize QueryResponsePayload to bytes with KMIP version-specific attributes and write to output buffer.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by stating 'Serialize QueryResponsePayload with version-specific attributes to output buffer.'
*** Final Score ***: 4
*** Final Comment ***: Serialize QueryResponsePayload to bytes with KMIP version-specific attributes and write to output buffer.


===============================[ 50 ]===============================
def insert_bytes(fobj, size, offset, BUFFER_SIZE=2**16):
    assert 0 < size
    assert 0 <= offset
    locked = False
    fobj.seek(0, 2)
    filesize = fobj.tell()
    movesize = filesize - offset
    fobj.write(b'\x00' * size)
    fobj.flush()
    try:
        try:
            import mmap
            file_map = mmap.mmap(fobj.fileno(), filesize + size)
            try:
                file_map.move(offset + size, offset, movesize)
            finally:
                file_map.close()
        except (ValueError, EnvironmentError, ImportError):
            locked = lock(fobj)
            fobj.truncate(filesize)
            fobj.seek(0, 2)
            padsize = size
            while padsize:
                addsize = min(BUFFER_SIZE, padsize)
                fobj.write(b"\x00" * addsize)
                padsize -= addsize
            fobj.seek(filesize, 0)
            while movesize:
                thismove = min(BUFFER_SIZE, movesize)
                fobj.seek(-thismove, 1)
                nextpos = fobj.tell()
                data = fobj.read(thismove)
                fobj.seek(-thismove + size, 1)
                fobj.write(data)
                fobj.seek(nextpos)
                movesize -= thismove
            fobj.flush()
    finally:
        if locked:
            unlock(fobj)
*** Original Comment ***: Inserts a specified number of bytes at a given offset in a file object, using mmap if available or manual byte shifting with a buffer.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by starting with an action verb and removing 'a specified number of' as it is implied by 'size'. Suggested comment: 'Inserts bytes at given offset in file object, utilizing mmap or manual byte shifting with buffer.'
----------------Refine 1----------------
*** Refine comment ***: Inserts bytes at given offset in file object, utilizing mmap or manual byte shifting with buffer.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by stating 'Inserts bytes at specified offset using mmap or fallback to byte shifting.'
*** Final Score ***: 4
*** Final Comment ***: Inserts bytes at given offset in file object, utilizing mmap or manual byte shifting with buffer.


===============================[ 51 ]===============================
def as_dagster_type(
    existing_type,
    name=None,
    description=None,
    input_schema=None,
    output_schema=None,
    serialization_strategy=None,
    storage_plugins=None,
):
    check.type_param(existing_type, 'existing_type')
    check.opt_str_param(name, 'name')
    check.opt_str_param(description, 'description')
    check.opt_inst_param(input_schema, 'input_schema', InputSchema)
    check.opt_inst_param(output_schema, 'output_schema', OutputSchema)
    check.opt_inst_param(serialization_strategy, 'serialization_strategy', SerializationStrategy)
    storage_plugins = check.opt_dict_param(storage_plugins, 'storage_plugins')
    if serialization_strategy is None:
        serialization_strategy = PickleSerializationStrategy()
    name = existing_type.__name__ if name is None else name
    return _decorate_as_dagster_type(
        existing_type,
        key=name,
        name=name,
        description=description,
        input_schema=input_schema,
        output_schema=output_schema,
        serialization_strategy=serialization_strategy,
        storage_plugins=storage_plugins,
    )
*** Original Comment ***: Create a Dagster type by decorating an existing Python type with additional properties like name, description, input/output schema, serialization strategy, and storage plugins.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'by decorating an existing Python type with' as it's implied by the function name and context.
----------------Refine 1----------------
*** Refine comment ***: Create a Dagster type with additional properties such as name, description, input/output schema, serialization strategy, and storage plugins.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by stating 'Creates a Dagster type with customizable properties.'
*** Final Score ***: 4
*** Final Comment ***: Create a Dagster type with additional properties such as name, description, input/output schema, serialization strategy, and storage plugins.


===============================[ 52 ]===============================
def url(self):
        if self._url is not None:
            url = self._url
        else:
            url = getattr(self.nb.metadata, 'url', None)
        if url is not None:
            return nbviewer_link(url)
*** Original Comment ***: Retrieves the URL from the notebook metadata or the instance variable, and returns a nbviewer formatted link.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'from the notebook metadata or the instance variable,' as it's implied by the code structure.
----------------Refine 1----------------
*** Refine comment ***: Retrieves the URL and returns a nbviewer formatted link.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'and returns' as 'Retrieves' implies the return action. Suggested comment: 'Retrieves the URL in nbviewer format.'
*** Final Score ***: 4
*** Final Comment ***: Retrieves the URL and returns a nbviewer formatted link.


===============================[ 53 ]===============================
def save_graph_only_from_checkpoint(input_checkpoint, output_file_path, output_node_names, as_text=False):
    check_input_checkpoint(input_checkpoint)
    output_node_names = output_node_names_string_as_list(output_node_names)
    with tf.Session() as sess:
        restore_from_checkpoint(sess, input_checkpoint)
        save_graph_only(sess, output_file_path, output_node_names, as_text=as_text)
*** Original Comment ***: Extract and save the graph from a TensorFlow checkpoint file to a specified output file path.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Extract and save the graph from a TensorFlow checkpoint file to a specified output file path.


===============================[ 54 ]===============================
def copy_notebook(self, notebook_id):
        last_mod, nb = self.get_notebook_object(notebook_id)
        name = nb.metadata.name + '-Copy'
        path, name = self.increment_filename(name)
        nb.metadata.name = name
        notebook_id = self.new_notebook_id(name)
        self.save_notebook_object(notebook_id, nb)
        return notebook_id
*** Original Comment ***: Creates a copy of the specified notebook and saves it with an incremented filename.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Creates a copy of the specified notebook and saves it with an incremented filename.


===============================[ 55 ]===============================
def debug_storage(storage, base_info=False, chars=True, runs=False):
    import codecs
    import locale
    import sys
    if six.PY2:
        stderr = codecs.getwriter(locale.getpreferredencoding())(sys.stderr)
    else:
        stderr = sys.stderr
    caller = inspect.stack()[1][3]
    stderr.write('in %s\n' % caller)
    if base_info:
        stderr.write(u'  base level  : %d\n' % storage['base_level'])
        stderr.write(u'  base dir    : %s\n' % storage['base_dir'])
    if runs:
        stderr.write(u'  runs        : %s\n' % list(storage['runs']))
    if chars:
        output = u'  Chars       : '
        for _ch in storage['chars']:
            if _ch != '\n':
                output += _ch['ch']
            else:
                output += 'C'
        stderr.write(output + u'\n')
        output = u'  Res. levels : %s\n' % u''.join(
            [six.text_type(_ch['level']) for _ch in storage['chars']])
        stderr.write(output)
        _types = [_ch['type'].ljust(3) for _ch in storage['chars']]
        for i in range(3):
            if i:
                output = u'                %s\n'
            else:
                output = u'  Res. types  : %s\n'
            stderr.write(output % u''.join([_t[i] for _t in _types]))
*** Original Comment ***: Function to assist in debugging by printing information about a storage object to stderr with options for base info, characters, and runs.
*** Validation Score ***: 3
*** Validation Suggestion ***: Be more concise by starting with an action verb and removing unnecessary details: 'Prints storage object details to stderr with options for base info, characters, and runs.'
----------------Refine 1----------------
*** Refine comment ***: Prints storage object details to stderr with options for base info, characters, and runs.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'with options for' as it is implied by the context that the function has options.
*** Final Score ***: 4
*** Final Comment ***: Prints storage object details to stderr with options for base info, characters, and runs.


===============================[ 56 ]===============================
def get(self, alert_type, alert_args=None):
        alert_args = alert_args or {}
        alerts = self.list()
        return [
            alert
            for alert
            in alerts
            if alert.get('type') == alert_type
            and dict_is_subset(alert_args, alert.get('args'))
        ]
*** Original Comment ***: The _get_recursive_difference method returns a list of differences between dictionary items based on the specified type ('intersect', 'added', 'removed', or 'all').
*** Validation Score ***: 1
*** Validation Suggestion ***: Be more concise by correcting the method name and focusing on the core functionality: 'Returns a list of alerts filtered by type and optional arguments.'
----------------Refine 1----------------
*** Refine comment ***: Returns a list of alerts filtered by type and optional arguments.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Returns a list of alerts filtered by type and optional arguments.


===============================[ 57 ]===============================
def _make_reversed_wildcards(self, old_length=-1):
        if len(self._reversed_wildcards) > 0:
            start = old_length
        else:
            start = -1
        for wildcards, func in self._wildcard_functions.items():
            for irun in range(start, len(self)):
                translated_name = func(irun)
                if not translated_name in self._reversed_wildcards:
                    self._reversed_wildcards[translated_name] = ([], wildcards)
                self._reversed_wildcards[translated_name][0].append(irun)
*** Original Comment ***: Updates the mapping of reversed wildcards to their corresponding indices in the sequence, starting from a given index or from the beginning if no index is provided.
*** Validation Score ***: 3
*** Validation Suggestion ***: Be more concise by removing 'in the sequence' as it is implied by context and 'to their corresponding indices', which can be simplified to 'with indices'. Also, 'starting from a given index or from the beginning if no index is provided' can be shortened to 'starting at a specified or default index'.
----------------Refine 1----------------
*** Refine comment ***: Updates the mapping of reversed wildcards with indices, starting at a specified or default index.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'with indices, starting at a specified or default index' as it is implied by 'Updates the mapping'.
*** Final Score ***: 4
*** Final Comment ***: Updates the mapping of reversed wildcards with indices, starting at a specified or default index.


===============================[ 58 ]===============================
def sanitize_block(self, block):
        embed_type = block.get('type', None)
        data = block.get('data', {})
        serializer = self.serializers.get(embed_type, None)
        if serializer is None:
            return block
        block['data'] = serializer.to_internal_value(data)
        return block
*** Original Comment ***: Sanitizes the block by using the appropriate serializer for its type to convert the block's data to a valid internal format.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'for its type' as 'appropriate serializer' implies type specificity.
----------------Refine 1----------------
*** Refine comment ***: Sanitizes the block by using the appropriate serializer to convert the block's data to a valid internal format.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Sanitizes the block by using the appropriate serializer to convert the block's data to a valid internal format.


===============================[ 59 ]===============================
def handle_stop(self, signame, set_stop):
        if set_stop:
            self.sigs[signame].b_stop       = True
            self.sigs[signame].print_method = self.dbgr.intf[-1].msg
            self.sigs[signame].pass_along   = False
        else:
            self.sigs[signame].b_stop       = False
            pass
        return set_stop
*** Original Comment ***: The function handle_stop sets or clears a stop flag for a given signal name and optionally sets a print method and pass_along flag.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'optionally' as the code clearly shows the condition, and 'for a given signal name' as it's implied by the function's parameter. A revised comment could be: 'Sets or clears the stop flag, assigns a print method, and updates the pass_along flag based on the set_stop parameter.'
----------------Refine 1----------------
*** Refine comment ***: Sets or clears the stop flag, assigns a print method, and updates the pass_along flag based on the set_stop parameter.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'and updates the pass_along flag' as it is implied by 'sets or clears'. Suggested comment: 'Sets or clears the stop flag and assigns a print method based on the set_stop parameter.'
*** Final Score ***: 4
*** Final Comment ***: Sets or clears the stop flag, assigns a print method, and updates the pass_along flag based on the set_stop parameter.


===============================[ 60 ]===============================
def classe(self, name):
        for klass in self.classes():
            if klass.node.name == name:
                return klass
        raise KeyError(name)
*** Original Comment ***: Searches for a class with the specified name among the classes and returns it, or raises a KeyError if not found.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'among the classes and' as it is implied by the context.
----------------Refine 1----------------
*** Refine comment ***: Searches for a class with the specified name and returns it, or raises a KeyError if not found.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'with the specified name' as 'specified' is implied by the context. Suggested comment: 'Searches for a class by name and returns it, or raises a KeyError if not found.'
*** Final Score ***: 4
*** Final Comment ***: Searches for a class with the specified name and returns it, or raises a KeyError if not found.


===============================[ 61 ]===============================
def delete(self, id, coordinates):
        p_mins, p_maxs = self.get_coordinate_pointers(coordinates)
        core.rt.Index_DeleteData(
            self.handle, id, p_mins, p_maxs, self.properties.dimension)
*** Original Comment ***: Deletes data from an index within specified coordinates.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Deletes data from an index within specified coordinates.


===============================[ 62 ]===============================
def trace(self, app):
        if app not in self._trace_cache:
            process = subprocess.Popen(
                "trace-deps.js {}".format(app), shell=True,
                stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                env=self.env, universal_newlines=True, cwd=self._package_json_dir
            )
            out, err = process.communicate()
            if err:
                raise TraceError(err)
            self._trace_cache[app] = json.loads(out)
        return self._trace_cache[app]
*** Original Comment ***: This function traces a given app's dependencies using a subprocess to run 'trace-deps.js', caches the result, and returns the dependency information.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'using a subprocess to run 'trace-deps.js',' as it describes the implementation rather than the function's responsibility. A revised comment could be: 'Traces and caches an app's dependencies, returning the dependency information.'
----------------Refine 1----------------
*** Refine comment ***: Traces and caches an app's dependencies, returning the dependency information.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Traces and caches an app's dependencies, returning the dependency information.


===============================[ 63 ]===============================
def getoutput(cmd):
    with AvoidUNCPath() as path:
        if path is not None:
            cmd = '"pushd %s &&"%s' % (path, cmd)
        out = process_handler(cmd, lambda p: p.communicate()[0], STDOUT)
    if out is None:
        out = b''
    return py3compat.bytes_to_str(out)
*** Original Comment ***: Execute a command using subprocess and return its output, handling exceptions.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'handling exceptions' as it is not explicitly mentioned in the code snippet.
----------------Refine 1----------------
*** Refine comment ***: Execute a command using subprocess and return its output.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'using subprocess' as it is implied by 'execute a command' and is not essential to the core responsibility of the function.
*** Final Score ***: 4
*** Final Comment ***: Execute a command using subprocess and return its output.


===============================[ 64 ]===============================
def get_safe_contract(w3: Web3, address=None):
    return w3.eth.contract(address,
                           abi=GNOSIS_SAFE_INTERFACE['abi'],
                           bytecode=GNOSIS_SAFE_INTERFACE['bytecode'])
*** Original Comment ***: Retrieve or create a Web3 contract instance for Gnosis Safe with the given address and predefined ABI and bytecode.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'Retrieve or create a' and starting with 'Returns a Web3 contract instance for Gnosis Safe using the given address, ABI, and bytecode.'
----------------Refine 1----------------
*** Refine comment ***: Returns a Web3 contract instance for Gnosis Safe using the given address, ABI, and bytecode.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Returns a Web3 contract instance for Gnosis Safe using the given address, ABI, and bytecode.


===============================[ 65 ]===============================
def headers_present(self, headers):
        headers = {name: re.compile('(.*)') for name in headers}
        self.add_matcher(matcher('HeadersMatcher', headers))
*** Original Comment ***: Add a matcher to check if specific headers are present in the request
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'to check if' as 'Add a matcher for specific request headers' implies the check.
----------------Refine 1----------------
*** Refine comment ***: Add a matcher for specific request headers
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Add a matcher for specific request headers


===============================[ 66 ]===============================
def is_method_call(func, types=(), methods=()):
    return (
        isinstance(func, astroid.BoundMethod)
        and isinstance(func.bound, astroid.Instance)
        and (func.bound.name in types if types else True)
        and (func.name in methods if methods else True)
    )
*** Original Comment ***: Checks if the provided function is a method call on specified types and methods.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed; the comment is already concise, starting with an action verb, focusing on the core responsibility, and without redundancy.
*** Final Score ***: 5
*** Final Comment ***: Checks if the provided function is a method call on specified types and methods.


===============================[ 67 ]===============================
def _uniform_unit_norm(dimension, shape, dtype, seed):
  raw = normal.Normal(
      loc=dtype_util.as_numpy_dtype(dtype)(0),
      scale=dtype_util.as_numpy_dtype(dtype)(1)).sample(
          tf.concat([shape, [dimension]], axis=0), seed=seed())
  unit_norm = raw / tf.norm(tensor=raw, ord=2, axis=-1)[..., tf.newaxis]
  return unit_norm
*** Original Comment ***: Generates a tensor with samples drawn from a normal distribution and normalizes it to have a unit norm along the specified dimension.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'and normalizes it' as 'Generates a unit-normed tensor from a normal distribution along the specified dimension.'
----------------Refine 1----------------
*** Refine comment ***: Generates a unit-normed tensor from a normal distribution along the specified dimension.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Generates a unit-normed tensor from a normal distribution along the specified dimension.


===============================[ 68 ]===============================
def is_from_fallback_block(node: astroid.node_classes.NodeNG) -> bool:
    context = find_try_except_wrapper_node(node)
    if not context:
        return False
    if isinstance(context, astroid.ExceptHandler):
        other_body = context.parent.body
        handlers = context.parent.handlers
    else:
        other_body = itertools.chain.from_iterable(
            handler.body for handler in context.handlers
        )
        handlers = context.handlers
    has_fallback_imports = any(
        isinstance(import_node, (astroid.ImportFrom, astroid.Import))
        for import_node in other_body
    )
    ignores_import_error = _except_handlers_ignores_exception(handlers, ImportError)
    return ignores_import_error or has_fallback_imports
*** Original Comment ***: Determines if a node is within a try-except block that serves as a fallback, typically for handling failed imports.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'typically for handling failed imports' as it is context-specific and not essential to the core responsibility of the function.
----------------Refine 1----------------
*** Refine comment ***: Determines if a node is within a try-except block that serves as a fallback.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'that serves as a fallback' as it is implied by the context of the try-except block.
*** Final Score ***: 4
*** Final Comment ***: Determines if a node is within a try-except block that serves as a fallback.


===============================[ 69 ]===============================
def before_app_request(self, f):
        self.record_once(lambda s: s.app.before_request_funcs
            .setdefault(None, []).append(f))
        return f
*** Original Comment ***: Register a function to run before each request in the application.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Register a function to run before each request in the application.


===============================[ 70 ]===============================
def cudnn_bi_gru(units,
                 n_hidden,
                 seq_lengths=None,
                 n_layers=1,
                 trainable_initial_states=False,
                 name='cudnn_bi_gru',
                 reuse=False):
    with tf.variable_scope(name, reuse=reuse):
        if seq_lengths is None:
            seq_lengths = tf.ones([tf.shape(units)[0]], dtype=tf.int32) * tf.shape(units)[1]
        with tf.variable_scope('Forward'):
            h_fw, h_last_fw = cudnn_gru_wrapper(units,
                                                n_hidden,
                                                n_layers=n_layers,
                                                trainable_initial_states=trainable_initial_states,
                                                seq_lengths=seq_lengths,
                                                reuse=reuse)
        with tf.variable_scope('Backward'):
            reversed_units = tf.reverse_sequence(units, seq_lengths=seq_lengths, seq_dim=1, batch_dim=0)
            h_bw, h_last_bw = cudnn_gru_wrapper(reversed_units,
                                                n_hidden,
                                                n_layers=n_layers,
                                                trainable_initial_states=trainable_initial_states,
                                                seq_lengths=seq_lengths,
                                                reuse=reuse)
            h_bw = tf.reverse_sequence(h_bw, seq_lengths=seq_lengths, seq_dim=1, batch_dim=0)
    return (h_fw, h_bw), (h_last_fw, h_last_bw)
*** Original Comment ***: Defines a bidirectional GRU layer using CuDNN, with options for sequence length, number of layers, and trainable initial states.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'with options for' as it is implied by the listed parameters. Suggested comment: 'Defines a bidirectional GRU layer using CuDNN, specifying sequence length, number of layers, and trainable initial states.'
----------------Refine 1----------------
*** Refine comment ***: Defines a bidirectional GRU layer using CuDNN, specifying sequence length, number of layers, and trainable initial states.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'specifying' as it is implied by the context. A revised comment could be: 'Defines a bidirectional GRU layer with CuDNN, including sequence length, number of layers, and trainable initial states.'
*** Final Score ***: 4
*** Final Comment ***: Defines a bidirectional GRU layer using CuDNN, specifying sequence length, number of layers, and trainable initial states.


===============================[ 71 ]===============================
def xml_report(self, morfs=None, outfile=None, ignore_errors=None,
                    omit=None, include=None):
        self._harvest_data()
        self.config.from_args(
            ignore_errors=ignore_errors, omit=omit, include=include,
            xml_output=outfile,
            )
        file_to_close = None
        delete_file = False
        if self.config.xml_output:
            if self.config.xml_output == '-':
                outfile = sys.stdout
            else:
                outfile = open(self.config.xml_output, "w")
                file_to_close = outfile
        try:
            try:
                reporter = XmlReporter(self, self.config)
                return reporter.report(morfs, outfile=outfile)
            except CoverageException:
                delete_file = True
                raise
        finally:
            if file_to_close:
                file_to_close.close()
                if delete_file:
                    file_be_gone(self.config.xml_output)
*** Original Comment ***: Generates an XML report for the coverage data, handling output file creation and error cleanup.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'for the coverage data,' as it is implied by the context of the function.
----------------Refine 1----------------
*** Refine comment ***: Generates an XML report, handling output file creation and error cleanup.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'handling' as it is implied by 'Generates'. Suggested comment: 'Generates an XML report, managing output file creation and error cleanup.'
*** Final Score ***: 4
*** Final Comment ***: Generates an XML report, handling output file creation and error cleanup.


===============================[ 72 ]===============================
def of(jvalue, bigdl_type="float"):
        def get_py_name(jclass_name):
            if jclass_name == "StaticGraph" or jclass_name == "DynamicGraph":
                return "Model"
            elif jclass_name == "Input":
                return "Layer"
            else:
                return jclass_name
        jname = callBigDlFunc(bigdl_type,
                                      "getRealClassNameOfJValue",
                                      jvalue)
        jpackage_name = ".".join(jname.split(".")[:-1])
        pclass_name = get_py_name(jname.split(".")[-1])
        if "com.intel.analytics.bigdl.nn.keras.Model" == jname or                        "com.intel.analytics.bigdl.nn.keras.Sequential" == jname:
            base_module = importlib.import_module('bigdl.nn.keras.topology')
        elif "com.intel.analytics.bigdl.nn.keras" == jpackage_name:
            base_module = importlib.import_module('bigdl.nn.keras.layer')
        else:
            base_module = importlib.import_module('bigdl.nn.layer')
        realClassName = "Layer" 
        if pclass_name in dir(base_module):
            realClassName = pclass_name
        module = getattr(base_module, realClassName)
        jvalue_creator = getattr(module, "from_jvalue")
        model = jvalue_creator(jvalue, bigdl_type)
        return model
*** Original Comment ***: The function 'of' dynamically loads a BigDL class from a Java value, mapping Java class names to Python class names, and instantiates the corresponding Python class.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by stating 'The 'of' function dynamically maps Java class names to Python equivalents and instantiates the corresponding BigDL class from a Java value.'
----------------Refine 1----------------
*** Refine comment ***: The 'of' function dynamically maps Java class names to Python equivalents and instantiates the corresponding BigDL class from a Java value.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by stating 'Maps Java class names to Python BigDL classes and instantiates them from a Java value.'
*** Final Score ***: 4
*** Final Comment ***: The 'of' function dynamically maps Java class names to Python equivalents and instantiates the corresponding BigDL class from a Java value.


===============================[ 73 ]===============================
async def resume(self, *, device: Optional[SomeDevice] = None):
        await self._user.http.play_playback(None, device_id=str(device))
*** Original Comment ***: Resume playback on a given device or the default one.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Resume playback on a given device or the default one.


===============================[ 74 ]===============================
def _get_access_token():
    access_token = os.environ.get(ACCESS_TOKEN_ENVIRONMENT_VARIABLE)
    if access_token:
        return access_token
    else:
        for access_token_variable in LEGACY_ACCESS_TOKEN_ENVIRONMENT_VARIABLES:
            access_token = os.environ.get(access_token_variable)
            if access_token:
                env_var_deprecation_warning = PendingDeprecationWarning(
                    "Use of the `{legacy}` environment variable will be "
                    "deprecated in the future.  Please update your "
                    "environment(s) to use the new `{new}` environment "
                    "variable.".format(
                        legacy=access_token,
                        new=ACCESS_TOKEN_ENVIRONMENT_VARIABLE,
                    )
                )
                warnings.warn(env_var_deprecation_warning)
                return access_token
*** Original Comment ***: Retrieve the access token from the environment variable, with support for legacy variables and a deprecation warning.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'with support for' as it's implied by the context. Suggested comment: 'Retrieve the access token from the environment, issuing a deprecation warning for legacy variables.'
----------------Refine 1----------------
*** Refine comment ***: Retrieve the access token from the environment, issuing a deprecation warning for legacy variables.
*** Validation Score ***: 5
*** Validation Suggestion ***: Be more concise by removing 'issuing a deprecation warning for legacy variables' as it is an additional action not central to the core responsibility of the function.
*** Final Score ***: 5
*** Final Comment ***: Retrieve the access token from the environment, issuing a deprecation warning for legacy variables.


===============================[ 75 ]===============================
def sanitize_for_archive(url, headers, payload):
        url = re.sub('bot.*/', 'botXXXXX/', url)
        return url, headers, payload
*** Original Comment ***: The function sanitize_for_archive sanitizes the URL by replacing the bot user ID with 'botXXXXX/' and returns the sanitized URL along with the unchanged headers and payload.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing the redundant explanation of the return value, as it's clear from the context. Suggested comment: 'Sanitizes the URL by replacing the bot user ID with 'botXXXXX/'.'
----------------Refine 1----------------
*** Refine comment ***: Sanitizes the URL by replacing the bot user ID with 'botXXXXX/'.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Sanitizes the URL by replacing the bot user ID with 'botXXXXX/'.


===============================[ 76 ]===============================
def remove_subscriber(self, ws):
        if ws in self.subscribers:
            self.subscribers.remove(ws)
        for name in self.available_events:
            self.remove_event_subscriber(name, ws)
*** Original Comment ***: Remove a WebSocket subscriber from the list of subscribers and from all event subscriptions.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'from the list of subscribers and' as it is implied by 'Remove a WebSocket subscriber'.
----------------Refine 1----------------
*** Refine comment ***: Remove a WebSocket subscriber from all event subscriptions.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Remove a WebSocket subscriber from all event subscriptions.


===============================[ 77 ]===============================
def backward_smoothing_update(filtered_mean,
                              filtered_cov,
                              predicted_mean,
                              predicted_cov,
                              next_posterior_mean,
                              next_posterior_cov,
                              transition_matrix):
  tmp_gain_cov = transition_matrix.matmul(filtered_cov)
  predicted_cov_chol = tf.linalg.cholesky(predicted_cov)
  gain_transpose = tf.linalg.cholesky_solve(predicted_cov_chol, tmp_gain_cov)
  posterior_mean = (filtered_mean +
                    tf.linalg.matmul(gain_transpose,
                                     next_posterior_mean - predicted_mean,
                                     adjoint_a=True))
  posterior_cov = (
      filtered_cov +
      tf.linalg.matmul(gain_transpose,
                       tf.linalg.matmul(
                           next_posterior_cov - predicted_cov, gain_transpose),
                       adjoint_a=True))
  return (posterior_mean, posterior_cov)
*** Original Comment ***: Perform a backward smoothing update step in a Kalman filter.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Perform a backward smoothing update step in a Kalman filter.


===============================[ 78 ]===============================
def plot(self, timestep="AUTO", metric="AUTO", server=False, **kwargs):
        assert_is_type(metric, "AUTO", "logloss", "auc", "classification_error", "rmse")
        if self._model_json["algo"] in ("deeplearning", "deepwater", "xgboost", "drf", "gbm"):
            if metric == "AUTO":
                metric = "logloss"
        self._plot(timestep=timestep, metric=metric, server=server)
*** Original Comment ***: The function 'plot_predict' generates a forecast plot using the estimated model, including confidence intervals if specified.
*** Validation Score ***: 2
*** Validation Suggestion ***: Be more concise by starting with an action verb and focusing on the core functionality. Modify to: 'Generates a plot of the model's forecast with optional confidence intervals.'
----------------Refine 1----------------
*** Refine comment ***: Generates a plot of the model's forecast with optional confidence intervals.
*** Validation Score ***: 2
*** Validation Suggestion ***: Be more concise by stating 'Generates a model forecast plot.'
----------------Refine 2----------------
*** Refine comment ***: Generates a model forecast plot.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by using 'Generates plot for model predictions.'
*** Final Score ***: 4
*** Final Comment ***: Generates a model forecast plot.


===============================[ 79 ]===============================
def create_py(self, nb, force=False):
        if list(map(int, re.findall('\d+', nbconvert.__version__))) >= [4, 2]:
            py_file = os.path.basename(self.py_file)
        else:
            py_file = self.py_file
        try:
            level = logger.logger.level
        except AttributeError:
            level = logger.level
        spr.call(['jupyter', 'nbconvert', '--to=python',
                  '--output=' + py_file, '--log-level=%s' % level,
                  self.outfile])
        with open(self.py_file) as f:
            py_content = f.read()
        py_content = re.sub('^\s*get_ipython\(\).magic.*', '# \g<0>',
                            py_content, flags=re.MULTILINE)
        with open(self.py_file, 'w') as f:
            f.write(py_content)
*** Original Comment ***: Convert a Jupyter notebook to a Python script, adjust logging level, and comment out IPython magic commands.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Convert a Jupyter notebook to a Python script, adjust logging level, and comment out IPython magic commands.


===============================[ 80 ]===============================
def _remove_overlaps(segmentation_mask, fronts):
    fidxs, sidxs = np.where((segmentation_mask != fronts) & (segmentation_mask != 0) & (fronts != 0))
    fronts[fidxs, sidxs] = 0
*** Original Comment ***: The function _remove_overlaps takes a segmentation mask and fronts, identifies overlapping non-zero pixels between them, and sets those pixels to zero in the fronts array.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'takes a segmentation mask and fronts,' as the parameters are implied by the context.
----------------Refine 1----------------
*** Refine comment ***: Identifies and removes overlapping non-zero pixels between the segmentation mask and fronts by setting those pixels to zero in the fronts array.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'by setting those pixels to zero in the fronts array' as it is implied by 'removes'.
*** Final Score ***: 4
*** Final Comment ***: Identifies and removes overlapping non-zero pixels between the segmentation mask and fronts by setting those pixels to zero in the fronts array.


===============================[ 81 ]===============================
def skip_module(*modules):
    modules = (modules and isinstance(modules[0], list)) and              modules[0] or modules
    for module in modules:
        if not module in SKIPPED_MODULES:
            SKIPPED_MODULES.append(module)
    traceback.extract_tb = _new_extract_tb
*** Original Comment ***: Add specified modules to the list of skipped modules and override the traceback.extract_tb function.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'specified' as it is implied and 'override the' as it is clear from context. Suggested comment: 'Add modules to skipped list and replace traceback.extract_tb function.'
----------------Refine 1----------------
*** Refine comment ***: Add modules to skipped list and replace traceback.extract_tb function.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'and replace traceback.extract_tb function' as it is implied by the context.
*** Final Score ***: 4
*** Final Comment ***: Add modules to skipped list and replace traceback.extract_tb function.


===============================[ 82 ]===============================
def get_config(self):
    return {
        'initializers': [
            tf.compat.v2.initializers.serialize(
                tf.keras.initializers.get(init))
            for init in self.initializers
        ],
        'sizes': self.sizes,
        'validate_args': self.validate_args,
    }
*** Original Comment ***: Returns a dictionary containing the serialized initializers, layer sizes, and a flag indicating if arguments should be validated.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'a flag indicating if' to simply state 'and argument validation flag'.
----------------Refine 1----------------
*** Refine comment ***: Returns a dictionary containing serialized initializers, layer sizes, and argument validation flag.
*** Validation Score ***: 5
*** Validation Suggestion ***: The comment is already concise, clearly stating the function's return values without redundancy.
*** Final Score ***: 5
*** Final Comment ***: Returns a dictionary containing serialized initializers, layer sizes, and argument validation flag.


===============================[ 83 ]===============================
def set_query_parameter(url, param_name, param_value):
    scheme, netloc, path, query_string, fragment = urlsplit(url)
    query_params = parse_qs(query_string)
    query_params[param_name] = [param_value]
    new_query_string = urlencode(query_params, doseq=True)
    return urlunsplit((scheme, netloc, path, new_query_string, fragment))
*** Original Comment ***: Set or replace a query parameter in the URL and return the modified URL.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'and return the modified URL' as it is implied by the function's action.
----------------Refine 1----------------
*** Refine comment ***: Set or replace a query parameter in the URL.
*** Validation Score ***: 5
*** Validation Suggestion ***: The comment is already concise and clearly states the core functionality of the code.
*** Final Score ***: 5
*** Final Comment ***: Set or replace a query parameter in the URL.


===============================[ 84 ]===============================
def alphavsks(self,autozoom=True,**kwargs):
        pylab.plot(self._alpha_values, self._xmin_kstest, '.')
        pylab.errorbar(self._alpha, self._ks, xerr=self._alphaerr, fmt='+')
        ax=pylab.gca()
        if autozoom:
            ax.set_ylim(0.8*(self._ks),3*(self._ks))
            ax.set_xlim((self._alpha)-5*self._alphaerr,(self._alpha)+5*self._alphaerr)
        ax.set_ylabel("KS statistic")
        ax.set_xlabel(r'$\alpha$')
        pylab.draw()
        return ax
*** Original Comment ***: Plots the relationship between alpha values and KS statistics, with optional zooming, using pylab.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'with optional zooming,' as it is implied by the parameter name 'autozoom' and by changing 'using pylab' to 'using error bars' to focus on the core functionality.
----------------Refine 1----------------
*** Refine comment ***: Plots the relationship between alpha values and KS statistics using error bars.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'using error bars' as it is implied by the context of the plotting function.
*** Final Score ***: 4
*** Final Comment ***: Plots the relationship between alpha values and KS statistics using error bars.


===============================[ 85 ]===============================
def put_container(self, container, headers=None, query=None, cdn=False,
                      body=None):
        path = self._container_path(container)
        return self.request(
            'PUT', path, body or '', headers, query=query, cdn=cdn)
*** Original Comment ***: Create or update a container with optional headers, query parameters, and body, and specify if it's a CDN container.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'and specify if it's a CDN container' as it is implied by the 'cdn' parameter in the method signature.
----------------Refine 1----------------
*** Refine comment ***: Create or update a container with optional headers, query parameters, and body.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'with optional' as it's implied by the context that headers, query parameters, and body are not mandatory.
*** Final Score ***: 4
*** Final Comment ***: Create or update a container with optional headers, query parameters, and body.


===============================[ 86 ]===============================
def assign_moving_mean_variance(
    mean_var, variance_var, value, decay, name=None):
  with tf.compat.v1.name_scope(name, "assign_moving_mean_variance",
                               [variance_var, mean_var, value, decay]):
    with tf.compat.v1.colocate_with(variance_var):
      with tf.compat.v1.colocate_with(mean_var):
        base_dtype = mean_var.dtype.base_dtype
        if not base_dtype.is_floating:
          raise TypeError(
              "mean_var.base_dtype({}) does not have float type "
              "`dtype`.".format(base_dtype.name))
        if base_dtype != variance_var.dtype.base_dtype:
          raise TypeError(
              "mean_var.base_dtype({}) != variance_var.base_dtype({})".format(
                  base_dtype.name,
                  variance_var.dtype.base_dtype.name))
        value = tf.convert_to_tensor(
            value=value, dtype=base_dtype, name="value")
        decay = tf.convert_to_tensor(
            value=decay, dtype=base_dtype, name="decay")
        delta = value - mean_var
        with tf.control_dependencies([delta]):
          mean_var = mean_var.assign_add((1. - decay) * delta)
          variance_var = variance_var.assign_sub(
              (1. - decay) * (variance_var - decay * tf.square(delta)))
        return mean_var, variance_var
*** Original Comment ***: Assigns the moving mean and variance to the given variables with specified decay.
*** Validation Score ***: 5
*** Validation Suggestion ***: The comment is already concise, clearly stating the function's action without unnecessary words.
*** Final Score ***: 5
*** Final Comment ***: Assigns the moving mean and variance to the given variables with specified decay.


===============================[ 87 ]===============================
def _dot_product(self, imgs_to_decode):
        return np.dot(imgs_to_decode.T, self.feature_images).T
*** Original Comment ***: Calculates the dot product between the transposed images to decode and the feature images, returning the transposed result.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'between the transposed images to decode and the feature images, returning the transposed result' and replacing with 'of transposed images to decode with feature images'.
----------------Refine 1----------------
*** Refine comment ***: Calculates the dot product of transposed images to decode with feature images.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'to decode' as it is implied by the context.
*** Final Score ***: 4
*** Final Comment ***: Calculates the dot product of transposed images to decode with feature images.


===============================[ 88 ]===============================
def _decode(self, data, decode_content, flush_decoder):
        try:
            if decode_content and self._decoder:
                data = self._decoder.decompress(data)
        except (IOError, zlib.error) as e:
            content_encoding = self.headers.get('content-encoding', '').lower()
            raise DecodeError(
                "Received response with content-encoding: %s, but "
                "failed to decode it." % content_encoding, e)
        if flush_decoder and decode_content and self._decoder:
            buf = self._decoder.decompress(binary_type())
            data += buf + self._decoder.flush()
        return data
*** Original Comment ***: The _decode method attempts to decompress data if decode_content is True and a decoder is available, handling exceptions and flushing the decoder if required.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'attempts to' and 'if required' as they are implied by the context.
----------------Refine 1----------------
*** Refine comment ***: The _decode method decompresses data when decode_content is True and a decoder is available, handling exceptions and flushing the decoder.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing redundancy and specifying the exception handling, e.g., 'Decompresses data if decode_content is True and a decoder exists, also handles related exceptions and flushes the decoder.'
*** Final Score ***: 4
*** Final Comment ***: The _decode method decompresses data when decode_content is True and a decoder is available, handling exceptions and flushing the decoder.


===============================[ 89 ]===============================
def get_tweets(user, pages=25):
    url = f'https://twitter.com/i/profiles/show/{user}/timeline/tweets?include_available_features=1&include_entities=1&include_new_items_bar=true'
    headers = {
        'Accept': 'application/json, text/javascript, */*; q=0.01',
        'Referer': f'https://twitter.com/{user}',
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/603.3.8 (KHTML, like Gecko) Version/10.1.2 Safari/603.3.8',
        'X-Twitter-Active-User': 'yes',
        'X-Requested-With': 'XMLHttpRequest',
        'Accept-Language': 'en-US'
    }
    def gen_tweets(pages):
        r = session.get(url, headers=headers)
        while pages > 0:
            try:
                html = HTML(html=r.json()['items_html'],
                            url='bunk', default_encoding='utf-8')
            except KeyError:
                raise ValueError(
                    f'Oops! Either "{user}" does not exist or is private.')
            comma = ","
            dot = "."
            tweets = []
            for tweet in html.find('html > .stream-item'):
                try:
                    text = tweet.find('.tweet-text')[0].full_text
                except IndexError:  
                    continue
                tweet_id = tweet.find('.js-permalink')[0].attrs['data-conversation-id']
                time = datetime.fromtimestamp(int(tweet.find('._timestamp')[0].attrs['data-time-ms']) / 1000.0)
                interactions = [
                    x.text
                    for x in tweet.find('.ProfileTweet-actionCount')
                ]
                replies = int(
                    interactions[0].split(' ')[0].replace(comma, '').replace(dot, '')
                    or interactions[3]
                )
                retweets = int(
                    interactions[1].split(' ')[0].replace(comma, '').replace(dot, '')
                    or interactions[4]
                    or interactions[5]
                )
                likes = int(
                    interactions[2].split(' ')[0].replace(comma, '').replace(dot, '')
                    or interactions[6]
                    or interactions[7]
                )
                hashtags = [
                    hashtag_node.full_text
                    for hashtag_node in tweet.find('.twitter-hashtag')
                ]
                urls = [
                    url_node.attrs['data-expanded-url']
                    for url_node in tweet.find('a.twitter-timeline-link:not(.u-hidden)')
                ]
                photos = [
                    photo_node.attrs['data-image-url']
                    for photo_node in tweet.find('.AdaptiveMedia-photoContainer')
                ]
                videos = []
                video_nodes = tweet.find(".PlayableMedia-player")
                for node in video_nodes:
                    styles = node.attrs['style'].split()
                    for style in styles:
                        if style.startswith('background'):
                            tmp = style.split('/')[-1]
                            video_id = tmp[:tmp.index('.jpg')]
                            videos.append({'id': video_id})
                tweets.append({
                    'tweetId': tweet_id,
                    'time': time,
                    'text': text,
                    'replies': replies,
                    'retweets': retweets,
                    'likes': likes,
                    'entries': {
                        'hashtags': hashtags, 'urls': urls,
                        'photos': photos, 'videos': videos
                    }
                })
            last_tweet = html.find('.stream-item')[-1].attrs['data-item-id']
            for tweet in tweets:
                if tweet:
                    tweet['text'] = re.sub('http', ' http', tweet['text'], 1)
                    yield tweet
            r = session.get(url, params={'max_position': last_tweet}, headers=headers)
            pages += -1
    yield from gen_tweets(pages)
*** Original Comment ***: Fetches tweets from a user's timeline and yields them with metadata such as interactions and media entries.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'such as interactions and media entries' as it is implied by 'metadata'.
----------------Refine 1----------------
*** Refine comment ***: Fetches tweets from a user's timeline and yields them with metadata.
*** Validation Score ***: 5
*** Validation Suggestion ***: The comment is already concise and effectively summarizes the core functionality of the code.
*** Final Score ***: 5
*** Final Comment ***: Fetches tweets from a user's timeline and yields them with metadata.


===============================[ 90 ]===============================
def __info_yenczlib_gen(self):
        escape = 0
        dcrc32 = 0
        inflate = zlib.decompressobj(-15)
        header = next(self.__info_plain_gen())
        if not header.startswith("=ybegin"):
            raise NNTPDataError("Bad yEnc header")
        buf, trailer = fifo.Fifo(), ""
        for line in self.__info_plain_gen():
            if line.startswith("=yend"):
                trailer = line
                continue
            data, escape, dcrc32 = yenc.decode(line, escape, dcrc32)
            try:
                data = inflate.decompress(data)
            except zlib.error:
                raise NNTPDataError("Decompression failed")
            if not data:
                continue
            buf.write(data)
            for l in buf:
                yield l
        if not trailer:
            raise NNTPDataError("Missing yEnc trailer")
        ecrc32 = yenc.crc32(trailer)
        if ecrc32 is None:
            raise NNTPDataError("Bad yEnc trailer")
        if ecrc32 != dcrc32 & 0xffffffff:
            raise NNTPDataError("Bad yEnc CRC")
*** Original Comment ***: Generates decoded and decompressed data from yEnc encoded input, handling errors and verifying CRC.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'handling errors and' as it is implied by 'verifying CRC'.
----------------Refine 1----------------
*** Refine comment ***: Generates decoded and decompressed data from yEnc encoded input, verifying CRC.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'Generates' and starting with 'Decodes' to focus on the core action.
*** Final Score ***: 4
*** Final Comment ***: Generates decoded and decompressed data from yEnc encoded input, verifying CRC.


===============================[ 91 ]===============================
def _load_attr(name: str, ctx: ast.AST = ast.Load()) -> ast.Attribute:
    attrs = name.split(".")
    def attr_node(node, idx):
        if idx >= len(attrs):
            node.ctx = ctx
            return node
        return attr_node(
            ast.Attribute(value=node, attr=attrs[idx], ctx=ast.Load()), idx + 1
        )
    return attr_node(ast.Name(id=attrs[0], ctx=ast.Load()), 1)
*** Original Comment ***: Recursively construct an AST Attribute node from a dotted attribute string.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed; the comment is already concise.
*** Final Score ***: 5
*** Final Comment ***: Recursively construct an AST Attribute node from a dotted attribute string.


===============================[ 92 ]===============================
def open_in_browser(doc, encoding=None):
    import os
    import webbrowser
    import tempfile
    if not isinstance(doc, etree._ElementTree):
        doc = etree.ElementTree(doc)
    handle, fn = tempfile.mkstemp(suffix='.html')
    f = os.fdopen(handle, 'wb')
    try:
        doc.write(f, method="html", encoding=encoding or doc.docinfo.encoding or "UTF-8")
    finally:
        f.close()
    url = 'file://' + fn.replace(os.path.sep, '/')
    print(url)
    webbrowser.open(url)
*** Original Comment ***: Writes an XML/HTML document to a temporary file and opens it in the default web browser.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'Writes' and starting with 'Write' to align with the action verb guideline, and by eliminating 'default' as it is implied by 'web browser'.
----------------Refine 1----------------
*** Refine comment ***: Write an XML/HTML document to a temporary file and opens it in a web browser.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'and' and combining actions: 'Writes an XML/HTML document to a temporary file, then opens it in a web browser.'
*** Final Score ***: 4
*** Final Comment ***: Write an XML/HTML document to a temporary file and opens it in a web browser.


===============================[ 93 ]===============================
def check_gpu_existence():
    global _gpu_available
    if _gpu_available is None:
        sess_config = tf.ConfigProto()
        sess_config.gpu_options.allow_growth = True
        try:
            with tf.Session(config=sess_config):
                device_list = device_lib.list_local_devices()
                _gpu_available = any(device.device_type == 'GPU' for device in device_list)
        except AttributeError as e:
            log.warning(f'Got an AttributeError `{e}`, assuming documentation building')
            _gpu_available = False
    return _gpu_available
*** Original Comment ***: Check if a GPU is available and cache the result for future reference.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'and cache the result for future reference' as it is implied by the function's behavior.
----------------Refine 1----------------
*** Refine comment ***: Check if a GPU is available.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Check if a GPU is available.


===============================[ 94 ]===============================
def __fetch_crate_data(self, crate_id):
        raw_crate = self.client.crate(crate_id)
        crate = json.loads(raw_crate)
        return crate['crate']
*** Original Comment ***: Fetches data for a specific crate by its crate_id.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Fetches data for a specific crate by its crate_id.


===============================[ 95 ]===============================
def current_lr(self):
        if self.optimizer is None:
            raise RuntimeError(
                'lr is not applicable because optimizer does not exist.')
        return [group['lr'] for group in self.optimizer.param_groups]
*** Original Comment ***: Retrieves the current learning rate from the optimizer's parameter groups.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Retrieves the current learning rate from the optimizer's parameter groups.


===============================[ 96 ]===============================
def delete_team(self, name):
        res = self.get_team(name)
        if res[0] == False:
            return res
        t = res[1]
        res = requests.delete(self.url + '/api/teams/' + str(t['id']), headers=self.hdrs, verify=self.ssl_verify)
        if not self._checkResponse(res):
            return [False, self.lasterr]
        return [True, None]
*** Original Comment ***: The function 'delete_team' attempts to delete a team by its name, returning a tuple with a boolean success flag and an error message if applicable.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'attempts to' and 'if applicable' as they are implied by the context. Suggested comment: 'Deletes a team by name, returning a tuple with a boolean success flag and an error message.'
----------------Refine 1----------------
*** Refine comment ***: Deletes a team by name, returning a tuple with a boolean success flag and an error message.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'returning a tuple with' as it's implied by the context.
*** Final Score ***: 4
*** Final Comment ***: Deletes a team by name, returning a tuple with a boolean success flag and an error message.


===============================[ 97 ]===============================
def __download_price(self, symbol: str, currency: str, agent: str):
        from finance_quote_python import Quote
        assert isinstance(symbol, str)
        assert isinstance(currency, str)
        assert isinstance(agent, str)
        if not symbol:
            return None
        dl = Quote()
        dl.logger = self.logger
        dl.set_source(agent)
        dl.set_currency(currency)
        result = dl.fetch(agent, [symbol])
        if not result:
            raise ValueError(f"Did not receive a response for {symbol}.")
        price = result[0]
        if not price:
            raise ValueError(f"Price not downloaded/parsed for {symbol}.")
        else:
            self.add_price(price)
        return price
*** Original Comment ***: Downloads and adds the price of a given symbol using a specified agent and currency
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'and adds' as it is implied by the context that the price will be used after downloading.
----------------Refine 1----------------
*** Refine comment ***: Downloads the price of a given symbol using a specified agent and currency
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'using a specified' as it is implied by the context.
*** Final Score ***: 4
*** Final Comment ***: Downloads the price of a given symbol using a specified agent and currency


===============================[ 98 ]===============================
def purge_queue(self, name):
        content = {"_object_id": {"_object_name": "org.apache.qpid.broker:queue:{0}".format(name)},
                   "_method_name": "purge",
                   "_arguments": {"type": "queue",
                                  "name": name,
                                  "filter": dict()}}
        logger.debug("Message content -> {0}".format(content))
        return content, self.method_properties
*** Original Comment ***: Purge the specified queue on the given virtual host.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'on the given virtual host' as it is not mentioned in the code snippet.
----------------Refine 1----------------
*** Refine comment ***: Purge the specified queue.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed as the comment is already concise.
*** Final Score ***: 5
*** Final Comment ***: Purge the specified queue.


===============================[ 99 ]===============================
def get_motion_vector(self):
        if any(self.move):
            x, y = self.actor._rot
            strafe = math.degrees(math.atan2(*self.move))
            y_angle = math.radians(y)
            x_angle = math.radians(x + strafe)
            dy = 0.0
            dx = math.cos(x_angle)
            dz = math.sin(x_angle)
        else:
            dy = 0.0
            dx = 0.0
            dz = 0.0
        return (dx, dy, dz)
*** Original Comment ***: Calculate the motion vector based on the actor's rotation and movement direction
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'based on the actor's rotation and movement direction' as it is implied by 'motion vector'.
----------------Refine 1----------------
*** Refine comment ***: Calculate the motion vector
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed as the comment is already concise, starting with an action verb, focusing on the core responsibility, and without redundancy.
*** Final Score ***: 5
*** Final Comment ***: Calculate the motion vector


===============================[ 100 ]===============================
def from_file(self, filename, table=None, delimiter='|', null='NULL',
            panic=True, quotechar='"', parse_dates=False):
        if not self.table:
            if not table:
                raise GiraffeError("Table must be set or specified to load a file.")
            self.table = table
        if not isinstance(null, basestring):
            raise GiraffeError("Expected 'null' to be str, received {}".format(type(null)))
        with Reader(filename, delimiter=delimiter, quotechar=quotechar) as f:
            if not isinstance(f.delimiter, basestring):
                raise GiraffeError("Expected 'delimiter' to be str, received {}".format(type(delimiter)))
            self.columns = f.header
            if isinstance(f, ArchiveFileReader):
                self.mload.set_encoding(ROW_ENCODING_RAW)
                self.preprocessor = lambda s: s
            if parse_dates:
                self.preprocessor = DateHandler(self.columns)
            self._initiate()
            self.mload.set_null(null)
            self.mload.set_delimiter(delimiter)
            i = 0
            for i, line in enumerate(f, 1):
                self.put(line, panic=panic)
                if i % self.checkpoint_interval == 1:
                    log.info("\rBulkLoad", "Processed {} rows".format(i), console=True)
                    checkpoint_status = self.checkpoint()
                    self.exit_code = self._exit_code()
                    if self.exit_code != 0:
                        return self.exit_code
            log.info("\rBulkLoad", "Processed {} rows".format(i))
            return self.finish()
*** Original Comment ***: Loads data from a file into a database table, handling various configurations such as delimiters, null representation, and date parsing, with error checking and logging.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'handling various configurations such as' and 'with error checking and logging' as these details are implied by the context.
----------------Refine 1----------------
*** Refine comment ***: Loads data from a file into a database table, supporting delimiters, null representation, and date parsing.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'supporting delimiters, null representation, and date parsing' as these details are implied by the parameters in the method signature.
*** Final Score ***: 4
*** Final Comment ***: Loads data from a file into a database table, supporting delimiters, null representation, and date parsing.


===============================[ 101 ]===============================
def fetchMore(self, index):
        if not index.isValid():
            item = self.root
        else:
            item = index.internalPointer()
        if item.canFetchMore():
            startIndex = len(item.children)
            additionalChildren = item.fetchChildren()
            endIndex = startIndex + len(additionalChildren) - 1
            if endIndex >= startIndex:
                self.beginInsertRows(index, startIndex, endIndex)
                for newChild in additionalChildren:
                    item.addChild(newChild)
                self.endInsertRows()
*** Original Comment ***: Fetch more data for a model item and update the view with new rows if available
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'if available' as it is implied by the conditional structure of the code.
----------------Refine 1----------------
*** Refine comment ***: Fetch more data for a model item and update the view with new rows
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'for a model item and' as it is implied by the context of the method.
*** Final Score ***: 4
*** Final Comment ***: Fetch more data for a model item and update the view with new rows


===============================[ 102 ]===============================
def update_suggestions_dictionary(request, object):
    if request.user.is_authenticated():
        user = request.user
        content_type = ContentType.objects.get_for_model(type(object))
        try:
            ObjectView.objects.get(
                user=user, object_id=object.id, content_type=content_type)
        except:
            ObjectView.objects.create(user=user, content_object=object)
        viewed = ObjectView.objects.filter(user=user)
    else:
        update_dict_for_guests(request, object, content_type)
        return
    if viewed:
        for obj in viewed:
            if content_type == obj.content_type:
                if not exists_in_dictionary(request, object,
                                            content_type,
                                            obj, True):
                    if object.id != obj.object_id:
                        ObjectViewDictionary.objects.create(
                            current_object=object,
                            visited_before_object=obj.content_object)
                        if not exists_in_dictionary(request, obj,
                                                    obj.content_type,
                                                    object, False):
                            ObjectViewDictionary.objects.create(
                                current_object=obj.content_object,
                                visited_before_object=object)
    return
*** Original Comment ***: Updates the suggestions dictionary with the viewed objects for authenticated users or guests.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'with the viewed objects' as it is implied by 'updates' and 'suggestions dictionary', and by replacing 'for authenticated users or guests' with 'based on user status'.
----------------Refine 1----------------
*** Refine comment ***: Updates the suggestions dictionary based on user status.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'based on user status' as it is implied by the context of the function.
*** Final Score ***: 4
*** Final Comment ***: Updates the suggestions dictionary based on user status.


===============================[ 103 ]===============================
def parse_version(version):
    global parse_version
    try:
        from pkg_resources import parse_version
    except ImportError:
        from distutils.version import LooseVersion as parse_version
    return parse_version(version)
*** Original Comment ***: Dynamically import and use the appropriate parse_version function from either pkg_resources or distutils.version based on availability.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing redundancy and simplifying the language: 'Import and use the suitable parse_version function based on availability.'
----------------Refine 1----------------
*** Refine comment ***: Import and use the suitable parse_version function based on availability.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing redundancy and stating 'Import the appropriate parse_version function.'
*** Final Score ***: 4
*** Final Comment ***: Import and use the suitable parse_version function based on availability.


===============================[ 104 ]===============================
def sum_mvns(distributions):
  graph_parents = [tensor for distribution in distributions
                   for tensor in distribution._graph_parents]  
  with tf.compat.v1.name_scope('sum_mvns', values=graph_parents):
    if all([isinstance(mvn, tfd.MultivariateNormalDiag)
            for mvn in distributions]):
      return tfd.MultivariateNormalDiag(
          loc=sum([mvn.mean() for mvn in distributions]),
          scale_diag=tf.sqrt(sum([
              mvn.scale.diag**2 for mvn in distributions])))
    else:
      raise NotImplementedError(
          'Sums of distributions other than MultivariateNormalDiag are not '
          'currently implemented. (given: {})'.format(distributions))
*** Original Comment ***: This function sums multiple MultivariateNormalDiag distributions, returning a new MultivariateNormalDiag distribution with the summed means and the square root of the summed variances.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing redundant information. Instead of 'sums multiple MultivariateNormalDiag distributions, returning a new MultivariateNormalDiag distribution with the summed means and the square root of the summed variances', simplify to 'Sums MultivariateNormalDiag distributions into a new one with combined means and variances'.
----------------Refine 1----------------
*** Refine comment ***: Sums MultivariateNormalDiag distributions into a new one with combined means and variances.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing redundant words 'into a new one' as it is implied by the action 'Sums'.
*** Final Score ***: 4
*** Final Comment ***: Sums MultivariateNormalDiag distributions into a new one with combined means and variances.


===============================[ 105 ]===============================
def crz(self, theta, ctl, tgt):
    return self.append(CrzGate(theta), [ctl, tgt], [])
*** Original Comment ***: Appends a controlled rotation around the Z-axis by an angle theta to the circuit.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Appends a controlled rotation around the Z-axis by an angle theta to the circuit.


===============================[ 106 ]===============================
def build_core_type(s_cdt):
    s_dt = nav_one(s_cdt).S_DT[17]()
    if s_dt.name == 'void':
        type_name = None
    elif s_dt.name == 'boolean':
        type_name = 'xs:boolean'
    elif s_dt.name == 'integer':
        type_name = 'xs:integer'
    elif s_dt.name == 'real':
        type_name = 'xs:decimal'
    elif s_dt.name == 'string':
        type_name = 'xs:string'
    elif s_dt.name == 'unique_id':
        type_name = 'xs:integer'
    else:
        type_name = None
    if type_name:
        mapped_type = ET.Element('xs:simpleType', name=s_dt.name)
        ET.SubElement(mapped_type, 'xs:restriction', base=type_name)
        return mapped_type
*** Original Comment ***: Builds an XML schema simpleType element based on the core data type provided.
*** Validation Score ***: 5
*** Validation Suggestion ***: The comment is already concise, clearly stating the function's core responsibility without redundancy.
*** Final Score ***: 5
*** Final Comment ***: Builds an XML schema simpleType element based on the core data type provided.


===============================[ 107 ]===============================
def _filter_deprecation_warnings():
    deprecation_filter = ('always', None, DeprecationWarning,
                          re.compile(r'^qiskit\.*', re.UNICODE), 0)
    try:
        warnings._add_filter(*deprecation_filter, append=False)
    except AttributeError:
        pass
    warnings.simplefilter('ignore', category=ChangedInMarshmallow3Warning)
*** Original Comment ***: This function configures the warnings filter to always show deprecation warnings originating from qiskit and to ignore warnings of type ChangedInMarshmallow3Warning.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'to always show' and 'of type' as they are implied by the context.
----------------Refine 1----------------
*** Refine comment ***: Configures the warnings filter to show deprecation warnings from qiskit and ignore ChangedInMarshmallow3Warning.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'Configures the warnings filter to' and starting with 'Shows deprecation warnings from qiskit and ignores ChangedInMarshmallow3Warning.'
*** Final Score ***: 4
*** Final Comment ***: Configures the warnings filter to show deprecation warnings from qiskit and ignore ChangedInMarshmallow3Warning.


===============================[ 108 ]===============================
def  make_file_list( args = None ):
    file_list = []
    if not args:
        args = sys.argv[1 :]
    for pathname in args:
        if string.find( pathname, '*' ) >= 0:
            newpath = glob.glob( pathname )
            newpath.sort()  
        else:
            newpath = [pathname]
        file_list.extend( newpath )
    if len( file_list ) == 0:
        file_list = None
    else:
        file_list = filter( file_exists, file_list )
    return file_list
*** Original Comment ***: Generates a list of file paths from command line arguments or provided arguments, including wildcard expansion and file existence check.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'from command line arguments or provided arguments, including' as it is implied by the context.
----------------Refine 1----------------
*** Refine comment ***: Generates a list of file paths with wildcard expansion and file existence check.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed; the comment is already concise.
*** Final Score ***: 5
*** Final Comment ***: Generates a list of file paths with wildcard expansion and file existence check.


===============================[ 109 ]===============================
def nwise(iter, n):
 iterset = [iter]
 while len(iterset) < n:
		iterset[-1:] = itertools.tee(iterset[-1])
  next(iterset[-1], None)
 return six.moves.zip(*iterset)
*** Original Comment ***: Creates an iterator that returns n-tuples of consecutive elements from the input iterable.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Creates an iterator that returns n-tuples of consecutive elements from the input iterable.


===============================[ 110 ]===============================
def handle_ping(self, payload):
        self.logger.info('server ping: %s' % payload)
        self.send('PONG %s' % payload, True)
*** Original Comment ***: The handle_ping method logs a 'server ping' message with the payload and sends back a 'PONG' response with the payload.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing redundancy and focusing on core actions: 'Logs 'server ping' and sends 'PONG' with payload.'
----------------Refine 1----------------
*** Refine comment ***: Logs 'server ping' and sends 'PONG' with payload.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed; the comment is already concise.
*** Final Score ***: 5
*** Final Comment ***: Logs 'server ping' and sends 'PONG' with payload.


===============================[ 111 ]===============================
def new(self, kind, *args, **kwargs):
        metaclass = self.find_metaclass(kind)
        return metaclass.new(*args, **kwargs)
*** Original Comment ***: Creates a new instance of a specified kind using the determined metaclass.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Creates a new instance of a specified kind using the determined metaclass.


===============================[ 112 ]===============================
def parse_questions(raw_page):
        raw_questions = json.loads(raw_page)
        questions = raw_questions['items']
        for question in questions:
            yield question
*** Original Comment ***: Parses a JSON string containing questions and yields each question.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Parses a JSON string containing questions and yields each question.


===============================[ 113 ]===============================
def wait_on_rate_limit(self, value):
        check_type(value, bool, may_be_none=False)
        self._wait_on_rate_limit = value
*** Original Comment ***: Set the behavior for rate limit waiting.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Set the behavior for rate limit waiting.


===============================[ 114 ]===============================
def add_text(self, text, cursor=None, justification=None):
        if cursor is None:
            cursor = self.page.cursor
        text = re.sub("\s\s+" , " ", text)
        if justification is None:
            justification = self.justification
        if '\n' in text:
            text_list = text.split('\n')
            for text in text_list:
                PDFText(self.session, self.page, text, self.font, self.text_color, cursor, justification, self.double_spacing)
                self.add_newline()
        else:
            PDFText(self.session, self.page, text, self.font, self.text_color, cursor, justification, self.double_spacing)
*** Original Comment ***: Adds text to a PDF page, handling newlines and text justification.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing redundancy and specifying the handling of newlines and justification, e.g., 'Adds text to PDF, managing newlines and justification.'
----------------Refine 1----------------
*** Refine comment ***: Adds text to PDF, managing newlines and justification.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'managing' as it is implied by the function's actions and by replacing 'Adds text to PDF' with 'Inserts text into PDF' for directness.
*** Final Score ***: 4
*** Final Comment ***: Adds text to PDF, managing newlines and justification.


===============================[ 115 ]===============================
def write_json(f: TextIO, deja_vu_sans_path: str,
               measurer: text_measurer.TextMeasurer,
               encodings: Iterable[str]) -> None:
    supported_characters = list(
        generate_supported_characters(deja_vu_sans_path))
    kerning_characters = ''.join(
        generate_encodeable_characters(supported_characters, encodings))
    char_to_length = calculate_character_to_length_mapping(measurer,
                                                           supported_characters)
    pair_to_kerning = calculate_pair_to_kern_mapping(measurer, char_to_length,
                                                     kerning_characters)
    json.dump(
        {'mean-character-length': statistics.mean(char_to_length.values()),
         'character-lengths': char_to_length,
         'kerning-characters': kerning_characters,
         'kerning-pairs': pair_to_kerning},
        f, sort_keys=True, indent=1)
*** Original Comment ***: Write character length and kerning information to a JSON file.
*** Validation Score ***: 5
*** Validation Suggestion ***: The comment is already concise and clearly states the core responsibility of the function.
*** Final Score ***: 5
*** Final Comment ***: Write character length and kerning information to a JSON file.


===============================[ 116 ]===============================
def _node_to_asn(self, node):
    if node.is_type(TokenType.identifier):
      return Identifier(node.svalue)
    elif node.is_type(TokenType.terminal):
      return Terminal(node.svalue)
    elif node.is_type(TokenType.option_group):
      expr = node.children[0]
      return OptionGroup(self._expression_to_asn(expr))
    elif node.is_type(TokenType.repetition_group):
      expr = node.children[0]
      return RepetitionGroup(self._expression_to_asn(expr))
    elif node.is_type(TokenType.grouping_group):
      expr = node.children[0]
      return GroupingGroup(self._expression_to_asn(expr))
    elif node.is_type(TokenType.special_handling):
      ident = node.children[0]
      return SpecialHandling(ident)
    elif node.is_type(TokenType.number):
      return Number(node.svalue)
    elif node.is_type((TokenType.operator, TokenType.op_mult, TokenType.op_add)):
      return OperatorNode(OPERATOR_INDEX[node.svalue], node.position)
    else:
      raise Exception("Unhandled parse tree node: {0}".format(node))
*** Original Comment ***: Converts a parse tree node to an abstract syntax node (ASN) based on its type.
*** Validation Score ***: 5
*** Validation Suggestion ***: The comment is already concise as it starts with an action verb, focuses on the core responsibility, and does not contain redundant information.
*** Final Score ***: 5
*** Final Comment ***: Converts a parse tree node to an abstract syntax node (ASN) based on its type.


===============================[ 117 ]===============================
def launch_ipython_5_shell(args):
    import IPython  
    from traitlets.config import Config
    c = Config()
    path = os.path.dirname(os.path.abspath(__file__))
    try:
        get_ipython  
        _print("WARNING: Running IPython within IPython.")
    except NameError:
        c.InteractiveShell.banner1 = 'SolveBio Python shell started.\n'
    c.InteractiveShellApp.exec_files = ['{}/ipython_init.py'.format(path)]
    IPython.start_ipython(argv=[], config=c)
*** Original Comment ***: Launches an IPython shell with a custom banner and initialization file.
*** Validation Score ***: 5
*** Validation Suggestion ***: The comment is already concise, clearly stating the action performed by the function without unnecessary details.
*** Final Score ***: 5
*** Final Comment ***: Launches an IPython shell with a custom banner and initialization file.


===============================[ 118 ]===============================
def indexes(self, collection=None):
        indexes = []
        for collection_name in self.collections():
            if collection and collection != collection_name:
                continue
            for index_name in self.db[collection_name].index_information():
                if index_name != '_id_':
                    indexes.append(index_name)
        return indexes
*** Original Comment ***: Retrieve a list of index names for the specified collection, excluding the default '_id_' index.
*** Validation Score ***: 5
*** Validation Suggestion ***: The comment is already concise and clear, no modification needed.
*** Final Score ***: 5
*** Final Comment ***: Retrieve a list of index names for the specified collection, excluding the default '_id_' index.


===============================[ 119 ]===============================
def redraw_canvas(self):
        from xdot_parser import XdotAttrParser
        xdot_parser = XdotAttrParser()
        canvas = self._component_default()
        for node in self.nodes:
            components = xdot_parser.parse_xdot_data( node._draw_ )
            canvas.add( *components )
            components = xdot_parser.parse_xdot_data( node._ldraw_ )
            canvas.add( *components )
        for edge in self.edges:
            components = xdot_parser.parse_xdot_data( edge._draw_ )
            canvas.add( *components )
            components = xdot_parser.parse_xdot_data( edge._ldraw_ )
            canvas.add( *components )
            components = xdot_parser.parse_xdot_data( edge._hdraw_ )
            canvas.add( *components )
            components = xdot_parser.parse_xdot_data( edge._tdraw_ )
            canvas.add( *components )
            components = xdot_parser.parse_xdot_data( edge._hldraw_ )
            canvas.add( *components )
            components = xdot_parser.parse_xdot_data( edge._tldraw_ )
            canvas.add( *components )
        self.component = canvas
        self.vp.request_redraw()
*** Original Comment ***: Parses and redraws the canvas with nodes and edges using xdot data.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'with nodes and edges' as it is implied by 'canvas'.
----------------Refine 1----------------
*** Refine comment ***: Parses and redraws the canvas using xdot data.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing redundancy and specifying the action: 'Redraws canvas with parsed xdot data.'
*** Final Score ***: 4
*** Final Comment ***: Parses and redraws the canvas using xdot data.


===============================[ 120 ]===============================
def _parse_dot_code_fired(self):
        parser = GodotDataParser()
        graph  = parser.parse_dot_data(self.dot_code)
        if graph is not None:
            self.model = graph
*** Original Comment ***: Parses the 'dot_code' attribute using the 'GodotDataParser' and sets the parsed graph to the 'model' attribute if a graph is successfully parsed.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing redundancy and simplifying the language: 'Parses 'dot_code' and updates 'model' with the resulting graph if not None.'
----------------Refine 1----------------
*** Refine comment ***: Parses 'dot_code' and updates 'model' with the resulting graph if not None.
*** Validation Score ***: 5
*** Validation Suggestion ***: No modification needed.
*** Final Score ***: 5
*** Final Comment ***: Parses 'dot_code' and updates 'model' with the resulting graph if not None.


===============================[ 121 ]===============================
def compute(self, *inputs, **kwargs):
        from deepy.core.neural_var import NeuralVariable
        from deepy.core.graph import graph
        if type(inputs[0]) != NeuralVariable:
            raise SystemError("The input of `compute` must be NeuralVar")
        dims = [t.dim() for t in inputs]
        if len(inputs) == 1:
            self.init(input_dim=dims[0])
        else:
            self.init(input_dims=dims)
        if self.parameters and not self._linked_block:
            self.belongs_to(graph.default_block())
        train_kwargs, _, _ = convert_to_theano_var(kwargs)
        output = self.compute_tensor(*[t.tensor for t in inputs], **train_kwargs)
        if type(output) != list and type(output) != tuple:
            return NeuralVariable(output, dim=self.output_dim)
        else:
            return [NeuralVariable(*item) for item in zip(output, self.output_dims)]
*** Original Comment ***: Compute the output of a neural network layer or block given the input tensors, ensuring they are NeuralVariable instances, and return the result as a NeuralVariable or list of NeuralVariables.
*** Validation Score ***: 3
*** Validation Suggestion ***: Be more concise by removing redundancy and focusing on the core action. Suggested comment: 'Computes and returns the neural network layer's output as NeuralVariable(s) from given inputs.'
----------------Refine 1----------------
*** Refine comment ***: Computes and returns the neural network layer's output as NeuralVariable(s) from given inputs.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'neural network layer's' as it is implied by context and 'from given inputs' as it is redundant.
*** Final Score ***: 4
*** Final Comment ***: Computes and returns the neural network layer's output as NeuralVariable(s) from given inputs.


===============================[ 122 ]===============================
def exec_command(ctx, section, command, map_files):
    try:
        logger.debug('Running exec command')
        existing_sections = config.ConfigFileResolver(config.LOCAL_CONFIG_FILE).sections()
        command = ' '.join(command)
        if section not in existing_sections:
            command = '{} {}'.format(section, command) if command else section
            section = None
        if not command:
            logger.warning('No command detected.')
            click.echo(exec_command.get_help(ctx))
            return
        settings = config.Settings(section=section)
        storage = STORAGES['s3'](settings=settings)
        conf = s3conf.S3Conf(storage=storage, settings=settings)
        env_vars = conf.get_envfile().as_dict()
        if env_vars.get('S3CONF_MAP') and map_files:
            conf.download_mapping(env_vars.get('S3CONF_MAP'))
        current_env = os.environ.copy()
        current_env.update(env_vars)
        logger.debug('Executing command "%s"', command)
        subprocess.run(shlex.split(command), env=current_env, check=True)
    except exceptions.EnvfilePathNotDefinedError:
        raise exceptions.EnvfilePathNotDefinedUsageError()
*** Original Comment ***: Execute a command with S3 configuration and environment variables, handling exceptions and logging appropriately.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by removing 'and logging appropriately' as it is implied by the context.
----------------Refine 1----------------
*** Refine comment ***: Execute a command with S3 configuration and environment variables, handling exceptions.
*** Validation Score ***: 5
*** Validation Suggestion ***: The comment is already concise, clearly stating the function's core responsibility without redundancy.
*** Final Score ***: 5
*** Final Comment ***: Execute a command with S3 configuration and environment variables, handling exceptions.


===============================[ 1 ]===============================
def _restrict_along_direction(value_and_gradients_function,
                              position,
                              direction):
  def _restricted_func(t):
    t = _broadcast(t, position)
    pt = position + tf.expand_dims(t, axis=-1) * direction
    objective_value, gradient = value_and_gradients_function(pt)
    return ValueAndGradient(
        x=t,
        f=objective_value,
        df=tf.reduce_sum(input_tensor=gradient * direction, axis=-1),
        full_gradient=gradient)
  return _restricted_func
*** Original Comment ***: Defines a function that restricts another function along a given direction, returning the value and projected gradient at that point.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by: (1. Remove 'another' as it is implied. 2. Replace 'returning the value and projected gradient at that point' with 'returns value and gradient projection.')
----------------Refine 1----------------
*** Refine comment ***: Defines a function that restricts a function along a given direction, returns value and gradient projection.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by: (1. Start with an action verb. 2. Remove 'a function that' as it is implied. 3. Eliminate 'given' as it is unnecessary.)
*** Final Score ***: 4
*** Final Comment ***: Defines a function that restricts a function along a given direction, returns value and gradient projection.


===============================[ 1 ]===============================
def _restrict_along_direction(value_and_gradients_function,
                              position,
                              direction):
  def _restricted_func(t):
    t = _broadcast(t, position)
    pt = position + tf.expand_dims(t, axis=-1) * direction
    objective_value, gradient = value_and_gradients_function(pt)
    return ValueAndGradient(
        x=t,
        f=objective_value,
        df=tf.reduce_sum(input_tensor=gradient * direction, axis=-1),
        full_gradient=gradient)
  return _restricted_func
*** Original Comment ***: Defines a function that restricts another function along a given direction, returning the value and projected gradient at that point.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by: 1. Starting with an action verb. 2. Removing 'another' as it is implied. 3. Eliminating 'at that point' since it's clear from context.
----------------Refine 1----------------
*** Refine comment ***: Restricts a function along a specified direction and returns its value and projected gradient.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by: 1. Starting with an action verb in the third person. 2. Removing 'specified' as it is implied by context. 3. Eliminating 'its' as the subject is clear.
*** Final Score ***: 4
*** Final Comment ***: Restricts a function along a specified direction and returns its value and projected gradient.


===============================[ 2 ]===============================
def _find_namespaces_from_child(parent, child, namespaces):
        for cur_child in parent.childNodes:
            if cur_child is child:
                return True
            if _MinidomXmlToObject._find_namespaces_from_child(cur_child, child, namespaces):
                for key in cur_child.attributes.keys():
                    if key.startswith('xmlns:') or key == 'xmlns':
                        namespaces[key] = cur_child.attributes[key]
                break
        return False
*** Original Comment ***: Recursively search for the child node within the parent, and if found, collect the namespaces defined on the path to the child.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by: 1. Starting with an action verb. 2. Removing 'within the parent' as it is implied. 3. Eliminating 'on the path to the child' as it is unnecessary detail.
----------------Refine 1----------------
*** Refine comment ***: Search recursively for the child node and collect namespaces if the child is found.
*** Validation Score ***: 4
*** Validation Suggestion ***: Be more concise by: 1. Starting with an action verb. 2. Removing 'recursively' as it is implied by the context. 3. Eliminating 'if the child is found' as it is redundant with 'search'.
*** Final Score ***: 4
*** Final Comment ***: Search recursively for the child node and collect namespaces if the child is found.


===============================[ 1 ]===============================
def _restrict_along_direction(value_and_gradients_function,
                              position,
                              direction):
  def _restricted_func(t):
    t = _broadcast(t, position)
    pt = position + tf.expand_dims(t, axis=-1) * direction
    objective_value, gradient = value_and_gradients_function(pt)
    return ValueAndGradient(
        x=t,
        f=objective_value,
        df=tf.reduce_sum(input_tensor=gradient * direction, axis=-1),
        full_gradient=gradient)
  return _restricted_func


===============================[ 1 ]===============================
def _restrict_along_direction(value_and_gradients_function,
                              position,
                              direction):
  def _restricted_func(t):
    t = _broadcast(t, position)
    pt = position + tf.expand_dims(t, axis=-1) * direction
    objective_value, gradient = value_and_gradients_function(pt)
    return ValueAndGradient(
        x=t,
        f=objective_value,
        df=tf.reduce_sum(input_tensor=gradient * direction, axis=-1),
        full_gradient=gradient)
  return _restricted_func


===============================[ 2 ]===============================
def _find_namespaces_from_child(parent, child, namespaces):
        for cur_child in parent.childNodes:
            if cur_child is child:
                return True
            if _MinidomXmlToObject._find_namespaces_from_child(cur_child, child, namespaces):
                for key in cur_child.attributes.keys():
                    if key.startswith('xmlns:') or key == 'xmlns':
                        namespaces[key] = cur_child.attributes[key]
                break
        return False


===============================[ 3 ]===============================
def CovInv(self):
        self.recurse(full_matrix=True)
        return self.tree.root.cinv


===============================[ 4 ]===============================
def _satisfies_wolfe(val_0,
                     val_c,
                     f_lim,
                     sufficient_decrease_param,
                     curvature_param):
  exact_wolfe_suff_dec = (sufficient_decrease_param * val_0.df >=
                          (val_c.f - val_0.f) / val_c.x)
  wolfe_curvature = val_c.df >= curvature_param * val_0.df
  exact_wolfe = exact_wolfe_suff_dec & wolfe_curvature
  approx_wolfe_applies = val_c.f <= f_lim
  approx_wolfe_suff_dec = ((2 * sufficient_decrease_param - 1) * val_0.df
                           >= val_c.df)
  approx_wolfe = approx_wolfe_applies & approx_wolfe_suff_dec & wolfe_curvature
  is_satisfied = exact_wolfe | approx_wolfe
  return is_satisfied


===============================[ 5 ]===============================
def get_misses(self):
        return [self.stats[cache_level]['MISS_count']/self.first_dim_factor
                for cache_level in range(len(self.machine['memory hierarchy']))]


===============================[ 6 ]===============================
def call(self, inputs):
    net = self.encoder_net(tf.cast(inputs, tf.float32))
    return ed.MultivariateNormalDiag(
        loc=net[..., :self.latent_size],
        scale_diag=tf.nn.softplus(net[..., self.latent_size:]),
        name="latent_code_posterior")


===============================[ 7 ]===============================
def ancestry(self, context):
        log.debug("get ancestry %s", context)
        if context is None:
            return
        if hasattr(context, 'im_class'):
            context = context.im_class
        elif hasattr(context, '__self__'):
            context = context.__self__.__class__
        if hasattr(context, '__module__'):
            ancestors = context.__module__.split('.')
        elif hasattr(context, '__name__'):
            ancestors = context.__name__.split('.')[:-1]
        else:
            raise TypeError("%s has no ancestors?" % context)
        while ancestors:
            log.debug(" %s ancestors %s", context, ancestors)
            yield resolve_name('.'.join(ancestors))
            ancestors.pop()


===============================[ 8 ]===============================
def get_item_abspath(self, identifier):
        admin_metadata = self.get_admin_metadata()
        uuid = admin_metadata["uuid"]
        dataset_cache_abspath = os.path.join(self._s3_cache_abspath, uuid)
        mkdir_parents(dataset_cache_abspath)
        bucket_fpath = self.data_key_prefix + identifier
        obj = self.s3resource.Object(self.bucket, bucket_fpath)
        relpath = obj.get()['Metadata']['handle']
        _, ext = os.path.splitext(relpath)
        local_item_abspath = os.path.join(
            dataset_cache_abspath,
            identifier + ext
        )
        if not os.path.isfile(local_item_abspath):
            tmp_local_item_abspath = local_item_abspath + ".tmp"
            self.s3resource.Bucket(self.bucket).download_file(
                bucket_fpath,
                tmp_local_item_abspath
            )
            os.rename(tmp_local_item_abspath, local_item_abspath)
        return local_item_abspath


===============================[ 9 ]===============================
def set_option(self, key, value):
        if key == "rtmpdump":
            key = "rtmp-rtmpdump"
        elif key == "rtmpdump-proxy":
            key = "rtmp-proxy"
        elif key == "errorlog":
            key = "subprocess-errorlog"
        elif key == "errorlog-path":
            key = "subprocess-errorlog-path"
        if key == "http-proxy":
            self.http.proxies["http"] = update_scheme("http://", value)
        elif key == "https-proxy":
            self.http.proxies["https"] = update_scheme("https://", value)
        elif key == "http-cookies":
            if isinstance(value, dict):
                self.http.cookies.update(value)
            else:
                self.http.parse_cookies(value)
        elif key == "http-headers":
            if isinstance(value, dict):
                self.http.headers.update(value)
            else:
                self.http.parse_headers(value)
        elif key == "http-query-params":
            if isinstance(value, dict):
                self.http.params.update(value)
            else:
                self.http.parse_query_params(value)
        elif key == "http-trust-env":
            self.http.trust_env = value
        elif key == "http-ssl-verify":
            self.http.verify = value
        elif key == "http-disable-dh":
            if value:
                requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += ':!DH'
                try:
                    requests.packages.urllib3.contrib.pyopenssl.DEFAULT_SSL_CIPHER_LIST =                        requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS.encode("ascii")
                except AttributeError:
                    pass
        elif key == "http-ssl-cert":
            self.http.cert = value
        elif key == "http-timeout":
            self.http.timeout = value
        else:
            self.options.set(key, value)


===============================[ 10 ]===============================
def paid_invoices_by_date(request, form):
    products = form.cleaned_data["product"]
    categories = form.cleaned_data["category"]
    invoices = commerce.Invoice.objects.filter(
        (
            Q(lineitem__product__in=products) |
            Q(lineitem__product__category__in=categories)
        ),
        status=commerce.Invoice.STATUS_PAID,
    )
    payments = commerce.PaymentBase.objects.all()
    payments = payments.filter(
        invoice__in=invoices,
    )
    payments = payments.order_by("invoice")
    invoice_max_time = payments.values("invoice").annotate(
        max_time=Max("time")
    )
    zero_value_invoices = invoices.filter(value=0)
    times = itertools.chain(
        (line["max_time"] for line in invoice_max_time),
        (invoice.issue_time for invoice in zero_value_invoices),
    )
    by_date = collections.defaultdict(int)
    for time in times:
        date = datetime.datetime(
            year=time.year, month=time.month, day=time.day
        )
        by_date[date] += 1
    data = [(date_, count) for date_, count in sorted(by_date.items())]
    data = [(date_.strftime("%Y-%m-%d"), count) for date_, count in data]
    return ListReport(
        "Paid Invoices By Date",
        ["date", "count"],
        data,
    )


===============================[ 11 ]===============================
def expand_and_standardize_dataset(response_index, response_header, data_set, col_vals, headers, standardizers, feats_to_ignore, columns_to_expand, outcome_trans_dict):
  modified_set = []
  for row_index, row in enumerate(data_set):
    new_row = []
    for col_index, val in enumerate(row):
      header = headers[col_index]
      if col_index == response_index:
        new_outcome = outcome_trans_dict[val]
        new_row.append(new_outcome)
      elif header in feats_to_ignore:
        pass
      elif header in columns_to_expand:
        for poss_val in col_vals[header]:
          if val == poss_val:
            new_cat_val = 1.0
          else:
            new_cat_val = -1.0
          new_row.append(new_cat_val)
      else:
        new_cont_val = float((val - standardizers[header]['mean']) / standardizers[header]['std_dev'])
        new_row.append(new_cont_val)
    modified_set.append(new_row)
  expanded_headers = []
  for header in headers:
    if header in feats_to_ignore:
      pass
    elif (header in columns_to_expand) and (header is not response_header):
      for poss_val in col_vals[header]:
        new_header = '{}_{}'.format(header,poss_val)
        expanded_headers.append(new_header)
    else:
      expanded_headers.append(header)
  return modified_set, expanded_headers


===============================[ 12 ]===============================
def _get_convert_to_tensor_fn(identifier):
  if identifier is None:
    return None
  if isinstance(identifier, six.string_types):
    identifier = str(identifier)
    return _deserialize(identifier)
  if isinstance(identifier, dict):
    return _deserialize(identifier)
  if isinstance(identifier, property):
    identifier = identifier.fget
  if callable(identifier):
    return identifier
  raise ValueError('Could not interpret '
                   'convert-to-tensor function identifier:', identifier)


===============================[ 13 ]===============================
def plot_rb_data(xdata, ydatas, yavg, yerr, fit, survival_prob, ax=None,
                 show_plt=True):
    if not HAS_MATPLOTLIB:
        raise ImportError('The function plot_rb_data needs matplotlib. '
                          'Run "pip install matplotlib" before.')
    if ax is None:
        plt.figure()
        ax = plt.gca()
    for ydata in ydatas:
        ax.plot(xdata, ydata, color='gray', linestyle='none', marker='x')
    ax.errorbar(xdata, yavg, yerr=yerr, color='r', linestyle='--', linewidth=3)
    ax.plot(xdata, survival_prob(xdata, *fit), color='blue', linestyle='-', linewidth=2)
    ax.tick_params(labelsize=14)
    ax.set_xlabel('Clifford Length', fontsize=16)
    ax.set_ylabel('Z', fontsize=16)
    ax.grid(True)
    if show_plt:
        plt.show()


===============================[ 14 ]===============================
def get_environ_vars(self):
        for key, val in os.environ.items():
            if _environ_prefix_re.search(key):
                yield (_environ_prefix_re.sub("", key).lower(), val)


===============================[ 15 ]===============================
def create_cname_record(self, name, values, ttl=60, weight=None, region=None,
                           set_identifier=None):
        self._halt_if_already_deleted()
        values = locals()
        del values['self']
        return self._add_record(CNAMEResourceRecordSet, **values)


===============================[ 16 ]===============================
def _wakeup(self):
        log.info("send: WAKEUP")
        for i in xrange(3):
            self.port.write('\n')  
            ack = self.port.read(len(self.WAKE_ACK))  
            log_raw('read', ack)
            if ack == self.WAKE_ACK:
                return
        raise NoDeviceException('Can not access weather station')


===============================[ 17 ]===============================
def _srvc_load_several_items(self, iterable, *args, **kwargs):
        for input_tuple in iterable:
            msg = input_tuple[0]
            item = input_tuple[1]
            if len(input_tuple) > 2:
                args = input_tuple[2]
            if len(input_tuple) > 3:
                kwargs = input_tuple[3]
            if len(input_tuple) > 4:
                raise RuntimeError('You shall not pass!')
            self.load(msg, item, *args, **kwargs)


===============================[ 18 ]===============================
def estimate_beats(self):
        if self._audio_percussive is None:
            self._audio_harmonic, self._audio_percussive = self.compute_HPSS()
        tempo, frames = librosa.beat.beat_track(
            y=self._audio_percussive, sr=self.sr,
            hop_length=self.hop_length)
        times = librosa.frames_to_time(frames, sr=self.sr,
                                       hop_length=self.hop_length)
        if len(times) > 0 and times[0] == 0:
            times = times[1:]
            frames = frames[1:]
        return times, frames


===============================[ 19 ]===============================
def updateLogicalInterface(self, logicalInterfaceId, name, schemaId, description=None):
        req = ApiClient.oneLogicalInterfaceUrl % (self.host, "/draft", logicalInterfaceId)
        body = {"name" : name, "schemaId" : schemaId, "id" : logicalInterfaceId}
        if description:
            body["description"] = description
        resp = requests.put(req, auth=self.credentials, headers={"Content-Type":"application/json"},
                            data=json.dumps(body),  verify=self.verify)
        if resp.status_code == 200:
            self.logger.debug("Logical interface updated")
        else:
            raise ibmiotf.APIException(resp.status_code, "HTTP error updating logical interface", resp)
        return resp.json()


===============================[ 20 ]===============================
def add_path(path, config=None):
    log.debug('Add path %s' % path)    
    if not path:
        return []
    added = []
    parent = os.path.dirname(path)
    if (parent
        and os.path.exists(os.path.join(path, '__init__.py'))):
        added.extend(add_path(parent, config))
    elif not path in sys.path:
        log.debug("insert %s into sys.path", path)
        sys.path.insert(0, path)
        added.append(path)
    if config and config.srcDirs:
        for dirname in config.srcDirs:
            dirpath = os.path.join(path, dirname)
            if os.path.isdir(dirpath):
                sys.path.insert(0, dirpath)
                added.append(dirpath)
    return added


===============================[ 21 ]===============================
def _convert_endpoint(endpoint):
    pb_endpoint = zipkin_pb2.Endpoint()
    if endpoint.service_name:
        pb_endpoint.service_name = endpoint.service_name
    if endpoint.port and endpoint.port != 0:
        pb_endpoint.port = endpoint.port
    if endpoint.ipv4:
        pb_endpoint.ipv4 = socket.inet_pton(socket.AF_INET, endpoint.ipv4)
    if endpoint.ipv6:
        pb_endpoint.ipv6 = socket.inet_pton(socket.AF_INET6, endpoint.ipv6)
    return pb_endpoint


===============================[ 22 ]===============================
def register_blueprints(app):
    app.register_blueprint(public.public_bp)
    app.register_blueprint(genes.genes_bp)
    app.register_blueprint(cases.cases_bp)
    app.register_blueprint(login.login_bp)
    app.register_blueprint(variants.variants_bp)
    app.register_blueprint(panels.panels_bp)
    app.register_blueprint(dashboard.dashboard_bp)
    app.register_blueprint(api.api_bp)
    app.register_blueprint(alignviewers.alignviewers_bp)
    app.register_blueprint(phenotypes.hpo_bp)
    app.register_blueprint(institutes.overview)


===============================[ 23 ]===============================
def peek(self, n):
        if self._len < n:
            raise ValueError("Not enough bytes in the queue")
        def part_generator(n):
            remaining = n
            for part in self._parts:
                if len(part) <= remaining:
                    yield part
                    remaining -= len(part)
                else:
                    yield part[:remaining]
                    remaining = 0 
                if remaining == 0:
                    break
        return "".join(part_generator(n))
===============================[ 13 ]===============================
def plot_rb_data(xdata, ydatas, yavg, yerr, fit, survival_prob, ax=None,
                 show_plt=True):
    if not HAS_MATPLOTLIB:
        raise ImportError('The function plot_rb_data needs matplotlib. '
                          'Run "pip install matplotlib" before.')
    if ax is None:
        plt.figure()
        ax = plt.gca()
    for ydata in ydatas:
        ax.plot(xdata, ydata, color='gray', linestyle='none', marker='x')
    ax.errorbar(xdata, yavg, yerr=yerr, color='r', linestyle='--', linewidth=3)
    ax.plot(xdata, survival_prob(xdata, *fit), color='blue', linestyle='-', linewidth=2)
    ax.tick_params(labelsize=14)
    ax.set_xlabel('Clifford Length', fontsize=16)
    ax.set_ylabel('Z', fontsize=16)
    ax.grid(True)
    if show_plt:
        plt.show()


===============================[ 13 ]===============================
def plot_rb_data(xdata, ydatas, yavg, yerr, fit, survival_prob, ax=None,
                 show_plt=True):
    if not HAS_MATPLOTLIB:
        raise ImportError('The function plot_rb_data needs matplotlib. '
                          'Run "pip install matplotlib" before.')
    if ax is None:
        plt.figure()
        ax = plt.gca()
    for ydata in ydatas:
        ax.plot(xdata, ydata, color='gray', linestyle='none', marker='x')
    ax.errorbar(xdata, yavg, yerr=yerr, color='r', linestyle='--', linewidth=3)
    ax.plot(xdata, survival_prob(xdata, *fit), color='blue', linestyle='-', linewidth=2)
    ax.tick_params(labelsize=14)
    ax.set_xlabel('Clifford Length', fontsize=16)
    ax.set_ylabel('Z', fontsize=16)
    ax.grid(True)
    if show_plt:
        plt.show()


===============================[ 13 ]===============================
def plot_rb_data(xdata, ydatas, yavg, yerr, fit, survival_prob, ax=None,
                 show_plt=True):
    if not HAS_MATPLOTLIB:
        raise ImportError('The function plot_rb_data needs matplotlib. '
                          'Run "pip install matplotlib" before.')
    if ax is None:
        plt.figure()
        ax = plt.gca()
    for ydata in ydatas:
        ax.plot(xdata, ydata, color='gray', linestyle='none', marker='x')
    ax.errorbar(xdata, yavg, yerr=yerr, color='r', linestyle='--', linewidth=3)
    ax.plot(xdata, survival_prob(xdata, *fit), color='blue', linestyle='-', linewidth=2)
    ax.tick_params(labelsize=14)
    ax.set_xlabel('Clifford Length', fontsize=16)
    ax.set_ylabel('Z', fontsize=16)
    ax.grid(True)
    if show_plt:
        plt.show()


===============================[ 13 ]===============================
def plot_rb_data(xdata, ydatas, yavg, yerr, fit, survival_prob, ax=None,
                 show_plt=True):
    if not HAS_MATPLOTLIB:
        raise ImportError('The function plot_rb_data needs matplotlib. '
                          'Run "pip install matplotlib" before.')
    if ax is None:
        plt.figure()
        ax = plt.gca()
    for ydata in ydatas:
        ax.plot(xdata, ydata, color='gray', linestyle='none', marker='x')
    ax.errorbar(xdata, yavg, yerr=yerr, color='r', linestyle='--', linewidth=3)
    ax.plot(xdata, survival_prob(xdata, *fit), color='blue', linestyle='-', linewidth=2)
    ax.tick_params(labelsize=14)
    ax.set_xlabel('Clifford Length', fontsize=16)
    ax.set_ylabel('Z', fontsize=16)
    ax.grid(True)
    if show_plt:
        plt.show()


===============================[ 2 ]===============================
def _find_namespaces_from_child(parent, child, namespaces):
        for cur_child in parent.childNodes:
            if cur_child is child:
                return True
            if _MinidomXmlToObject._find_namespaces_from_child(cur_child, child, namespaces):
                for key in cur_child.attributes.keys():
                    if key.startswith('xmlns:') or key == 'xmlns':
                        namespaces[key] = cur_child.attributes[key]
                break
        return False


===============================[ 2 ]===============================
def _find_namespaces_from_child(parent, child, namespaces):
        for cur_child in parent.childNodes:
            if cur_child is child:
                return True
            if _MinidomXmlToObject._find_namespaces_from_child(cur_child, child, namespaces):
                for key in cur_child.attributes.keys():
                    if key.startswith('xmlns:') or key == 'xmlns':
                        namespaces[key] = cur_child.attributes[key]
                break
        return False
===============================[ 2 ]===============================
def _find_namespaces_from_child(parent, child, namespaces):
        for cur_child in parent.childNodes:
            if cur_child is child:
                return True
            if _MinidomXmlToObject._find_namespaces_from_child(cur_child, child, namespaces):
                for key in cur_child.attributes.keys():
                    if key.startswith('xmlns:') or key == 'xmlns':
                        namespaces[key] = cur_child.attributes[key]
                break
        return False


===============================[ 2 ]===============================
def _find_namespaces_from_child(parent, child, namespaces):
        for cur_child in parent.childNodes:
            if cur_child is child:
                return True
            if _MinidomXmlToObject._find_namespaces_from_child(cur_child, child, namespaces):
                for key in cur_child.attributes.keys():
                    if key.startswith('xmlns:') or key == 'xmlns':
                        namespaces[key] = cur_child.attributes[key]
                break
        return False


===============================[ 2 ]===============================
def _find_namespaces_from_child(parent, child, namespaces):
        for cur_child in parent.childNodes:
            if cur_child is child:
                return True
            if _MinidomXmlToObject._find_namespaces_from_child(cur_child, child, namespaces):
                for key in cur_child.attributes.keys():
                    if key.startswith('xmlns:') or key == 'xmlns':
                        namespaces[key] = cur_child.attributes[key]
                break
        return False


===============================[ 1 ]===============================
def _restrict_along_direction(value_and_gradients_function,
                              position,
                              direction):
  def _restricted_func(t):
    t = _broadcast(t, position)
    pt = position + tf.expand_dims(t, axis=-1) * direction
    objective_value, gradient = value_and_gradients_function(pt)
    return ValueAndGradient(
        x=t,
        f=objective_value,
        df=tf.reduce_sum(input_tensor=gradient * direction, axis=-1),
        full_gradient=gradient)
  return _restricted_func


===============================[ 13 ]===============================
def plot_rb_data(xdata, ydatas, yavg, yerr, fit, survival_prob, ax=None,
                 show_plt=True):
    if not HAS_MATPLOTLIB:
        raise ImportError('The function plot_rb_data needs matplotlib. '
                          'Run "pip install matplotlib" before.')
    if ax is None:
        plt.figure()
        ax = plt.gca()
    for ydata in ydatas:
        ax.plot(xdata, ydata, color='gray', linestyle='none', marker='x')
    ax.errorbar(xdata, yavg, yerr=yerr, color='r', linestyle='--', linewidth=3)
    ax.plot(xdata, survival_prob(xdata, *fit), color='blue', linestyle='-', linewidth=2)
    ax.tick_params(labelsize=14)
    ax.set_xlabel('Clifford Length', fontsize=16)
    ax.set_ylabel('Z', fontsize=16)
    ax.grid(True)
    if show_plt:
        plt.show()


===============================[ 13 ]===============================
def plot_rb_data(xdata, ydatas, yavg, yerr, fit, survival_prob, ax=None,
                 show_plt=True):
    if not HAS_MATPLOTLIB:
        raise ImportError('The function plot_rb_data needs matplotlib. '
                          'Run "pip install matplotlib" before.')
    if ax is None:
        plt.figure()
        ax = plt.gca()
    for ydata in ydatas:
        ax.plot(xdata, ydata, color='gray', linestyle='none', marker='x')
    ax.errorbar(xdata, yavg, yerr=yerr, color='r', linestyle='--', linewidth=3)
    ax.plot(xdata, survival_prob(xdata, *fit), color='blue', linestyle='-', linewidth=2)
    ax.tick_params(labelsize=14)
    ax.set_xlabel('Clifford Length', fontsize=16)
    ax.set_ylabel('Z', fontsize=16)
    ax.grid(True)
    if show_plt:
        plt.show()


===============================[ 1 ]===============================
def _restrict_along_direction(value_and_gradients_function,
                              position,
                              direction):
  def _restricted_func(t):
    t = _broadcast(t, position)
    pt = position + tf.expand_dims(t, axis=-1) * direction
    objective_value, gradient = value_and_gradients_function(pt)
    return ValueAndGradient(
        x=t,
        f=objective_value,
        df=tf.reduce_sum(input_tensor=gradient * direction, axis=-1),
        full_gradient=gradient)
  return _restricted_func


===============================[ 1 ]===============================
def _restrict_along_direction(value_and_gradients_function,
                              position,
                              direction):
  def _restricted_func(t):
    t = _broadcast(t, position)
    pt = position + tf.expand_dims(t, axis=-1) * direction
    objective_value, gradient = value_and_gradients_function(pt)
    return ValueAndGradient(
        x=t,
        f=objective_value,
        df=tf.reduce_sum(input_tensor=gradient * direction, axis=-1),
        full_gradient=gradient)
  return _restricted_func
===============================[ 1 ]===============================
def _restrict_along_direction(value_and_gradients_function,
                              position,
                              direction):
  def _restricted_func(t):
    t = _broadcast(t, position)
    pt = position + tf.expand_dims(t, axis=-1) * direction
    objective_value, gradient = value_and_gradients_function(pt)
    return ValueAndGradient(
        x=t,
        f=objective_value,
        df=tf.reduce_sum(input_tensor=gradient * direction, axis=-1),
        full_gradient=gradient)
  return _restricted_func
===============================[ 1 ]===============================
def _restrict_along_direction(value_and_gradients_function,
                              position,
                              direction):
  def _restricted_func(t):
    t = _broadcast(t, position)
    pt = position + tf.expand_dims(t, axis=-1) * direction
    objective_value, gradient = value_and_gradients_function(pt)
    return ValueAndGradient(
        x=t,
        f=objective_value,
        df=tf.reduce_sum(input_tensor=gradient * direction, axis=-1),
        full_gradient=gradient)
  return _restricted_func
===============================[ 1 ]===============================
def _restrict_along_direction(value_and_gradients_function,
                              position,
                              direction):
  def _restricted_func(t):
    t = _broadcast(t, position)
    pt = position + tf.expand_dims(t, axis=-1) * direction
    objective_value, gradient = value_and_gradients_function(pt)
    return ValueAndGradient(
        x=t,
        f=objective_value,
        df=tf.reduce_sum(input_tensor=gradient * direction, axis=-1),
        full_gradient=gradient)
  return _restricted_func


===============================[ 2 ]===============================
def _find_namespaces_from_child(parent, child, namespaces):
        for cur_child in parent.childNodes:
            if cur_child is child:
                return True
            if _MinidomXmlToObject._find_namespaces_from_child(cur_child, child, namespaces):
                for key in cur_child.attributes.keys():
                    if key.startswith('xmlns:') or key == 'xmlns':
                        namespaces[key] = cur_child.attributes[key]
                break
        return False


===============================[ 3 ]===============================
def CovInv(self):
        self.recurse(full_matrix=True)
        return self.tree.root.cinv
===============================[ 1 ]===============================
def _restrict_along_direction(value_and_gradients_function,
                              position,
                              direction):
  def _restricted_func(t):
    t = _broadcast(t, position)
    pt = position + tf.expand_dims(t, axis=-1) * direction
    objective_value, gradient = value_and_gradients_function(pt)
    return ValueAndGradient(
        x=t,
        f=objective_value,
        df=tf.reduce_sum(input_tensor=gradient * direction, axis=-1),
        full_gradient=gradient)
  return _restricted_func
===============================[ 1 ]===============================
def _restrict_along_direction(value_and_gradients_function,
                              position,
                              direction):
  def _restricted_func(t):
    t = _broadcast(t, position)
    pt = position + tf.expand_dims(t, axis=-1) * direction
    objective_value, gradient = value_and_gradients_function(pt)
    return ValueAndGradient(
        x=t,
        f=objective_value,
        df=tf.reduce_sum(input_tensor=gradient * direction, axis=-1),
        full_gradient=gradient)
  return _restricted_func
===============================[ 1 ]===============================
def _restrict_along_direction(value_and_gradients_function,
                              position,
                              direction):
  def _restricted_func(t):
    t = _broadcast(t, position)
    pt = position + tf.expand_dims(t, axis=-1) * direction
    objective_value, gradient = value_and_gradients_function(pt)
    return ValueAndGradient(
        x=t,
        f=objective_value,
        df=tf.reduce_sum(input_tensor=gradient * direction, axis=-1),
        full_gradient=gradient)
  return _restricted_func


===============================[ 2 ]===============================
def _find_namespaces_from_child(parent, child, namespaces):
        for cur_child in parent.childNodes:
            if cur_child is child:
                return True
            if _MinidomXmlToObject._find_namespaces_from_child(cur_child, child, namespaces):
                for key in cur_child.attributes.keys():
                    if key.startswith('xmlns:') or key == 'xmlns':
                        namespaces[key] = cur_child.attributes[key]
                break
        return False


===============================[ 3 ]===============================
def CovInv(self):
        self.recurse(full_matrix=True)
        return self.tree.root.cinv


===============================[ 4 ]===============================
def _satisfies_wolfe(val_0,
                     val_c,
                     f_lim,
                     sufficient_decrease_param,
                     curvature_param):
  exact_wolfe_suff_dec = (sufficient_decrease_param * val_0.df >=
                          (val_c.f - val_0.f) / val_c.x)
  wolfe_curvature = val_c.df >= curvature_param * val_0.df
  exact_wolfe = exact_wolfe_suff_dec & wolfe_curvature
  approx_wolfe_applies = val_c.f <= f_lim
  approx_wolfe_suff_dec = ((2 * sufficient_decrease_param - 1) * val_0.df
                           >= val_c.df)
  approx_wolfe = approx_wolfe_applies & approx_wolfe_suff_dec & wolfe_curvature
  is_satisfied = exact_wolfe | approx_wolfe
  return is_satisfied


===============================[ 5 ]===============================
def get_misses(self):
        return [self.stats[cache_level]['MISS_count']/self.first_dim_factor
                for cache_level in range(len(self.machine['memory hierarchy']))]


===============================[ 6 ]===============================
def call(self, inputs):
    net = self.encoder_net(tf.cast(inputs, tf.float32))
    return ed.MultivariateNormalDiag(
        loc=net[..., :self.latent_size],
        scale_diag=tf.nn.softplus(net[..., self.latent_size:]),
        name="latent_code_posterior")


===============================[ 7 ]===============================
def ancestry(self, context):
        log.debug("get ancestry %s", context)
        if context is None:
            return
        if hasattr(context, 'im_class'):
            context = context.im_class
        elif hasattr(context, '__self__'):
            context = context.__self__.__class__
        if hasattr(context, '__module__'):
            ancestors = context.__module__.split('.')
        elif hasattr(context, '__name__'):
            ancestors = context.__name__.split('.')[:-1]
        else:
            raise TypeError("%s has no ancestors?" % context)
        while ancestors:
            log.debug(" %s ancestors %s", context, ancestors)
            yield resolve_name('.'.join(ancestors))
            ancestors.pop()


===============================[ 8 ]===============================
def get_item_abspath(self, identifier):
        admin_metadata = self.get_admin_metadata()
        uuid = admin_metadata["uuid"]
        dataset_cache_abspath = os.path.join(self._s3_cache_abspath, uuid)
        mkdir_parents(dataset_cache_abspath)
        bucket_fpath = self.data_key_prefix + identifier
        obj = self.s3resource.Object(self.bucket, bucket_fpath)
        relpath = obj.get()['Metadata']['handle']
        _, ext = os.path.splitext(relpath)
        local_item_abspath = os.path.join(
            dataset_cache_abspath,
            identifier + ext
        )
        if not os.path.isfile(local_item_abspath):
            tmp_local_item_abspath = local_item_abspath + ".tmp"
            self.s3resource.Bucket(self.bucket).download_file(
                bucket_fpath,
                tmp_local_item_abspath
            )
            os.rename(tmp_local_item_abspath, local_item_abspath)
        return local_item_abspath


===============================[ 9 ]===============================
def set_option(self, key, value):
        if key == "rtmpdump":
            key = "rtmp-rtmpdump"
        elif key == "rtmpdump-proxy":
            key = "rtmp-proxy"
        elif key == "errorlog":
            key = "subprocess-errorlog"
        elif key == "errorlog-path":
            key = "subprocess-errorlog-path"
        if key == "http-proxy":
            self.http.proxies["http"] = update_scheme("http://", value)
        elif key == "https-proxy":
            self.http.proxies["https"] = update_scheme("https://", value)
        elif key == "http-cookies":
            if isinstance(value, dict):
                self.http.cookies.update(value)
            else:
                self.http.parse_cookies(value)
        elif key == "http-headers":
            if isinstance(value, dict):
                self.http.headers.update(value)
            else:
                self.http.parse_headers(value)
        elif key == "http-query-params":
            if isinstance(value, dict):
                self.http.params.update(value)
            else:
                self.http.parse_query_params(value)
        elif key == "http-trust-env":
            self.http.trust_env = value
        elif key == "http-ssl-verify":
            self.http.verify = value
        elif key == "http-disable-dh":
            if value:
                requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += ':!DH'
                try:
                    requests.packages.urllib3.contrib.pyopenssl.DEFAULT_SSL_CIPHER_LIST =                        requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS.encode("ascii")
                except AttributeError:
                    pass
        elif key == "http-ssl-cert":
            self.http.cert = value
        elif key == "http-timeout":
            self.http.timeout = value
        else:
            self.options.set(key, value)


===============================[ 10 ]===============================
def paid_invoices_by_date(request, form):
    products = form.cleaned_data["product"]
    categories = form.cleaned_data["category"]
    invoices = commerce.Invoice.objects.filter(
        (
            Q(lineitem__product__in=products) |
            Q(lineitem__product__category__in=categories)
        ),
        status=commerce.Invoice.STATUS_PAID,
    )
    payments = commerce.PaymentBase.objects.all()
    payments = payments.filter(
        invoice__in=invoices,
    )
    payments = payments.order_by("invoice")
    invoice_max_time = payments.values("invoice").annotate(
        max_time=Max("time")
    )
    zero_value_invoices = invoices.filter(value=0)
    times = itertools.chain(
        (line["max_time"] for line in invoice_max_time),
        (invoice.issue_time for invoice in zero_value_invoices),
    )
    by_date = collections.defaultdict(int)
    for time in times:
        date = datetime.datetime(
            year=time.year, month=time.month, day=time.day
        )
        by_date[date] += 1
    data = [(date_, count) for date_, count in sorted(by_date.items())]
    data = [(date_.strftime("%Y-%m-%d"), count) for date_, count in data]
    return ListReport(
        "Paid Invoices By Date",
        ["date", "count"],
        data,
    )


===============================[ 11 ]===============================
def expand_and_standardize_dataset(response_index, response_header, data_set, col_vals, headers, standardizers, feats_to_ignore, columns_to_expand, outcome_trans_dict):
  modified_set = []
  for row_index, row in enumerate(data_set):
    new_row = []
    for col_index, val in enumerate(row):
      header = headers[col_index]
      if col_index == response_index:
        new_outcome = outcome_trans_dict[val]
        new_row.append(new_outcome)
      elif header in feats_to_ignore:
        pass
      elif header in columns_to_expand:
        for poss_val in col_vals[header]:
          if val == poss_val:
            new_cat_val = 1.0
          else:
            new_cat_val = -1.0
          new_row.append(new_cat_val)
      else:
        new_cont_val = float((val - standardizers[header]['mean']) / standardizers[header]['std_dev'])
        new_row.append(new_cont_val)
    modified_set.append(new_row)
  expanded_headers = []
  for header in headers:
    if header in feats_to_ignore:
      pass
    elif (header in columns_to_expand) and (header is not response_header):
      for poss_val in col_vals[header]:
        new_header = '{}_{}'.format(header,poss_val)
        expanded_headers.append(new_header)
    else:
      expanded_headers.append(header)
  return modified_set, expanded_headers


===============================[ 12 ]===============================
def _get_convert_to_tensor_fn(identifier):
  if identifier is None:
    return None
  if isinstance(identifier, six.string_types):
    identifier = str(identifier)
    return _deserialize(identifier)
  if isinstance(identifier, dict):
    return _deserialize(identifier)
  if isinstance(identifier, property):
    identifier = identifier.fget
  if callable(identifier):
    return identifier
  raise ValueError('Could not interpret '
                   'convert-to-tensor function identifier:', identifier)


===============================[ 13 ]===============================
def plot_rb_data(xdata, ydatas, yavg, yerr, fit, survival_prob, ax=None,
                 show_plt=True):
    if not HAS_MATPLOTLIB:
        raise ImportError('The function plot_rb_data needs matplotlib. '
                          'Run "pip install matplotlib" before.')
    if ax is None:
        plt.figure()
        ax = plt.gca()
    for ydata in ydatas:
        ax.plot(xdata, ydata, color='gray', linestyle='none', marker='x')
    ax.errorbar(xdata, yavg, yerr=yerr, color='r', linestyle='--', linewidth=3)
    ax.plot(xdata, survival_prob(xdata, *fit), color='blue', linestyle='-', linewidth=2)
    ax.tick_params(labelsize=14)
    ax.set_xlabel('Clifford Length', fontsize=16)
    ax.set_ylabel('Z', fontsize=16)
    ax.grid(True)
    if show_plt:
        plt.show()


===============================[ 14 ]===============================
def get_environ_vars(self):
        for key, val in os.environ.items():
            if _environ_prefix_re.search(key):
                yield (_environ_prefix_re.sub("", key).lower(), val)


===============================[ 15 ]===============================
def create_cname_record(self, name, values, ttl=60, weight=None, region=None,
                           set_identifier=None):
        self._halt_if_already_deleted()
        values = locals()
        del values['self']
        return self._add_record(CNAMEResourceRecordSet, **values)


===============================[ 16 ]===============================
def _wakeup(self):
        log.info("send: WAKEUP")
        for i in xrange(3):
            self.port.write('\n')  
            ack = self.port.read(len(self.WAKE_ACK))  
            log_raw('read', ack)
            if ack == self.WAKE_ACK:
                return
        raise NoDeviceException('Can not access weather station')


===============================[ 17 ]===============================
def _srvc_load_several_items(self, iterable, *args, **kwargs):
        for input_tuple in iterable:
            msg = input_tuple[0]
            item = input_tuple[1]
            if len(input_tuple) > 2:
                args = input_tuple[2]
            if len(input_tuple) > 3:
                kwargs = input_tuple[3]
            if len(input_tuple) > 4:
                raise RuntimeError('You shall not pass!')
            self.load(msg, item, *args, **kwargs)


===============================[ 18 ]===============================
def estimate_beats(self):
        if self._audio_percussive is None:
            self._audio_harmonic, self._audio_percussive = self.compute_HPSS()
        tempo, frames = librosa.beat.beat_track(
            y=self._audio_percussive, sr=self.sr,
            hop_length=self.hop_length)
        times = librosa.frames_to_time(frames, sr=self.sr,
                                       hop_length=self.hop_length)
        if len(times) > 0 and times[0] == 0:
            times = times[1:]
            frames = frames[1:]
        return times, frames


===============================[ 19 ]===============================
def updateLogicalInterface(self, logicalInterfaceId, name, schemaId, description=None):
        req = ApiClient.oneLogicalInterfaceUrl % (self.host, "/draft", logicalInterfaceId)
        body = {"name" : name, "schemaId" : schemaId, "id" : logicalInterfaceId}
        if description:
            body["description"] = description
        resp = requests.put(req, auth=self.credentials, headers={"Content-Type":"application/json"},
                            data=json.dumps(body),  verify=self.verify)
        if resp.status_code == 200:
            self.logger.debug("Logical interface updated")
        else:
            raise ibmiotf.APIException(resp.status_code, "HTTP error updating logical interface", resp)
        return resp.json()


===============================[ 20 ]===============================
def add_path(path, config=None):
    log.debug('Add path %s' % path)    
    if not path:
        return []
    added = []
    parent = os.path.dirname(path)
    if (parent
        and os.path.exists(os.path.join(path, '__init__.py'))):
        added.extend(add_path(parent, config))
    elif not path in sys.path:
        log.debug("insert %s into sys.path", path)
        sys.path.insert(0, path)
        added.append(path)
    if config and config.srcDirs:
        for dirname in config.srcDirs:
            dirpath = os.path.join(path, dirname)
            if os.path.isdir(dirpath):
                sys.path.insert(0, dirpath)
                added.append(dirpath)
    return added


===============================[ 21 ]===============================
def _convert_endpoint(endpoint):
    pb_endpoint = zipkin_pb2.Endpoint()
    if endpoint.service_name:
        pb_endpoint.service_name = endpoint.service_name
    if endpoint.port and endpoint.port != 0:
        pb_endpoint.port = endpoint.port
    if endpoint.ipv4:
        pb_endpoint.ipv4 = socket.inet_pton(socket.AF_INET, endpoint.ipv4)
    if endpoint.ipv6:
        pb_endpoint.ipv6 = socket.inet_pton(socket.AF_INET6, endpoint.ipv6)
    return pb_endpoint


===============================[ 22 ]===============================
def register_blueprints(app):
    app.register_blueprint(public.public_bp)
    app.register_blueprint(genes.genes_bp)
    app.register_blueprint(cases.cases_bp)
    app.register_blueprint(login.login_bp)
    app.register_blueprint(variants.variants_bp)
    app.register_blueprint(panels.panels_bp)
    app.register_blueprint(dashboard.dashboard_bp)
    app.register_blueprint(api.api_bp)
    app.register_blueprint(alignviewers.alignviewers_bp)
    app.register_blueprint(phenotypes.hpo_bp)
    app.register_blueprint(institutes.overview)


===============================[ 23 ]===============================
def peek(self, n):
        if self._len < n:
            raise ValueError("Not enough bytes in the queue")
        def part_generator(n):
            remaining = n
            for part in self._parts:
                if len(part) <= remaining:
                    yield part
                    remaining -= len(part)
                else:
                    yield part[:remaining]
                    remaining = 0 
                if remaining == 0:
                    break
        return "".join(part_generator(n))


===============================[ 24 ]===============================
def get_memory_info(self):
        rss, vms = _psutil_bsd.get_process_memory_info(self.pid)[:2]
        return nt_meminfo(rss, vms)


===============================[ 25 ]===============================
def dist_in_usersite(dist):
    norm_path = normalize_path(dist_location(dist))
    return norm_path.startswith(normalize_path(user_site))


===============================[ 26 ]===============================
def quadrature_scheme_softmaxnormal_quantiles(
    normal_loc, normal_scale, quadrature_size,
    validate_args=False, name=None):
  with tf.name_scope(name or "softmax_normal_grid_and_probs"):
    normal_loc = tf.convert_to_tensor(value=normal_loc, name="normal_loc")
    dt = dtype_util.base_dtype(normal_loc.dtype)
    normal_scale = tf.convert_to_tensor(
        value=normal_scale, dtype=dt, name="normal_scale")
    normal_scale = maybe_check_quadrature_param(
        normal_scale, "normal_scale", validate_args)
    dist = normal.Normal(loc=normal_loc, scale=normal_scale)
    def _get_batch_ndims():
      ndims = tensorshape_util.rank(dist.batch_shape)
      if ndims is None:
        ndims = tf.shape(input=dist.batch_shape_tensor())[0]
      return ndims
    batch_ndims = _get_batch_ndims()
    def _get_final_shape(qs):
      bs = tensorshape_util.with_rank_at_least(dist.batch_shape, 1)
      num_components = tf.compat.dimension_value(bs[-1])
      if num_components is not None:
        num_components += 1
      tail = tf.TensorShape([num_components, qs])
      return bs[:-1].concatenate(tail)
    def _compute_quantiles():
      zero = tf.zeros([], dtype=dist.dtype)
      edges = tf.linspace(zero, 1., quadrature_size + 3)[1:-1]
      edges = tf.reshape(
          edges,
          shape=tf.concat(
              [[-1], tf.ones([batch_ndims], dtype=tf.int32)], axis=0))
      quantiles = dist.quantile(edges)
      quantiles = softmax_centered_bijector.SoftmaxCentered().forward(quantiles)
      perm = tf.concat([tf.range(1, 1 + batch_ndims), [0]], axis=0)
      quantiles = tf.transpose(a=quantiles, perm=perm)
      tensorshape_util.set_shape(
          quantiles, _get_final_shape(quadrature_size + 1))
      return quantiles
    quantiles = _compute_quantiles()
    grid = (quantiles[..., :-1] + quantiles[..., 1:]) / 2.
    tensorshape_util.set_shape(grid, _get_final_shape(quadrature_size))
    probs = tf.fill(
        dims=[quadrature_size], value=1. / tf.cast(quadrature_size, dist.dtype))
    return grid, probs


===============================[ 27 ]===============================
def _run_sql(self, sql, params, raw=True, output=False):
        toget = 'source_raw' if raw else 'source'
        sqlfrom = "history"
        if output:
            sqlfrom = "history LEFT JOIN output_history USING (session, line)"
            toget = "history.%s, output_history.output" % toget
        cur = self.db.execute("SELECT session, line, %s FROM %s " %                                (toget, sqlfrom) + sql, params)
        if output:    
            return ((ses, lin, (inp, out)) for ses, lin, inp, out in cur)
        return cur


===============================[ 28 ]===============================
def read_image(filepath):
  im_bytes = tf.io.read_file(filepath)
  im = tf.image.decode_image(im_bytes, channels=CHANNELS)
  im = tf.image.convert_image_dtype(im, tf.float32)
  return im


===============================[ 29 ]===============================
def highlight(string, keywords, cls_name='highlighted'):
    if not keywords:
        return string
    if not string:
        return ''
    include, exclude = get_text_tokenizer(keywords)
    highlighted = highlight_text(include, string, cls_name)
    return highlighted


===============================[ 30 ]===============================
def expand_to_vector(x, tensor_name=None, op_name=None, validate_args=False):
  with tf.name_scope(op_name or "expand_to_vector"):
    x = tf.convert_to_tensor(value=x, name="x")
    ndims = tensorshape_util.rank(x.shape)
    if ndims is None:
      if validate_args:
        x = with_dependencies([
            assert_util.assert_rank_at_most(
                x, 1, message="Input is neither scalar nor vector.")
        ], x)
      ndims = tf.rank(x)
      expanded_shape = pick_vector(
          tf.equal(ndims, 0), np.array([1], dtype=np.int32), tf.shape(input=x))
      return tf.reshape(x, expanded_shape)
    elif ndims == 0:
      x_const = tf.get_static_value(x)
      if x_const is not None:
        return tf.convert_to_tensor(
            value=dtype_util.as_numpy_dtype(x.dtype)([x_const]),
            name=tensor_name)
      else:
        return tf.reshape(x, [1])
    elif ndims != 1:
      raise ValueError("Input is neither scalar nor vector.")
    return x


===============================[ 31 ]===============================
def godot_options(self, info):
        if info.initialized:
            self.edit_traits( parent = info.ui.control,
                              kind   = "livemodal",
                              view   = "options_view" )


===============================[ 32 ]===============================
def _get_rule_transform(self, rule):
    rd = self._find_directive(lambda d: d.name == "rule" and d.args.get("name") == rule.name)
    if rd:
      args = rd.args
    else:
      args = {}
    transform = args.get("transform", "retype")
    if transform == "retype":
      new_name = args.get("to_type", "TokenType.{0}".format(rule.name))
      return ".retyped({0})".format(new_name)
    elif transform == "compress":
      new_name = args.get("to_type", "TokenType.{0}".format(rule.name))
      if new_name == "identity":
        return ".compressed()"
      else:
        return ".compressed({0})".format(new_name)
    elif transform == "identity":
      return ""


===============================[ 33 ]===============================
def picard_mark_duplicates(job, bam, bai, validation_stringency='LENIENT'):
    work_dir = job.fileStore.getLocalTempDir()
    job.fileStore.readGlobalFile(bam, os.path.join(work_dir, 'sorted.bam'))
    job.fileStore.readGlobalFile(bai, os.path.join(work_dir, 'sorted.bai'))
    command = ['MarkDuplicates',
               'INPUT=sorted.bam',
               'OUTPUT=mkdups.bam',
               'METRICS_FILE=metrics.txt',
               'ASSUME_SORTED=true',
               'CREATE_INDEX=true',
               'VALIDATION_STRINGENCY=%s' % validation_stringency.upper()]
    docker_parameters = ['--rm',
                         '--log-driver', 'none',
                         '-e', 'JAVA_OPTIONS=-Djava.io.tmpdir=/data/ -Xmx{}'.format(job.memory),
                         '-v', '{}:/data'.format(work_dir)]
    start_time = time.time()
    dockerCall(job=job, workDir=work_dir,
               parameters=command,
               tool='quay.io/ucsc_cgl/picardtools:1.95--dd5ac549b95eb3e5d166a5e310417ef13651994e',
               dockerParameters=docker_parameters)
    end_time = time.time()
    _log_runtime(job, start_time, end_time, "Picard MarkDuplicates")
    bam = job.fileStore.writeGlobalFile(os.path.join(work_dir, 'mkdups.bam'))
    bai = job.fileStore.writeGlobalFile(os.path.join(work_dir, 'mkdups.bai'))
    return bam, bai


===============================[ 34 ]===============================
def patch_transport_fake_push_producer(transport):
    patch_if_missing(transport, 'pauseProducing', lambda: None)
    patch_if_missing(transport, 'resumeProducing', lambda: None)
    patch_if_missing(transport, 'stopProducing', transport.loseConnection)


===============================[ 35 ]===============================
def format_function(
    func_body,
    func_type=None,
    indent=2,
    format_locals=True,
):
    if func_type is None:
        yield 'func'
    else:
        param_section = ' (param {})'.format(' '.join(
            map(format_lang_type, func_type.param_types)
        )) if func_type.param_types else ''
        result_section = ' (result {})'.format(
            format_lang_type(func_type.return_type)
        ) if func_type.return_type else ''
        yield 'func' + param_section + result_section
    if format_locals and func_body.locals:
        yield '(locals {})'.format(' '.join(itertools.chain.from_iterable(
            itertools.repeat(format_lang_type(x.type), x.count)
            for x in func_body.locals
        )))
    level = 1
    for cur_insn in decode_bytecode(func_body.code):
        if cur_insn.op.flags & INSN_LEAVE_BLOCK:
            level -= 1
        yield ' ' * (level * indent) + format_instruction(cur_insn)
        if cur_insn.op.flags & INSN_ENTER_BLOCK:
            level += 1


===============================[ 36 ]===============================
def get_description(self):
        def split_header(s, get_header=True):
            s = s.lstrip().rstrip()
            parts = s.splitlines()
            if parts[0].startswith('#'):
                if get_header:
                    header = re.sub('#+\s*', '', parts.pop(0))
                    if not parts:
                        return header, ''
                else:
                    header = ''
                rest = '\n'.join(parts).lstrip().split('\n\n')
                desc = rest[0].replace('\n', ' ')
                return header, desc
            else:
                if get_header:
                    if parts[0].startswith(('=', '-')):
                        parts = parts[1:]
                    header = parts.pop(0)
                    if parts and parts[0].startswith(('=', '-')):
                        parts.pop(0)
                    if not parts:
                        return header, ''
                else:
                    header = ''
                rest = '\n'.join(parts).lstrip().split('\n\n')
                desc = rest[0].replace('\n', ' ')
                return header, desc
        first_cell = self.nb['cells'][0]
        if not first_cell['cell_type'] == 'markdown':
            return '', ''
        header, desc = split_header(first_cell['source'])
        if not desc and len(self.nb['cells']) > 1:
            second_cell = self.nb['cells'][1]
            if second_cell['cell_type'] == 'markdown':
                _, desc = split_header(second_cell['source'], False)
        return header, desc


===============================[ 37 ]===============================
def has_comment(src):
    readline = StringIO(src).readline
    toktypes = set()
    try:
        for t in tokenize.generate_tokens(readline):
            toktypes.add(t[0])
    except tokenize.TokenError:
        pass
    return(tokenize.COMMENT in toktypes)


===============================[ 38 ]===============================
def _topic(self, topic):
        if self.int_id >= 0:
            base = "engine.%i" % self.int_id
        else:
            base = "kernel.%s" % self.ident
        return py3compat.cast_bytes("%s.%s" % (base, topic))


===============================[ 39 ]===============================
def rdiscover_modules(directory):
    found = list()
    if os.path.isdir(directory):
        for entry in os.listdir(directory):
            next_dir = os.path.join(directory, entry)
            if os.path.isfile(os.path.join(next_dir, MODULE_INIT_FILE)):
                modules = _search_for_modules(next_dir, True, entry)
                found.extend(modules)
    return found


===============================[ 40 ]===============================
def _event_filter_console_keypress(self, event):
        intercepted = False
        cursor = self._control.textCursor()
        position = cursor.position()
        key = event.key()
        ctrl_down = self._control_key_down(event.modifiers())
        alt_down = event.modifiers() & QtCore.Qt.AltModifier
        shift_down = event.modifiers() & QtCore.Qt.ShiftModifier
        if event.matches(QtGui.QKeySequence.Copy):
            self.copy()
            intercepted = True
        elif event.matches(QtGui.QKeySequence.Cut):
            self.cut()
            intercepted = True
        elif event.matches(QtGui.QKeySequence.Paste):
            self.paste()
            intercepted = True
        elif key in (QtCore.Qt.Key_Return, QtCore.Qt.Key_Enter):
            intercepted = True
            self._cancel_completion()
            if self._in_buffer(position):
                if self._reading:
                    self._append_plain_text('\n')
                    self._reading = False
                    if self._reading_callback:
                        self._reading_callback()
                elif not self._executing:
                    cursor.movePosition(QtGui.QTextCursor.End,
                                        QtGui.QTextCursor.KeepAnchor)
                    at_end = len(cursor.selectedText().strip()) == 0
                    single_line = (self._get_end_cursor().blockNumber() ==
                                   self._get_prompt_cursor().blockNumber())
                    if (at_end or shift_down or single_line) and not ctrl_down:
                        self.execute(interactive = not shift_down)
                    else:
                        cursor.beginEditBlock()
                        cursor.setPosition(position)
                        cursor.insertText('\n')
                        self._insert_continuation_prompt(cursor)
                        cursor.endEditBlock()
                        self._control.moveCursor(QtGui.QTextCursor.End)
                        self._control.setTextCursor(cursor)
        elif ctrl_down:
            if key == QtCore.Qt.Key_G:
                self._keyboard_quit()
                intercepted = True
            elif key == QtCore.Qt.Key_K:
                if self._in_buffer(position):
                    cursor.clearSelection()
                    cursor.movePosition(QtGui.QTextCursor.EndOfLine,
                                        QtGui.QTextCursor.KeepAnchor)
                    if not cursor.hasSelection():
                        cursor.movePosition(QtGui.QTextCursor.NextBlock,
                                            QtGui.QTextCursor.KeepAnchor)
                        cursor.movePosition(QtGui.QTextCursor.Right,
                                            QtGui.QTextCursor.KeepAnchor,
                                            len(self._continuation_prompt))
                    self._kill_ring.kill_cursor(cursor)
                    self._set_cursor(cursor)
                intercepted = True
            elif key == QtCore.Qt.Key_L:
                self.prompt_to_top()
                intercepted = True
            elif key == QtCore.Qt.Key_O:
                if self._page_control and self._page_control.isVisible():
                    self._page_control.setFocus()
                intercepted = True
            elif key == QtCore.Qt.Key_U:
                if self._in_buffer(position):
                    cursor.clearSelection()
                    start_line = cursor.blockNumber()
                    if start_line == self._get_prompt_cursor().blockNumber():
                        offset = len(self._prompt)
                    else:
                        offset = len(self._continuation_prompt)
                    cursor.movePosition(QtGui.QTextCursor.StartOfBlock,
                                        QtGui.QTextCursor.KeepAnchor)
                    cursor.movePosition(QtGui.QTextCursor.Right,
                                        QtGui.QTextCursor.KeepAnchor, offset)
                    self._kill_ring.kill_cursor(cursor)
                    self._set_cursor(cursor)
                intercepted = True
            elif key == QtCore.Qt.Key_Y:
                self._keep_cursor_in_buffer()
                self._kill_ring.yank()
                intercepted = True
            elif key in (QtCore.Qt.Key_Backspace, QtCore.Qt.Key_Delete):
                if key == QtCore.Qt.Key_Backspace:
                    cursor = self._get_word_start_cursor(position)
                else: 
                    cursor = self._get_word_end_cursor(position)
                cursor.setPosition(position, QtGui.QTextCursor.KeepAnchor)
                self._kill_ring.kill_cursor(cursor)
                intercepted = True
            elif key == QtCore.Qt.Key_D:
                if len(self.input_buffer) == 0:
                    self.exit_requested.emit(self)
                else:
                    new_event = QtGui.QKeyEvent(QtCore.QEvent.KeyPress,
                                                QtCore.Qt.Key_Delete,
                                                QtCore.Qt.NoModifier)
                    QtGui.qApp.sendEvent(self._control, new_event)
                    intercepted = True
        elif alt_down:
            if key == QtCore.Qt.Key_B:
                self._set_cursor(self._get_word_start_cursor(position))
                intercepted = True
            elif key == QtCore.Qt.Key_F:
                self._set_cursor(self._get_word_end_cursor(position))
                intercepted = True
            elif key == QtCore.Qt.Key_Y:
                self._kill_ring.rotate()
                intercepted = True
            elif key == QtCore.Qt.Key_Backspace:
                cursor = self._get_word_start_cursor(position)
                cursor.setPosition(position, QtGui.QTextCursor.KeepAnchor)
                self._kill_ring.kill_cursor(cursor)
                intercepted = True
            elif key == QtCore.Qt.Key_D:
                cursor = self._get_word_end_cursor(position)
                cursor.setPosition(position, QtGui.QTextCursor.KeepAnchor)
                self._kill_ring.kill_cursor(cursor)
                intercepted = True
            elif key == QtCore.Qt.Key_Delete:
                intercepted = True
            elif key == QtCore.Qt.Key_Greater:
                self._control.moveCursor(QtGui.QTextCursor.End)
                intercepted = True
            elif key == QtCore.Qt.Key_Less:
                self._control.setTextCursor(self._get_prompt_cursor())
                intercepted = True
        else:
            if shift_down:
                anchormode = QtGui.QTextCursor.KeepAnchor
            else:
                anchormode = QtGui.QTextCursor.MoveAnchor
            if key == QtCore.Qt.Key_Escape:
                self._keyboard_quit()
                intercepted = True
            elif key == QtCore.Qt.Key_Up:
                if self._reading or not self._up_pressed(shift_down):
                    intercepted = True
                else:
                    prompt_line = self._get_prompt_cursor().blockNumber()
                    intercepted = cursor.blockNumber() <= prompt_line
            elif key == QtCore.Qt.Key_Down:
                if self._reading or not self._down_pressed(shift_down):
                    intercepted = True
                else:
                    end_line = self._get_end_cursor().blockNumber()
                    intercepted = cursor.blockNumber() == end_line
            elif key == QtCore.Qt.Key_Tab:
                if not self._reading:
                    if self._tab_pressed():
                        cursor.insertText(' '*4)
                    intercepted = True
            elif key == QtCore.Qt.Key_Left:
                line, col = cursor.blockNumber(), cursor.columnNumber()
                if line > self._get_prompt_cursor().blockNumber() and                        col == len(self._continuation_prompt):
                    self._control.moveCursor(QtGui.QTextCursor.PreviousBlock,
                                             mode=anchormode)
                    self._control.moveCursor(QtGui.QTextCursor.EndOfBlock,
                                             mode=anchormode)
                    intercepted = True
                else:
                    intercepted = not self._in_buffer(position - 1)
            elif key == QtCore.Qt.Key_Right:
                original_block_number = cursor.blockNumber()
                cursor.movePosition(QtGui.QTextCursor.Right,
                                mode=anchormode)
                if cursor.blockNumber() != original_block_number:
                    cursor.movePosition(QtGui.QTextCursor.Right,
                                        n=len(self._continuation_prompt),
                                        mode=anchormode)
                self._set_cursor(cursor)
                intercepted = True
            elif key == QtCore.Qt.Key_Home:
                start_line = cursor.blockNumber()
                if start_line == self._get_prompt_cursor().blockNumber():
                    start_pos = self._prompt_pos
                else:
                    cursor.movePosition(QtGui.QTextCursor.StartOfBlock,
                                        QtGui.QTextCursor.KeepAnchor)
                    start_pos = cursor.position()
                    start_pos += len(self._continuation_prompt)
                    cursor.setPosition(position)
                if shift_down and self._in_buffer(position):
                    cursor.setPosition(start_pos, QtGui.QTextCursor.KeepAnchor)
                else:
                    cursor.setPosition(start_pos)
                self._set_cursor(cursor)
                intercepted = True
            elif key == QtCore.Qt.Key_Backspace:
                line, col = cursor.blockNumber(), cursor.columnNumber()
                if not self._reading and                        col == len(self._continuation_prompt) and                        line > self._get_prompt_cursor().blockNumber():
                    cursor.beginEditBlock()
                    cursor.movePosition(QtGui.QTextCursor.StartOfBlock,
                                        QtGui.QTextCursor.KeepAnchor)
                    cursor.removeSelectedText()
                    cursor.deletePreviousChar()
                    cursor.endEditBlock()
                    intercepted = True
                else:
                    anchor = cursor.anchor()
                    if anchor == position:
                        intercepted = not self._in_buffer(position - 1)
                    else:
                        intercepted = not self._in_buffer(min(anchor, position))
            elif key == QtCore.Qt.Key_Delete:
                if not self._reading and self._in_buffer(position) and                        cursor.atBlockEnd() and not cursor.hasSelection():
                    cursor.movePosition(QtGui.QTextCursor.NextBlock,
                                        QtGui.QTextCursor.KeepAnchor)
                    cursor.movePosition(QtGui.QTextCursor.Right,
                                        QtGui.QTextCursor.KeepAnchor,
                                        len(self._continuation_prompt))
                    cursor.removeSelectedText()
                    intercepted = True
                else:
                    anchor = cursor.anchor()
                    intercepted = (not self._in_buffer(anchor) or
                                   not self._in_buffer(position))
        if not (self._control_key_down(event.modifiers(), include_command=True)
                or key in (QtCore.Qt.Key_PageUp, QtCore.Qt.Key_PageDown)
                or (self._executing and not self._reading)):
            self._keep_cursor_in_buffer()
        return intercepted


===============================[ 41 ]===============================
def new_code_cell(code=None, prompt_number=None):
    cell = NotebookNode()
    cell.cell_type = u'code'
    if code is not None:
        cell.code = unicode(code)
    if prompt_number is not None:
        cell.prompt_number = int(prompt_number)
    return cell


===============================[ 42 ]===============================
def roc_auc_score(y_true: Union[List[List[float]], List[List[int]], np.ndarray],
                  y_pred: Union[List[List[float]], List[List[int]], np.ndarray]) -> float:
    try:
        return sklearn.metrics.roc_auc_score(np.squeeze(np.array(y_true)),
                                             np.squeeze(np.array(y_pred)), average="macro")
    except ValueError:
        return 0.


===============================[ 43 ]===============================
def crop_image(img, start_y, start_x, h, w):
    return img[start_y:start_y + h, start_x:start_x + w, :].copy()


===============================[ 44 ]===============================
def advance_robots(self):
        self = lens.robots.Each().call_step_towards(self.player)(self)
        self = lens.crashes.call_union(duplicates(self.robots))(self)
        self = lens.robots.modify(lambda r: list(set(r) - self.crashes))(self)
        return self


===============================[ 45 ]===============================
def check_standard_dir(module_path):
    if 'site-packages' in module_path:
        return True
    for stdlib_path in _STDLIB_PATHS:
        if fnmatch.fnmatchcase(module_path, stdlib_path + '*'):
            return True
    return False


===============================[ 46 ]===============================
def table_exists(self, table_name, db='default'):
        try:
            self.get_table(table_name, db)
            return True
        except Exception:
            return False


===============================[ 47 ]===============================
def delete_vacation(self, index, vacation):
        body = {"selection": {
                    "selectionType": "thermostats",
                    "selectionMatch": self.thermostats[index]['identifier']},
                "functions": [{"type": "deleteVacation", "params": {
                    "name": vacation
                }}]}
        log_msg_action = "delete a vacation"
        return self.make_request(body, log_msg_action)


===============================[ 48 ]===============================
def get_transfers(self, start=0, stop=None, inclusion_states=False):
        return extended.GetTransfersCommand(self.adapter)(
            seed=self.seed,
            start=start,
            stop=stop,
            inclusionStates=inclusion_states,
        )


===============================[ 49 ]===============================
def write(self, output_buffer, kmip_version=enums.KMIPVersion.KMIP_1_0):
        local_buffer = utils.BytearrayStream()
        if self._operations:
            for operation in self._operations:
                operation.write(local_buffer, kmip_version=kmip_version)
        if self._object_types:
            for object_type in self._object_types:
                object_type.write(local_buffer, kmip_version=kmip_version)
        if self._vendor_identification:
            self._vendor_identification.write(
                local_buffer,
                kmip_version=kmip_version
            )
        if self._server_information:
            self._server_information.write(
                local_buffer,
                kmip_version=kmip_version
            )
        if self._application_namespaces:
            for application_namespace in self._application_namespaces:
                application_namespace.write(
                    local_buffer,
                    kmip_version=kmip_version
                )
        if kmip_version >= enums.KMIPVersion.KMIP_1_1:
            if self._extension_information:
                for extension_information in self._extension_information:
                    extension_information.write(
                        local_buffer,
                        kmip_version=kmip_version
                    )
        if kmip_version >= enums.KMIPVersion.KMIP_1_2:
            if self._attestation_types:
                for attestation_type in self._attestation_types:
                    attestation_type.write(
                        local_buffer,
                        kmip_version=kmip_version
                    )
        if kmip_version >= enums.KMIPVersion.KMIP_1_3:
            if self._rng_parameters:
                for rng_parameters in self._rng_parameters:
                    rng_parameters.write(
                        local_buffer,
                        kmip_version=kmip_version
                    )
            if self._profile_information:
                for profile_information in self._profile_information:
                    profile_information.write(
                        local_buffer,
                        kmip_version=kmip_version
                    )
            if self._validation_information:
                for validation_information in self._validation_information:
                    validation_information.write(
                        local_buffer,
                        kmip_version=kmip_version
                    )
            if self._capability_information:
                for capability_information in self._capability_information:
                    capability_information.write(
                        local_buffer,
                        kmip_version=kmip_version
                    )
            if self._client_registration_methods:
                for client_reg_method in self._client_registration_methods:
                    client_reg_method.write(
                        local_buffer,
                        kmip_version=kmip_version
                    )
        if kmip_version >= enums.KMIPVersion.KMIP_2_0:
            if self._defaults_information:
                self._defaults_information.write(
                    local_buffer,
                    kmip_version=kmip_version
                )
            if self._storage_protection_masks:
                for storage_protection_mask in self._storage_protection_masks:
                    storage_protection_mask.write(
                        local_buffer,
                        kmip_version=kmip_version
                    )
        self.length = local_buffer.length()
        super(QueryResponsePayload, self).write(
            output_buffer,
            kmip_version=kmip_version
        )
        output_buffer.write(local_buffer.buffer)


===============================[ 50 ]===============================
def insert_bytes(fobj, size, offset, BUFFER_SIZE=2**16):
    assert 0 < size
    assert 0 <= offset
    locked = False
    fobj.seek(0, 2)
    filesize = fobj.tell()
    movesize = filesize - offset
    fobj.write(b'\x00' * size)
    fobj.flush()
    try:
        try:
            import mmap
            file_map = mmap.mmap(fobj.fileno(), filesize + size)
            try:
                file_map.move(offset + size, offset, movesize)
            finally:
                file_map.close()
        except (ValueError, EnvironmentError, ImportError):
            locked = lock(fobj)
            fobj.truncate(filesize)
            fobj.seek(0, 2)
            padsize = size
            while padsize:
                addsize = min(BUFFER_SIZE, padsize)
                fobj.write(b"\x00" * addsize)
                padsize -= addsize
            fobj.seek(filesize, 0)
            while movesize:
                thismove = min(BUFFER_SIZE, movesize)
                fobj.seek(-thismove, 1)
                nextpos = fobj.tell()
                data = fobj.read(thismove)
                fobj.seek(-thismove + size, 1)
                fobj.write(data)
                fobj.seek(nextpos)
                movesize -= thismove
            fobj.flush()
    finally:
        if locked:
            unlock(fobj)


===============================[ 51 ]===============================
def as_dagster_type(
    existing_type,
    name=None,
    description=None,
    input_schema=None,
    output_schema=None,
    serialization_strategy=None,
    storage_plugins=None,
):
    check.type_param(existing_type, 'existing_type')
    check.opt_str_param(name, 'name')
    check.opt_str_param(description, 'description')
    check.opt_inst_param(input_schema, 'input_schema', InputSchema)
    check.opt_inst_param(output_schema, 'output_schema', OutputSchema)
    check.opt_inst_param(serialization_strategy, 'serialization_strategy', SerializationStrategy)
    storage_plugins = check.opt_dict_param(storage_plugins, 'storage_plugins')
    if serialization_strategy is None:
        serialization_strategy = PickleSerializationStrategy()
    name = existing_type.__name__ if name is None else name
    return _decorate_as_dagster_type(
        existing_type,
        key=name,
        name=name,
        description=description,
        input_schema=input_schema,
        output_schema=output_schema,
        serialization_strategy=serialization_strategy,
        storage_plugins=storage_plugins,
    )


===============================[ 52 ]===============================
def url(self):
        if self._url is not None:
            url = self._url
        else:
            url = getattr(self.nb.metadata, 'url', None)
        if url is not None:
            return nbviewer_link(url)


===============================[ 53 ]===============================
def save_graph_only_from_checkpoint(input_checkpoint, output_file_path, output_node_names, as_text=False):
    check_input_checkpoint(input_checkpoint)
    output_node_names = output_node_names_string_as_list(output_node_names)
    with tf.Session() as sess:
        restore_from_checkpoint(sess, input_checkpoint)
        save_graph_only(sess, output_file_path, output_node_names, as_text=as_text)


===============================[ 54 ]===============================
def copy_notebook(self, notebook_id):
        last_mod, nb = self.get_notebook_object(notebook_id)
        name = nb.metadata.name + '-Copy'
        path, name = self.increment_filename(name)
        nb.metadata.name = name
        notebook_id = self.new_notebook_id(name)
        self.save_notebook_object(notebook_id, nb)
        return notebook_id


===============================[ 55 ]===============================
def debug_storage(storage, base_info=False, chars=True, runs=False):
    import codecs
    import locale
    import sys
    if six.PY2:
        stderr = codecs.getwriter(locale.getpreferredencoding())(sys.stderr)
    else:
        stderr = sys.stderr
    caller = inspect.stack()[1][3]
    stderr.write('in %s\n' % caller)
    if base_info:
        stderr.write(u'  base level  : %d\n' % storage['base_level'])
        stderr.write(u'  base dir    : %s\n' % storage['base_dir'])
    if runs:
        stderr.write(u'  runs        : %s\n' % list(storage['runs']))
    if chars:
        output = u'  Chars       : '
        for _ch in storage['chars']:
            if _ch != '\n':
                output += _ch['ch']
            else:
                output += 'C'
        stderr.write(output + u'\n')
        output = u'  Res. levels : %s\n' % u''.join(
            [six.text_type(_ch['level']) for _ch in storage['chars']])
        stderr.write(output)
        _types = [_ch['type'].ljust(3) for _ch in storage['chars']]
        for i in range(3):
            if i:
                output = u'                %s\n'
            else:
                output = u'  Res. types  : %s\n'
            stderr.write(output % u''.join([_t[i] for _t in _types]))


===============================[ 56 ]===============================
def get(self, alert_type, alert_args=None):
        alert_args = alert_args or {}
        alerts = self.list()
        return [
            alert
            for alert
            in alerts
            if alert.get('type') == alert_type
            and dict_is_subset(alert_args, alert.get('args'))
        ]


===============================[ 57 ]===============================
def _make_reversed_wildcards(self, old_length=-1):
        if len(self._reversed_wildcards) > 0:
            start = old_length
        else:
            start = -1
        for wildcards, func in self._wildcard_functions.items():
            for irun in range(start, len(self)):
                translated_name = func(irun)
                if not translated_name in self._reversed_wildcards:
                    self._reversed_wildcards[translated_name] = ([], wildcards)
                self._reversed_wildcards[translated_name][0].append(irun)


===============================[ 58 ]===============================
def sanitize_block(self, block):
        embed_type = block.get('type', None)
        data = block.get('data', {})
        serializer = self.serializers.get(embed_type, None)
        if serializer is None:
            return block
        block['data'] = serializer.to_internal_value(data)
        return block


===============================[ 59 ]===============================
def handle_stop(self, signame, set_stop):
        if set_stop:
            self.sigs[signame].b_stop       = True
            self.sigs[signame].print_method = self.dbgr.intf[-1].msg
            self.sigs[signame].pass_along   = False
        else:
            self.sigs[signame].b_stop       = False
            pass
        return set_stop


===============================[ 60 ]===============================
def classe(self, name):
        for klass in self.classes():
            if klass.node.name == name:
                return klass
        raise KeyError(name)


===============================[ 61 ]===============================
def delete(self, id, coordinates):
        p_mins, p_maxs = self.get_coordinate_pointers(coordinates)
        core.rt.Index_DeleteData(
            self.handle, id, p_mins, p_maxs, self.properties.dimension)


===============================[ 62 ]===============================
def trace(self, app):
        if app not in self._trace_cache:
            process = subprocess.Popen(
                "trace-deps.js {}".format(app), shell=True,
                stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                env=self.env, universal_newlines=True, cwd=self._package_json_dir
            )
            out, err = process.communicate()
            if err:
                raise TraceError(err)
            self._trace_cache[app] = json.loads(out)
        return self._trace_cache[app]


===============================[ 63 ]===============================
def getoutput(cmd):
    with AvoidUNCPath() as path:
        if path is not None:
            cmd = '"pushd %s &&"%s' % (path, cmd)
        out = process_handler(cmd, lambda p: p.communicate()[0], STDOUT)
    if out is None:
        out = b''
    return py3compat.bytes_to_str(out)


===============================[ 64 ]===============================
def get_safe_contract(w3: Web3, address=None):
    return w3.eth.contract(address,
                           abi=GNOSIS_SAFE_INTERFACE['abi'],
                           bytecode=GNOSIS_SAFE_INTERFACE['bytecode'])


===============================[ 65 ]===============================
def headers_present(self, headers):
        headers = {name: re.compile('(.*)') for name in headers}
        self.add_matcher(matcher('HeadersMatcher', headers))


===============================[ 66 ]===============================
def is_method_call(func, types=(), methods=()):
    return (
        isinstance(func, astroid.BoundMethod)
        and isinstance(func.bound, astroid.Instance)
        and (func.bound.name in types if types else True)
        and (func.name in methods if methods else True)
    )


===============================[ 67 ]===============================
def _uniform_unit_norm(dimension, shape, dtype, seed):
  raw = normal.Normal(
      loc=dtype_util.as_numpy_dtype(dtype)(0),
      scale=dtype_util.as_numpy_dtype(dtype)(1)).sample(
          tf.concat([shape, [dimension]], axis=0), seed=seed())
  unit_norm = raw / tf.norm(tensor=raw, ord=2, axis=-1)[..., tf.newaxis]
  return unit_norm


===============================[ 68 ]===============================
def is_from_fallback_block(node: astroid.node_classes.NodeNG) -> bool:
    context = find_try_except_wrapper_node(node)
    if not context:
        return False
    if isinstance(context, astroid.ExceptHandler):
        other_body = context.parent.body
        handlers = context.parent.handlers
    else:
        other_body = itertools.chain.from_iterable(
            handler.body for handler in context.handlers
        )
        handlers = context.handlers
    has_fallback_imports = any(
        isinstance(import_node, (astroid.ImportFrom, astroid.Import))
        for import_node in other_body
    )
    ignores_import_error = _except_handlers_ignores_exception(handlers, ImportError)
    return ignores_import_error or has_fallback_imports


===============================[ 69 ]===============================
def before_app_request(self, f):
        self.record_once(lambda s: s.app.before_request_funcs
            .setdefault(None, []).append(f))
        return f


===============================[ 70 ]===============================
def cudnn_bi_gru(units,
                 n_hidden,
                 seq_lengths=None,
                 n_layers=1,
                 trainable_initial_states=False,
                 name='cudnn_bi_gru',
                 reuse=False):
    with tf.variable_scope(name, reuse=reuse):
        if seq_lengths is None:
            seq_lengths = tf.ones([tf.shape(units)[0]], dtype=tf.int32) * tf.shape(units)[1]
        with tf.variable_scope('Forward'):
            h_fw, h_last_fw = cudnn_gru_wrapper(units,
                                                n_hidden,
                                                n_layers=n_layers,
                                                trainable_initial_states=trainable_initial_states,
                                                seq_lengths=seq_lengths,
                                                reuse=reuse)
        with tf.variable_scope('Backward'):
            reversed_units = tf.reverse_sequence(units, seq_lengths=seq_lengths, seq_dim=1, batch_dim=0)
            h_bw, h_last_bw = cudnn_gru_wrapper(reversed_units,
                                                n_hidden,
                                                n_layers=n_layers,
                                                trainable_initial_states=trainable_initial_states,
                                                seq_lengths=seq_lengths,
                                                reuse=reuse)
            h_bw = tf.reverse_sequence(h_bw, seq_lengths=seq_lengths, seq_dim=1, batch_dim=0)
    return (h_fw, h_bw), (h_last_fw, h_last_bw)


===============================[ 71 ]===============================
def xml_report(self, morfs=None, outfile=None, ignore_errors=None,
                    omit=None, include=None):
        self._harvest_data()
        self.config.from_args(
            ignore_errors=ignore_errors, omit=omit, include=include,
            xml_output=outfile,
            )
        file_to_close = None
        delete_file = False
        if self.config.xml_output:
            if self.config.xml_output == '-':
                outfile = sys.stdout
            else:
                outfile = open(self.config.xml_output, "w")
                file_to_close = outfile
        try:
            try:
                reporter = XmlReporter(self, self.config)
                return reporter.report(morfs, outfile=outfile)
            except CoverageException:
                delete_file = True
                raise
        finally:
            if file_to_close:
                file_to_close.close()
                if delete_file:
                    file_be_gone(self.config.xml_output)


===============================[ 72 ]===============================
def of(jvalue, bigdl_type="float"):
        def get_py_name(jclass_name):
            if jclass_name == "StaticGraph" or jclass_name == "DynamicGraph":
                return "Model"
            elif jclass_name == "Input":
                return "Layer"
            else:
                return jclass_name
        jname = callBigDlFunc(bigdl_type,
                                      "getRealClassNameOfJValue",
                                      jvalue)
        jpackage_name = ".".join(jname.split(".")[:-1])
        pclass_name = get_py_name(jname.split(".")[-1])
        if "com.intel.analytics.bigdl.nn.keras.Model" == jname or                        "com.intel.analytics.bigdl.nn.keras.Sequential" == jname:
            base_module = importlib.import_module('bigdl.nn.keras.topology')
        elif "com.intel.analytics.bigdl.nn.keras" == jpackage_name:
            base_module = importlib.import_module('bigdl.nn.keras.layer')
        else:
            base_module = importlib.import_module('bigdl.nn.layer')
        realClassName = "Layer" 
        if pclass_name in dir(base_module):
            realClassName = pclass_name
        module = getattr(base_module, realClassName)
        jvalue_creator = getattr(module, "from_jvalue")
        model = jvalue_creator(jvalue, bigdl_type)
        return model


===============================[ 73 ]===============================
async def resume(self, *, device: Optional[SomeDevice] = None):
        await self._user.http.play_playback(None, device_id=str(device))


===============================[ 74 ]===============================
def _get_access_token():
    access_token = os.environ.get(ACCESS_TOKEN_ENVIRONMENT_VARIABLE)
    if access_token:
        return access_token
    else:
        for access_token_variable in LEGACY_ACCESS_TOKEN_ENVIRONMENT_VARIABLES:
            access_token = os.environ.get(access_token_variable)
            if access_token:
                env_var_deprecation_warning = PendingDeprecationWarning(
                    "Use of the `{legacy}` environment variable will be "
                    "deprecated in the future.  Please update your "
                    "environment(s) to use the new `{new}` environment "
                    "variable.".format(
                        legacy=access_token,
                        new=ACCESS_TOKEN_ENVIRONMENT_VARIABLE,
                    )
                )
                warnings.warn(env_var_deprecation_warning)
                return access_token


===============================[ 75 ]===============================
def sanitize_for_archive(url, headers, payload):
        url = re.sub('bot.*/', 'botXXXXX/', url)
        return url, headers, payload


===============================[ 76 ]===============================
def remove_subscriber(self, ws):
        if ws in self.subscribers:
            self.subscribers.remove(ws)
        for name in self.available_events:
            self.remove_event_subscriber(name, ws)


===============================[ 77 ]===============================
def backward_smoothing_update(filtered_mean,
                              filtered_cov,
                              predicted_mean,
                              predicted_cov,
                              next_posterior_mean,
                              next_posterior_cov,
                              transition_matrix):
  tmp_gain_cov = transition_matrix.matmul(filtered_cov)
  predicted_cov_chol = tf.linalg.cholesky(predicted_cov)
  gain_transpose = tf.linalg.cholesky_solve(predicted_cov_chol, tmp_gain_cov)
  posterior_mean = (filtered_mean +
                    tf.linalg.matmul(gain_transpose,
                                     next_posterior_mean - predicted_mean,
                                     adjoint_a=True))
  posterior_cov = (
      filtered_cov +
      tf.linalg.matmul(gain_transpose,
                       tf.linalg.matmul(
                           next_posterior_cov - predicted_cov, gain_transpose),
                       adjoint_a=True))
  return (posterior_mean, posterior_cov)


===============================[ 78 ]===============================
def plot(self, timestep="AUTO", metric="AUTO", server=False, **kwargs):
        assert_is_type(metric, "AUTO", "logloss", "auc", "classification_error", "rmse")
        if self._model_json["algo"] in ("deeplearning", "deepwater", "xgboost", "drf", "gbm"):
            if metric == "AUTO":
                metric = "logloss"
        self._plot(timestep=timestep, metric=metric, server=server)


===============================[ 79 ]===============================
def create_py(self, nb, force=False):
        if list(map(int, re.findall('\d+', nbconvert.__version__))) >= [4, 2]:
            py_file = os.path.basename(self.py_file)
        else:
            py_file = self.py_file
        try:
            level = logger.logger.level
        except AttributeError:
            level = logger.level
        spr.call(['jupyter', 'nbconvert', '--to=python',
                  '--output=' + py_file, '--log-level=%s' % level,
                  self.outfile])
        with open(self.py_file) as f:
            py_content = f.read()
        py_content = re.sub('^\s*get_ipython\(\).magic.*', '# \g<0>',
                            py_content, flags=re.MULTILINE)
        with open(self.py_file, 'w') as f:
            f.write(py_content)


===============================[ 80 ]===============================
def _remove_overlaps(segmentation_mask, fronts):
    fidxs, sidxs = np.where((segmentation_mask != fronts) & (segmentation_mask != 0) & (fronts != 0))
    fronts[fidxs, sidxs] = 0


===============================[ 81 ]===============================
def skip_module(*modules):
    modules = (modules and isinstance(modules[0], list)) and              modules[0] or modules
    for module in modules:
        if not module in SKIPPED_MODULES:
            SKIPPED_MODULES.append(module)
    traceback.extract_tb = _new_extract_tb


===============================[ 82 ]===============================
def get_config(self):
    return {
        'initializers': [
            tf.compat.v2.initializers.serialize(
                tf.keras.initializers.get(init))
            for init in self.initializers
        ],
        'sizes': self.sizes,
        'validate_args': self.validate_args,
    }


===============================[ 83 ]===============================
def set_query_parameter(url, param_name, param_value):
    scheme, netloc, path, query_string, fragment = urlsplit(url)
    query_params = parse_qs(query_string)
    query_params[param_name] = [param_value]
    new_query_string = urlencode(query_params, doseq=True)
    return urlunsplit((scheme, netloc, path, new_query_string, fragment))


===============================[ 84 ]===============================
def alphavsks(self,autozoom=True,**kwargs):
        pylab.plot(self._alpha_values, self._xmin_kstest, '.')
        pylab.errorbar(self._alpha, self._ks, xerr=self._alphaerr, fmt='+')
        ax=pylab.gca()
        if autozoom:
            ax.set_ylim(0.8*(self._ks),3*(self._ks))
            ax.set_xlim((self._alpha)-5*self._alphaerr,(self._alpha)+5*self._alphaerr)
        ax.set_ylabel("KS statistic")
        ax.set_xlabel(r'$\alpha$')
        pylab.draw()
        return ax


===============================[ 85 ]===============================
def put_container(self, container, headers=None, query=None, cdn=False,
                      body=None):
        path = self._container_path(container)
        return self.request(
            'PUT', path, body or '', headers, query=query, cdn=cdn)


===============================[ 86 ]===============================
def assign_moving_mean_variance(
    mean_var, variance_var, value, decay, name=None):
  with tf.compat.v1.name_scope(name, "assign_moving_mean_variance",
                               [variance_var, mean_var, value, decay]):
    with tf.compat.v1.colocate_with(variance_var):
      with tf.compat.v1.colocate_with(mean_var):
        base_dtype = mean_var.dtype.base_dtype
        if not base_dtype.is_floating:
          raise TypeError(
              "mean_var.base_dtype({}) does not have float type "
              "`dtype`.".format(base_dtype.name))
        if base_dtype != variance_var.dtype.base_dtype:
          raise TypeError(
              "mean_var.base_dtype({}) != variance_var.base_dtype({})".format(
                  base_dtype.name,
                  variance_var.dtype.base_dtype.name))
        value = tf.convert_to_tensor(
            value=value, dtype=base_dtype, name="value")
        decay = tf.convert_to_tensor(
            value=decay, dtype=base_dtype, name="decay")
        delta = value - mean_var
        with tf.control_dependencies([delta]):
          mean_var = mean_var.assign_add((1. - decay) * delta)
          variance_var = variance_var.assign_sub(
              (1. - decay) * (variance_var - decay * tf.square(delta)))
        return mean_var, variance_var


===============================[ 87 ]===============================
def _dot_product(self, imgs_to_decode):
        return np.dot(imgs_to_decode.T, self.feature_images).T


===============================[ 88 ]===============================
def _decode(self, data, decode_content, flush_decoder):
        try:
            if decode_content and self._decoder:
                data = self._decoder.decompress(data)
        except (IOError, zlib.error) as e:
            content_encoding = self.headers.get('content-encoding', '').lower()
            raise DecodeError(
                "Received response with content-encoding: %s, but "
                "failed to decode it." % content_encoding, e)
        if flush_decoder and decode_content and self._decoder:
            buf = self._decoder.decompress(binary_type())
            data += buf + self._decoder.flush()
        return data


===============================[ 89 ]===============================
def get_tweets(user, pages=25):
    url = f'https://twitter.com/i/profiles/show/{user}/timeline/tweets?include_available_features=1&include_entities=1&include_new_items_bar=true'
    headers = {
        'Accept': 'application/json, text/javascript, */*; q=0.01',
        'Referer': f'https://twitter.com/{user}',
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/603.3.8 (KHTML, like Gecko) Version/10.1.2 Safari/603.3.8',
        'X-Twitter-Active-User': 'yes',
        'X-Requested-With': 'XMLHttpRequest',
        'Accept-Language': 'en-US'
    }
    def gen_tweets(pages):
        r = session.get(url, headers=headers)
        while pages > 0:
            try:
                html = HTML(html=r.json()['items_html'],
                            url='bunk', default_encoding='utf-8')
            except KeyError:
                raise ValueError(
                    f'Oops! Either "{user}" does not exist or is private.')
            comma = ","
            dot = "."
            tweets = []
            for tweet in html.find('html > .stream-item'):
                try:
                    text = tweet.find('.tweet-text')[0].full_text
                except IndexError:  
                    continue
                tweet_id = tweet.find('.js-permalink')[0].attrs['data-conversation-id']
                time = datetime.fromtimestamp(int(tweet.find('._timestamp')[0].attrs['data-time-ms']) / 1000.0)
                interactions = [
                    x.text
                    for x in tweet.find('.ProfileTweet-actionCount')
                ]
                replies = int(
                    interactions[0].split(' ')[0].replace(comma, '').replace(dot, '')
                    or interactions[3]
                )
                retweets = int(
                    interactions[1].split(' ')[0].replace(comma, '').replace(dot, '')
                    or interactions[4]
                    or interactions[5]
                )
                likes = int(
                    interactions[2].split(' ')[0].replace(comma, '').replace(dot, '')
                    or interactions[6]
                    or interactions[7]
                )
                hashtags = [
                    hashtag_node.full_text
                    for hashtag_node in tweet.find('.twitter-hashtag')
                ]
                urls = [
                    url_node.attrs['data-expanded-url']
                    for url_node in tweet.find('a.twitter-timeline-link:not(.u-hidden)')
                ]
                photos = [
                    photo_node.attrs['data-image-url']
                    for photo_node in tweet.find('.AdaptiveMedia-photoContainer')
                ]
                videos = []
                video_nodes = tweet.find(".PlayableMedia-player")
                for node in video_nodes:
                    styles = node.attrs['style'].split()
                    for style in styles:
                        if style.startswith('background'):
                            tmp = style.split('/')[-1]
                            video_id = tmp[:tmp.index('.jpg')]
                            videos.append({'id': video_id})
                tweets.append({
                    'tweetId': tweet_id,
                    'time': time,
                    'text': text,
                    'replies': replies,
                    'retweets': retweets,
                    'likes': likes,
                    'entries': {
                        'hashtags': hashtags, 'urls': urls,
                        'photos': photos, 'videos': videos
                    }
                })
            last_tweet = html.find('.stream-item')[-1].attrs['data-item-id']
            for tweet in tweets:
                if tweet:
                    tweet['text'] = re.sub('http', ' http', tweet['text'], 1)
                    yield tweet
            r = session.get(url, params={'max_position': last_tweet}, headers=headers)
            pages += -1
    yield from gen_tweets(pages)
===============================[ 89 ]===============================
def get_tweets(user, pages=25):
    url = f'https://twitter.com/i/profiles/show/{user}/timeline/tweets?include_available_features=1&include_entities=1&include_new_items_bar=true'
    headers = {
        'Accept': 'application/json, text/javascript, */*; q=0.01',
        'Referer': f'https://twitter.com/{user}',
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/603.3.8 (KHTML, like Gecko) Version/10.1.2 Safari/603.3.8',
        'X-Twitter-Active-User': 'yes',
        'X-Requested-With': 'XMLHttpRequest',
        'Accept-Language': 'en-US'
    }
    def gen_tweets(pages):
        r = session.get(url, headers=headers)
        while pages > 0:
            try:
                html = HTML(html=r.json()['items_html'],
                            url='bunk', default_encoding='utf-8')
            except KeyError:
                raise ValueError(
                    f'Oops! Either "{user}" does not exist or is private.')
            comma = ","
            dot = "."
            tweets = []
            for tweet in html.find('html > .stream-item'):
                try:
                    text = tweet.find('.tweet-text')[0].full_text
                except IndexError:  
                    continue
                tweet_id = tweet.find('.js-permalink')[0].attrs['data-conversation-id']
                time = datetime.fromtimestamp(int(tweet.find('._timestamp')[0].attrs['data-time-ms']) / 1000.0)
                interactions = [
                    x.text
                    for x in tweet.find('.ProfileTweet-actionCount')
                ]
                replies = int(
                    interactions[0].split(' ')[0].replace(comma, '').replace(dot, '')
                    or interactions[3]
                )
                retweets = int(
                    interactions[1].split(' ')[0].replace(comma, '').replace(dot, '')
                    or interactions[4]
                    or interactions[5]
                )
                likes = int(
                    interactions[2].split(' ')[0].replace(comma, '').replace(dot, '')
                    or interactions[6]
                    or interactions[7]
                )
                hashtags = [
                    hashtag_node.full_text
                    for hashtag_node in tweet.find('.twitter-hashtag')
                ]
                urls = [
                    url_node.attrs['data-expanded-url']
                    for url_node in tweet.find('a.twitter-timeline-link:not(.u-hidden)')
                ]
                photos = [
                    photo_node.attrs['data-image-url']
                    for photo_node in tweet.find('.AdaptiveMedia-photoContainer')
                ]
                videos = []
                video_nodes = tweet.find(".PlayableMedia-player")
                for node in video_nodes:
                    styles = node.attrs['style'].split()
                    for style in styles:
                        if style.startswith('background'):
                            tmp = style.split('/')[-1]
                            video_id = tmp[:tmp.index('.jpg')]
                            videos.append({'id': video_id})
                tweets.append({
                    'tweetId': tweet_id,
                    'time': time,
                    'text': text,
                    'replies': replies,
                    'retweets': retweets,
                    'likes': likes,
                    'entries': {
                        'hashtags': hashtags, 'urls': urls,
                        'photos': photos, 'videos': videos
                    }
                })
            last_tweet = html.find('.stream-item')[-1].attrs['data-item-id']
            for tweet in tweets:
                if tweet:
                    tweet['text'] = re.sub('http', ' http', tweet['text'], 1)
                    yield tweet
            r = session.get(url, params={'max_position': last_tweet}, headers=headers)
            pages += -1
    yield from gen_tweets(pages)


===============================[ 90 ]===============================
def __info_yenczlib_gen(self):
        escape = 0
        dcrc32 = 0
        inflate = zlib.decompressobj(-15)
        header = next(self.__info_plain_gen())
        if not header.startswith("=ybegin"):
            raise NNTPDataError("Bad yEnc header")
        buf, trailer = fifo.Fifo(), ""
        for line in self.__info_plain_gen():
            if line.startswith("=yend"):
                trailer = line
                continue
            data, escape, dcrc32 = yenc.decode(line, escape, dcrc32)
            try:
                data = inflate.decompress(data)
            except zlib.error:
                raise NNTPDataError("Decompression failed")
            if not data:
                continue
            buf.write(data)
            for l in buf:
                yield l
        if not trailer:
            raise NNTPDataError("Missing yEnc trailer")
        ecrc32 = yenc.crc32(trailer)
        if ecrc32 is None:
            raise NNTPDataError("Bad yEnc trailer")
        if ecrc32 != dcrc32 & 0xffffffff:
            raise NNTPDataError("Bad yEnc CRC")


===============================[ 91 ]===============================
def _load_attr(name: str, ctx: ast.AST = ast.Load()) -> ast.Attribute:
    attrs = name.split(".")
    def attr_node(node, idx):
        if idx >= len(attrs):
            node.ctx = ctx
            return node
        return attr_node(
            ast.Attribute(value=node, attr=attrs[idx], ctx=ast.Load()), idx + 1
        )
    return attr_node(ast.Name(id=attrs[0], ctx=ast.Load()), 1)


===============================[ 92 ]===============================
def open_in_browser(doc, encoding=None):
    import os
    import webbrowser
    import tempfile
    if not isinstance(doc, etree._ElementTree):
        doc = etree.ElementTree(doc)
    handle, fn = tempfile.mkstemp(suffix='.html')
    f = os.fdopen(handle, 'wb')
    try:
        doc.write(f, method="html", encoding=encoding or doc.docinfo.encoding or "UTF-8")
    finally:
        f.close()
    url = 'file://' + fn.replace(os.path.sep, '/')
    print(url)
    webbrowser.open(url)


===============================[ 93 ]===============================
def check_gpu_existence():
    global _gpu_available
    if _gpu_available is None:
        sess_config = tf.ConfigProto()
        sess_config.gpu_options.allow_growth = True
        try:
            with tf.Session(config=sess_config):
                device_list = device_lib.list_local_devices()
                _gpu_available = any(device.device_type == 'GPU' for device in device_list)
        except AttributeError as e:
            log.warning(f'Got an AttributeError `{e}`, assuming documentation building')
            _gpu_available = False
    return _gpu_available


===============================[ 94 ]===============================
def __fetch_crate_data(self, crate_id):
        raw_crate = self.client.crate(crate_id)
        crate = json.loads(raw_crate)
        return crate['crate']


===============================[ 95 ]===============================
def current_lr(self):
        if self.optimizer is None:
            raise RuntimeError(
                'lr is not applicable because optimizer does not exist.')
        return [group['lr'] for group in self.optimizer.param_groups]


===============================[ 96 ]===============================
def delete_team(self, name):
        res = self.get_team(name)
        if res[0] == False:
            return res
        t = res[1]
        res = requests.delete(self.url + '/api/teams/' + str(t['id']), headers=self.hdrs, verify=self.ssl_verify)
        if not self._checkResponse(res):
            return [False, self.lasterr]
        return [True, None]


===============================[ 97 ]===============================
def __download_price(self, symbol: str, currency: str, agent: str):
        from finance_quote_python import Quote
        assert isinstance(symbol, str)
        assert isinstance(currency, str)
        assert isinstance(agent, str)
        if not symbol:
            return None
        dl = Quote()
        dl.logger = self.logger
        dl.set_source(agent)
        dl.set_currency(currency)
        result = dl.fetch(agent, [symbol])
        if not result:
            raise ValueError(f"Did not receive a response for {symbol}.")
        price = result[0]
        if not price:
            raise ValueError(f"Price not downloaded/parsed for {symbol}.")
        else:
            self.add_price(price)
        return price


===============================[ 98 ]===============================
def purge_queue(self, name):
        content = {"_object_id": {"_object_name": "org.apache.qpid.broker:queue:{0}".format(name)},
                   "_method_name": "purge",
                   "_arguments": {"type": "queue",
                                  "name": name,
                                  "filter": dict()}}
        logger.debug("Message content -> {0}".format(content))
        return content, self.method_properties


===============================[ 99 ]===============================
def get_motion_vector(self):
        if any(self.move):
            x, y = self.actor._rot
            strafe = math.degrees(math.atan2(*self.move))
            y_angle = math.radians(y)
            x_angle = math.radians(x + strafe)
            dy = 0.0
            dx = math.cos(x_angle)
            dz = math.sin(x_angle)
        else:
            dy = 0.0
            dx = 0.0
            dz = 0.0
        return (dx, dy, dz)


===============================[ 100 ]===============================
def from_file(self, filename, table=None, delimiter='|', null='NULL',
            panic=True, quotechar='"', parse_dates=False):
        if not self.table:
            if not table:
                raise GiraffeError("Table must be set or specified to load a file.")
            self.table = table
        if not isinstance(null, basestring):
            raise GiraffeError("Expected 'null' to be str, received {}".format(type(null)))
        with Reader(filename, delimiter=delimiter, quotechar=quotechar) as f:
            if not isinstance(f.delimiter, basestring):
                raise GiraffeError("Expected 'delimiter' to be str, received {}".format(type(delimiter)))
            self.columns = f.header
            if isinstance(f, ArchiveFileReader):
                self.mload.set_encoding(ROW_ENCODING_RAW)
                self.preprocessor = lambda s: s
            if parse_dates:
                self.preprocessor = DateHandler(self.columns)
            self._initiate()
            self.mload.set_null(null)
            self.mload.set_delimiter(delimiter)
            i = 0
            for i, line in enumerate(f, 1):
                self.put(line, panic=panic)
                if i % self.checkpoint_interval == 1:
                    log.info("\rBulkLoad", "Processed {} rows".format(i), console=True)
                    checkpoint_status = self.checkpoint()
                    self.exit_code = self._exit_code()
                    if self.exit_code != 0:
                        return self.exit_code
            log.info("\rBulkLoad", "Processed {} rows".format(i))
            return self.finish()


===============================[ 101 ]===============================
def fetchMore(self, index):
        if not index.isValid():
            item = self.root
        else:
            item = index.internalPointer()
        if item.canFetchMore():
            startIndex = len(item.children)
            additionalChildren = item.fetchChildren()
            endIndex = startIndex + len(additionalChildren) - 1
            if endIndex >= startIndex:
                self.beginInsertRows(index, startIndex, endIndex)
                for newChild in additionalChildren:
                    item.addChild(newChild)
                self.endInsertRows()


===============================[ 102 ]===============================
def update_suggestions_dictionary(request, object):
    if request.user.is_authenticated():
        user = request.user
        content_type = ContentType.objects.get_for_model(type(object))
        try:
            ObjectView.objects.get(
                user=user, object_id=object.id, content_type=content_type)
        except:
            ObjectView.objects.create(user=user, content_object=object)
        viewed = ObjectView.objects.filter(user=user)
    else:
        update_dict_for_guests(request, object, content_type)
        return
    if viewed:
        for obj in viewed:
            if content_type == obj.content_type:
                if not exists_in_dictionary(request, object,
                                            content_type,
                                            obj, True):
                    if object.id != obj.object_id:
                        ObjectViewDictionary.objects.create(
                            current_object=object,
                            visited_before_object=obj.content_object)
                        if not exists_in_dictionary(request, obj,
                                                    obj.content_type,
                                                    object, False):
                            ObjectViewDictionary.objects.create(
                                current_object=obj.content_object,
                                visited_before_object=object)
    return


===============================[ 103 ]===============================
def parse_version(version):
    global parse_version
    try:
        from pkg_resources import parse_version
    except ImportError:
        from distutils.version import LooseVersion as parse_version
    return parse_version(version)


===============================[ 104 ]===============================
def sum_mvns(distributions):
  graph_parents = [tensor for distribution in distributions
                   for tensor in distribution._graph_parents]  
  with tf.compat.v1.name_scope('sum_mvns', values=graph_parents):
    if all([isinstance(mvn, tfd.MultivariateNormalDiag)
            for mvn in distributions]):
      return tfd.MultivariateNormalDiag(
          loc=sum([mvn.mean() for mvn in distributions]),
          scale_diag=tf.sqrt(sum([
              mvn.scale.diag**2 for mvn in distributions])))
    else:
      raise NotImplementedError(
          'Sums of distributions other than MultivariateNormalDiag are not '
          'currently implemented. (given: {})'.format(distributions))


===============================[ 105 ]===============================
def crz(self, theta, ctl, tgt):
    return self.append(CrzGate(theta), [ctl, tgt], [])


===============================[ 106 ]===============================
def build_core_type(s_cdt):
    s_dt = nav_one(s_cdt).S_DT[17]()
    if s_dt.name == 'void':
        type_name = None
    elif s_dt.name == 'boolean':
        type_name = 'xs:boolean'
    elif s_dt.name == 'integer':
        type_name = 'xs:integer'
    elif s_dt.name == 'real':
        type_name = 'xs:decimal'
    elif s_dt.name == 'string':
        type_name = 'xs:string'
    elif s_dt.name == 'unique_id':
        type_name = 'xs:integer'
    else:
        type_name = None
    if type_name:
        mapped_type = ET.Element('xs:simpleType', name=s_dt.name)
        ET.SubElement(mapped_type, 'xs:restriction', base=type_name)
        return mapped_type


===============================[ 107 ]===============================
def _filter_deprecation_warnings():
    deprecation_filter = ('always', None, DeprecationWarning,
                          re.compile(r'^qiskit\.*', re.UNICODE), 0)
    try:
        warnings._add_filter(*deprecation_filter, append=False)
    except AttributeError:
        pass
    warnings.simplefilter('ignore', category=ChangedInMarshmallow3Warning)


===============================[ 108 ]===============================
def  make_file_list( args = None ):
    file_list = []
    if not args:
        args = sys.argv[1 :]
    for pathname in args:
        if string.find( pathname, '*' ) >= 0:
            newpath = glob.glob( pathname )
            newpath.sort()  
        else:
            newpath = [pathname]
        file_list.extend( newpath )
    if len( file_list ) == 0:
        file_list = None
    else:
        file_list = filter( file_exists, file_list )
    return file_list


===============================[ 109 ]===============================
def nwise(iter, n):
 iterset = [iter]
 while len(iterset) < n:
		iterset[-1:] = itertools.tee(iterset[-1])
  next(iterset[-1], None)
 return six.moves.zip(*iterset)


===============================[ 110 ]===============================
def handle_ping(self, payload):
        self.logger.info('server ping: %s' % payload)
        self.send('PONG %s' % payload, True)


===============================[ 111 ]===============================
def new(self, kind, *args, **kwargs):
        metaclass = self.find_metaclass(kind)
        return metaclass.new(*args, **kwargs)


===============================[ 112 ]===============================
def parse_questions(raw_page):
        raw_questions = json.loads(raw_page)
        questions = raw_questions['items']
        for question in questions:
            yield question


===============================[ 113 ]===============================
def wait_on_rate_limit(self, value):
        check_type(value, bool, may_be_none=False)
        self._wait_on_rate_limit = value


===============================[ 114 ]===============================
def add_text(self, text, cursor=None, justification=None):
        if cursor is None:
            cursor = self.page.cursor
        text = re.sub("\s\s+" , " ", text)
        if justification is None:
            justification = self.justification
        if '\n' in text:
            text_list = text.split('\n')
            for text in text_list:
                PDFText(self.session, self.page, text, self.font, self.text_color, cursor, justification, self.double_spacing)
                self.add_newline()
        else:
            PDFText(self.session, self.page, text, self.font, self.text_color, cursor, justification, self.double_spacing)


===============================[ 115 ]===============================
def write_json(f: TextIO, deja_vu_sans_path: str,
               measurer: text_measurer.TextMeasurer,
               encodings: Iterable[str]) -> None:
    supported_characters = list(
        generate_supported_characters(deja_vu_sans_path))
    kerning_characters = ''.join(
        generate_encodeable_characters(supported_characters, encodings))
    char_to_length = calculate_character_to_length_mapping(measurer,
                                                           supported_characters)
    pair_to_kerning = calculate_pair_to_kern_mapping(measurer, char_to_length,
                                                     kerning_characters)
    json.dump(
        {'mean-character-length': statistics.mean(char_to_length.values()),
         'character-lengths': char_to_length,
         'kerning-characters': kerning_characters,
         'kerning-pairs': pair_to_kerning},
        f, sort_keys=True, indent=1)


===============================[ 116 ]===============================
def _node_to_asn(self, node):
    if node.is_type(TokenType.identifier):
      return Identifier(node.svalue)
    elif node.is_type(TokenType.terminal):
      return Terminal(node.svalue)
    elif node.is_type(TokenType.option_group):
      expr = node.children[0]
      return OptionGroup(self._expression_to_asn(expr))
    elif node.is_type(TokenType.repetition_group):
      expr = node.children[0]
      return RepetitionGroup(self._expression_to_asn(expr))
    elif node.is_type(TokenType.grouping_group):
      expr = node.children[0]
      return GroupingGroup(self._expression_to_asn(expr))
    elif node.is_type(TokenType.special_handling):
      ident = node.children[0]
      return SpecialHandling(ident)
    elif node.is_type(TokenType.number):
      return Number(node.svalue)
    elif node.is_type((TokenType.operator, TokenType.op_mult, TokenType.op_add)):
      return OperatorNode(OPERATOR_INDEX[node.svalue], node.position)
    else:
      raise Exception("Unhandled parse tree node: {0}".format(node))


===============================[ 117 ]===============================
def launch_ipython_5_shell(args):
    import IPython  
    from traitlets.config import Config
    c = Config()
    path = os.path.dirname(os.path.abspath(__file__))
    try:
        get_ipython  
        _print("WARNING: Running IPython within IPython.")
    except NameError:
        c.InteractiveShell.banner1 = 'SolveBio Python shell started.\n'
    c.InteractiveShellApp.exec_files = ['{}/ipython_init.py'.format(path)]
    IPython.start_ipython(argv=[], config=c)


===============================[ 118 ]===============================
def indexes(self, collection=None):
        indexes = []
        for collection_name in self.collections():
            if collection and collection != collection_name:
                continue
            for index_name in self.db[collection_name].index_information():
                if index_name != '_id_':
                    indexes.append(index_name)
        return indexes


===============================[ 119 ]===============================
def redraw_canvas(self):
        from xdot_parser import XdotAttrParser
        xdot_parser = XdotAttrParser()
        canvas = self._component_default()
        for node in self.nodes:
            components = xdot_parser.parse_xdot_data( node._draw_ )
            canvas.add( *components )
            components = xdot_parser.parse_xdot_data( node._ldraw_ )
            canvas.add( *components )
        for edge in self.edges:
            components = xdot_parser.parse_xdot_data( edge._draw_ )
            canvas.add( *components )
            components = xdot_parser.parse_xdot_data( edge._ldraw_ )
            canvas.add( *components )
            components = xdot_parser.parse_xdot_data( edge._hdraw_ )
            canvas.add( *components )
            components = xdot_parser.parse_xdot_data( edge._tdraw_ )
            canvas.add( *components )
            components = xdot_parser.parse_xdot_data( edge._hldraw_ )
            canvas.add( *components )
            components = xdot_parser.parse_xdot_data( edge._tldraw_ )
            canvas.add( *components )
        self.component = canvas
        self.vp.request_redraw()


===============================[ 120 ]===============================
def _parse_dot_code_fired(self):
        parser = GodotDataParser()
        graph  = parser.parse_dot_data(self.dot_code)
        if graph is not None:
            self.model = graph


===============================[ 121 ]===============================
def compute(self, *inputs, **kwargs):
        from deepy.core.neural_var import NeuralVariable
        from deepy.core.graph import graph
        if type(inputs[0]) != NeuralVariable:
            raise SystemError("The input of `compute` must be NeuralVar")
        dims = [t.dim() for t in inputs]
        if len(inputs) == 1:
            self.init(input_dim=dims[0])
        else:
            self.init(input_dims=dims)
        if self.parameters and not self._linked_block:
            self.belongs_to(graph.default_block())
        train_kwargs, _, _ = convert_to_theano_var(kwargs)
        output = self.compute_tensor(*[t.tensor for t in inputs], **train_kwargs)
        if type(output) != list and type(output) != tuple:
            return NeuralVariable(output, dim=self.output_dim)
        else:
            return [NeuralVariable(*item) for item in zip(output, self.output_dims)]


===============================[ 122 ]===============================
def exec_command(ctx, section, command, map_files):
    try:
        logger.debug('Running exec command')
        existing_sections = config.ConfigFileResolver(config.LOCAL_CONFIG_FILE).sections()
        command = ' '.join(command)
        if section not in existing_sections:
            command = '{} {}'.format(section, command) if command else section
            section = None
        if not command:
            logger.warning('No command detected.')
            click.echo(exec_command.get_help(ctx))
            return
        settings = config.Settings(section=section)
        storage = STORAGES['s3'](settings=settings)
        conf = s3conf.S3Conf(storage=storage, settings=settings)
        env_vars = conf.get_envfile().as_dict()
        if env_vars.get('S3CONF_MAP') and map_files:
            conf.download_mapping(env_vars.get('S3CONF_MAP'))
        current_env = os.environ.copy()
        current_env.update(env_vars)
        logger.debug('Executing command "%s"', command)
        subprocess.run(shlex.split(command), env=current_env, check=True)
    except exceptions.EnvfilePathNotDefinedError:
        raise exceptions.EnvfilePathNotDefinedUsageError()


