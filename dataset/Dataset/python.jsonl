{"repo": "StaticCube/python-synology", "path": "SynologyDSM/SynologyDSM.py", "func_name": "SynoUtilization.network_up", "original_string": "def network_up(self, human_readable=True):\r\n        \"\"\"Total upload speed being used\"\"\"\r\n        network = self._get_network(\"total\")\r\n        if network is not None:\r\n            return_data = int(network[\"tx\"])\r\n            if human_readable:\r\n                return SynoFormatHelper.bytes_to_readable(\r\n                    return_data)\r\n            else:\r\n                return return_data", "language": "python", "code": "def network_up(self, human_readable=True):\r\n        \"\"\"Total upload speed being used\"\"\"\r\n        network = self._get_network(\"total\")\r\n        if network is not None:\r\n            return_data = int(network[\"tx\"])\r\n            if human_readable:\r\n                return SynoFormatHelper.bytes_to_readable(\r\n                    return_data)\r\n            else:\r\n                return return_data", "code_tokens": ["def", "network_up", "(", "self", ",", "human_readable", "=", "True", ")", ":", "network", "=", "self", ".", "_get_network", "(", "\"total\"", ")", "if", "network", "is", "not", "None", ":", "return_data", "=", "int", "(", "network", "[", "\"tx\"", "]", ")", "if", "human_readable", ":", "return", "SynoFormatHelper", ".", "bytes_to_readable", "(", "return_data", ")", "else", ":", "return", "return_data"], "docstring": "Total upload speed being used", "docstring_tokens": ["Total", "upload", "speed", "being", "used"], "sha": "a5446a052fc91a38f7589803dc7a654180db2566", "url": "https://github.com/StaticCube/python-synology/blob/a5446a052fc91a38f7589803dc7a654180db2566/SynologyDSM/SynologyDSM.py#L185-L194", "partition": "test"}
{"repo": "bolt-project/bolt", "path": "bolt/utils.py", "func_name": "slicify", "original_string": "def slicify(slc, dim):\n    \"\"\"\n    Force a slice to have defined start, stop, and step from a known dim.\n    Start and stop will always be positive. Step may be negative.\n\n    There is an exception where a negative step overflows the stop needs to have\n    the default value set to -1. This is the only case of a negative start/stop\n    value.\n\n    Parameters\n    ----------\n    slc : slice or int\n        The slice to modify, or int to convert to a slice\n\n    dim : tuple\n        Bound for slice\n    \"\"\"\n    if isinstance(slc, slice):\n\n        # default limits\n        start = 0 if slc.start is None else slc.start\n        stop = dim if slc.stop is None else slc.stop\n        step = 1 if slc.step is None else slc.step\n        # account for negative indices\n        if start < 0: start += dim\n        if stop < 0: stop += dim\n        # account for over-flowing the bounds\n        if step > 0:\n            if start < 0: start = 0\n            if stop > dim: stop = dim\n        else:\n            if stop < 0: stop = -1\n            if start > dim: start = dim-1\n\n        return slice(start, stop, step)\n\n    elif isinstance(slc, int):\n        if slc < 0:\n            slc += dim\n        return slice(slc, slc+1, 1)\n\n    else:\n        raise ValueError(\"Type for slice %s not recongized\" % type(slc))", "language": "python", "code": "def slicify(slc, dim):\n    \"\"\"\n    Force a slice to have defined start, stop, and step from a known dim.\n    Start and stop will always be positive. Step may be negative.\n\n    There is an exception where a negative step overflows the stop needs to have\n    the default value set to -1. This is the only case of a negative start/stop\n    value.\n\n    Parameters\n    ----------\n    slc : slice or int\n        The slice to modify, or int to convert to a slice\n\n    dim : tuple\n        Bound for slice\n    \"\"\"\n    if isinstance(slc, slice):\n\n        # default limits\n        start = 0 if slc.start is None else slc.start\n        stop = dim if slc.stop is None else slc.stop\n        step = 1 if slc.step is None else slc.step\n        # account for negative indices\n        if start < 0: start += dim\n        if stop < 0: stop += dim\n        # account for over-flowing the bounds\n        if step > 0:\n            if start < 0: start = 0\n            if stop > dim: stop = dim\n        else:\n            if stop < 0: stop = -1\n            if start > dim: start = dim-1\n\n        return slice(start, stop, step)\n\n    elif isinstance(slc, int):\n        if slc < 0:\n            slc += dim\n        return slice(slc, slc+1, 1)\n\n    else:\n        raise ValueError(\"Type for slice %s not recongized\" % type(slc))", "code_tokens": ["def", "slicify", "(", "slc", ",", "dim", ")", ":", "if", "isinstance", "(", "slc", ",", "slice", ")", ":", "# default limits", "start", "=", "0", "if", "slc", ".", "start", "is", "None", "else", "slc", ".", "start", "stop", "=", "dim", "if", "slc", ".", "stop", "is", "None", "else", "slc", ".", "stop", "step", "=", "1", "if", "slc", ".", "step", "is", "None", "else", "slc", ".", "step", "# account for negative indices", "if", "start", "<", "0", ":", "start", "+=", "dim", "if", "stop", "<", "0", ":", "stop", "+=", "dim", "# account for over-flowing the bounds", "if", "step", ">", "0", ":", "if", "start", "<", "0", ":", "start", "=", "0", "if", "stop", ">", "dim", ":", "stop", "=", "dim", "else", ":", "if", "stop", "<", "0", ":", "stop", "=", "-", "1", "if", "start", ">", "dim", ":", "start", "=", "dim", "-", "1", "return", "slice", "(", "start", ",", "stop", ",", "step", ")", "elif", "isinstance", "(", "slc", ",", "int", ")", ":", "if", "slc", "<", "0", ":", "slc", "+=", "dim", "return", "slice", "(", "slc", ",", "slc", "+", "1", ",", "1", ")", "else", ":", "raise", "ValueError", "(", "\"Type for slice %s not recongized\"", "%", "type", "(", "slc", ")", ")"], "docstring": "Force a slice to have defined start, stop, and step from a known dim.\n    Start and stop will always be positive. Step may be negative.\n\n    There is an exception where a negative step overflows the stop needs to have\n    the default value set to -1. This is the only case of a negative start/stop\n    value.\n\n    Parameters\n    ----------\n    slc : slice or int\n        The slice to modify, or int to convert to a slice\n\n    dim : tuple\n        Bound for slice", "docstring_tokens": ["Force", "a", "slice", "to", "have", "defined", "start", "stop", "and", "step", "from", "a", "known", "dim", ".", "Start", "and", "stop", "will", "always", "be", "positive", ".", "Step", "may", "be", "negative", "."], "sha": "9cd7104aa085498da3097b72696184b9d3651c51", "url": "https://github.com/bolt-project/bolt/blob/9cd7104aa085498da3097b72696184b9d3651c51/bolt/utils.py#L105-L147", "partition": "test"}
{"repo": "Microsoft/botbuilder-python", "path": "libraries/botbuilder-azure/botbuilder/azure/cosmosdb_storage.py", "func_name": "CosmosDbStorage.__get_or_create_container", "original_string": "def __get_or_create_container(self, doc_client, container) -> str:\n        \"\"\"Return the container link.\n\n        Check if the container exists or create the container.\n\n        :param doc_client:\n        :param container:\n        :return str:\n        \"\"\"\n        # query CosmosDB for a container in the database with that name\n        containers = list(doc_client.QueryContainers(\n            self.__database_link,\n            {\n                \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                \"parameters\": [\n                    {\"name\": \"@id\", \"value\": container}\n                ]\n            }\n        ))\n        # if there are results, return the first (container names are unique)\n        if len(containers) > 0:\n            return containers[0]['id']\n        else:\n            # Create a container if it didn't exist\n            res = doc_client.CreateContainer(\n                self.__database_link, {'id': container})\n            return res['id']", "language": "python", "code": "def __get_or_create_container(self, doc_client, container) -> str:\n        \"\"\"Return the container link.\n\n        Check if the container exists or create the container.\n\n        :param doc_client:\n        :param container:\n        :return str:\n        \"\"\"\n        # query CosmosDB for a container in the database with that name\n        containers = list(doc_client.QueryContainers(\n            self.__database_link,\n            {\n                \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                \"parameters\": [\n                    {\"name\": \"@id\", \"value\": container}\n                ]\n            }\n        ))\n        # if there are results, return the first (container names are unique)\n        if len(containers) > 0:\n            return containers[0]['id']\n        else:\n            # Create a container if it didn't exist\n            res = doc_client.CreateContainer(\n                self.__database_link, {'id': container})\n            return res['id']", "code_tokens": ["def", "__get_or_create_container", "(", "self", ",", "doc_client", ",", "container", ")", "->", "str", ":", "# query CosmosDB for a container in the database with that name", "containers", "=", "list", "(", "doc_client", ".", "QueryContainers", "(", "self", ".", "__database_link", ",", "{", "\"query\"", ":", "\"SELECT * FROM r WHERE r.id=@id\"", ",", "\"parameters\"", ":", "[", "{", "\"name\"", ":", "\"@id\"", ",", "\"value\"", ":", "container", "}", "]", "}", ")", ")", "# if there are results, return the first (container names are unique)", "if", "len", "(", "containers", ")", ">", "0", ":", "return", "containers", "[", "0", "]", "[", "'id'", "]", "else", ":", "# Create a container if it didn't exist", "res", "=", "doc_client", ".", "CreateContainer", "(", "self", ".", "__database_link", ",", "{", "'id'", ":", "container", "}", ")", "return", "res", "[", "'id'", "]"], "docstring": "Return the container link.\n\n        Check if the container exists or create the container.\n\n        :param doc_client:\n        :param container:\n        :return str:", "docstring_tokens": ["Return", "the", "container", "link", "."], "sha": "274663dd91c811bae6ac4488915ba5880771b0a7", "url": "https://github.com/Microsoft/botbuilder-python/blob/274663dd91c811bae6ac4488915ba5880771b0a7/libraries/botbuilder-azure/botbuilder/azure/cosmosdb_storage.py#L275-L301", "partition": "test"}
{"repo": "tonyseek/flask-navigation", "path": "flask_navigation/api.py", "func_name": "Navigation.initialize_bars", "original_string": "def initialize_bars(self, sender=None, **kwargs):\n        \"\"\"Calls the initializers of all bound navigation bars.\"\"\"\n        for bar in self.bars.values():\n            for initializer in bar.initializers:\n                initializer(self)", "language": "python", "code": "def initialize_bars(self, sender=None, **kwargs):\n        \"\"\"Calls the initializers of all bound navigation bars.\"\"\"\n        for bar in self.bars.values():\n            for initializer in bar.initializers:\n                initializer(self)", "code_tokens": ["def", "initialize_bars", "(", "self", ",", "sender", "=", "None", ",", "*", "*", "kwargs", ")", ":", "for", "bar", "in", "self", ".", "bars", ".", "values", "(", ")", ":", "for", "initializer", "in", "bar", ".", "initializers", ":", "initializer", "(", "self", ")"], "docstring": "Calls the initializers of all bound navigation bars.", "docstring_tokens": ["Calls", "the", "initializers", "of", "all", "bound", "navigation", "bars", "."], "sha": "38fa83addcbe62f31516763fbe3c0bbdc793dc96", "url": "https://github.com/tonyseek/flask-navigation/blob/38fa83addcbe62f31516763fbe3c0bbdc793dc96/flask_navigation/api.py#L50-L54", "partition": "test"}
{"repo": "unistra/django-rest-framework-custom-exceptions", "path": "rest_framework_custom_exceptions/exceptions.py", "func_name": "simple_error_handler", "original_string": "def simple_error_handler(exc, *args):\n    \"\"\"\n    Returns the response that should be used for any given exception.\n\n    By default we handle the REST framework `APIException`, and also\n    Django's builtin `Http404` and `PermissionDenied` exceptions.\n\n    Any unhandled exceptions may return `None`, which will cause a 500 error\n    to be raised.\n    \"\"\"\n    if isinstance(exc, exceptions.APIException):\n        headers = {}\n        if getattr(exc, 'auth_header', None):\n            headers['WWW-Authenticate'] = exc.auth_header\n        if getattr(exc, 'wait', None):\n            headers['X-Throttle-Wait-Seconds'] = '%d' % exc.wait\n\n        return Response({'error': exc.detail},\n                        status=exc.status_code,\n                        headers=headers)\n\n    elif isinstance(exc, Http404):\n        return Response({'error': 'Not found'},\n                        status=status.HTTP_404_NOT_FOUND)\n\n    elif isinstance(exc, PermissionDenied):\n        return Response({'error': 'Permission denied'},\n                        status=status.HTTP_403_FORBIDDEN)\n\n    # Note: Unhandled exceptions will raise a 500 error.\n    return None", "language": "python", "code": "def simple_error_handler(exc, *args):\n    \"\"\"\n    Returns the response that should be used for any given exception.\n\n    By default we handle the REST framework `APIException`, and also\n    Django's builtin `Http404` and `PermissionDenied` exceptions.\n\n    Any unhandled exceptions may return `None`, which will cause a 500 error\n    to be raised.\n    \"\"\"\n    if isinstance(exc, exceptions.APIException):\n        headers = {}\n        if getattr(exc, 'auth_header', None):\n            headers['WWW-Authenticate'] = exc.auth_header\n        if getattr(exc, 'wait', None):\n            headers['X-Throttle-Wait-Seconds'] = '%d' % exc.wait\n\n        return Response({'error': exc.detail},\n                        status=exc.status_code,\n                        headers=headers)\n\n    elif isinstance(exc, Http404):\n        return Response({'error': 'Not found'},\n                        status=status.HTTP_404_NOT_FOUND)\n\n    elif isinstance(exc, PermissionDenied):\n        return Response({'error': 'Permission denied'},\n                        status=status.HTTP_403_FORBIDDEN)\n\n    # Note: Unhandled exceptions will raise a 500 error.\n    return None", "code_tokens": ["def", "simple_error_handler", "(", "exc", ",", "*", "args", ")", ":", "if", "isinstance", "(", "exc", ",", "exceptions", ".", "APIException", ")", ":", "headers", "=", "{", "}", "if", "getattr", "(", "exc", ",", "'auth_header'", ",", "None", ")", ":", "headers", "[", "'WWW-Authenticate'", "]", "=", "exc", ".", "auth_header", "if", "getattr", "(", "exc", ",", "'wait'", ",", "None", ")", ":", "headers", "[", "'X-Throttle-Wait-Seconds'", "]", "=", "'%d'", "%", "exc", ".", "wait", "return", "Response", "(", "{", "'error'", ":", "exc", ".", "detail", "}", ",", "status", "=", "exc", ".", "status_code", ",", "headers", "=", "headers", ")", "elif", "isinstance", "(", "exc", ",", "Http404", ")", ":", "return", "Response", "(", "{", "'error'", ":", "'Not found'", "}", ",", "status", "=", "status", ".", "HTTP_404_NOT_FOUND", ")", "elif", "isinstance", "(", "exc", ",", "PermissionDenied", ")", ":", "return", "Response", "(", "{", "'error'", ":", "'Permission denied'", "}", ",", "status", "=", "status", ".", "HTTP_403_FORBIDDEN", ")", "# Note: Unhandled exceptions will raise a 500 error.", "return", "None"], "docstring": "Returns the response that should be used for any given exception.\n\n    By default we handle the REST framework `APIException`, and also\n    Django's builtin `Http404` and `PermissionDenied` exceptions.\n\n    Any unhandled exceptions may return `None`, which will cause a 500 error\n    to be raised.", "docstring_tokens": ["Returns", "the", "response", "that", "should", "be", "used", "for", "any", "given", "exception", "."], "sha": "b0fdd5c64146c4f25fb893bc84c58d7774ccb5ef", "url": "https://github.com/unistra/django-rest-framework-custom-exceptions/blob/b0fdd5c64146c4f25fb893bc84c58d7774ccb5ef/rest_framework_custom_exceptions/exceptions.py#L9-L39", "partition": "test"}
{"repo": "Yelp/py_zipkin", "path": "py_zipkin/encoding/protobuf/__init__.py", "func_name": "_convert_endpoint", "original_string": "def _convert_endpoint(endpoint):\n    \"\"\"Converts py_zipkin's Endpoint to Protobuf's Endpoint.\n\n    :param endpoint: py_zipkins' endpoint to convert.\n    :type endpoint: py_zipkin.encoding.Endpoint\n    :return: corresponding protobuf's endpoint.\n    :rtype: zipkin_pb2.Endpoint\n    \"\"\"\n    pb_endpoint = zipkin_pb2.Endpoint()\n\n    if endpoint.service_name:\n        pb_endpoint.service_name = endpoint.service_name\n    if endpoint.port and endpoint.port != 0:\n        pb_endpoint.port = endpoint.port\n    if endpoint.ipv4:\n        pb_endpoint.ipv4 = socket.inet_pton(socket.AF_INET, endpoint.ipv4)\n    if endpoint.ipv6:\n        pb_endpoint.ipv6 = socket.inet_pton(socket.AF_INET6, endpoint.ipv6)\n\n    return pb_endpoint", "language": "python", "code": "def _convert_endpoint(endpoint):\n    \"\"\"Converts py_zipkin's Endpoint to Protobuf's Endpoint.\n\n    :param endpoint: py_zipkins' endpoint to convert.\n    :type endpoint: py_zipkin.encoding.Endpoint\n    :return: corresponding protobuf's endpoint.\n    :rtype: zipkin_pb2.Endpoint\n    \"\"\"\n    pb_endpoint = zipkin_pb2.Endpoint()\n\n    if endpoint.service_name:\n        pb_endpoint.service_name = endpoint.service_name\n    if endpoint.port and endpoint.port != 0:\n        pb_endpoint.port = endpoint.port\n    if endpoint.ipv4:\n        pb_endpoint.ipv4 = socket.inet_pton(socket.AF_INET, endpoint.ipv4)\n    if endpoint.ipv6:\n        pb_endpoint.ipv6 = socket.inet_pton(socket.AF_INET6, endpoint.ipv6)\n\n    return pb_endpoint", "code_tokens": ["def", "_convert_endpoint", "(", "endpoint", ")", ":", "pb_endpoint", "=", "zipkin_pb2", ".", "Endpoint", "(", ")", "if", "endpoint", ".", "service_name", ":", "pb_endpoint", ".", "service_name", "=", "endpoint", ".", "service_name", "if", "endpoint", ".", "port", "and", "endpoint", ".", "port", "!=", "0", ":", "pb_endpoint", ".", "port", "=", "endpoint", ".", "port", "if", "endpoint", ".", "ipv4", ":", "pb_endpoint", ".", "ipv4", "=", "socket", ".", "inet_pton", "(", "socket", ".", "AF_INET", ",", "endpoint", ".", "ipv4", ")", "if", "endpoint", ".", "ipv6", ":", "pb_endpoint", ".", "ipv6", "=", "socket", ".", "inet_pton", "(", "socket", ".", "AF_INET6", ",", "endpoint", ".", "ipv6", ")", "return", "pb_endpoint"], "docstring": "Converts py_zipkin's Endpoint to Protobuf's Endpoint.\n\n    :param endpoint: py_zipkins' endpoint to convert.\n    :type endpoint: py_zipkin.encoding.Endpoint\n    :return: corresponding protobuf's endpoint.\n    :rtype: zipkin_pb2.Endpoint", "docstring_tokens": ["Converts", "py_zipkin", "s", "Endpoint", "to", "Protobuf", "s", "Endpoint", "."], "sha": "0944d9a3fb1f1798dbb276694aeed99f2b4283ba", "url": "https://github.com/Yelp/py_zipkin/blob/0944d9a3fb1f1798dbb276694aeed99f2b4283ba/py_zipkin/encoding/protobuf/__init__.py#L134-L153", "partition": "test"}
{"repo": "mcs07/MolVS", "path": "molvs/resonance.py", "func_name": "ResonanceEnumerator.enumerate", "original_string": "def enumerate(self, mol):\n        \"\"\"Enumerate all possible resonance forms and return them as a list.\n\n        :param mol: The input molecule.\n        :type mol: rdkit.Chem.rdchem.Mol\n        :return: A list of all possible resonance forms of the molecule.\n        :rtype: list of rdkit.Chem.rdchem.Mol\n        \"\"\"\n        flags = 0\n        if self.kekule_all:\n            flags = flags | Chem.KEKULE_ALL\n        if self.allow_incomplete_octets:\n            flags = flags | Chem.ALLOW_INCOMPLETE_OCTETS\n        if self.allow_charge_separation:\n            flags = flags | Chem.ALLOW_CHARGE_SEPARATION\n        if self.unconstrained_anions:\n            flags = flags | Chem.UNCONSTRAINED_ANIONS\n        if self.unconstrained_cations:\n            flags = flags | Chem.UNCONSTRAINED_CATIONS\n        results = []\n        for result in Chem.ResonanceMolSupplier(mol, flags=flags, maxStructs=self.max_structures):\n            # This seems necessary? ResonanceMolSupplier only does a partial sanitization\n            Chem.SanitizeMol(result)\n            results.append(result)\n        return results", "language": "python", "code": "def enumerate(self, mol):\n        \"\"\"Enumerate all possible resonance forms and return them as a list.\n\n        :param mol: The input molecule.\n        :type mol: rdkit.Chem.rdchem.Mol\n        :return: A list of all possible resonance forms of the molecule.\n        :rtype: list of rdkit.Chem.rdchem.Mol\n        \"\"\"\n        flags = 0\n        if self.kekule_all:\n            flags = flags | Chem.KEKULE_ALL\n        if self.allow_incomplete_octets:\n            flags = flags | Chem.ALLOW_INCOMPLETE_OCTETS\n        if self.allow_charge_separation:\n            flags = flags | Chem.ALLOW_CHARGE_SEPARATION\n        if self.unconstrained_anions:\n            flags = flags | Chem.UNCONSTRAINED_ANIONS\n        if self.unconstrained_cations:\n            flags = flags | Chem.UNCONSTRAINED_CATIONS\n        results = []\n        for result in Chem.ResonanceMolSupplier(mol, flags=flags, maxStructs=self.max_structures):\n            # This seems necessary? ResonanceMolSupplier only does a partial sanitization\n            Chem.SanitizeMol(result)\n            results.append(result)\n        return results", "code_tokens": ["def", "enumerate", "(", "self", ",", "mol", ")", ":", "flags", "=", "0", "if", "self", ".", "kekule_all", ":", "flags", "=", "flags", "|", "Chem", ".", "KEKULE_ALL", "if", "self", ".", "allow_incomplete_octets", ":", "flags", "=", "flags", "|", "Chem", ".", "ALLOW_INCOMPLETE_OCTETS", "if", "self", ".", "allow_charge_separation", ":", "flags", "=", "flags", "|", "Chem", ".", "ALLOW_CHARGE_SEPARATION", "if", "self", ".", "unconstrained_anions", ":", "flags", "=", "flags", "|", "Chem", ".", "UNCONSTRAINED_ANIONS", "if", "self", ".", "unconstrained_cations", ":", "flags", "=", "flags", "|", "Chem", ".", "UNCONSTRAINED_CATIONS", "results", "=", "[", "]", "for", "result", "in", "Chem", ".", "ResonanceMolSupplier", "(", "mol", ",", "flags", "=", "flags", ",", "maxStructs", "=", "self", ".", "max_structures", ")", ":", "# This seems necessary? ResonanceMolSupplier only does a partial sanitization", "Chem", ".", "SanitizeMol", "(", "result", ")", "results", ".", "append", "(", "result", ")", "return", "results"], "docstring": "Enumerate all possible resonance forms and return them as a list.\n\n        :param mol: The input molecule.\n        :type mol: rdkit.Chem.rdchem.Mol\n        :return: A list of all possible resonance forms of the molecule.\n        :rtype: list of rdkit.Chem.rdchem.Mol", "docstring_tokens": ["Enumerate", "all", "possible", "resonance", "forms", "and", "return", "them", "as", "a", "list", "."], "sha": "d815fe52d160abcecbcbf117e6437bf727dbd8ad", "url": "https://github.com/mcs07/MolVS/blob/d815fe52d160abcecbcbf117e6437bf727dbd8ad/molvs/resonance.py#L52-L76", "partition": "test"}
{"repo": "SmokinCaterpillar/pypet", "path": "pypet/trajectory.py", "func_name": "Trajectory._preset", "original_string": "def _preset(self, name, args, kwargs):\n        \"\"\"Generic preset function, marks a parameter or config for presetting.\"\"\"\n        if self.f_contains(name, shortcuts=False):\n            raise ValueError('Parameter `%s` is already part of your trajectory, use the normal'\n                             'accessing routine to change config.' % name)\n        else:\n            self._changed_default_parameters[name] = (args, kwargs)", "language": "python", "code": "def _preset(self, name, args, kwargs):\n        \"\"\"Generic preset function, marks a parameter or config for presetting.\"\"\"\n        if self.f_contains(name, shortcuts=False):\n            raise ValueError('Parameter `%s` is already part of your trajectory, use the normal'\n                             'accessing routine to change config.' % name)\n        else:\n            self._changed_default_parameters[name] = (args, kwargs)", "code_tokens": ["def", "_preset", "(", "self", ",", "name", ",", "args", ",", "kwargs", ")", ":", "if", "self", ".", "f_contains", "(", "name", ",", "shortcuts", "=", "False", ")", ":", "raise", "ValueError", "(", "'Parameter `%s` is already part of your trajectory, use the normal'", "'accessing routine to change config.'", "%", "name", ")", "else", ":", "self", ".", "_changed_default_parameters", "[", "name", "]", "=", "(", "args", ",", "kwargs", ")"], "docstring": "Generic preset function, marks a parameter or config for presetting.", "docstring_tokens": ["Generic", "preset", "function", "marks", "a", "parameter", "or", "config", "for", "presetting", "."], "sha": "97ad3e80d46dbdea02deeb98ea41f05a19565826", "url": "https://github.com/SmokinCaterpillar/pypet/blob/97ad3e80d46dbdea02deeb98ea41f05a19565826/pypet/trajectory.py#L731-L737", "partition": "test"}
{"repo": "NateFerrero/oauth2lib", "path": "oauth2lib/provider.py", "func_name": "AuthorizationProvider.get_authorization_code", "original_string": "def get_authorization_code(self,\n                               response_type,\n                               client_id,\n                               redirect_uri,\n                               **params):\n        \"\"\"Generate authorization code HTTP response.\n\n        :param response_type: Desired response type. Must be exactly \"code\".\n        :type response_type: str\n        :param client_id: Client ID.\n        :type client_id: str\n        :param redirect_uri: Client redirect URI.\n        :type redirect_uri: str\n        :rtype: requests.Response\n        \"\"\"\n\n        # Ensure proper response_type\n        if response_type != 'code':\n            err = 'unsupported_response_type'\n            return self._make_redirect_error_response(redirect_uri, err)\n\n        # Check redirect URI\n        is_valid_redirect_uri = self.validate_redirect_uri(client_id,\n                                                           redirect_uri)\n        if not is_valid_redirect_uri:\n            return self._invalid_redirect_uri_response()\n\n        # Check conditions\n        is_valid_client_id = self.validate_client_id(client_id)\n        is_valid_access = self.validate_access()\n        scope = params.get('scope', '')\n        is_valid_scope = self.validate_scope(client_id, scope)\n\n        # Return proper error responses on invalid conditions\n        if not is_valid_client_id:\n            err = 'unauthorized_client'\n            return self._make_redirect_error_response(redirect_uri, err)\n\n        if not is_valid_access:\n            err = 'access_denied'\n            return self._make_redirect_error_response(redirect_uri, err)\n\n        if not is_valid_scope:\n            err = 'invalid_scope'\n            return self._make_redirect_error_response(redirect_uri, err)\n\n        # Generate authorization code\n        code = self.generate_authorization_code()\n\n        # Save information to be used to validate later requests\n        self.persist_authorization_code(client_id=client_id,\n                                        code=code,\n                                        scope=scope)\n\n        # Return redirection response\n        params.update({\n            'code': code,\n            'response_type': None,\n            'client_id': None,\n            'redirect_uri': None\n        })\n        redirect = utils.build_url(redirect_uri, params)\n        return self._make_response(headers={'Location': redirect},\n                                   status_code=302)", "language": "python", "code": "def get_authorization_code(self,\n                               response_type,\n                               client_id,\n                               redirect_uri,\n                               **params):\n        \"\"\"Generate authorization code HTTP response.\n\n        :param response_type: Desired response type. Must be exactly \"code\".\n        :type response_type: str\n        :param client_id: Client ID.\n        :type client_id: str\n        :param redirect_uri: Client redirect URI.\n        :type redirect_uri: str\n        :rtype: requests.Response\n        \"\"\"\n\n        # Ensure proper response_type\n        if response_type != 'code':\n            err = 'unsupported_response_type'\n            return self._make_redirect_error_response(redirect_uri, err)\n\n        # Check redirect URI\n        is_valid_redirect_uri = self.validate_redirect_uri(client_id,\n                                                           redirect_uri)\n        if not is_valid_redirect_uri:\n            return self._invalid_redirect_uri_response()\n\n        # Check conditions\n        is_valid_client_id = self.validate_client_id(client_id)\n        is_valid_access = self.validate_access()\n        scope = params.get('scope', '')\n        is_valid_scope = self.validate_scope(client_id, scope)\n\n        # Return proper error responses on invalid conditions\n        if not is_valid_client_id:\n            err = 'unauthorized_client'\n            return self._make_redirect_error_response(redirect_uri, err)\n\n        if not is_valid_access:\n            err = 'access_denied'\n            return self._make_redirect_error_response(redirect_uri, err)\n\n        if not is_valid_scope:\n            err = 'invalid_scope'\n            return self._make_redirect_error_response(redirect_uri, err)\n\n        # Generate authorization code\n        code = self.generate_authorization_code()\n\n        # Save information to be used to validate later requests\n        self.persist_authorization_code(client_id=client_id,\n                                        code=code,\n                                        scope=scope)\n\n        # Return redirection response\n        params.update({\n            'code': code,\n            'response_type': None,\n            'client_id': None,\n            'redirect_uri': None\n        })\n        redirect = utils.build_url(redirect_uri, params)\n        return self._make_response(headers={'Location': redirect},\n                                   status_code=302)", "code_tokens": ["def", "get_authorization_code", "(", "self", ",", "response_type", ",", "client_id", ",", "redirect_uri", ",", "*", "*", "params", ")", ":", "# Ensure proper response_type", "if", "response_type", "!=", "'code'", ":", "err", "=", "'unsupported_response_type'", "return", "self", ".", "_make_redirect_error_response", "(", "redirect_uri", ",", "err", ")", "# Check redirect URI", "is_valid_redirect_uri", "=", "self", ".", "validate_redirect_uri", "(", "client_id", ",", "redirect_uri", ")", "if", "not", "is_valid_redirect_uri", ":", "return", "self", ".", "_invalid_redirect_uri_response", "(", ")", "# Check conditions", "is_valid_client_id", "=", "self", ".", "validate_client_id", "(", "client_id", ")", "is_valid_access", "=", "self", ".", "validate_access", "(", ")", "scope", "=", "params", ".", "get", "(", "'scope'", ",", "''", ")", "is_valid_scope", "=", "self", ".", "validate_scope", "(", "client_id", ",", "scope", ")", "# Return proper error responses on invalid conditions", "if", "not", "is_valid_client_id", ":", "err", "=", "'unauthorized_client'", "return", "self", ".", "_make_redirect_error_response", "(", "redirect_uri", ",", "err", ")", "if", "not", "is_valid_access", ":", "err", "=", "'access_denied'", "return", "self", ".", "_make_redirect_error_response", "(", "redirect_uri", ",", "err", ")", "if", "not", "is_valid_scope", ":", "err", "=", "'invalid_scope'", "return", "self", ".", "_make_redirect_error_response", "(", "redirect_uri", ",", "err", ")", "# Generate authorization code", "code", "=", "self", ".", "generate_authorization_code", "(", ")", "# Save information to be used to validate later requests", "self", ".", "persist_authorization_code", "(", "client_id", "=", "client_id", ",", "code", "=", "code", ",", "scope", "=", "scope", ")", "# Return redirection response", "params", ".", "update", "(", "{", "'code'", ":", "code", ",", "'response_type'", ":", "None", ",", "'client_id'", ":", "None", ",", "'redirect_uri'", ":", "None", "}", ")", "redirect", "=", "utils", ".", "build_url", "(", "redirect_uri", ",", "params", ")", "return", "self", ".", "_make_response", "(", "headers", "=", "{", "'Location'", ":", "redirect", "}", ",", "status_code", "=", "302", ")"], "docstring": "Generate authorization code HTTP response.\n\n        :param response_type: Desired response type. Must be exactly \"code\".\n        :type response_type: str\n        :param client_id: Client ID.\n        :type client_id: str\n        :param redirect_uri: Client redirect URI.\n        :type redirect_uri: str\n        :rtype: requests.Response", "docstring_tokens": ["Generate", "authorization", "code", "HTTP", "response", "."], "sha": "d161b010f8a596826050a09e5e94d59443cc12d9", "url": "https://github.com/NateFerrero/oauth2lib/blob/d161b010f8a596826050a09e5e94d59443cc12d9/oauth2lib/provider.py#L205-L268", "partition": "test"}
{"repo": "dopefishh/pympi", "path": "pympi/Elan.py", "func_name": "Eaf.rename_tier", "original_string": "def rename_tier(self, id_from, id_to):\n        \"\"\"Rename a tier. Note that this renames also the child tiers that have\n        the tier as a parent.\n\n        :param str id_from: Original name of the tier.\n        :param str id_to: Target name of the tier.\n        :throws KeyError: If the tier doesnt' exist.\n        \"\"\"\n        childs = self.get_child_tiers_for(id_from)\n        self.tiers[id_to] = self.tiers.pop(id_from)\n        self.tiers[id_to][2]['TIER_ID'] = id_to\n        for child in childs:\n            self.tiers[child][2]['PARENT_REF'] = id_to", "language": "python", "code": "def rename_tier(self, id_from, id_to):\n        \"\"\"Rename a tier. Note that this renames also the child tiers that have\n        the tier as a parent.\n\n        :param str id_from: Original name of the tier.\n        :param str id_to: Target name of the tier.\n        :throws KeyError: If the tier doesnt' exist.\n        \"\"\"\n        childs = self.get_child_tiers_for(id_from)\n        self.tiers[id_to] = self.tiers.pop(id_from)\n        self.tiers[id_to][2]['TIER_ID'] = id_to\n        for child in childs:\n            self.tiers[child][2]['PARENT_REF'] = id_to", "code_tokens": ["def", "rename_tier", "(", "self", ",", "id_from", ",", "id_to", ")", ":", "childs", "=", "self", ".", "get_child_tiers_for", "(", "id_from", ")", "self", ".", "tiers", "[", "id_to", "]", "=", "self", ".", "tiers", ".", "pop", "(", "id_from", ")", "self", ".", "tiers", "[", "id_to", "]", "[", "2", "]", "[", "'TIER_ID'", "]", "=", "id_to", "for", "child", "in", "childs", ":", "self", ".", "tiers", "[", "child", "]", "[", "2", "]", "[", "'PARENT_REF'", "]", "=", "id_to"], "docstring": "Rename a tier. Note that this renames also the child tiers that have\n        the tier as a parent.\n\n        :param str id_from: Original name of the tier.\n        :param str id_to: Target name of the tier.\n        :throws KeyError: If the tier doesnt' exist.", "docstring_tokens": ["Rename", "a", "tier", ".", "Note", "that", "this", "renames", "also", "the", "child", "tiers", "that", "have", "the", "tier", "as", "a", "parent", "."], "sha": "79c747cde45b5ba203ed93154d8c123ac9c3ef56", "url": "https://github.com/dopefishh/pympi/blob/79c747cde45b5ba203ed93154d8c123ac9c3ef56/pympi/Elan.py#L1276-L1288", "partition": "test"}
{"repo": "dagster-io/dagster", "path": "python_modules/dagster/dagster/core/definitions/repository.py", "func_name": "RepositoryDefinition.get_pipeline", "original_string": "def get_pipeline(self, name):\n        '''Get a pipeline by name. Only constructs that pipeline and caches it.\n\n        Args:\n            name (str): Name of the pipeline to retriever\n\n        Returns:\n            PipelineDefinition: Instance of PipelineDefinition with that name.\n        '''\n        check.str_param(name, 'name')\n\n        if name in self._pipeline_cache:\n            return self._pipeline_cache[name]\n\n        try:\n            pipeline = self.pipeline_dict[name]()\n        except KeyError:\n            raise DagsterInvariantViolationError(\n                'Could not find pipeline \"{name}\". Found: {pipeline_names}.'.format(\n                    name=name,\n                    pipeline_names=', '.join(\n                        [\n                            '\"{pipeline_name}\"'.format(pipeline_name=pipeline_name)\n                            for pipeline_name in self.pipeline_dict.keys()\n                        ]\n                    ),\n                )\n            )\n        check.invariant(\n            pipeline.name == name,\n            'Name does not match. Name in dict {name}. Name in pipeline {pipeline.name}'.format(\n                name=name, pipeline=pipeline\n            ),\n        )\n\n        self._pipeline_cache[name] = check.inst(\n            pipeline,\n            PipelineDefinition,\n            (\n                'Function passed into pipeline_dict with key {key} must return a '\n                'PipelineDefinition'\n            ).format(key=name),\n        )\n\n        return pipeline", "language": "python", "code": "def get_pipeline(self, name):\n        '''Get a pipeline by name. Only constructs that pipeline and caches it.\n\n        Args:\n            name (str): Name of the pipeline to retriever\n\n        Returns:\n            PipelineDefinition: Instance of PipelineDefinition with that name.\n        '''\n        check.str_param(name, 'name')\n\n        if name in self._pipeline_cache:\n            return self._pipeline_cache[name]\n\n        try:\n            pipeline = self.pipeline_dict[name]()\n        except KeyError:\n            raise DagsterInvariantViolationError(\n                'Could not find pipeline \"{name}\". Found: {pipeline_names}.'.format(\n                    name=name,\n                    pipeline_names=', '.join(\n                        [\n                            '\"{pipeline_name}\"'.format(pipeline_name=pipeline_name)\n                            for pipeline_name in self.pipeline_dict.keys()\n                        ]\n                    ),\n                )\n            )\n        check.invariant(\n            pipeline.name == name,\n            'Name does not match. Name in dict {name}. Name in pipeline {pipeline.name}'.format(\n                name=name, pipeline=pipeline\n            ),\n        )\n\n        self._pipeline_cache[name] = check.inst(\n            pipeline,\n            PipelineDefinition,\n            (\n                'Function passed into pipeline_dict with key {key} must return a '\n                'PipelineDefinition'\n            ).format(key=name),\n        )\n\n        return pipeline", "code_tokens": ["def", "get_pipeline", "(", "self", ",", "name", ")", ":", "check", ".", "str_param", "(", "name", ",", "'name'", ")", "if", "name", "in", "self", ".", "_pipeline_cache", ":", "return", "self", ".", "_pipeline_cache", "[", "name", "]", "try", ":", "pipeline", "=", "self", ".", "pipeline_dict", "[", "name", "]", "(", ")", "except", "KeyError", ":", "raise", "DagsterInvariantViolationError", "(", "'Could not find pipeline \"{name}\". Found: {pipeline_names}.'", ".", "format", "(", "name", "=", "name", ",", "pipeline_names", "=", "', '", ".", "join", "(", "[", "'\"{pipeline_name}\"'", ".", "format", "(", "pipeline_name", "=", "pipeline_name", ")", "for", "pipeline_name", "in", "self", ".", "pipeline_dict", ".", "keys", "(", ")", "]", ")", ",", ")", ")", "check", ".", "invariant", "(", "pipeline", ".", "name", "==", "name", ",", "'Name does not match. Name in dict {name}. Name in pipeline {pipeline.name}'", ".", "format", "(", "name", "=", "name", ",", "pipeline", "=", "pipeline", ")", ",", ")", "self", ".", "_pipeline_cache", "[", "name", "]", "=", "check", ".", "inst", "(", "pipeline", ",", "PipelineDefinition", ",", "(", "'Function passed into pipeline_dict with key {key} must return a '", "'PipelineDefinition'", ")", ".", "format", "(", "key", "=", "name", ")", ",", ")", "return", "pipeline"], "docstring": "Get a pipeline by name. Only constructs that pipeline and caches it.\n\n        Args:\n            name (str): Name of the pipeline to retriever\n\n        Returns:\n            PipelineDefinition: Instance of PipelineDefinition with that name.", "docstring_tokens": ["Get", "a", "pipeline", "by", "name", ".", "Only", "constructs", "that", "pipeline", "and", "caches", "it", "."], "sha": "4119f8c773089de64831b1dfb9e168e353d401dc", "url": "https://github.com/dagster-io/dagster/blob/4119f8c773089de64831b1dfb9e168e353d401dc/python_modules/dagster/dagster/core/definitions/repository.py#L56-L100", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/visualization/interactive/iplot_qsphere.py", "func_name": "n_choose_k", "original_string": "def n_choose_k(n, k):\n    \"\"\"Return the number of combinations for n choose k.\n\n    Args:\n        n (int): the total number of options .\n        k (int): The number of elements.\n\n    Returns:\n        int: returns the binomial coefficient\n    \"\"\"\n    if n == 0:\n        return 0\n    return reduce(lambda x, y: x * y[0] / y[1],\n                  zip(range(n - k + 1, n + 1),\n                      range(1, k + 1)), 1)", "language": "python", "code": "def n_choose_k(n, k):\n    \"\"\"Return the number of combinations for n choose k.\n\n    Args:\n        n (int): the total number of options .\n        k (int): The number of elements.\n\n    Returns:\n        int: returns the binomial coefficient\n    \"\"\"\n    if n == 0:\n        return 0\n    return reduce(lambda x, y: x * y[0] / y[1],\n                  zip(range(n - k + 1, n + 1),\n                      range(1, k + 1)), 1)", "code_tokens": ["def", "n_choose_k", "(", "n", ",", "k", ")", ":", "if", "n", "==", "0", ":", "return", "0", "return", "reduce", "(", "lambda", "x", ",", "y", ":", "x", "*", "y", "[", "0", "]", "/", "y", "[", "1", "]", ",", "zip", "(", "range", "(", "n", "-", "k", "+", "1", ",", "n", "+", "1", ")", ",", "range", "(", "1", ",", "k", "+", "1", ")", ")", ",", "1", ")"], "docstring": "Return the number of combinations for n choose k.\n\n    Args:\n        n (int): the total number of options .\n        k (int): The number of elements.\n\n    Returns:\n        int: returns the binomial coefficient", "docstring_tokens": ["Return", "the", "number", "of", "combinations", "for", "n", "choose", "k", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/visualization/interactive/iplot_qsphere.py#L158-L172", "partition": "test"}
{"repo": "assemblerflow/flowcraft", "path": "flowcraft/templates/flowcraft_utils/flowcraft_base.py", "func_name": "MainWrapper.build_versions", "original_string": "def build_versions(self):\n        \"\"\"Writes versions JSON for a template file\n\n        This method creates the JSON file ``.versions`` based on the metadata\n        and specific functions that are present in a given template script.\n\n        It starts by fetching the template metadata, which can be specified\n        via the ``__version__``, ``__template__`` and ``__build__``\n        attributes. If all of these attributes exist, it starts to populate\n        a JSON/dict array (Note that the absence of any one of them will\n        prevent the version from being written).\n\n        Then, it will search the\n        template scope for functions that start with the substring\n        ``__set_version`` (For example ``def __set_version_fastqc()`).\n        These functions should gather the version of\n        an arbitrary program and return a JSON/dict object with the following\n        information::\n\n            {\n                \"program\": <program_name>,\n                \"version\": <version>\n                \"build\": <build>\n            }\n\n        This JSON/dict object is then written in the ``.versions`` file.\n        \"\"\"\n\n        version_storage = []\n\n        template_version = self.context.get(\"__version__\", None)\n        template_program = self.context.get(\"__template__\", None)\n        template_build = self.context.get(\"__build__\", None)\n\n        if template_version and template_program and template_build:\n            if self.logger:\n                self.logger.debug(\"Adding template version: {}; {}; \"\n                                  \"{}\".format(template_program,\n                                              template_version,\n                                              template_build))\n            version_storage.append({\n                \"program\": template_program,\n                \"version\": template_version,\n                \"build\": template_build\n            })\n\n        for var, obj in self.context.items():\n            if var.startswith(\"__get_version\"):\n                ver = obj()\n                version_storage.append(ver)\n                if self.logger:\n                    self.logger.debug(\"Found additional software version\"\n                                      \"{}\".format(ver))\n\n        with open(\".versions\", \"w\") as fh:\n            fh.write(json.dumps(version_storage, separators=(\",\", \":\")))", "language": "python", "code": "def build_versions(self):\n        \"\"\"Writes versions JSON for a template file\n\n        This method creates the JSON file ``.versions`` based on the metadata\n        and specific functions that are present in a given template script.\n\n        It starts by fetching the template metadata, which can be specified\n        via the ``__version__``, ``__template__`` and ``__build__``\n        attributes. If all of these attributes exist, it starts to populate\n        a JSON/dict array (Note that the absence of any one of them will\n        prevent the version from being written).\n\n        Then, it will search the\n        template scope for functions that start with the substring\n        ``__set_version`` (For example ``def __set_version_fastqc()`).\n        These functions should gather the version of\n        an arbitrary program and return a JSON/dict object with the following\n        information::\n\n            {\n                \"program\": <program_name>,\n                \"version\": <version>\n                \"build\": <build>\n            }\n\n        This JSON/dict object is then written in the ``.versions`` file.\n        \"\"\"\n\n        version_storage = []\n\n        template_version = self.context.get(\"__version__\", None)\n        template_program = self.context.get(\"__template__\", None)\n        template_build = self.context.get(\"__build__\", None)\n\n        if template_version and template_program and template_build:\n            if self.logger:\n                self.logger.debug(\"Adding template version: {}; {}; \"\n                                  \"{}\".format(template_program,\n                                              template_version,\n                                              template_build))\n            version_storage.append({\n                \"program\": template_program,\n                \"version\": template_version,\n                \"build\": template_build\n            })\n\n        for var, obj in self.context.items():\n            if var.startswith(\"__get_version\"):\n                ver = obj()\n                version_storage.append(ver)\n                if self.logger:\n                    self.logger.debug(\"Found additional software version\"\n                                      \"{}\".format(ver))\n\n        with open(\".versions\", \"w\") as fh:\n            fh.write(json.dumps(version_storage, separators=(\",\", \":\")))", "code_tokens": ["def", "build_versions", "(", "self", ")", ":", "version_storage", "=", "[", "]", "template_version", "=", "self", ".", "context", ".", "get", "(", "\"__version__\"", ",", "None", ")", "template_program", "=", "self", ".", "context", ".", "get", "(", "\"__template__\"", ",", "None", ")", "template_build", "=", "self", ".", "context", ".", "get", "(", "\"__build__\"", ",", "None", ")", "if", "template_version", "and", "template_program", "and", "template_build", ":", "if", "self", ".", "logger", ":", "self", ".", "logger", ".", "debug", "(", "\"Adding template version: {}; {}; \"", "\"{}\"", ".", "format", "(", "template_program", ",", "template_version", ",", "template_build", ")", ")", "version_storage", ".", "append", "(", "{", "\"program\"", ":", "template_program", ",", "\"version\"", ":", "template_version", ",", "\"build\"", ":", "template_build", "}", ")", "for", "var", ",", "obj", "in", "self", ".", "context", ".", "items", "(", ")", ":", "if", "var", ".", "startswith", "(", "\"__get_version\"", ")", ":", "ver", "=", "obj", "(", ")", "version_storage", ".", "append", "(", "ver", ")", "if", "self", ".", "logger", ":", "self", ".", "logger", ".", "debug", "(", "\"Found additional software version\"", "\"{}\"", ".", "format", "(", "ver", ")", ")", "with", "open", "(", "\".versions\"", ",", "\"w\"", ")", "as", "fh", ":", "fh", ".", "write", "(", "json", ".", "dumps", "(", "version_storage", ",", "separators", "=", "(", "\",\"", ",", "\":\"", ")", ")", ")"], "docstring": "Writes versions JSON for a template file\n\n        This method creates the JSON file ``.versions`` based on the metadata\n        and specific functions that are present in a given template script.\n\n        It starts by fetching the template metadata, which can be specified\n        via the ``__version__``, ``__template__`` and ``__build__``\n        attributes. If all of these attributes exist, it starts to populate\n        a JSON/dict array (Note that the absence of any one of them will\n        prevent the version from being written).\n\n        Then, it will search the\n        template scope for functions that start with the substring\n        ``__set_version`` (For example ``def __set_version_fastqc()`).\n        These functions should gather the version of\n        an arbitrary program and return a JSON/dict object with the following\n        information::\n\n            {\n                \"program\": <program_name>,\n                \"version\": <version>\n                \"build\": <build>\n            }\n\n        This JSON/dict object is then written in the ``.versions`` file.", "docstring_tokens": ["Writes", "versions", "JSON", "for", "a", "template", "file"], "sha": "fc3f4bddded1efc76006600016dc71a06dd908c0", "url": "https://github.com/assemblerflow/flowcraft/blob/fc3f4bddded1efc76006600016dc71a06dd908c0/flowcraft/templates/flowcraft_utils/flowcraft_base.py#L67-L122", "partition": "test"}
{"repo": "LordSputnik/mutagen", "path": "mutagen/apev2.py", "func_name": "APEValue", "original_string": "def APEValue(value, kind):\n    \"\"\"APEv2 tag value factory.\n\n    Use this if you need to specify the value's type manually.  Binary\n    and text data are automatically detected by APEv2.__setitem__.\n    \"\"\"\n\n    if kind in (TEXT, EXTERNAL):\n        if not isinstance(value, text_type):\n            # stricter with py3\n            if PY3:\n                raise TypeError(\"str only for text/external values\")\n        else:\n            value = value.encode(\"utf-8\")\n\n    if kind == TEXT:\n        return APETextValue(value, kind)\n    elif kind == BINARY:\n        return APEBinaryValue(value, kind)\n    elif kind == EXTERNAL:\n        return APEExtValue(value, kind)\n    else:\n        raise ValueError(\"kind must be TEXT, BINARY, or EXTERNAL\")", "language": "python", "code": "def APEValue(value, kind):\n    \"\"\"APEv2 tag value factory.\n\n    Use this if you need to specify the value's type manually.  Binary\n    and text data are automatically detected by APEv2.__setitem__.\n    \"\"\"\n\n    if kind in (TEXT, EXTERNAL):\n        if not isinstance(value, text_type):\n            # stricter with py3\n            if PY3:\n                raise TypeError(\"str only for text/external values\")\n        else:\n            value = value.encode(\"utf-8\")\n\n    if kind == TEXT:\n        return APETextValue(value, kind)\n    elif kind == BINARY:\n        return APEBinaryValue(value, kind)\n    elif kind == EXTERNAL:\n        return APEExtValue(value, kind)\n    else:\n        raise ValueError(\"kind must be TEXT, BINARY, or EXTERNAL\")", "code_tokens": ["def", "APEValue", "(", "value", ",", "kind", ")", ":", "if", "kind", "in", "(", "TEXT", ",", "EXTERNAL", ")", ":", "if", "not", "isinstance", "(", "value", ",", "text_type", ")", ":", "# stricter with py3", "if", "PY3", ":", "raise", "TypeError", "(", "\"str only for text/external values\"", ")", "else", ":", "value", "=", "value", ".", "encode", "(", "\"utf-8\"", ")", "if", "kind", "==", "TEXT", ":", "return", "APETextValue", "(", "value", ",", "kind", ")", "elif", "kind", "==", "BINARY", ":", "return", "APEBinaryValue", "(", "value", ",", "kind", ")", "elif", "kind", "==", "EXTERNAL", ":", "return", "APEExtValue", "(", "value", ",", "kind", ")", "else", ":", "raise", "ValueError", "(", "\"kind must be TEXT, BINARY, or EXTERNAL\"", ")"], "docstring": "APEv2 tag value factory.\n\n    Use this if you need to specify the value's type manually.  Binary\n    and text data are automatically detected by APEv2.__setitem__.", "docstring_tokens": ["APEv2", "tag", "value", "factory", "."], "sha": "38e62c8dc35c72b16554f5dbe7c0fde91acc3411", "url": "https://github.com/LordSputnik/mutagen/blob/38e62c8dc35c72b16554f5dbe7c0fde91acc3411/mutagen/apev2.py#L460-L482", "partition": "test"}
{"repo": "reingart/gui2py", "path": "gui/dialog.py", "func_name": "prompt", "original_string": "def prompt(message=\"\", title=\"\", default=\"\", multiline=False, password=None, \r\n           parent=None):\r\n    \"Modal dialog asking for an input, returns string or None if cancelled\"\r\n    if password:\r\n        style = wx.TE_PASSWORD | wx.OK | wx.CANCEL\r\n        result = dialogs.textEntryDialog(parent, message, title, default, style)\r\n    elif multiline:\r\n        style = wx.TE_MULTILINE | wx.OK | wx.CANCEL\r\n        result = dialogs.textEntryDialog(parent, message, title, default, style)\r\n        # workaround for Mac OS X\r\n        result.text = '\\n'.join(result.text.splitlines())\r\n    else:\r\n        result = dialogs.textEntryDialog(parent, message, title, default)\r\n    if result.accepted:\r\n        return result.text", "language": "python", "code": "def prompt(message=\"\", title=\"\", default=\"\", multiline=False, password=None, \r\n           parent=None):\r\n    \"Modal dialog asking for an input, returns string or None if cancelled\"\r\n    if password:\r\n        style = wx.TE_PASSWORD | wx.OK | wx.CANCEL\r\n        result = dialogs.textEntryDialog(parent, message, title, default, style)\r\n    elif multiline:\r\n        style = wx.TE_MULTILINE | wx.OK | wx.CANCEL\r\n        result = dialogs.textEntryDialog(parent, message, title, default, style)\r\n        # workaround for Mac OS X\r\n        result.text = '\\n'.join(result.text.splitlines())\r\n    else:\r\n        result = dialogs.textEntryDialog(parent, message, title, default)\r\n    if result.accepted:\r\n        return result.text", "code_tokens": ["def", "prompt", "(", "message", "=", "\"\"", ",", "title", "=", "\"\"", ",", "default", "=", "\"\"", ",", "multiline", "=", "False", ",", "password", "=", "None", ",", "parent", "=", "None", ")", ":", "if", "password", ":", "style", "=", "wx", ".", "TE_PASSWORD", "|", "wx", ".", "OK", "|", "wx", ".", "CANCEL", "result", "=", "dialogs", ".", "textEntryDialog", "(", "parent", ",", "message", ",", "title", ",", "default", ",", "style", ")", "elif", "multiline", ":", "style", "=", "wx", ".", "TE_MULTILINE", "|", "wx", ".", "OK", "|", "wx", ".", "CANCEL", "result", "=", "dialogs", ".", "textEntryDialog", "(", "parent", ",", "message", ",", "title", ",", "default", ",", "style", ")", "# workaround for Mac OS X\r", "result", ".", "text", "=", "'\\n'", ".", "join", "(", "result", ".", "text", ".", "splitlines", "(", ")", ")", "else", ":", "result", "=", "dialogs", ".", "textEntryDialog", "(", "parent", ",", "message", ",", "title", ",", "default", ")", "if", "result", ".", "accepted", ":", "return", "result", ".", "text"], "docstring": "Modal dialog asking for an input, returns string or None if cancelled", "docstring_tokens": ["Modal", "dialog", "asking", "for", "an", "input", "returns", "string", "or", "None", "if", "cancelled"], "sha": "aca0a05f6fcde55c94ad7cc058671a06608b01a4", "url": "https://github.com/reingart/gui2py/blob/aca0a05f6fcde55c94ad7cc058671a06608b01a4/gui/dialog.py#L26-L40", "partition": "test"}
{"repo": "gtaylor/python-route53", "path": "route53/xml_parsers/common_change_info.py", "func_name": "parse_change_info", "original_string": "def parse_change_info(e_change_info):\n    \"\"\"\n    Parses a ChangeInfo tag. Seen in CreateHostedZone, DeleteHostedZone,\n    and ChangeResourceRecordSetsRequest.\n\n    :param lxml.etree._Element e_change_info: A ChangeInfo element.\n    :rtype: dict\n    :returns: A dict representation of the change info.\n    \"\"\"\n\n    if e_change_info is None:\n        return e_change_info\n\n    status = e_change_info.find('./{*}Status').text\n    submitted_at = e_change_info.find('./{*}SubmittedAt').text\n    submitted_at = parse_iso_8601_time_str(submitted_at)\n\n    return {\n        'request_id': id,\n        'request_status': status,\n        'request_submitted_at': submitted_at\n    }", "language": "python", "code": "def parse_change_info(e_change_info):\n    \"\"\"\n    Parses a ChangeInfo tag. Seen in CreateHostedZone, DeleteHostedZone,\n    and ChangeResourceRecordSetsRequest.\n\n    :param lxml.etree._Element e_change_info: A ChangeInfo element.\n    :rtype: dict\n    :returns: A dict representation of the change info.\n    \"\"\"\n\n    if e_change_info is None:\n        return e_change_info\n\n    status = e_change_info.find('./{*}Status').text\n    submitted_at = e_change_info.find('./{*}SubmittedAt').text\n    submitted_at = parse_iso_8601_time_str(submitted_at)\n\n    return {\n        'request_id': id,\n        'request_status': status,\n        'request_submitted_at': submitted_at\n    }", "code_tokens": ["def", "parse_change_info", "(", "e_change_info", ")", ":", "if", "e_change_info", "is", "None", ":", "return", "e_change_info", "status", "=", "e_change_info", ".", "find", "(", "'./{*}Status'", ")", ".", "text", "submitted_at", "=", "e_change_info", ".", "find", "(", "'./{*}SubmittedAt'", ")", ".", "text", "submitted_at", "=", "parse_iso_8601_time_str", "(", "submitted_at", ")", "return", "{", "'request_id'", ":", "id", ",", "'request_status'", ":", "status", ",", "'request_submitted_at'", ":", "submitted_at", "}"], "docstring": "Parses a ChangeInfo tag. Seen in CreateHostedZone, DeleteHostedZone,\n    and ChangeResourceRecordSetsRequest.\n\n    :param lxml.etree._Element e_change_info: A ChangeInfo element.\n    :rtype: dict\n    :returns: A dict representation of the change info.", "docstring_tokens": ["Parses", "a", "ChangeInfo", "tag", ".", "Seen", "in", "CreateHostedZone", "DeleteHostedZone", "and", "ChangeResourceRecordSetsRequest", "."], "sha": "b9fc7e258a79551c9ed61e4a71668b7f06f9e774", "url": "https://github.com/gtaylor/python-route53/blob/b9fc7e258a79551c9ed61e4a71668b7f06f9e774/route53/xml_parsers/common_change_info.py#L8-L29", "partition": "test"}
{"repo": "amorison/loam", "path": "loam/manager.py", "func_name": "ConfigurationManager.update_", "original_string": "def update_(self, conf_dict, conf_arg=True):\n        \"\"\"Update values of configuration options with dict.\n\n        Args:\n            conf_dict (dict): dict of dict indexed with section and option\n                names.\n            conf_arg (bool): if True, only options that can be set in a config\n                file are updated.\n        \"\"\"\n        for section, secdict in conf_dict.items():\n            self[section].update_(secdict, conf_arg)", "language": "python", "code": "def update_(self, conf_dict, conf_arg=True):\n        \"\"\"Update values of configuration options with dict.\n\n        Args:\n            conf_dict (dict): dict of dict indexed with section and option\n                names.\n            conf_arg (bool): if True, only options that can be set in a config\n                file are updated.\n        \"\"\"\n        for section, secdict in conf_dict.items():\n            self[section].update_(secdict, conf_arg)", "code_tokens": ["def", "update_", "(", "self", ",", "conf_dict", ",", "conf_arg", "=", "True", ")", ":", "for", "section", ",", "secdict", "in", "conf_dict", ".", "items", "(", ")", ":", "self", "[", "section", "]", ".", "update_", "(", "secdict", ",", "conf_arg", ")"], "docstring": "Update values of configuration options with dict.\n\n        Args:\n            conf_dict (dict): dict of dict indexed with section and option\n                names.\n            conf_arg (bool): if True, only options that can be set in a config\n                file are updated.", "docstring_tokens": ["Update", "values", "of", "configuration", "options", "with", "dict", "."], "sha": "a566c943a75e068a4510099331a1ddfe5bbbdd94", "url": "https://github.com/amorison/loam/blob/a566c943a75e068a4510099331a1ddfe5bbbdd94/loam/manager.py#L339-L349", "partition": "test"}
{"repo": "nicfit/nicfit.py", "path": "nicfit/logger.py", "func_name": "applyLoggingOpts", "original_string": "def applyLoggingOpts(log_levels, log_files):\n    \"\"\"Apply logging options produced by LogLevelAction and LogFileAction.\n\n    More often then not this function is not needed, the actions have already\n    been taken during the parse, but it can be used in the case they need to be\n    applied again (e.g. when command line opts take precedence but were\n    overridded by a fileConfig, etc.).\n    \"\"\"\n    for l, lvl in log_levels:\n        l.setLevel(lvl)\n    for l, hdl in log_files:\n        for h in l.handlers:\n            l.removeHandler(h)\n        l.addHandler(hdl)", "language": "python", "code": "def applyLoggingOpts(log_levels, log_files):\n    \"\"\"Apply logging options produced by LogLevelAction and LogFileAction.\n\n    More often then not this function is not needed, the actions have already\n    been taken during the parse, but it can be used in the case they need to be\n    applied again (e.g. when command line opts take precedence but were\n    overridded by a fileConfig, etc.).\n    \"\"\"\n    for l, lvl in log_levels:\n        l.setLevel(lvl)\n    for l, hdl in log_files:\n        for h in l.handlers:\n            l.removeHandler(h)\n        l.addHandler(hdl)", "code_tokens": ["def", "applyLoggingOpts", "(", "log_levels", ",", "log_files", ")", ":", "for", "l", ",", "lvl", "in", "log_levels", ":", "l", ".", "setLevel", "(", "lvl", ")", "for", "l", ",", "hdl", "in", "log_files", ":", "for", "h", "in", "l", ".", "handlers", ":", "l", ".", "removeHandler", "(", "h", ")", "l", ".", "addHandler", "(", "hdl", ")"], "docstring": "Apply logging options produced by LogLevelAction and LogFileAction.\n\n    More often then not this function is not needed, the actions have already\n    been taken during the parse, but it can be used in the case they need to be\n    applied again (e.g. when command line opts take precedence but were\n    overridded by a fileConfig, etc.).", "docstring_tokens": ["Apply", "logging", "options", "produced", "by", "LogLevelAction", "and", "LogFileAction", "."], "sha": "8313f8edbc5e7361ddad496d6d818324b5236c7a", "url": "https://github.com/nicfit/nicfit.py/blob/8313f8edbc5e7361ddad496d6d818324b5236c7a/nicfit/logger.py#L125-L138", "partition": "test"}
{"repo": "HumanBrainProject/hbp-service-client", "path": "hbp_service_client/storage_service/client.py", "func_name": "Client.get_parent", "original_string": "def get_parent(self, path):\n        '''Get the parent entity of the entity pointed by the given path.\n\n        Args:\n            path (str): The path of the entity whose parent is needed\n\n        Returns:\n            A JSON object of the parent entity if found.\n\n        Raises:\n            StorageArgumentException: Invalid arguments\n            StorageForbiddenException: Server response code 403\n            StorageNotFoundException: Server response code 404\n            StorageException: other 400-600 error codes\n        '''\n\n        self.__validate_storage_path(path, projects_allowed=False)\n        path_steps = [step for step in path.split('/') if step]\n        del path_steps[-1]\n        parent_path = '/{0}'.format('/'.join(path_steps))\n        return self.api_client.get_entity_by_query(path=parent_path)", "language": "python", "code": "def get_parent(self, path):\n        '''Get the parent entity of the entity pointed by the given path.\n\n        Args:\n            path (str): The path of the entity whose parent is needed\n\n        Returns:\n            A JSON object of the parent entity if found.\n\n        Raises:\n            StorageArgumentException: Invalid arguments\n            StorageForbiddenException: Server response code 403\n            StorageNotFoundException: Server response code 404\n            StorageException: other 400-600 error codes\n        '''\n\n        self.__validate_storage_path(path, projects_allowed=False)\n        path_steps = [step for step in path.split('/') if step]\n        del path_steps[-1]\n        parent_path = '/{0}'.format('/'.join(path_steps))\n        return self.api_client.get_entity_by_query(path=parent_path)", "code_tokens": ["def", "get_parent", "(", "self", ",", "path", ")", ":", "self", ".", "__validate_storage_path", "(", "path", ",", "projects_allowed", "=", "False", ")", "path_steps", "=", "[", "step", "for", "step", "in", "path", ".", "split", "(", "'/'", ")", "if", "step", "]", "del", "path_steps", "[", "-", "1", "]", "parent_path", "=", "'/{0}'", ".", "format", "(", "'/'", ".", "join", "(", "path_steps", ")", ")", "return", "self", ".", "api_client", ".", "get_entity_by_query", "(", "path", "=", "parent_path", ")"], "docstring": "Get the parent entity of the entity pointed by the given path.\n\n        Args:\n            path (str): The path of the entity whose parent is needed\n\n        Returns:\n            A JSON object of the parent entity if found.\n\n        Raises:\n            StorageArgumentException: Invalid arguments\n            StorageForbiddenException: Server response code 403\n            StorageNotFoundException: Server response code 404\n            StorageException: other 400-600 error codes", "docstring_tokens": ["Get", "the", "parent", "entity", "of", "the", "entity", "pointed", "by", "the", "given", "path", "."], "sha": "b338fb41a7f0e7b9d654ff28fcf13a56d03bff4d", "url": "https://github.com/HumanBrainProject/hbp-service-client/blob/b338fb41a7f0e7b9d654ff28fcf13a56d03bff4d/hbp_service_client/storage_service/client.py#L145-L165", "partition": "test"}
{"repo": "streamlink/streamlink", "path": "src/streamlink_cli/main.py", "func_name": "load_plugins", "original_string": "def load_plugins(dirs):\n    \"\"\"Attempts to load plugins from a list of directories.\"\"\"\n\n    dirs = [os.path.expanduser(d) for d in dirs]\n\n    for directory in dirs:\n        if os.path.isdir(directory):\n            streamlink.load_plugins(directory)\n        else:\n            log.warning(\"Plugin path {0} does not exist or is not \"\n                        \"a directory!\", directory)", "language": "python", "code": "def load_plugins(dirs):\n    \"\"\"Attempts to load plugins from a list of directories.\"\"\"\n\n    dirs = [os.path.expanduser(d) for d in dirs]\n\n    for directory in dirs:\n        if os.path.isdir(directory):\n            streamlink.load_plugins(directory)\n        else:\n            log.warning(\"Plugin path {0} does not exist or is not \"\n                        \"a directory!\", directory)", "code_tokens": ["def", "load_plugins", "(", "dirs", ")", ":", "dirs", "=", "[", "os", ".", "path", ".", "expanduser", "(", "d", ")", "for", "d", "in", "dirs", "]", "for", "directory", "in", "dirs", ":", "if", "os", ".", "path", ".", "isdir", "(", "directory", ")", ":", "streamlink", ".", "load_plugins", "(", "directory", ")", "else", ":", "log", ".", "warning", "(", "\"Plugin path {0} does not exist or is not \"", "\"a directory!\"", ",", "directory", ")"], "docstring": "Attempts to load plugins from a list of directories.", "docstring_tokens": ["Attempts", "to", "load", "plugins", "from", "a", "list", "of", "directories", "."], "sha": "c8ed1daff14ac03195870238b9b900c1109dd5c1", "url": "https://github.com/streamlink/streamlink/blob/c8ed1daff14ac03195870238b9b900c1109dd5c1/src/streamlink_cli/main.py#L650-L660", "partition": "test"}
{"repo": "vaexio/vaex", "path": "packages/vaex-core/vaex/dataframe.py", "func_name": "DataFrame.add_virtual_columns_rotation", "original_string": "def add_virtual_columns_rotation(self, x, y, xnew, ynew, angle_degrees, propagate_uncertainties=False):\n        \"\"\"Rotation in 2d.\n\n        :param str x: Name/expression of x column\n        :param str y: idem for y\n        :param str xnew: name of transformed x column\n        :param str ynew:\n        :param float angle_degrees: rotation in degrees, anti clockwise\n        :return:\n        \"\"\"\n        x = _ensure_string_from_expression(x)\n        y = _ensure_string_from_expression(y)\n        theta = np.radians(angle_degrees)\n        matrix = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n        m = matrix_name = x + \"_\" + y + \"_rot\"\n        for i in range(2):\n            for j in range(2):\n                self.set_variable(matrix_name + \"_%d%d\" % (i, j), matrix[i, j].item())\n        self[xnew] = self._expr(\"{m}_00 * {x} + {m}_01 * {y}\".format(**locals()))\n        self[ynew] = self._expr(\"{m}_10 * {x} + {m}_11 * {y}\".format(**locals()))\n        if propagate_uncertainties:\n            self.propagate_uncertainties([self[xnew], self[ynew]])", "language": "python", "code": "def add_virtual_columns_rotation(self, x, y, xnew, ynew, angle_degrees, propagate_uncertainties=False):\n        \"\"\"Rotation in 2d.\n\n        :param str x: Name/expression of x column\n        :param str y: idem for y\n        :param str xnew: name of transformed x column\n        :param str ynew:\n        :param float angle_degrees: rotation in degrees, anti clockwise\n        :return:\n        \"\"\"\n        x = _ensure_string_from_expression(x)\n        y = _ensure_string_from_expression(y)\n        theta = np.radians(angle_degrees)\n        matrix = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n        m = matrix_name = x + \"_\" + y + \"_rot\"\n        for i in range(2):\n            for j in range(2):\n                self.set_variable(matrix_name + \"_%d%d\" % (i, j), matrix[i, j].item())\n        self[xnew] = self._expr(\"{m}_00 * {x} + {m}_01 * {y}\".format(**locals()))\n        self[ynew] = self._expr(\"{m}_10 * {x} + {m}_11 * {y}\".format(**locals()))\n        if propagate_uncertainties:\n            self.propagate_uncertainties([self[xnew], self[ynew]])", "code_tokens": ["def", "add_virtual_columns_rotation", "(", "self", ",", "x", ",", "y", ",", "xnew", ",", "ynew", ",", "angle_degrees", ",", "propagate_uncertainties", "=", "False", ")", ":", "x", "=", "_ensure_string_from_expression", "(", "x", ")", "y", "=", "_ensure_string_from_expression", "(", "y", ")", "theta", "=", "np", ".", "radians", "(", "angle_degrees", ")", "matrix", "=", "np", ".", "array", "(", "[", "[", "np", ".", "cos", "(", "theta", ")", ",", "-", "np", ".", "sin", "(", "theta", ")", "]", ",", "[", "np", ".", "sin", "(", "theta", ")", ",", "np", ".", "cos", "(", "theta", ")", "]", "]", ")", "m", "=", "matrix_name", "=", "x", "+", "\"_\"", "+", "y", "+", "\"_rot\"", "for", "i", "in", "range", "(", "2", ")", ":", "for", "j", "in", "range", "(", "2", ")", ":", "self", ".", "set_variable", "(", "matrix_name", "+", "\"_%d%d\"", "%", "(", "i", ",", "j", ")", ",", "matrix", "[", "i", ",", "j", "]", ".", "item", "(", ")", ")", "self", "[", "xnew", "]", "=", "self", ".", "_expr", "(", "\"{m}_00 * {x} + {m}_01 * {y}\"", ".", "format", "(", "*", "*", "locals", "(", ")", ")", ")", "self", "[", "ynew", "]", "=", "self", ".", "_expr", "(", "\"{m}_10 * {x} + {m}_11 * {y}\"", ".", "format", "(", "*", "*", "locals", "(", ")", ")", ")", "if", "propagate_uncertainties", ":", "self", ".", "propagate_uncertainties", "(", "[", "self", "[", "xnew", "]", ",", "self", "[", "ynew", "]", "]", ")"], "docstring": "Rotation in 2d.\n\n        :param str x: Name/expression of x column\n        :param str y: idem for y\n        :param str xnew: name of transformed x column\n        :param str ynew:\n        :param float angle_degrees: rotation in degrees, anti clockwise\n        :return:", "docstring_tokens": ["Rotation", "in", "2d", "."], "sha": "a45b672f8287afca2ada8e36b74b604b9b28dd85", "url": "https://github.com/vaexio/vaex/blob/a45b672f8287afca2ada8e36b74b604b9b28dd85/packages/vaex-core/vaex/dataframe.py#L3105-L3126", "partition": "test"}
{"repo": "uogbuji/amara3-xml", "path": "pylib/uxml/uxpath/functions.py", "func_name": "lookup_", "original_string": "def lookup_(ctx, tableid, key):\n    '''\n    Yields a sequence of a single value, the result of looking up a value from the tables provided in the context, or an empty sequence if lookup is unsuccessful\n\n    * tableid: id of the lookup table to use\n    * expr: expression to be converted to string, then dynamically evaluated for each item on the sequence to produce the result\n    '''\n    tableid = next(string_arg(ctx, tableid), '')\n    key = next(string_arg(ctx, key), '')\n    #value = ctx.\n    for item in seq:\n        innerctx = ctx.copy(item=item)\n        yield from pexpr.compute(innerctx)", "language": "python", "code": "def lookup_(ctx, tableid, key):\n    '''\n    Yields a sequence of a single value, the result of looking up a value from the tables provided in the context, or an empty sequence if lookup is unsuccessful\n\n    * tableid: id of the lookup table to use\n    * expr: expression to be converted to string, then dynamically evaluated for each item on the sequence to produce the result\n    '''\n    tableid = next(string_arg(ctx, tableid), '')\n    key = next(string_arg(ctx, key), '')\n    #value = ctx.\n    for item in seq:\n        innerctx = ctx.copy(item=item)\n        yield from pexpr.compute(innerctx)", "code_tokens": ["def", "lookup_", "(", "ctx", ",", "tableid", ",", "key", ")", ":", "tableid", "=", "next", "(", "string_arg", "(", "ctx", ",", "tableid", ")", ",", "''", ")", "key", "=", "next", "(", "string_arg", "(", "ctx", ",", "key", ")", ",", "''", ")", "#value = ctx.", "for", "item", "in", "seq", ":", "innerctx", "=", "ctx", ".", "copy", "(", "item", "=", "item", ")", "yield", "from", "pexpr", ".", "compute", "(", "innerctx", ")"], "docstring": "Yields a sequence of a single value, the result of looking up a value from the tables provided in the context, or an empty sequence if lookup is unsuccessful\n\n    * tableid: id of the lookup table to use\n    * expr: expression to be converted to string, then dynamically evaluated for each item on the sequence to produce the result", "docstring_tokens": ["Yields", "a", "sequence", "of", "a", "single", "value", "the", "result", "of", "looking", "up", "a", "value", "from", "the", "tables", "provided", "in", "the", "context", "or", "an", "empty", "sequence", "if", "lookup", "is", "unsuccessful"], "sha": "88c18876418cffc89bb85b4a3193e5002b6b39a6", "url": "https://github.com/uogbuji/amara3-xml/blob/88c18876418cffc89bb85b4a3193e5002b6b39a6/pylib/uxml/uxpath/functions.py#L302-L314", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/frontend/qt/console/history_console_widget.py", "func_name": "HistoryConsoleWidget.history_previous", "original_string": "def history_previous(self, substring='', as_prefix=True):\n        \"\"\" If possible, set the input buffer to a previous history item.\n\n        Parameters:\n        -----------\n        substring : str, optional\n            If specified, search for an item with this substring.\n        as_prefix : bool, optional\n            If True, the substring must match at the beginning (default).\n\n        Returns:\n        --------\n        Whether the input buffer was changed.\n        \"\"\"\n        index = self._history_index\n        replace = False\n        while index > 0:\n            index -= 1\n            history = self._get_edited_history(index)\n            if (as_prefix and history.startswith(substring)) \\\n                or (not as_prefix and substring in history):\n                replace = True\n                break\n\n        if replace:\n            self._store_edits()\n            self._history_index = index\n            self.input_buffer = history\n\n        return replace", "language": "python", "code": "def history_previous(self, substring='', as_prefix=True):\n        \"\"\" If possible, set the input buffer to a previous history item.\n\n        Parameters:\n        -----------\n        substring : str, optional\n            If specified, search for an item with this substring.\n        as_prefix : bool, optional\n            If True, the substring must match at the beginning (default).\n\n        Returns:\n        --------\n        Whether the input buffer was changed.\n        \"\"\"\n        index = self._history_index\n        replace = False\n        while index > 0:\n            index -= 1\n            history = self._get_edited_history(index)\n            if (as_prefix and history.startswith(substring)) \\\n                or (not as_prefix and substring in history):\n                replace = True\n                break\n\n        if replace:\n            self._store_edits()\n            self._history_index = index\n            self.input_buffer = history\n\n        return replace", "code_tokens": ["def", "history_previous", "(", "self", ",", "substring", "=", "''", ",", "as_prefix", "=", "True", ")", ":", "index", "=", "self", ".", "_history_index", "replace", "=", "False", "while", "index", ">", "0", ":", "index", "-=", "1", "history", "=", "self", ".", "_get_edited_history", "(", "index", ")", "if", "(", "as_prefix", "and", "history", ".", "startswith", "(", "substring", ")", ")", "or", "(", "not", "as_prefix", "and", "substring", "in", "history", ")", ":", "replace", "=", "True", "break", "if", "replace", ":", "self", ".", "_store_edits", "(", ")", "self", ".", "_history_index", "=", "index", "self", ".", "input_buffer", "=", "history", "return", "replace"], "docstring": "If possible, set the input buffer to a previous history item.\n\n        Parameters:\n        -----------\n        substring : str, optional\n            If specified, search for an item with this substring.\n        as_prefix : bool, optional\n            If True, the substring must match at the beginning (default).\n\n        Returns:\n        --------\n        Whether the input buffer was changed.", "docstring_tokens": ["If", "possible", "set", "the", "input", "buffer", "to", "a", "previous", "history", "item", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/frontend/qt/console/history_console_widget.py#L135-L164", "partition": "test"}
{"repo": "Nic30/hwt", "path": "hwt/hdl/portItem.py", "func_name": "PortItem.getInternSig", "original_string": "def getInternSig(self):\n        \"\"\"\n        return signal inside unit which has this port\n        \"\"\"\n        d = self.direction\n        if d == DIRECTION.IN:\n            return self.dst\n        elif d == DIRECTION.OUT:\n            return self.src\n        else:\n            raise NotImplementedError(d)", "language": "python", "code": "def getInternSig(self):\n        \"\"\"\n        return signal inside unit which has this port\n        \"\"\"\n        d = self.direction\n        if d == DIRECTION.IN:\n            return self.dst\n        elif d == DIRECTION.OUT:\n            return self.src\n        else:\n            raise NotImplementedError(d)", "code_tokens": ["def", "getInternSig", "(", "self", ")", ":", "d", "=", "self", ".", "direction", "if", "d", "==", "DIRECTION", ".", "IN", ":", "return", "self", ".", "dst", "elif", "d", "==", "DIRECTION", ".", "OUT", ":", "return", "self", ".", "src", "else", ":", "raise", "NotImplementedError", "(", "d", ")"], "docstring": "return signal inside unit which has this port", "docstring_tokens": ["return", "signal", "inside", "unit", "which", "has", "this", "port"], "sha": "8cbb399e326da3b22c233b98188a9d08dec057e6", "url": "https://github.com/Nic30/hwt/blob/8cbb399e326da3b22c233b98188a9d08dec057e6/hwt/hdl/portItem.py#L84-L94", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval", "path": "perceval/archive.py", "func_name": "ArchiveManager._search_files", "original_string": "def _search_files(self):\n        \"\"\"Retrieve the file paths stored under the base path.\"\"\"\n\n        for root, _, files in os.walk(self.dirpath):\n            for filename in files:\n                location = os.path.join(root, filename)\n                yield location", "language": "python", "code": "def _search_files(self):\n        \"\"\"Retrieve the file paths stored under the base path.\"\"\"\n\n        for root, _, files in os.walk(self.dirpath):\n            for filename in files:\n                location = os.path.join(root, filename)\n                yield location", "code_tokens": ["def", "_search_files", "(", "self", ")", ":", "for", "root", ",", "_", ",", "files", "in", "os", ".", "walk", "(", "self", ".", "dirpath", ")", ":", "for", "filename", "in", "files", ":", "location", "=", "os", ".", "path", ".", "join", "(", "root", ",", "filename", ")", "yield", "location"], "docstring": "Retrieve the file paths stored under the base path.", "docstring_tokens": ["Retrieve", "the", "file", "paths", "stored", "under", "the", "base", "path", "."], "sha": "41c908605e88b7ebc3a536c643fa0f212eaf9e0e", "url": "https://github.com/chaoss/grimoirelab-perceval/blob/41c908605e88b7ebc3a536c643fa0f212eaf9e0e/perceval/archive.py#L459-L465", "partition": "test"}
{"repo": "Azure/azure-sdk-for-python", "path": "azure-servicebus/azure/servicebus/control_client/models.py", "func_name": "Message.delete", "original_string": "def delete(self):\n        ''' Deletes itself if find queue name or topic name and subscription\n        name. '''\n        if self._queue_name:\n            self.service_bus_service.delete_queue_message(\n                self._queue_name,\n                self.broker_properties['SequenceNumber'],\n                self.broker_properties['LockToken'])\n        elif self._topic_name and self._subscription_name:\n            self.service_bus_service.delete_subscription_message(\n                self._topic_name,\n                self._subscription_name,\n                self.broker_properties['SequenceNumber'],\n                self.broker_properties['LockToken'])\n        else:\n            raise AzureServiceBusPeekLockError(_ERROR_MESSAGE_NOT_PEEK_LOCKED_ON_DELETE)", "language": "python", "code": "def delete(self):\n        ''' Deletes itself if find queue name or topic name and subscription\n        name. '''\n        if self._queue_name:\n            self.service_bus_service.delete_queue_message(\n                self._queue_name,\n                self.broker_properties['SequenceNumber'],\n                self.broker_properties['LockToken'])\n        elif self._topic_name and self._subscription_name:\n            self.service_bus_service.delete_subscription_message(\n                self._topic_name,\n                self._subscription_name,\n                self.broker_properties['SequenceNumber'],\n                self.broker_properties['LockToken'])\n        else:\n            raise AzureServiceBusPeekLockError(_ERROR_MESSAGE_NOT_PEEK_LOCKED_ON_DELETE)", "code_tokens": ["def", "delete", "(", "self", ")", ":", "if", "self", ".", "_queue_name", ":", "self", ".", "service_bus_service", ".", "delete_queue_message", "(", "self", ".", "_queue_name", ",", "self", ".", "broker_properties", "[", "'SequenceNumber'", "]", ",", "self", ".", "broker_properties", "[", "'LockToken'", "]", ")", "elif", "self", ".", "_topic_name", "and", "self", ".", "_subscription_name", ":", "self", ".", "service_bus_service", ".", "delete_subscription_message", "(", "self", ".", "_topic_name", ",", "self", ".", "_subscription_name", ",", "self", ".", "broker_properties", "[", "'SequenceNumber'", "]", ",", "self", ".", "broker_properties", "[", "'LockToken'", "]", ")", "else", ":", "raise", "AzureServiceBusPeekLockError", "(", "_ERROR_MESSAGE_NOT_PEEK_LOCKED_ON_DELETE", ")"], "docstring": "Deletes itself if find queue name or topic name and subscription\n        name.", "docstring_tokens": ["Deletes", "itself", "if", "find", "queue", "name", "or", "topic", "name", "and", "subscription", "name", "."], "sha": "d7306fde32f60a293a7567678692bdad31e4b667", "url": "https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicebus/azure/servicebus/control_client/models.py#L185-L200", "partition": "test"}
{"repo": "nats-io/asyncio-nats", "path": "nats/aio/client.py", "func_name": "Client._process_op_err", "original_string": "def _process_op_err(self, e):\n        \"\"\"\n        Process errors which occured while reading or parsing\n        the protocol. If allow_reconnect is enabled it will\n        try to switch the server to which it is currently connected\n        otherwise it will disconnect.\n        \"\"\"\n        if self.is_connecting or self.is_closed or self.is_reconnecting:\n            return\n\n        if self.options[\"allow_reconnect\"] and self.is_connected:\n            self._status = Client.RECONNECTING\n            self._ps.reset()\n\n            if self._reconnection_task is not None and not self._reconnection_task.cancelled():\n                # Cancel the previous task in case it may still be running.\n                self._reconnection_task.cancel()\n\n            self._reconnection_task = self._loop.create_task(self._attempt_reconnect())\n        else:\n            self._process_disconnect()\n            self._err = e\n            yield from self._close(Client.CLOSED, True)", "language": "python", "code": "def _process_op_err(self, e):\n        \"\"\"\n        Process errors which occured while reading or parsing\n        the protocol. If allow_reconnect is enabled it will\n        try to switch the server to which it is currently connected\n        otherwise it will disconnect.\n        \"\"\"\n        if self.is_connecting or self.is_closed or self.is_reconnecting:\n            return\n\n        if self.options[\"allow_reconnect\"] and self.is_connected:\n            self._status = Client.RECONNECTING\n            self._ps.reset()\n\n            if self._reconnection_task is not None and not self._reconnection_task.cancelled():\n                # Cancel the previous task in case it may still be running.\n                self._reconnection_task.cancel()\n\n            self._reconnection_task = self._loop.create_task(self._attempt_reconnect())\n        else:\n            self._process_disconnect()\n            self._err = e\n            yield from self._close(Client.CLOSED, True)", "code_tokens": ["def", "_process_op_err", "(", "self", ",", "e", ")", ":", "if", "self", ".", "is_connecting", "or", "self", ".", "is_closed", "or", "self", ".", "is_reconnecting", ":", "return", "if", "self", ".", "options", "[", "\"allow_reconnect\"", "]", "and", "self", ".", "is_connected", ":", "self", ".", "_status", "=", "Client", ".", "RECONNECTING", "self", ".", "_ps", ".", "reset", "(", ")", "if", "self", ".", "_reconnection_task", "is", "not", "None", "and", "not", "self", ".", "_reconnection_task", ".", "cancelled", "(", ")", ":", "# Cancel the previous task in case it may still be running.", "self", ".", "_reconnection_task", ".", "cancel", "(", ")", "self", ".", "_reconnection_task", "=", "self", ".", "_loop", ".", "create_task", "(", "self", ".", "_attempt_reconnect", "(", ")", ")", "else", ":", "self", ".", "_process_disconnect", "(", ")", "self", ".", "_err", "=", "e", "yield", "from", "self", ".", "_close", "(", "Client", ".", "CLOSED", ",", "True", ")"], "docstring": "Process errors which occured while reading or parsing\n        the protocol. If allow_reconnect is enabled it will\n        try to switch the server to which it is currently connected\n        otherwise it will disconnect.", "docstring_tokens": ["Process", "errors", "which", "occured", "while", "reading", "or", "parsing", "the", "protocol", ".", "If", "allow_reconnect", "is", "enabled", "it", "will", "try", "to", "switch", "the", "server", "to", "which", "it", "is", "currently", "connected", "otherwise", "it", "will", "disconnect", "."], "sha": "39e840be0b12ce326edac0bba69aeb1be930dcb8", "url": "https://github.com/nats-io/asyncio-nats/blob/39e840be0b12ce326edac0bba69aeb1be930dcb8/nats/aio/client.py#L1010-L1032", "partition": "test"}
{"repo": "ghcollin/multitables", "path": "multitables.py", "func_name": "SharedCircBuf.put_direct", "original_string": "def put_direct(self):\n        \"\"\"\n        Allows direct access to the buffer element.\n        Blocks until there is room to write into the buffer.\n\n        :return: A guard object that returns the buffer element.\n        \"\"\"\n\n        # Once the guard is released, write_idx will be placed into read_queue.\n        return self.Guard(self.read_queue, self.arys, self.__put_idx)", "language": "python", "code": "def put_direct(self):\n        \"\"\"\n        Allows direct access to the buffer element.\n        Blocks until there is room to write into the buffer.\n\n        :return: A guard object that returns the buffer element.\n        \"\"\"\n\n        # Once the guard is released, write_idx will be placed into read_queue.\n        return self.Guard(self.read_queue, self.arys, self.__put_idx)", "code_tokens": ["def", "put_direct", "(", "self", ")", ":", "# Once the guard is released, write_idx will be placed into read_queue.", "return", "self", ".", "Guard", "(", "self", ".", "read_queue", ",", "self", ".", "arys", ",", "self", ".", "__put_idx", ")"], "docstring": "Allows direct access to the buffer element.\n        Blocks until there is room to write into the buffer.\n\n        :return: A guard object that returns the buffer element.", "docstring_tokens": ["Allows", "direct", "access", "to", "the", "buffer", "element", ".", "Blocks", "until", "there", "is", "room", "to", "write", "into", "the", "buffer", "."], "sha": "9654a45800289a20e66d2b0e0666149f0d370f93", "url": "https://github.com/ghcollin/multitables/blob/9654a45800289a20e66d2b0e0666149f0d370f93/multitables.py#L298-L307", "partition": "test"}
{"repo": "open-mmlab/mmcv", "path": "mmcv/image/transforms/geometry.py", "func_name": "imcrop", "original_string": "def imcrop(img, bboxes, scale=1.0, pad_fill=None):\n    \"\"\"Crop image patches.\n\n    3 steps: scale the bboxes -> clip bboxes -> crop and pad.\n\n    Args:\n        img (ndarray): Image to be cropped.\n        bboxes (ndarray): Shape (k, 4) or (4, ), location of cropped bboxes.\n        scale (float, optional): Scale ratio of bboxes, the default value\n            1.0 means no padding.\n        pad_fill (number or list): Value to be filled for padding, None for\n            no padding.\n\n    Returns:\n        list or ndarray: The cropped image patches.\n    \"\"\"\n    chn = 1 if img.ndim == 2 else img.shape[2]\n    if pad_fill is not None:\n        if isinstance(pad_fill, (int, float)):\n            pad_fill = [pad_fill for _ in range(chn)]\n        assert len(pad_fill) == chn\n\n    _bboxes = bboxes[None, ...] if bboxes.ndim == 1 else bboxes\n    scaled_bboxes = bbox_scaling(_bboxes, scale).astype(np.int32)\n    clipped_bbox = bbox_clip(scaled_bboxes, img.shape)\n\n    patches = []\n    for i in range(clipped_bbox.shape[0]):\n        x1, y1, x2, y2 = tuple(clipped_bbox[i, :])\n        if pad_fill is None:\n            patch = img[y1:y2 + 1, x1:x2 + 1, ...]\n        else:\n            _x1, _y1, _x2, _y2 = tuple(scaled_bboxes[i, :])\n            if chn == 2:\n                patch_shape = (_y2 - _y1 + 1, _x2 - _x1 + 1)\n            else:\n                patch_shape = (_y2 - _y1 + 1, _x2 - _x1 + 1, chn)\n            patch = np.array(\n                pad_fill, dtype=img.dtype) * np.ones(\n                    patch_shape, dtype=img.dtype)\n            x_start = 0 if _x1 >= 0 else -_x1\n            y_start = 0 if _y1 >= 0 else -_y1\n            w = x2 - x1 + 1\n            h = y2 - y1 + 1\n            patch[y_start:y_start + h, x_start:x_start +\n                  w, ...] = img[y1:y1 + h, x1:x1 + w, ...]\n        patches.append(patch)\n\n    if bboxes.ndim == 1:\n        return patches[0]\n    else:\n        return patches", "language": "python", "code": "def imcrop(img, bboxes, scale=1.0, pad_fill=None):\n    \"\"\"Crop image patches.\n\n    3 steps: scale the bboxes -> clip bboxes -> crop and pad.\n\n    Args:\n        img (ndarray): Image to be cropped.\n        bboxes (ndarray): Shape (k, 4) or (4, ), location of cropped bboxes.\n        scale (float, optional): Scale ratio of bboxes, the default value\n            1.0 means no padding.\n        pad_fill (number or list): Value to be filled for padding, None for\n            no padding.\n\n    Returns:\n        list or ndarray: The cropped image patches.\n    \"\"\"\n    chn = 1 if img.ndim == 2 else img.shape[2]\n    if pad_fill is not None:\n        if isinstance(pad_fill, (int, float)):\n            pad_fill = [pad_fill for _ in range(chn)]\n        assert len(pad_fill) == chn\n\n    _bboxes = bboxes[None, ...] if bboxes.ndim == 1 else bboxes\n    scaled_bboxes = bbox_scaling(_bboxes, scale).astype(np.int32)\n    clipped_bbox = bbox_clip(scaled_bboxes, img.shape)\n\n    patches = []\n    for i in range(clipped_bbox.shape[0]):\n        x1, y1, x2, y2 = tuple(clipped_bbox[i, :])\n        if pad_fill is None:\n            patch = img[y1:y2 + 1, x1:x2 + 1, ...]\n        else:\n            _x1, _y1, _x2, _y2 = tuple(scaled_bboxes[i, :])\n            if chn == 2:\n                patch_shape = (_y2 - _y1 + 1, _x2 - _x1 + 1)\n            else:\n                patch_shape = (_y2 - _y1 + 1, _x2 - _x1 + 1, chn)\n            patch = np.array(\n                pad_fill, dtype=img.dtype) * np.ones(\n                    patch_shape, dtype=img.dtype)\n            x_start = 0 if _x1 >= 0 else -_x1\n            y_start = 0 if _y1 >= 0 else -_y1\n            w = x2 - x1 + 1\n            h = y2 - y1 + 1\n            patch[y_start:y_start + h, x_start:x_start +\n                  w, ...] = img[y1:y1 + h, x1:x1 + w, ...]\n        patches.append(patch)\n\n    if bboxes.ndim == 1:\n        return patches[0]\n    else:\n        return patches", "code_tokens": ["def", "imcrop", "(", "img", ",", "bboxes", ",", "scale", "=", "1.0", ",", "pad_fill", "=", "None", ")", ":", "chn", "=", "1", "if", "img", ".", "ndim", "==", "2", "else", "img", ".", "shape", "[", "2", "]", "if", "pad_fill", "is", "not", "None", ":", "if", "isinstance", "(", "pad_fill", ",", "(", "int", ",", "float", ")", ")", ":", "pad_fill", "=", "[", "pad_fill", "for", "_", "in", "range", "(", "chn", ")", "]", "assert", "len", "(", "pad_fill", ")", "==", "chn", "_bboxes", "=", "bboxes", "[", "None", ",", "...", "]", "if", "bboxes", ".", "ndim", "==", "1", "else", "bboxes", "scaled_bboxes", "=", "bbox_scaling", "(", "_bboxes", ",", "scale", ")", ".", "astype", "(", "np", ".", "int32", ")", "clipped_bbox", "=", "bbox_clip", "(", "scaled_bboxes", ",", "img", ".", "shape", ")", "patches", "=", "[", "]", "for", "i", "in", "range", "(", "clipped_bbox", ".", "shape", "[", "0", "]", ")", ":", "x1", ",", "y1", ",", "x2", ",", "y2", "=", "tuple", "(", "clipped_bbox", "[", "i", ",", ":", "]", ")", "if", "pad_fill", "is", "None", ":", "patch", "=", "img", "[", "y1", ":", "y2", "+", "1", ",", "x1", ":", "x2", "+", "1", ",", "...", "]", "else", ":", "_x1", ",", "_y1", ",", "_x2", ",", "_y2", "=", "tuple", "(", "scaled_bboxes", "[", "i", ",", ":", "]", ")", "if", "chn", "==", "2", ":", "patch_shape", "=", "(", "_y2", "-", "_y1", "+", "1", ",", "_x2", "-", "_x1", "+", "1", ")", "else", ":", "patch_shape", "=", "(", "_y2", "-", "_y1", "+", "1", ",", "_x2", "-", "_x1", "+", "1", ",", "chn", ")", "patch", "=", "np", ".", "array", "(", "pad_fill", ",", "dtype", "=", "img", ".", "dtype", ")", "*", "np", ".", "ones", "(", "patch_shape", ",", "dtype", "=", "img", ".", "dtype", ")", "x_start", "=", "0", "if", "_x1", ">=", "0", "else", "-", "_x1", "y_start", "=", "0", "if", "_y1", ">=", "0", "else", "-", "_y1", "w", "=", "x2", "-", "x1", "+", "1", "h", "=", "y2", "-", "y1", "+", "1", "patch", "[", "y_start", ":", "y_start", "+", "h", ",", "x_start", ":", "x_start", "+", "w", ",", "...", "]", "=", "img", "[", "y1", ":", "y1", "+", "h", ",", "x1", ":", "x1", "+", "w", ",", "...", "]", "patches", ".", "append", "(", "patch", ")", "if", "bboxes", ".", "ndim", "==", "1", ":", "return", "patches", "[", "0", "]", "else", ":", "return", "patches"], "docstring": "Crop image patches.\n\n    3 steps: scale the bboxes -> clip bboxes -> crop and pad.\n\n    Args:\n        img (ndarray): Image to be cropped.\n        bboxes (ndarray): Shape (k, 4) or (4, ), location of cropped bboxes.\n        scale (float, optional): Scale ratio of bboxes, the default value\n            1.0 means no padding.\n        pad_fill (number or list): Value to be filled for padding, None for\n            no padding.\n\n    Returns:\n        list or ndarray: The cropped image patches.", "docstring_tokens": ["Crop", "image", "patches", "."], "sha": "0d77f61450aab4dde8b8585a577cc496acb95d7f", "url": "https://github.com/open-mmlab/mmcv/blob/0d77f61450aab4dde8b8585a577cc496acb95d7f/mmcv/image/transforms/geometry.py#L112-L163", "partition": "test"}
{"repo": "streamlink/streamlink", "path": "src/streamlink_cli/main.py", "func_name": "check_file_output", "original_string": "def check_file_output(filename, force):\n    \"\"\"Checks if file already exists and ask the user if it should\n    be overwritten if it does.\"\"\"\n\n    log.debug(\"Checking file output\")\n\n    if os.path.isfile(filename) and not force:\n        if sys.stdin.isatty():\n            answer = console.ask(\"File {0} already exists! Overwrite it? [y/N] \",\n                                 filename)\n\n            if answer.lower() != \"y\":\n                sys.exit()\n        else:\n            log.error(\"File {0} already exists, use --force to overwrite it.\".format(filename))\n            sys.exit()\n\n    return FileOutput(filename)", "language": "python", "code": "def check_file_output(filename, force):\n    \"\"\"Checks if file already exists and ask the user if it should\n    be overwritten if it does.\"\"\"\n\n    log.debug(\"Checking file output\")\n\n    if os.path.isfile(filename) and not force:\n        if sys.stdin.isatty():\n            answer = console.ask(\"File {0} already exists! Overwrite it? [y/N] \",\n                                 filename)\n\n            if answer.lower() != \"y\":\n                sys.exit()\n        else:\n            log.error(\"File {0} already exists, use --force to overwrite it.\".format(filename))\n            sys.exit()\n\n    return FileOutput(filename)", "code_tokens": ["def", "check_file_output", "(", "filename", ",", "force", ")", ":", "log", ".", "debug", "(", "\"Checking file output\"", ")", "if", "os", ".", "path", ".", "isfile", "(", "filename", ")", "and", "not", "force", ":", "if", "sys", ".", "stdin", ".", "isatty", "(", ")", ":", "answer", "=", "console", ".", "ask", "(", "\"File {0} already exists! Overwrite it? [y/N] \"", ",", "filename", ")", "if", "answer", ".", "lower", "(", ")", "!=", "\"y\"", ":", "sys", ".", "exit", "(", ")", "else", ":", "log", ".", "error", "(", "\"File {0} already exists, use --force to overwrite it.\"", ".", "format", "(", "filename", ")", ")", "sys", ".", "exit", "(", ")", "return", "FileOutput", "(", "filename", ")"], "docstring": "Checks if file already exists and ask the user if it should\n    be overwritten if it does.", "docstring_tokens": ["Checks", "if", "file", "already", "exists", "and", "ask", "the", "user", "if", "it", "should", "be", "overwritten", "if", "it", "does", "."], "sha": "c8ed1daff14ac03195870238b9b900c1109dd5c1", "url": "https://github.com/streamlink/streamlink/blob/c8ed1daff14ac03195870238b9b900c1109dd5c1/src/streamlink_cli/main.py#L53-L70", "partition": "test"}
{"repo": "zenodo/zenodo-accessrequests", "path": "zenodo_accessrequests/tokens.py", "func_name": "EmailConfirmationSerializer.compat_validate_token", "original_string": "def compat_validate_token(cls, *args, **kwargs):\n        \"\"\"Multiple algorithm-compatible token validation.\"\"\"\n        data = None\n        for algorithm in SUPPORTED_DIGEST_ALGORITHMS:\n            data = cls(algorithm_name=algorithm).validate_token(\n                *args, **kwargs)\n            if not data:  # move to next algorithm\n                continue\n        return data", "language": "python", "code": "def compat_validate_token(cls, *args, **kwargs):\n        \"\"\"Multiple algorithm-compatible token validation.\"\"\"\n        data = None\n        for algorithm in SUPPORTED_DIGEST_ALGORITHMS:\n            data = cls(algorithm_name=algorithm).validate_token(\n                *args, **kwargs)\n            if not data:  # move to next algorithm\n                continue\n        return data", "code_tokens": ["def", "compat_validate_token", "(", "cls", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "data", "=", "None", "for", "algorithm", "in", "SUPPORTED_DIGEST_ALGORITHMS", ":", "data", "=", "cls", "(", "algorithm_name", "=", "algorithm", ")", ".", "validate_token", "(", "*", "args", ",", "*", "*", "kwargs", ")", "if", "not", "data", ":", "# move to next algorithm", "continue", "return", "data"], "docstring": "Multiple algorithm-compatible token validation.", "docstring_tokens": ["Multiple", "algorithm", "-", "compatible", "token", "validation", "."], "sha": "ce2cf3f1425d02ba4f3ad3202cfca43a1892558a", "url": "https://github.com/zenodo/zenodo-accessrequests/blob/ce2cf3f1425d02ba4f3ad3202cfca43a1892558a/zenodo_accessrequests/tokens.py#L151-L159", "partition": "test"}
{"repo": "mental32/spotify.py", "path": "spotify/http.py", "func_name": "HTTPClient.artist_albums", "original_string": "def artist_albums(self, spotify_id, include_groups=None, limit=20, offset=0, market='US'):\n        \"\"\"Get an artists tracks by their ID.\n\n        Parameters\n        ----------\n        spotify_id : str\n            The spotify_id to search by.\n        include_groups : INCLUDE_GROUPS_TP\n            INCLUDE_GROUPS\n        limit : Optional[int]\n            The maximum number of items to return. Default: 20. Minimum: 1. Maximum: 50.\n        offset : Optiona[int]\n            The offset of which Spotify should start yielding from.\n        market : Optional[str]\n            An ISO 3166-1 alpha-2 country code.\n        \"\"\"\n        route = Route('GET', '/artists/{spotify_id}/albums', spotify_id=spotify_id)\n        payload = {'limit': limit, 'offset': offset}\n\n        if include_groups:\n            payload['include_groups'] = include_groups\n\n        if market:\n            payload['market'] = market\n\n        return self.request(route, params=payload)", "language": "python", "code": "def artist_albums(self, spotify_id, include_groups=None, limit=20, offset=0, market='US'):\n        \"\"\"Get an artists tracks by their ID.\n\n        Parameters\n        ----------\n        spotify_id : str\n            The spotify_id to search by.\n        include_groups : INCLUDE_GROUPS_TP\n            INCLUDE_GROUPS\n        limit : Optional[int]\n            The maximum number of items to return. Default: 20. Minimum: 1. Maximum: 50.\n        offset : Optiona[int]\n            The offset of which Spotify should start yielding from.\n        market : Optional[str]\n            An ISO 3166-1 alpha-2 country code.\n        \"\"\"\n        route = Route('GET', '/artists/{spotify_id}/albums', spotify_id=spotify_id)\n        payload = {'limit': limit, 'offset': offset}\n\n        if include_groups:\n            payload['include_groups'] = include_groups\n\n        if market:\n            payload['market'] = market\n\n        return self.request(route, params=payload)", "code_tokens": ["def", "artist_albums", "(", "self", ",", "spotify_id", ",", "include_groups", "=", "None", ",", "limit", "=", "20", ",", "offset", "=", "0", ",", "market", "=", "'US'", ")", ":", "route", "=", "Route", "(", "'GET'", ",", "'/artists/{spotify_id}/albums'", ",", "spotify_id", "=", "spotify_id", ")", "payload", "=", "{", "'limit'", ":", "limit", ",", "'offset'", ":", "offset", "}", "if", "include_groups", ":", "payload", "[", "'include_groups'", "]", "=", "include_groups", "if", "market", ":", "payload", "[", "'market'", "]", "=", "market", "return", "self", ".", "request", "(", "route", ",", "params", "=", "payload", ")"], "docstring": "Get an artists tracks by their ID.\n\n        Parameters\n        ----------\n        spotify_id : str\n            The spotify_id to search by.\n        include_groups : INCLUDE_GROUPS_TP\n            INCLUDE_GROUPS\n        limit : Optional[int]\n            The maximum number of items to return. Default: 20. Minimum: 1. Maximum: 50.\n        offset : Optiona[int]\n            The offset of which Spotify should start yielding from.\n        market : Optional[str]\n            An ISO 3166-1 alpha-2 country code.", "docstring_tokens": ["Get", "an", "artists", "tracks", "by", "their", "ID", "."], "sha": "bb296cac7c3dd289908906b7069bd80f43950515", "url": "https://github.com/mental32/spotify.py/blob/bb296cac7c3dd289908906b7069bd80f43950515/spotify/http.py#L245-L270", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/core/history.py", "func_name": "HistoryManager.reset", "original_string": "def reset(self, new_session=True):\n        \"\"\"Clear the session history, releasing all object references, and\n        optionally open a new session.\"\"\"\n        self.output_hist.clear()\n        # The directory history can't be completely empty\n        self.dir_hist[:] = [os.getcwdu()]\n        \n        if new_session:\n            if self.session_number:\n                self.end_session()\n            self.input_hist_parsed[:] = [\"\"]\n            self.input_hist_raw[:] = [\"\"]\n            self.new_session()", "language": "python", "code": "def reset(self, new_session=True):\n        \"\"\"Clear the session history, releasing all object references, and\n        optionally open a new session.\"\"\"\n        self.output_hist.clear()\n        # The directory history can't be completely empty\n        self.dir_hist[:] = [os.getcwdu()]\n        \n        if new_session:\n            if self.session_number:\n                self.end_session()\n            self.input_hist_parsed[:] = [\"\"]\n            self.input_hist_raw[:] = [\"\"]\n            self.new_session()", "code_tokens": ["def", "reset", "(", "self", ",", "new_session", "=", "True", ")", ":", "self", ".", "output_hist", ".", "clear", "(", ")", "# The directory history can't be completely empty", "self", ".", "dir_hist", "[", ":", "]", "=", "[", "os", ".", "getcwdu", "(", ")", "]", "if", "new_session", ":", "if", "self", ".", "session_number", ":", "self", ".", "end_session", "(", ")", "self", ".", "input_hist_parsed", "[", ":", "]", "=", "[", "\"\"", "]", "self", ".", "input_hist_raw", "[", ":", "]", "=", "[", "\"\"", "]", "self", ".", "new_session", "(", ")"], "docstring": "Clear the session history, releasing all object references, and\n        optionally open a new session.", "docstring_tokens": ["Clear", "the", "session", "history", "releasing", "all", "object", "references", "and", "optionally", "open", "a", "new", "session", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/core/history.py#L445-L457", "partition": "test"}
{"repo": "vaexio/vaex", "path": "packages/vaex-core/vaex/dataframe.py", "func_name": "DataFrame.state_set", "original_string": "def state_set(self, state, use_active_range=False):\n        \"\"\"Sets the internal state of the df\n\n        Example:\n\n        >>> import vaex\n        >>> df = vaex.from_scalars(x=1, y=2)\n        >>> df\n          #    x    y        r\n          0    1    2  2.23607\n        >>> df['r'] = (df.x**2 + df.y**2)**0.5\n        >>> state = df.state_get()\n        >>> state\n        {'active_range': [0, 1],\n        'column_names': ['x', 'y', 'r'],\n        'description': None,\n        'descriptions': {},\n        'functions': {},\n        'renamed_columns': [],\n        'selections': {'__filter__': None},\n        'ucds': {},\n        'units': {},\n        'variables': {},\n        'virtual_columns': {'r': '(((x ** 2) + (y ** 2)) ** 0.5)'}}\n        >>> df2 = vaex.from_scalars(x=3, y=4)\n        >>> df2.state_set(state)  # now the virtual functions are 'copied'\n        >>> df2\n          #    x    y    r\n          0    3    4    5\n\n        :param state: dict as returned by :meth:`DataFrame.state_get`.\n        :param bool use_active_range: Whether to use the active range or not.\n        \"\"\"\n        self.description = state['description']\n        if use_active_range:\n            self._index_start, self._index_end = state['active_range']\n        self._length_unfiltered = self._index_end - self._index_start\n        if 'renamed_columns' in state:\n            for old, new in state['renamed_columns']:\n                self._rename(old, new)\n        for name, value in state['functions'].items():\n            self.add_function(name, vaex.serialize.from_dict(value))\n        if 'column_names' in state:\n            # we clear all columns, and add them later on, since otherwise self[name] = ... will try\n            # to rename the columns (which is unsupported for remote dfs)\n            self.column_names = []\n            self.virtual_columns = collections.OrderedDict()\n            for name, value in state['virtual_columns'].items():\n                self[name] = self._expr(value)\n                # self._save_assign_expression(name)\n            self.column_names = state['column_names']\n        else:\n            # old behaviour\n            self.virtual_columns = collections.OrderedDict()\n            for name, value in state['virtual_columns'].items():\n                self[name] = self._expr(value)\n        self.variables = state['variables']\n        import astropy  # TODO: make this dep optional?\n        units = {key: astropy.units.Unit(value) for key, value in state[\"units\"].items()}\n        self.units.update(units)\n        for name, selection_dict in state['selections'].items():\n            # TODO: make selection use the vaex.serialize framework\n            if selection_dict is None:\n                selection = None\n            else:\n                selection = selections.selection_from_dict(selection_dict)\n            self.set_selection(selection, name=name)", "language": "python", "code": "def state_set(self, state, use_active_range=False):\n        \"\"\"Sets the internal state of the df\n\n        Example:\n\n        >>> import vaex\n        >>> df = vaex.from_scalars(x=1, y=2)\n        >>> df\n          #    x    y        r\n          0    1    2  2.23607\n        >>> df['r'] = (df.x**2 + df.y**2)**0.5\n        >>> state = df.state_get()\n        >>> state\n        {'active_range': [0, 1],\n        'column_names': ['x', 'y', 'r'],\n        'description': None,\n        'descriptions': {},\n        'functions': {},\n        'renamed_columns': [],\n        'selections': {'__filter__': None},\n        'ucds': {},\n        'units': {},\n        'variables': {},\n        'virtual_columns': {'r': '(((x ** 2) + (y ** 2)) ** 0.5)'}}\n        >>> df2 = vaex.from_scalars(x=3, y=4)\n        >>> df2.state_set(state)  # now the virtual functions are 'copied'\n        >>> df2\n          #    x    y    r\n          0    3    4    5\n\n        :param state: dict as returned by :meth:`DataFrame.state_get`.\n        :param bool use_active_range: Whether to use the active range or not.\n        \"\"\"\n        self.description = state['description']\n        if use_active_range:\n            self._index_start, self._index_end = state['active_range']\n        self._length_unfiltered = self._index_end - self._index_start\n        if 'renamed_columns' in state:\n            for old, new in state['renamed_columns']:\n                self._rename(old, new)\n        for name, value in state['functions'].items():\n            self.add_function(name, vaex.serialize.from_dict(value))\n        if 'column_names' in state:\n            # we clear all columns, and add them later on, since otherwise self[name] = ... will try\n            # to rename the columns (which is unsupported for remote dfs)\n            self.column_names = []\n            self.virtual_columns = collections.OrderedDict()\n            for name, value in state['virtual_columns'].items():\n                self[name] = self._expr(value)\n                # self._save_assign_expression(name)\n            self.column_names = state['column_names']\n        else:\n            # old behaviour\n            self.virtual_columns = collections.OrderedDict()\n            for name, value in state['virtual_columns'].items():\n                self[name] = self._expr(value)\n        self.variables = state['variables']\n        import astropy  # TODO: make this dep optional?\n        units = {key: astropy.units.Unit(value) for key, value in state[\"units\"].items()}\n        self.units.update(units)\n        for name, selection_dict in state['selections'].items():\n            # TODO: make selection use the vaex.serialize framework\n            if selection_dict is None:\n                selection = None\n            else:\n                selection = selections.selection_from_dict(selection_dict)\n            self.set_selection(selection, name=name)", "code_tokens": ["def", "state_set", "(", "self", ",", "state", ",", "use_active_range", "=", "False", ")", ":", "self", ".", "description", "=", "state", "[", "'description'", "]", "if", "use_active_range", ":", "self", ".", "_index_start", ",", "self", ".", "_index_end", "=", "state", "[", "'active_range'", "]", "self", ".", "_length_unfiltered", "=", "self", ".", "_index_end", "-", "self", ".", "_index_start", "if", "'renamed_columns'", "in", "state", ":", "for", "old", ",", "new", "in", "state", "[", "'renamed_columns'", "]", ":", "self", ".", "_rename", "(", "old", ",", "new", ")", "for", "name", ",", "value", "in", "state", "[", "'functions'", "]", ".", "items", "(", ")", ":", "self", ".", "add_function", "(", "name", ",", "vaex", ".", "serialize", ".", "from_dict", "(", "value", ")", ")", "if", "'column_names'", "in", "state", ":", "# we clear all columns, and add them later on, since otherwise self[name] = ... will try", "# to rename the columns (which is unsupported for remote dfs)", "self", ".", "column_names", "=", "[", "]", "self", ".", "virtual_columns", "=", "collections", ".", "OrderedDict", "(", ")", "for", "name", ",", "value", "in", "state", "[", "'virtual_columns'", "]", ".", "items", "(", ")", ":", "self", "[", "name", "]", "=", "self", ".", "_expr", "(", "value", ")", "# self._save_assign_expression(name)", "self", ".", "column_names", "=", "state", "[", "'column_names'", "]", "else", ":", "# old behaviour", "self", ".", "virtual_columns", "=", "collections", ".", "OrderedDict", "(", ")", "for", "name", ",", "value", "in", "state", "[", "'virtual_columns'", "]", ".", "items", "(", ")", ":", "self", "[", "name", "]", "=", "self", ".", "_expr", "(", "value", ")", "self", ".", "variables", "=", "state", "[", "'variables'", "]", "import", "astropy", "# TODO: make this dep optional?", "units", "=", "{", "key", ":", "astropy", ".", "units", ".", "Unit", "(", "value", ")", "for", "key", ",", "value", "in", "state", "[", "\"units\"", "]", ".", "items", "(", ")", "}", "self", ".", "units", ".", "update", "(", "units", ")", "for", "name", ",", "selection_dict", "in", "state", "[", "'selections'", "]", ".", "items", "(", ")", ":", "# TODO: make selection use the vaex.serialize framework", "if", "selection_dict", "is", "None", ":", "selection", "=", "None", "else", ":", "selection", "=", "selections", ".", "selection_from_dict", "(", "selection_dict", ")", "self", ".", "set_selection", "(", "selection", ",", "name", "=", "name", ")"], "docstring": "Sets the internal state of the df\n\n        Example:\n\n        >>> import vaex\n        >>> df = vaex.from_scalars(x=1, y=2)\n        >>> df\n          #    x    y        r\n          0    1    2  2.23607\n        >>> df['r'] = (df.x**2 + df.y**2)**0.5\n        >>> state = df.state_get()\n        >>> state\n        {'active_range': [0, 1],\n        'column_names': ['x', 'y', 'r'],\n        'description': None,\n        'descriptions': {},\n        'functions': {},\n        'renamed_columns': [],\n        'selections': {'__filter__': None},\n        'ucds': {},\n        'units': {},\n        'variables': {},\n        'virtual_columns': {'r': '(((x ** 2) + (y ** 2)) ** 0.5)'}}\n        >>> df2 = vaex.from_scalars(x=3, y=4)\n        >>> df2.state_set(state)  # now the virtual functions are 'copied'\n        >>> df2\n          #    x    y    r\n          0    3    4    5\n\n        :param state: dict as returned by :meth:`DataFrame.state_get`.\n        :param bool use_active_range: Whether to use the active range or not.", "docstring_tokens": ["Sets", "the", "internal", "state", "of", "the", "df"], "sha": "a45b672f8287afca2ada8e36b74b604b9b28dd85", "url": "https://github.com/vaexio/vaex/blob/a45b672f8287afca2ada8e36b74b604b9b28dd85/packages/vaex-core/vaex/dataframe.py#L2135-L2201", "partition": "test"}
{"repo": "streamlink/streamlink", "path": "src/streamlink/plugins/ceskatelevize.py", "func_name": "_find_player_url", "original_string": "def _find_player_url(response):\n    \"\"\"\n    Finds embedded player url in HTTP response.\n\n    :param response: Response object.\n    :returns: Player url (str).\n    \"\"\"\n    url = ''\n    matches = _player_re.search(response.text)\n    if matches:\n        tmp_url = matches.group(0).replace('&amp;', '&')\n        if 'hash' not in tmp_url:\n            # there's no hash in the URL, try to find it\n            matches = _hash_re.search(response.text)\n            if matches:\n                url = tmp_url + '&hash=' + matches.group(1)\n        else:\n            url = tmp_url\n\n    return 'http://ceskatelevize.cz/' + url", "language": "python", "code": "def _find_player_url(response):\n    \"\"\"\n    Finds embedded player url in HTTP response.\n\n    :param response: Response object.\n    :returns: Player url (str).\n    \"\"\"\n    url = ''\n    matches = _player_re.search(response.text)\n    if matches:\n        tmp_url = matches.group(0).replace('&amp;', '&')\n        if 'hash' not in tmp_url:\n            # there's no hash in the URL, try to find it\n            matches = _hash_re.search(response.text)\n            if matches:\n                url = tmp_url + '&hash=' + matches.group(1)\n        else:\n            url = tmp_url\n\n    return 'http://ceskatelevize.cz/' + url", "code_tokens": ["def", "_find_player_url", "(", "response", ")", ":", "url", "=", "''", "matches", "=", "_player_re", ".", "search", "(", "response", ".", "text", ")", "if", "matches", ":", "tmp_url", "=", "matches", ".", "group", "(", "0", ")", ".", "replace", "(", "'&amp;'", ",", "'&'", ")", "if", "'hash'", "not", "in", "tmp_url", ":", "# there's no hash in the URL, try to find it", "matches", "=", "_hash_re", ".", "search", "(", "response", ".", "text", ")", "if", "matches", ":", "url", "=", "tmp_url", "+", "'&hash='", "+", "matches", ".", "group", "(", "1", ")", "else", ":", "url", "=", "tmp_url", "return", "'http://ceskatelevize.cz/'", "+", "url"], "docstring": "Finds embedded player url in HTTP response.\n\n    :param response: Response object.\n    :returns: Player url (str).", "docstring_tokens": ["Finds", "embedded", "player", "url", "in", "HTTP", "response", "."], "sha": "c8ed1daff14ac03195870238b9b900c1109dd5c1", "url": "https://github.com/streamlink/streamlink/blob/c8ed1daff14ac03195870238b9b900c1109dd5c1/src/streamlink/plugins/ceskatelevize.py#L70-L89", "partition": "test"}
{"repo": "celiao/tmdbsimple", "path": "tmdbsimple/movies.py", "func_name": "Movies.credits", "original_string": "def credits(self, **kwargs):\n        \"\"\"\n        Get the cast and crew information for a specific movie id.\n\n        Args:\n            append_to_response: (optional) Comma separated, any movie method.\n\n        Returns:\n            A dict representation of the JSON returned from the API.\n        \"\"\"\n        path = self._get_id_path('credits')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response", "language": "python", "code": "def credits(self, **kwargs):\n        \"\"\"\n        Get the cast and crew information for a specific movie id.\n\n        Args:\n            append_to_response: (optional) Comma separated, any movie method.\n\n        Returns:\n            A dict representation of the JSON returned from the API.\n        \"\"\"\n        path = self._get_id_path('credits')\n\n        response = self._GET(path, kwargs)\n        self._set_attrs_to_values(response)\n        return response", "code_tokens": ["def", "credits", "(", "self", ",", "*", "*", "kwargs", ")", ":", "path", "=", "self", ".", "_get_id_path", "(", "'credits'", ")", "response", "=", "self", ".", "_GET", "(", "path", ",", "kwargs", ")", "self", ".", "_set_attrs_to_values", "(", "response", ")", "return", "response"], "docstring": "Get the cast and crew information for a specific movie id.\n\n        Args:\n            append_to_response: (optional) Comma separated, any movie method.\n\n        Returns:\n            A dict representation of the JSON returned from the API.", "docstring_tokens": ["Get", "the", "cast", "and", "crew", "information", "for", "a", "specific", "movie", "id", "."], "sha": "ff17893110c99771d6398a62c35d36dd9735f4b9", "url": "https://github.com/celiao/tmdbsimple/blob/ff17893110c99771d6398a62c35d36dd9735f4b9/tmdbsimple/movies.py#L87-L101", "partition": "test"}
{"repo": "rocky/python3-trepan", "path": "trepan/post_mortem.py", "func_name": "post_mortem", "original_string": "def post_mortem(exc=None, frameno=1, dbg=None):\n    \"\"\"Enter debugger read loop after your program has crashed.\n\n    exc is a triple like you get back from sys.exc_info.  If no exc\n    parameter, is supplied, the values from sys.last_type,\n    sys.last_value, sys.last_traceback are used. And if these don't\n    exist either we'll assume that sys.exc_info() contains what we\n    want and frameno is the index location of where we want to start.\n\n    'frameno' specifies how many frames to ignore in the traceback.\n    The default is 1, that is, we don't need to show the immediate\n    call into post_mortem. If you have wrapper functions that call\n    this one, you may want to increase frameno.\n    \"\"\"\n\n    if dbg is None:\n        # Check for a global debugger object\n        if Mdebugger.debugger_obj is None:\n            Mdebugger.debugger_obj = Mdebugger.Trepan()\n            pass\n        dbg = Mdebugger.debugger_obj\n        pass\n    re_bogus_file = re.compile(\"^<.+>$\")\n\n    if exc[0] is None:\n        # frameno+1 because we are about to add one more level of call\n        # in get_last_or_frame_exception\n        exc = get_last_or_frame_exception()\n        if exc[0] is None:\n            print(\"Can't find traceback for post_mortem \"\n                  \"in sys.last_traceback or sys.exec_info()\")\n            return\n        pass\n    exc_type, exc_value, exc_tb = exc\n    dbg.core.execution_status = ('Terminated with unhandled exception %s'\n                                 % exc_type)\n\n    # tb has least-recent traceback entry first. We want the most-recent\n    # entry. Also we'll pick out a mainpyfile name if it hasn't previously\n    # been set.\n    if exc_tb is not None:\n        while exc_tb.tb_next is not None:\n            filename = exc_tb.tb_frame.f_code.co_filename\n            if (dbg.mainpyfile and 0 == len(dbg.mainpyfile)\n                and not re_bogus_file.match(filename)):\n                dbg.mainpyfile = filename\n                pass\n            exc_tb = exc_tb.tb_next\n            pass\n        dbg.core.processor.curframe = exc_tb.tb_frame\n        pass\n\n    if 0 == len(dbg.program_sys_argv):\n        # Fake program (run command) args since we weren't called with any\n        dbg.program_sys_argv = list(sys.argv[1:])\n        dbg.program_sys_argv[:0] = [dbg.mainpyfile]\n\n    # if 0 == len(dbg._sys_argv):\n    #     # Fake script invocation (restart) args since we don't have any\n    #     dbg._sys_argv = list(dbg.program_sys_argv)\n    #     dbg._sys_argv[:0] = [__title__]\n\n    try:\n        # # FIXME: This can be called from except hook in which case we\n        # # need this. Dunno why though.\n        # try:\n        #     _pydb_trace.set_trace(t.tb_frame)\n        # except:\n        #     pass\n\n        # Possibly a bug in Python 2.5. Why f.f_lineno is\n        # not always equal to t.tb_lineno, I don't know.\n        f = exc_tb.tb_frame\n        if f and f.f_lineno != exc_tb.tb_lineno : f = f.f_back\n        dbg.core.processor.event_processor(f, 'exception', exc, 'Trepan3k:pm')\n    except DebuggerRestart:\n        while True:\n            sys.argv = list(dbg._program_sys_argv)\n            dbg.msg(\"Restarting %s with arguments:\\n\\t%s\"\n                  % (dbg.filename(dbg.mainpyfile),\n                     \" \".join(dbg._program_sys_argv[1:])))\n            try:\n                dbg.run_script(dbg.mainpyfile)\n            except DebuggerRestart:\n                pass\n            pass\n    except DebuggerQuit:\n        pass\n    return", "language": "python", "code": "def post_mortem(exc=None, frameno=1, dbg=None):\n    \"\"\"Enter debugger read loop after your program has crashed.\n\n    exc is a triple like you get back from sys.exc_info.  If no exc\n    parameter, is supplied, the values from sys.last_type,\n    sys.last_value, sys.last_traceback are used. And if these don't\n    exist either we'll assume that sys.exc_info() contains what we\n    want and frameno is the index location of where we want to start.\n\n    'frameno' specifies how many frames to ignore in the traceback.\n    The default is 1, that is, we don't need to show the immediate\n    call into post_mortem. If you have wrapper functions that call\n    this one, you may want to increase frameno.\n    \"\"\"\n\n    if dbg is None:\n        # Check for a global debugger object\n        if Mdebugger.debugger_obj is None:\n            Mdebugger.debugger_obj = Mdebugger.Trepan()\n            pass\n        dbg = Mdebugger.debugger_obj\n        pass\n    re_bogus_file = re.compile(\"^<.+>$\")\n\n    if exc[0] is None:\n        # frameno+1 because we are about to add one more level of call\n        # in get_last_or_frame_exception\n        exc = get_last_or_frame_exception()\n        if exc[0] is None:\n            print(\"Can't find traceback for post_mortem \"\n                  \"in sys.last_traceback or sys.exec_info()\")\n            return\n        pass\n    exc_type, exc_value, exc_tb = exc\n    dbg.core.execution_status = ('Terminated with unhandled exception %s'\n                                 % exc_type)\n\n    # tb has least-recent traceback entry first. We want the most-recent\n    # entry. Also we'll pick out a mainpyfile name if it hasn't previously\n    # been set.\n    if exc_tb is not None:\n        while exc_tb.tb_next is not None:\n            filename = exc_tb.tb_frame.f_code.co_filename\n            if (dbg.mainpyfile and 0 == len(dbg.mainpyfile)\n                and not re_bogus_file.match(filename)):\n                dbg.mainpyfile = filename\n                pass\n            exc_tb = exc_tb.tb_next\n            pass\n        dbg.core.processor.curframe = exc_tb.tb_frame\n        pass\n\n    if 0 == len(dbg.program_sys_argv):\n        # Fake program (run command) args since we weren't called with any\n        dbg.program_sys_argv = list(sys.argv[1:])\n        dbg.program_sys_argv[:0] = [dbg.mainpyfile]\n\n    # if 0 == len(dbg._sys_argv):\n    #     # Fake script invocation (restart) args since we don't have any\n    #     dbg._sys_argv = list(dbg.program_sys_argv)\n    #     dbg._sys_argv[:0] = [__title__]\n\n    try:\n        # # FIXME: This can be called from except hook in which case we\n        # # need this. Dunno why though.\n        # try:\n        #     _pydb_trace.set_trace(t.tb_frame)\n        # except:\n        #     pass\n\n        # Possibly a bug in Python 2.5. Why f.f_lineno is\n        # not always equal to t.tb_lineno, I don't know.\n        f = exc_tb.tb_frame\n        if f and f.f_lineno != exc_tb.tb_lineno : f = f.f_back\n        dbg.core.processor.event_processor(f, 'exception', exc, 'Trepan3k:pm')\n    except DebuggerRestart:\n        while True:\n            sys.argv = list(dbg._program_sys_argv)\n            dbg.msg(\"Restarting %s with arguments:\\n\\t%s\"\n                  % (dbg.filename(dbg.mainpyfile),\n                     \" \".join(dbg._program_sys_argv[1:])))\n            try:\n                dbg.run_script(dbg.mainpyfile)\n            except DebuggerRestart:\n                pass\n            pass\n    except DebuggerQuit:\n        pass\n    return", "code_tokens": ["def", "post_mortem", "(", "exc", "=", "None", ",", "frameno", "=", "1", ",", "dbg", "=", "None", ")", ":", "if", "dbg", "is", "None", ":", "# Check for a global debugger object", "if", "Mdebugger", ".", "debugger_obj", "is", "None", ":", "Mdebugger", ".", "debugger_obj", "=", "Mdebugger", ".", "Trepan", "(", ")", "pass", "dbg", "=", "Mdebugger", ".", "debugger_obj", "pass", "re_bogus_file", "=", "re", ".", "compile", "(", "\"^<.+>$\"", ")", "if", "exc", "[", "0", "]", "is", "None", ":", "# frameno+1 because we are about to add one more level of call", "# in get_last_or_frame_exception", "exc", "=", "get_last_or_frame_exception", "(", ")", "if", "exc", "[", "0", "]", "is", "None", ":", "print", "(", "\"Can't find traceback for post_mortem \"", "\"in sys.last_traceback or sys.exec_info()\"", ")", "return", "pass", "exc_type", ",", "exc_value", ",", "exc_tb", "=", "exc", "dbg", ".", "core", ".", "execution_status", "=", "(", "'Terminated with unhandled exception %s'", "%", "exc_type", ")", "# tb has least-recent traceback entry first. We want the most-recent", "# entry. Also we'll pick out a mainpyfile name if it hasn't previously", "# been set.", "if", "exc_tb", "is", "not", "None", ":", "while", "exc_tb", ".", "tb_next", "is", "not", "None", ":", "filename", "=", "exc_tb", ".", "tb_frame", ".", "f_code", ".", "co_filename", "if", "(", "dbg", ".", "mainpyfile", "and", "0", "==", "len", "(", "dbg", ".", "mainpyfile", ")", "and", "not", "re_bogus_file", ".", "match", "(", "filename", ")", ")", ":", "dbg", ".", "mainpyfile", "=", "filename", "pass", "exc_tb", "=", "exc_tb", ".", "tb_next", "pass", "dbg", ".", "core", ".", "processor", ".", "curframe", "=", "exc_tb", ".", "tb_frame", "pass", "if", "0", "==", "len", "(", "dbg", ".", "program_sys_argv", ")", ":", "# Fake program (run command) args since we weren't called with any", "dbg", ".", "program_sys_argv", "=", "list", "(", "sys", ".", "argv", "[", "1", ":", "]", ")", "dbg", ".", "program_sys_argv", "[", ":", "0", "]", "=", "[", "dbg", ".", "mainpyfile", "]", "# if 0 == len(dbg._sys_argv):", "#     # Fake script invocation (restart) args since we don't have any", "#     dbg._sys_argv = list(dbg.program_sys_argv)", "#     dbg._sys_argv[:0] = [__title__]", "try", ":", "# # FIXME: This can be called from except hook in which case we", "# # need this. Dunno why though.", "# try:", "#     _pydb_trace.set_trace(t.tb_frame)", "# except:", "#     pass", "# Possibly a bug in Python 2.5. Why f.f_lineno is", "# not always equal to t.tb_lineno, I don't know.", "f", "=", "exc_tb", ".", "tb_frame", "if", "f", "and", "f", ".", "f_lineno", "!=", "exc_tb", ".", "tb_lineno", ":", "f", "=", "f", ".", "f_back", "dbg", ".", "core", ".", "processor", ".", "event_processor", "(", "f", ",", "'exception'", ",", "exc", ",", "'Trepan3k:pm'", ")", "except", "DebuggerRestart", ":", "while", "True", ":", "sys", ".", "argv", "=", "list", "(", "dbg", ".", "_program_sys_argv", ")", "dbg", ".", "msg", "(", "\"Restarting %s with arguments:\\n\\t%s\"", "%", "(", "dbg", ".", "filename", "(", "dbg", ".", "mainpyfile", ")", ",", "\" \"", ".", "join", "(", "dbg", ".", "_program_sys_argv", "[", "1", ":", "]", ")", ")", ")", "try", ":", "dbg", ".", "run_script", "(", "dbg", ".", "mainpyfile", ")", "except", "DebuggerRestart", ":", "pass", "pass", "except", "DebuggerQuit", ":", "pass", "return"], "docstring": "Enter debugger read loop after your program has crashed.\n\n    exc is a triple like you get back from sys.exc_info.  If no exc\n    parameter, is supplied, the values from sys.last_type,\n    sys.last_value, sys.last_traceback are used. And if these don't\n    exist either we'll assume that sys.exc_info() contains what we\n    want and frameno is the index location of where we want to start.\n\n    'frameno' specifies how many frames to ignore in the traceback.\n    The default is 1, that is, we don't need to show the immediate\n    call into post_mortem. If you have wrapper functions that call\n    this one, you may want to increase frameno.", "docstring_tokens": ["Enter", "debugger", "read", "loop", "after", "your", "program", "has", "crashed", "."], "sha": "14e91bc0acce090d67be145b1ac040cab92ac5f3", "url": "https://github.com/rocky/python3-trepan/blob/14e91bc0acce090d67be145b1ac040cab92ac5f3/trepan/post_mortem.py#L80-L168", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval", "path": "perceval/backend.py", "func_name": "BackendCommandArgumentParser._set_archive_arguments", "original_string": "def _set_archive_arguments(self):\n        \"\"\"Activate archive arguments parsing\"\"\"\n\n        group = self.parser.add_argument_group('archive arguments')\n        group.add_argument('--archive-path', dest='archive_path', default=None,\n                           help=\"directory path to the archives\")\n        group.add_argument('--no-archive', dest='no_archive', action='store_true',\n                           help=\"do not archive data\")\n        group.add_argument('--fetch-archive', dest='fetch_archive', action='store_true',\n                           help=\"fetch data from the archives\")\n        group.add_argument('--archived-since', dest='archived_since', default='1970-01-01',\n                           help=\"retrieve items archived since the given date\")", "language": "python", "code": "def _set_archive_arguments(self):\n        \"\"\"Activate archive arguments parsing\"\"\"\n\n        group = self.parser.add_argument_group('archive arguments')\n        group.add_argument('--archive-path', dest='archive_path', default=None,\n                           help=\"directory path to the archives\")\n        group.add_argument('--no-archive', dest='no_archive', action='store_true',\n                           help=\"do not archive data\")\n        group.add_argument('--fetch-archive', dest='fetch_archive', action='store_true',\n                           help=\"fetch data from the archives\")\n        group.add_argument('--archived-since', dest='archived_since', default='1970-01-01',\n                           help=\"retrieve items archived since the given date\")", "code_tokens": ["def", "_set_archive_arguments", "(", "self", ")", ":", "group", "=", "self", ".", "parser", ".", "add_argument_group", "(", "'archive arguments'", ")", "group", ".", "add_argument", "(", "'--archive-path'", ",", "dest", "=", "'archive_path'", ",", "default", "=", "None", ",", "help", "=", "\"directory path to the archives\"", ")", "group", ".", "add_argument", "(", "'--no-archive'", ",", "dest", "=", "'no_archive'", ",", "action", "=", "'store_true'", ",", "help", "=", "\"do not archive data\"", ")", "group", ".", "add_argument", "(", "'--fetch-archive'", ",", "dest", "=", "'fetch_archive'", ",", "action", "=", "'store_true'", ",", "help", "=", "\"fetch data from the archives\"", ")", "group", ".", "add_argument", "(", "'--archived-since'", ",", "dest", "=", "'archived_since'", ",", "default", "=", "'1970-01-01'", ",", "help", "=", "\"retrieve items archived since the given date\"", ")"], "docstring": "Activate archive arguments parsing", "docstring_tokens": ["Activate", "archive", "arguments", "parsing"], "sha": "41c908605e88b7ebc3a536c643fa0f212eaf9e0e", "url": "https://github.com/chaoss/grimoirelab-perceval/blob/41c908605e88b7ebc3a536c643fa0f212eaf9e0e/perceval/backend.py#L391-L402", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/pulse/timeslots.py", "func_name": "TimeslotCollection.is_mergeable_with", "original_string": "def is_mergeable_with(self, timeslots: 'TimeslotCollection') -> bool:\n        \"\"\"Return if self is mergeable with `timeslots`.\n\n        Args:\n            timeslots: TimeslotCollection to be checked\n        \"\"\"\n        for slot in timeslots.timeslots:\n            for interval in self._table[slot.channel]:\n                if slot.interval.has_overlap(interval):\n                    return False\n        return True", "language": "python", "code": "def is_mergeable_with(self, timeslots: 'TimeslotCollection') -> bool:\n        \"\"\"Return if self is mergeable with `timeslots`.\n\n        Args:\n            timeslots: TimeslotCollection to be checked\n        \"\"\"\n        for slot in timeslots.timeslots:\n            for interval in self._table[slot.channel]:\n                if slot.interval.has_overlap(interval):\n                    return False\n        return True", "code_tokens": ["def", "is_mergeable_with", "(", "self", ",", "timeslots", ":", "'TimeslotCollection'", ")", "->", "bool", ":", "for", "slot", "in", "timeslots", ".", "timeslots", ":", "for", "interval", "in", "self", ".", "_table", "[", "slot", ".", "channel", "]", ":", "if", "slot", ".", "interval", ".", "has_overlap", "(", "interval", ")", ":", "return", "False", "return", "True"], "docstring": "Return if self is mergeable with `timeslots`.\n\n        Args:\n            timeslots: TimeslotCollection to be checked", "docstring_tokens": ["Return", "if", "self", "is", "mergeable", "with", "timeslots", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/pulse/timeslots.py#L211-L221", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/hooks/http_hook.py", "func_name": "HttpHook.run_and_check", "original_string": "def run_and_check(self, session, prepped_request, extra_options):\n        \"\"\"\n        Grabs extra options like timeout and actually runs the request,\n        checking for the result\n\n        :param session: the session to be used to execute the request\n        :type session: requests.Session\n        :param prepped_request: the prepared request generated in run()\n        :type prepped_request: session.prepare_request\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX\n            or 3XX status codes\n        :type extra_options: dict\n        \"\"\"\n        extra_options = extra_options or {}\n\n        try:\n            response = session.send(\n                prepped_request,\n                stream=extra_options.get(\"stream\", False),\n                verify=extra_options.get(\"verify\", True),\n                proxies=extra_options.get(\"proxies\", {}),\n                cert=extra_options.get(\"cert\"),\n                timeout=extra_options.get(\"timeout\"),\n                allow_redirects=extra_options.get(\"allow_redirects\", True))\n\n            if extra_options.get('check_response', True):\n                self.check_response(response)\n            return response\n\n        except requests.exceptions.ConnectionError as ex:\n            self.log.warn(str(ex) + ' Tenacity will retry to execute the operation')\n            raise ex", "language": "python", "code": "def run_and_check(self, session, prepped_request, extra_options):\n        \"\"\"\n        Grabs extra options like timeout and actually runs the request,\n        checking for the result\n\n        :param session: the session to be used to execute the request\n        :type session: requests.Session\n        :param prepped_request: the prepared request generated in run()\n        :type prepped_request: session.prepare_request\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX\n            or 3XX status codes\n        :type extra_options: dict\n        \"\"\"\n        extra_options = extra_options or {}\n\n        try:\n            response = session.send(\n                prepped_request,\n                stream=extra_options.get(\"stream\", False),\n                verify=extra_options.get(\"verify\", True),\n                proxies=extra_options.get(\"proxies\", {}),\n                cert=extra_options.get(\"cert\"),\n                timeout=extra_options.get(\"timeout\"),\n                allow_redirects=extra_options.get(\"allow_redirects\", True))\n\n            if extra_options.get('check_response', True):\n                self.check_response(response)\n            return response\n\n        except requests.exceptions.ConnectionError as ex:\n            self.log.warn(str(ex) + ' Tenacity will retry to execute the operation')\n            raise ex", "code_tokens": ["def", "run_and_check", "(", "self", ",", "session", ",", "prepped_request", ",", "extra_options", ")", ":", "extra_options", "=", "extra_options", "or", "{", "}", "try", ":", "response", "=", "session", ".", "send", "(", "prepped_request", ",", "stream", "=", "extra_options", ".", "get", "(", "\"stream\"", ",", "False", ")", ",", "verify", "=", "extra_options", ".", "get", "(", "\"verify\"", ",", "True", ")", ",", "proxies", "=", "extra_options", ".", "get", "(", "\"proxies\"", ",", "{", "}", ")", ",", "cert", "=", "extra_options", ".", "get", "(", "\"cert\"", ")", ",", "timeout", "=", "extra_options", ".", "get", "(", "\"timeout\"", ")", ",", "allow_redirects", "=", "extra_options", ".", "get", "(", "\"allow_redirects\"", ",", "True", ")", ")", "if", "extra_options", ".", "get", "(", "'check_response'", ",", "True", ")", ":", "self", ".", "check_response", "(", "response", ")", "return", "response", "except", "requests", ".", "exceptions", ".", "ConnectionError", "as", "ex", ":", "self", ".", "log", ".", "warn", "(", "str", "(", "ex", ")", "+", "' Tenacity will retry to execute the operation'", ")", "raise", "ex"], "docstring": "Grabs extra options like timeout and actually runs the request,\n        checking for the result\n\n        :param session: the session to be used to execute the request\n        :type session: requests.Session\n        :param prepped_request: the prepared request generated in run()\n        :type prepped_request: session.prepare_request\n        :param extra_options: additional options to be used when executing the request\n            i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX\n            or 3XX status codes\n        :type extra_options: dict", "docstring_tokens": ["Grabs", "extra", "options", "like", "timeout", "and", "actually", "runs", "the", "request", "checking", "for", "the", "result"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/http_hook.py#L149-L181", "partition": "test"}
{"repo": "h2non/pook", "path": "pook/engine.py", "func_name": "Engine.enable_network", "original_string": "def enable_network(self, *hostnames):\n        \"\"\"\n        Enables real networking mode, optionally passing one or multiple\n        hostnames that would be used as filter.\n\n        If at least one hostname matches with the outgoing traffic, the\n        request will be executed via the real network.\n\n        Arguments:\n            *hostnames: optional list of host names to enable real network\n                against them. hostname value can be a regular expression.\n        \"\"\"\n        def hostname_filter(hostname, req):\n            if isregex(hostname):\n                return hostname.match(req.url.hostname)\n            return req.url.hostname == hostname\n\n        for hostname in hostnames:\n            self.use_network_filter(partial(hostname_filter, hostname))\n\n        self.networking = True", "language": "python", "code": "def enable_network(self, *hostnames):\n        \"\"\"\n        Enables real networking mode, optionally passing one or multiple\n        hostnames that would be used as filter.\n\n        If at least one hostname matches with the outgoing traffic, the\n        request will be executed via the real network.\n\n        Arguments:\n            *hostnames: optional list of host names to enable real network\n                against them. hostname value can be a regular expression.\n        \"\"\"\n        def hostname_filter(hostname, req):\n            if isregex(hostname):\n                return hostname.match(req.url.hostname)\n            return req.url.hostname == hostname\n\n        for hostname in hostnames:\n            self.use_network_filter(partial(hostname_filter, hostname))\n\n        self.networking = True", "code_tokens": ["def", "enable_network", "(", "self", ",", "*", "hostnames", ")", ":", "def", "hostname_filter", "(", "hostname", ",", "req", ")", ":", "if", "isregex", "(", "hostname", ")", ":", "return", "hostname", ".", "match", "(", "req", ".", "url", ".", "hostname", ")", "return", "req", ".", "url", ".", "hostname", "==", "hostname", "for", "hostname", "in", "hostnames", ":", "self", ".", "use_network_filter", "(", "partial", "(", "hostname_filter", ",", "hostname", ")", ")", "self", ".", "networking", "=", "True"], "docstring": "Enables real networking mode, optionally passing one or multiple\n        hostnames that would be used as filter.\n\n        If at least one hostname matches with the outgoing traffic, the\n        request will be executed via the real network.\n\n        Arguments:\n            *hostnames: optional list of host names to enable real network\n                against them. hostname value can be a regular expression.", "docstring_tokens": ["Enables", "real", "networking", "mode", "optionally", "passing", "one", "or", "multiple", "hostnames", "that", "would", "be", "used", "as", "filter", "."], "sha": "e64094e41e4d89d98d2d29af7608ef27dc50cf19", "url": "https://github.com/h2non/pook/blob/e64094e41e4d89d98d2d29af7608ef27dc50cf19/pook/engine.py#L84-L104", "partition": "test"}
{"repo": "trec-kba/streamcorpus-pipeline", "path": "streamcorpus_pipeline/_taggers.py", "func_name": "TaggerBatchTransform.shutdown", "original_string": "def shutdown(self):\n        '''\n        send SIGTERM to the tagger child process\n        '''\n        if self._child:\n            try:\n                self._child.terminate()\n            except OSError, exc:\n                if exc.errno == 3:\n                    ## child is already gone, possibly because it ran\n                    ## out of memory and caused us to shutdown\n                    pass", "language": "python", "code": "def shutdown(self):\n        '''\n        send SIGTERM to the tagger child process\n        '''\n        if self._child:\n            try:\n                self._child.terminate()\n            except OSError, exc:\n                if exc.errno == 3:\n                    ## child is already gone, possibly because it ran\n                    ## out of memory and caused us to shutdown\n                    pass", "code_tokens": ["def", "shutdown", "(", "self", ")", ":", "if", "self", ".", "_child", ":", "try", ":", "self", ".", "_child", ".", "terminate", "(", ")", "except", "OSError", ",", "exc", ":", "if", "exc", ".", "errno", "==", "3", ":", "## child is already gone, possibly because it ran", "## out of memory and caused us to shutdown", "pass"], "docstring": "send SIGTERM to the tagger child process", "docstring_tokens": ["send", "SIGTERM", "to", "the", "tagger", "child", "process"], "sha": "8bb82ea1beb83c6b40ed03fa1659df2897c2292a", "url": "https://github.com/trec-kba/streamcorpus-pipeline/blob/8bb82ea1beb83c6b40ed03fa1659df2897c2292a/streamcorpus_pipeline/_taggers.py#L763-L774", "partition": "test"}
{"repo": "tnkteja/myhelp", "path": "virtualEnvironment/lib/python2.7/site-packages/coverage/summary.py", "func_name": "SummaryReporter.report", "original_string": "def report(self, morfs, outfile=None):\n        \"\"\"Writes a report summarizing coverage statistics per module.\n\n        `outfile` is a file object to write the summary to.\n\n        \"\"\"\n        self.find_code_units(morfs)\n\n        # Prepare the formatting strings\n        max_name = max([len(cu.name) for cu in self.code_units] + [5])\n        fmt_name = \"%%- %ds  \" % max_name\n        fmt_err = \"%s   %s: %s\\n\"\n        header = (fmt_name % \"Name\") + \" Stmts   Miss\"\n        fmt_coverage = fmt_name + \"%6d %6d\"\n        if self.branches:\n            header += \" Branch BrMiss\"\n            fmt_coverage += \" %6d %6d\"\n        width100 = Numbers.pc_str_width()\n        header += \"%*s\" % (width100+4, \"Cover\")\n        fmt_coverage += \"%%%ds%%%%\" % (width100+3,)\n        if self.config.show_missing:\n            header += \"   Missing\"\n            fmt_coverage += \"   %s\"\n        rule = \"-\" * len(header) + \"\\n\"\n        header += \"\\n\"\n        fmt_coverage += \"\\n\"\n\n        if not outfile:\n            outfile = sys.stdout\n\n        # Write the header\n        outfile.write(header)\n        outfile.write(rule)\n\n        total = Numbers()\n\n        for cu in self.code_units:\n            try:\n                analysis = self.coverage._analyze(cu)\n                nums = analysis.numbers\n                args = (cu.name, nums.n_statements, nums.n_missing)\n                if self.branches:\n                    args += (nums.n_branches, nums.n_missing_branches)\n                args += (nums.pc_covered_str,)\n                if self.config.show_missing:\n                    args += (analysis.missing_formatted(),)\n                outfile.write(fmt_coverage % args)\n                total += nums\n            except KeyboardInterrupt:                   # pragma: not covered\n                raise\n            except:\n                report_it = not self.config.ignore_errors\n                if report_it:\n                    typ, msg = sys.exc_info()[:2]\n                    if typ is NotPython and not cu.should_be_python():\n                        report_it = False\n                if report_it:\n                    outfile.write(fmt_err % (cu.name, typ.__name__, msg))\n\n        if total.n_files > 1:\n            outfile.write(rule)\n            args = (\"TOTAL\", total.n_statements, total.n_missing)\n            if self.branches:\n                args += (total.n_branches, total.n_missing_branches)\n            args += (total.pc_covered_str,)\n            if self.config.show_missing:\n                args += (\"\",)\n            outfile.write(fmt_coverage % args)\n\n        return total.pc_covered", "language": "python", "code": "def report(self, morfs, outfile=None):\n        \"\"\"Writes a report summarizing coverage statistics per module.\n\n        `outfile` is a file object to write the summary to.\n\n        \"\"\"\n        self.find_code_units(morfs)\n\n        # Prepare the formatting strings\n        max_name = max([len(cu.name) for cu in self.code_units] + [5])\n        fmt_name = \"%%- %ds  \" % max_name\n        fmt_err = \"%s   %s: %s\\n\"\n        header = (fmt_name % \"Name\") + \" Stmts   Miss\"\n        fmt_coverage = fmt_name + \"%6d %6d\"\n        if self.branches:\n            header += \" Branch BrMiss\"\n            fmt_coverage += \" %6d %6d\"\n        width100 = Numbers.pc_str_width()\n        header += \"%*s\" % (width100+4, \"Cover\")\n        fmt_coverage += \"%%%ds%%%%\" % (width100+3,)\n        if self.config.show_missing:\n            header += \"   Missing\"\n            fmt_coverage += \"   %s\"\n        rule = \"-\" * len(header) + \"\\n\"\n        header += \"\\n\"\n        fmt_coverage += \"\\n\"\n\n        if not outfile:\n            outfile = sys.stdout\n\n        # Write the header\n        outfile.write(header)\n        outfile.write(rule)\n\n        total = Numbers()\n\n        for cu in self.code_units:\n            try:\n                analysis = self.coverage._analyze(cu)\n                nums = analysis.numbers\n                args = (cu.name, nums.n_statements, nums.n_missing)\n                if self.branches:\n                    args += (nums.n_branches, nums.n_missing_branches)\n                args += (nums.pc_covered_str,)\n                if self.config.show_missing:\n                    args += (analysis.missing_formatted(),)\n                outfile.write(fmt_coverage % args)\n                total += nums\n            except KeyboardInterrupt:                   # pragma: not covered\n                raise\n            except:\n                report_it = not self.config.ignore_errors\n                if report_it:\n                    typ, msg = sys.exc_info()[:2]\n                    if typ is NotPython and not cu.should_be_python():\n                        report_it = False\n                if report_it:\n                    outfile.write(fmt_err % (cu.name, typ.__name__, msg))\n\n        if total.n_files > 1:\n            outfile.write(rule)\n            args = (\"TOTAL\", total.n_statements, total.n_missing)\n            if self.branches:\n                args += (total.n_branches, total.n_missing_branches)\n            args += (total.pc_covered_str,)\n            if self.config.show_missing:\n                args += (\"\",)\n            outfile.write(fmt_coverage % args)\n\n        return total.pc_covered", "code_tokens": ["def", "report", "(", "self", ",", "morfs", ",", "outfile", "=", "None", ")", ":", "self", ".", "find_code_units", "(", "morfs", ")", "# Prepare the formatting strings", "max_name", "=", "max", "(", "[", "len", "(", "cu", ".", "name", ")", "for", "cu", "in", "self", ".", "code_units", "]", "+", "[", "5", "]", ")", "fmt_name", "=", "\"%%- %ds  \"", "%", "max_name", "fmt_err", "=", "\"%s   %s: %s\\n\"", "header", "=", "(", "fmt_name", "%", "\"Name\"", ")", "+", "\" Stmts   Miss\"", "fmt_coverage", "=", "fmt_name", "+", "\"%6d %6d\"", "if", "self", ".", "branches", ":", "header", "+=", "\" Branch BrMiss\"", "fmt_coverage", "+=", "\" %6d %6d\"", "width100", "=", "Numbers", ".", "pc_str_width", "(", ")", "header", "+=", "\"%*s\"", "%", "(", "width100", "+", "4", ",", "\"Cover\"", ")", "fmt_coverage", "+=", "\"%%%ds%%%%\"", "%", "(", "width100", "+", "3", ",", ")", "if", "self", ".", "config", ".", "show_missing", ":", "header", "+=", "\"   Missing\"", "fmt_coverage", "+=", "\"   %s\"", "rule", "=", "\"-\"", "*", "len", "(", "header", ")", "+", "\"\\n\"", "header", "+=", "\"\\n\"", "fmt_coverage", "+=", "\"\\n\"", "if", "not", "outfile", ":", "outfile", "=", "sys", ".", "stdout", "# Write the header", "outfile", ".", "write", "(", "header", ")", "outfile", ".", "write", "(", "rule", ")", "total", "=", "Numbers", "(", ")", "for", "cu", "in", "self", ".", "code_units", ":", "try", ":", "analysis", "=", "self", ".", "coverage", ".", "_analyze", "(", "cu", ")", "nums", "=", "analysis", ".", "numbers", "args", "=", "(", "cu", ".", "name", ",", "nums", ".", "n_statements", ",", "nums", ".", "n_missing", ")", "if", "self", ".", "branches", ":", "args", "+=", "(", "nums", ".", "n_branches", ",", "nums", ".", "n_missing_branches", ")", "args", "+=", "(", "nums", ".", "pc_covered_str", ",", ")", "if", "self", ".", "config", ".", "show_missing", ":", "args", "+=", "(", "analysis", ".", "missing_formatted", "(", ")", ",", ")", "outfile", ".", "write", "(", "fmt_coverage", "%", "args", ")", "total", "+=", "nums", "except", "KeyboardInterrupt", ":", "# pragma: not covered", "raise", "except", ":", "report_it", "=", "not", "self", ".", "config", ".", "ignore_errors", "if", "report_it", ":", "typ", ",", "msg", "=", "sys", ".", "exc_info", "(", ")", "[", ":", "2", "]", "if", "typ", "is", "NotPython", "and", "not", "cu", ".", "should_be_python", "(", ")", ":", "report_it", "=", "False", "if", "report_it", ":", "outfile", ".", "write", "(", "fmt_err", "%", "(", "cu", ".", "name", ",", "typ", ".", "__name__", ",", "msg", ")", ")", "if", "total", ".", "n_files", ">", "1", ":", "outfile", ".", "write", "(", "rule", ")", "args", "=", "(", "\"TOTAL\"", ",", "total", ".", "n_statements", ",", "total", ".", "n_missing", ")", "if", "self", ".", "branches", ":", "args", "+=", "(", "total", ".", "n_branches", ",", "total", ".", "n_missing_branches", ")", "args", "+=", "(", "total", ".", "pc_covered_str", ",", ")", "if", "self", ".", "config", ".", "show_missing", ":", "args", "+=", "(", "\"\"", ",", ")", "outfile", ".", "write", "(", "fmt_coverage", "%", "args", ")", "return", "total", ".", "pc_covered"], "docstring": "Writes a report summarizing coverage statistics per module.\n\n        `outfile` is a file object to write the summary to.", "docstring_tokens": ["Writes", "a", "report", "summarizing", "coverage", "statistics", "per", "module", "."], "sha": "fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb", "url": "https://github.com/tnkteja/myhelp/blob/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb/virtualEnvironment/lib/python2.7/site-packages/coverage/summary.py#L17-L86", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/quantum_info/synthesis/two_qubit_kak.py", "func_name": "simplify_U", "original_string": "def simplify_U(theta, phi, lam):\n    \"\"\"Return the gate u1, u2, or u3 implementing U with the fewest pulses.\n\n    The returned gate implements U exactly, not up to a global phase.\n\n    Args:\n        theta, phi, lam: input Euler rotation angles for a general U gate\n\n    Returns:\n        Gate: one of IdGate, U1Gate, U2Gate, U3Gate.\n    \"\"\"\n    gate = U3Gate(theta, phi, lam)\n    # Y rotation is 0 mod 2*pi, so the gate is a u1\n    if abs(gate.params[0] % (2.0 * math.pi)) < _CUTOFF_PRECISION:\n        gate = U1Gate(gate.params[0] + gate.params[1] + gate.params[2])\n    # Y rotation is pi/2 or -pi/2 mod 2*pi, so the gate is a u2\n    if isinstance(gate, U3Gate):\n        # theta = pi/2 + 2*k*pi\n        if abs((gate.params[0] - math.pi / 2) % (2.0 * math.pi)) < _CUTOFF_PRECISION:\n            gate = U2Gate(gate.params[1],\n                          gate.params[2] + (gate.params[0] - math.pi / 2))\n        # theta = -pi/2 + 2*k*pi\n        if abs((gate.params[0] + math.pi / 2) % (2.0 * math.pi)) < _CUTOFF_PRECISION:\n            gate = U2Gate(gate.params[1] + math.pi,\n                          gate.params[2] - math.pi + (gate.params[0] + math.pi / 2))\n    # u1 and lambda is 0 mod 4*pi so gate is nop\n    if isinstance(gate, U1Gate) and abs(gate.params[0] % (4.0 * math.pi)) < _CUTOFF_PRECISION:\n        gate = IdGate()\n    return gate", "language": "python", "code": "def simplify_U(theta, phi, lam):\n    \"\"\"Return the gate u1, u2, or u3 implementing U with the fewest pulses.\n\n    The returned gate implements U exactly, not up to a global phase.\n\n    Args:\n        theta, phi, lam: input Euler rotation angles for a general U gate\n\n    Returns:\n        Gate: one of IdGate, U1Gate, U2Gate, U3Gate.\n    \"\"\"\n    gate = U3Gate(theta, phi, lam)\n    # Y rotation is 0 mod 2*pi, so the gate is a u1\n    if abs(gate.params[0] % (2.0 * math.pi)) < _CUTOFF_PRECISION:\n        gate = U1Gate(gate.params[0] + gate.params[1] + gate.params[2])\n    # Y rotation is pi/2 or -pi/2 mod 2*pi, so the gate is a u2\n    if isinstance(gate, U3Gate):\n        # theta = pi/2 + 2*k*pi\n        if abs((gate.params[0] - math.pi / 2) % (2.0 * math.pi)) < _CUTOFF_PRECISION:\n            gate = U2Gate(gate.params[1],\n                          gate.params[2] + (gate.params[0] - math.pi / 2))\n        # theta = -pi/2 + 2*k*pi\n        if abs((gate.params[0] + math.pi / 2) % (2.0 * math.pi)) < _CUTOFF_PRECISION:\n            gate = U2Gate(gate.params[1] + math.pi,\n                          gate.params[2] - math.pi + (gate.params[0] + math.pi / 2))\n    # u1 and lambda is 0 mod 4*pi so gate is nop\n    if isinstance(gate, U1Gate) and abs(gate.params[0] % (4.0 * math.pi)) < _CUTOFF_PRECISION:\n        gate = IdGate()\n    return gate", "code_tokens": ["def", "simplify_U", "(", "theta", ",", "phi", ",", "lam", ")", ":", "gate", "=", "U3Gate", "(", "theta", ",", "phi", ",", "lam", ")", "# Y rotation is 0 mod 2*pi, so the gate is a u1", "if", "abs", "(", "gate", ".", "params", "[", "0", "]", "%", "(", "2.0", "*", "math", ".", "pi", ")", ")", "<", "_CUTOFF_PRECISION", ":", "gate", "=", "U1Gate", "(", "gate", ".", "params", "[", "0", "]", "+", "gate", ".", "params", "[", "1", "]", "+", "gate", ".", "params", "[", "2", "]", ")", "# Y rotation is pi/2 or -pi/2 mod 2*pi, so the gate is a u2", "if", "isinstance", "(", "gate", ",", "U3Gate", ")", ":", "# theta = pi/2 + 2*k*pi", "if", "abs", "(", "(", "gate", ".", "params", "[", "0", "]", "-", "math", ".", "pi", "/", "2", ")", "%", "(", "2.0", "*", "math", ".", "pi", ")", ")", "<", "_CUTOFF_PRECISION", ":", "gate", "=", "U2Gate", "(", "gate", ".", "params", "[", "1", "]", ",", "gate", ".", "params", "[", "2", "]", "+", "(", "gate", ".", "params", "[", "0", "]", "-", "math", ".", "pi", "/", "2", ")", ")", "# theta = -pi/2 + 2*k*pi", "if", "abs", "(", "(", "gate", ".", "params", "[", "0", "]", "+", "math", ".", "pi", "/", "2", ")", "%", "(", "2.0", "*", "math", ".", "pi", ")", ")", "<", "_CUTOFF_PRECISION", ":", "gate", "=", "U2Gate", "(", "gate", ".", "params", "[", "1", "]", "+", "math", ".", "pi", ",", "gate", ".", "params", "[", "2", "]", "-", "math", ".", "pi", "+", "(", "gate", ".", "params", "[", "0", "]", "+", "math", ".", "pi", "/", "2", ")", ")", "# u1 and lambda is 0 mod 4*pi so gate is nop", "if", "isinstance", "(", "gate", ",", "U1Gate", ")", "and", "abs", "(", "gate", ".", "params", "[", "0", "]", "%", "(", "4.0", "*", "math", ".", "pi", ")", ")", "<", "_CUTOFF_PRECISION", ":", "gate", "=", "IdGate", "(", ")", "return", "gate"], "docstring": "Return the gate u1, u2, or u3 implementing U with the fewest pulses.\n\n    The returned gate implements U exactly, not up to a global phase.\n\n    Args:\n        theta, phi, lam: input Euler rotation angles for a general U gate\n\n    Returns:\n        Gate: one of IdGate, U1Gate, U2Gate, U3Gate.", "docstring_tokens": ["Return", "the", "gate", "u1", "u2", "or", "u3", "implementing", "U", "with", "the", "fewest", "pulses", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/quantum_info/synthesis/two_qubit_kak.py#L100-L128", "partition": "test"}
{"repo": "filepreviews/filepreviews-python", "path": "filepreviews/__main__.py", "func_name": "generate", "original_string": "def generate(ctx, url, *args, **kwargs):\n    \"\"\"\n    Generate preview for URL.\n    \"\"\"\n    file_previews = ctx.obj['file_previews']\n\n    options = {}\n    metadata = kwargs['metadata']\n    width = kwargs['width']\n    height = kwargs['height']\n    output_format = kwargs['format']\n\n    if metadata:\n        options['metadata'] = metadata.split(',')\n\n    if width:\n        options.setdefault('size', {})\n        options['size']['width'] = width\n\n    if height:\n        options.setdefault('size', {})\n        options['size']['height'] = height\n\n    if output_format:\n        options['format'] = output_format\n\n    results = file_previews.generate(url, **options)\n\n    click.echo(results)", "language": "python", "code": "def generate(ctx, url, *args, **kwargs):\n    \"\"\"\n    Generate preview for URL.\n    \"\"\"\n    file_previews = ctx.obj['file_previews']\n\n    options = {}\n    metadata = kwargs['metadata']\n    width = kwargs['width']\n    height = kwargs['height']\n    output_format = kwargs['format']\n\n    if metadata:\n        options['metadata'] = metadata.split(',')\n\n    if width:\n        options.setdefault('size', {})\n        options['size']['width'] = width\n\n    if height:\n        options.setdefault('size', {})\n        options['size']['height'] = height\n\n    if output_format:\n        options['format'] = output_format\n\n    results = file_previews.generate(url, **options)\n\n    click.echo(results)", "code_tokens": ["def", "generate", "(", "ctx", ",", "url", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "file_previews", "=", "ctx", ".", "obj", "[", "'file_previews'", "]", "options", "=", "{", "}", "metadata", "=", "kwargs", "[", "'metadata'", "]", "width", "=", "kwargs", "[", "'width'", "]", "height", "=", "kwargs", "[", "'height'", "]", "output_format", "=", "kwargs", "[", "'format'", "]", "if", "metadata", ":", "options", "[", "'metadata'", "]", "=", "metadata", ".", "split", "(", "','", ")", "if", "width", ":", "options", ".", "setdefault", "(", "'size'", ",", "{", "}", ")", "options", "[", "'size'", "]", "[", "'width'", "]", "=", "width", "if", "height", ":", "options", ".", "setdefault", "(", "'size'", ",", "{", "}", ")", "options", "[", "'size'", "]", "[", "'height'", "]", "=", "height", "if", "output_format", ":", "options", "[", "'format'", "]", "=", "output_format", "results", "=", "file_previews", ".", "generate", "(", "url", ",", "*", "*", "options", ")", "click", ".", "echo", "(", "results", ")"], "docstring": "Generate preview for URL.", "docstring_tokens": ["Generate", "preview", "for", "URL", "."], "sha": "11be871a07438e3ab5d87ab1f2c163bbac4d4570", "url": "https://github.com/filepreviews/filepreviews-python/blob/11be871a07438e3ab5d87ab1f2c163bbac4d4570/filepreviews/__main__.py#L30-L58", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/frontend/qt/console/console_widget.py", "func_name": "ConsoleWidget._create_page_control", "original_string": "def _create_page_control(self):\n        \"\"\" Creates and connects the underlying paging widget.\n        \"\"\"\n        if self.custom_page_control:\n            control = self.custom_page_control()\n        elif self.kind == 'plain':\n            control = QtGui.QPlainTextEdit()\n        elif self.kind == 'rich':\n            control = QtGui.QTextEdit()\n        control.installEventFilter(self)\n        viewport = control.viewport()\n        viewport.installEventFilter(self)\n        control.setReadOnly(True)\n        control.setUndoRedoEnabled(False)\n        control.setVerticalScrollBarPolicy(QtCore.Qt.ScrollBarAlwaysOn)\n        return control", "language": "python", "code": "def _create_page_control(self):\n        \"\"\" Creates and connects the underlying paging widget.\n        \"\"\"\n        if self.custom_page_control:\n            control = self.custom_page_control()\n        elif self.kind == 'plain':\n            control = QtGui.QPlainTextEdit()\n        elif self.kind == 'rich':\n            control = QtGui.QTextEdit()\n        control.installEventFilter(self)\n        viewport = control.viewport()\n        viewport.installEventFilter(self)\n        control.setReadOnly(True)\n        control.setUndoRedoEnabled(False)\n        control.setVerticalScrollBarPolicy(QtCore.Qt.ScrollBarAlwaysOn)\n        return control", "code_tokens": ["def", "_create_page_control", "(", "self", ")", ":", "if", "self", ".", "custom_page_control", ":", "control", "=", "self", ".", "custom_page_control", "(", ")", "elif", "self", ".", "kind", "==", "'plain'", ":", "control", "=", "QtGui", ".", "QPlainTextEdit", "(", ")", "elif", "self", ".", "kind", "==", "'rich'", ":", "control", "=", "QtGui", ".", "QTextEdit", "(", ")", "control", ".", "installEventFilter", "(", "self", ")", "viewport", "=", "control", ".", "viewport", "(", ")", "viewport", ".", "installEventFilter", "(", "self", ")", "control", ".", "setReadOnly", "(", "True", ")", "control", ".", "setUndoRedoEnabled", "(", "False", ")", "control", ".", "setVerticalScrollBarPolicy", "(", "QtCore", ".", "Qt", ".", "ScrollBarAlwaysOn", ")", "return", "control"], "docstring": "Creates and connects the underlying paging widget.", "docstring_tokens": ["Creates", "and", "connects", "the", "underlying", "paging", "widget", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/frontend/qt/console/console_widget.py#L1036-L1051", "partition": "test"}
{"repo": "tnkteja/myhelp", "path": "virtualEnvironment/lib/python2.7/site-packages/coverage/templite.py", "func_name": "CodeBuilder.get_function", "original_string": "def get_function(self, fn_name):\n        \"\"\"Compile the code, and return the function `fn_name`.\"\"\"\n        assert self.indent_amount == 0\n        g = {}\n        code_text = str(self)\n        exec(code_text, g)\n        return g[fn_name]", "language": "python", "code": "def get_function(self, fn_name):\n        \"\"\"Compile the code, and return the function `fn_name`.\"\"\"\n        assert self.indent_amount == 0\n        g = {}\n        code_text = str(self)\n        exec(code_text, g)\n        return g[fn_name]", "code_tokens": ["def", "get_function", "(", "self", ",", "fn_name", ")", ":", "assert", "self", ".", "indent_amount", "==", "0", "g", "=", "{", "}", "code_text", "=", "str", "(", "self", ")", "exec", "(", "code_text", ",", "g", ")", "return", "g", "[", "fn_name", "]"], "docstring": "Compile the code, and return the function `fn_name`.", "docstring_tokens": ["Compile", "the", "code", "and", "return", "the", "function", "fn_name", "."], "sha": "fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb", "url": "https://github.com/tnkteja/myhelp/blob/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb/virtualEnvironment/lib/python2.7/site-packages/coverage/templite.py#L44-L50", "partition": "test"}
{"repo": "pmacosta/peng", "path": "peng/wave_functions.py", "func_name": "nmax", "original_string": "def nmax(wave, indep_min=None, indep_max=None):\n    r\"\"\"\n    Return the maximum of a waveform's dependent variable vector.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :param indep_min: Independent vector start point of computation\n    :type  indep_min: integer or float\n\n    :param indep_max: Independent vector stop point of computation\n    :type  indep_max: integer or float\n\n    :rtype: float\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc(raised=True)) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.nmax\n\n    :raises:\n     * RuntimeError (Argument \\`indep_max\\` is not valid)\n\n     * RuntimeError (Argument \\`indep_min\\` is not valid)\n\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * RuntimeError (Incongruent \\`indep_min\\` and \\`indep_max\\`\n       arguments)\n\n    .. [[[end]]]\n    \"\"\"\n    ret = copy.copy(wave)\n    _bound_waveform(ret, indep_min, indep_max)\n    return np.max(ret._dep_vector)", "language": "python", "code": "def nmax(wave, indep_min=None, indep_max=None):\n    r\"\"\"\n    Return the maximum of a waveform's dependent variable vector.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :param indep_min: Independent vector start point of computation\n    :type  indep_min: integer or float\n\n    :param indep_max: Independent vector stop point of computation\n    :type  indep_max: integer or float\n\n    :rtype: float\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc(raised=True)) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.nmax\n\n    :raises:\n     * RuntimeError (Argument \\`indep_max\\` is not valid)\n\n     * RuntimeError (Argument \\`indep_min\\` is not valid)\n\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * RuntimeError (Incongruent \\`indep_min\\` and \\`indep_max\\`\n       arguments)\n\n    .. [[[end]]]\n    \"\"\"\n    ret = copy.copy(wave)\n    _bound_waveform(ret, indep_min, indep_max)\n    return np.max(ret._dep_vector)", "code_tokens": ["def", "nmax", "(", "wave", ",", "indep_min", "=", "None", ",", "indep_max", "=", "None", ")", ":", "ret", "=", "copy", ".", "copy", "(", "wave", ")", "_bound_waveform", "(", "ret", ",", "indep_min", ",", "indep_max", ")", "return", "np", ".", "max", "(", "ret", ".", "_dep_vector", ")"], "docstring": "r\"\"\"\n    Return the maximum of a waveform's dependent variable vector.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :param indep_min: Independent vector start point of computation\n    :type  indep_min: integer or float\n\n    :param indep_max: Independent vector stop point of computation\n    :type  indep_max: integer or float\n\n    :rtype: float\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc(raised=True)) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.nmax\n\n    :raises:\n     * RuntimeError (Argument \\`indep_max\\` is not valid)\n\n     * RuntimeError (Argument \\`indep_min\\` is not valid)\n\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * RuntimeError (Incongruent \\`indep_min\\` and \\`indep_max\\`\n       arguments)\n\n    .. [[[end]]]", "docstring_tokens": ["r", "Return", "the", "maximum", "of", "a", "waveform", "s", "dependent", "variable", "vector", "."], "sha": "976935377adaa3de26fc5677aceb2cdfbd6f93a7", "url": "https://github.com/pmacosta/peng/blob/976935377adaa3de26fc5677aceb2cdfbd6f93a7/peng/wave_functions.py#L1555-L1588", "partition": "test"}
{"repo": "mental32/spotify.py", "path": "spotify/models/user.py", "func_name": "User.get_player", "original_string": "async def get_player(self) -> Player:\n        \"\"\"Get information about the users current playback.\n\n        Returns\n        -------\n        player : Player\n            A player object representing the current playback.\n        \"\"\"\n        self._player = player = Player(self.__client, self, await self.http.current_player())\n        return player", "language": "python", "code": "async def get_player(self) -> Player:\n        \"\"\"Get information about the users current playback.\n\n        Returns\n        -------\n        player : Player\n            A player object representing the current playback.\n        \"\"\"\n        self._player = player = Player(self.__client, self, await self.http.current_player())\n        return player", "code_tokens": ["async", "def", "get_player", "(", "self", ")", "->", "Player", ":", "self", ".", "_player", "=", "player", "=", "Player", "(", "self", ".", "__client", ",", "self", ",", "await", "self", ".", "http", ".", "current_player", "(", ")", ")", "return", "player"], "docstring": "Get information about the users current playback.\n\n        Returns\n        -------\n        player : Player\n            A player object representing the current playback.", "docstring_tokens": ["Get", "information", "about", "the", "users", "current", "playback", "."], "sha": "bb296cac7c3dd289908906b7069bd80f43950515", "url": "https://github.com/mental32/spotify.py/blob/bb296cac7c3dd289908906b7069bd80f43950515/spotify/models/user.py#L174-L183", "partition": "test"}
{"repo": "LuminosoInsight/luminoso-api-client-python", "path": "luminoso_api/v4_json_stream.py", "func_name": "stream_json_lines", "original_string": "def stream_json_lines(file):\n    \"\"\"\n    Load a JSON stream and return a generator, yielding one object at a time.\n    \"\"\"\n    if isinstance(file, string_type):\n        file = open(file, 'rb')\n    for line in file:\n        line = line.strip()\n        if line:\n            if isinstance(line, bytes):\n                line = line.decode('utf-8')\n            yield json.loads(line)", "language": "python", "code": "def stream_json_lines(file):\n    \"\"\"\n    Load a JSON stream and return a generator, yielding one object at a time.\n    \"\"\"\n    if isinstance(file, string_type):\n        file = open(file, 'rb')\n    for line in file:\n        line = line.strip()\n        if line:\n            if isinstance(line, bytes):\n                line = line.decode('utf-8')\n            yield json.loads(line)", "code_tokens": ["def", "stream_json_lines", "(", "file", ")", ":", "if", "isinstance", "(", "file", ",", "string_type", ")", ":", "file", "=", "open", "(", "file", ",", "'rb'", ")", "for", "line", "in", "file", ":", "line", "=", "line", ".", "strip", "(", ")", "if", "line", ":", "if", "isinstance", "(", "line", ",", "bytes", ")", ":", "line", "=", "line", ".", "decode", "(", "'utf-8'", ")", "yield", "json", ".", "loads", "(", "line", ")"], "docstring": "Load a JSON stream and return a generator, yielding one object at a time.", "docstring_tokens": ["Load", "a", "JSON", "stream", "and", "return", "a", "generator", "yielding", "one", "object", "at", "a", "time", "."], "sha": "3bedf2a454aee39214c11fbf556ead3eecc27881", "url": "https://github.com/LuminosoInsight/luminoso-api-client-python/blob/3bedf2a454aee39214c11fbf556ead3eecc27881/luminoso_api/v4_json_stream.py#L175-L186", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/backend/numpy/linalg.py", "func_name": "_matmul", "original_string": "def _matmul(a, b,\n            transpose_a=False, transpose_b=False,\n            adjoint_a=False, adjoint_b=False,\n            a_is_sparse=False, b_is_sparse=False,\n            name=None):  # pylint: disable=unused-argument\n  \"\"\"Numpy matmul wrapper.\"\"\"\n  if a_is_sparse or b_is_sparse:\n    raise NotImplementedError('Numpy backend does not support sparse matmul.')\n  if transpose_a or adjoint_a:\n    a = _matrix_transpose(a, conjugate=adjoint_a)\n  if transpose_b or adjoint_b:\n    b = _matrix_transpose(b, conjugate=adjoint_b)\n  return np.matmul(a, b)", "language": "python", "code": "def _matmul(a, b,\n            transpose_a=False, transpose_b=False,\n            adjoint_a=False, adjoint_b=False,\n            a_is_sparse=False, b_is_sparse=False,\n            name=None):  # pylint: disable=unused-argument\n  \"\"\"Numpy matmul wrapper.\"\"\"\n  if a_is_sparse or b_is_sparse:\n    raise NotImplementedError('Numpy backend does not support sparse matmul.')\n  if transpose_a or adjoint_a:\n    a = _matrix_transpose(a, conjugate=adjoint_a)\n  if transpose_b or adjoint_b:\n    b = _matrix_transpose(b, conjugate=adjoint_b)\n  return np.matmul(a, b)", "code_tokens": ["def", "_matmul", "(", "a", ",", "b", ",", "transpose_a", "=", "False", ",", "transpose_b", "=", "False", ",", "adjoint_a", "=", "False", ",", "adjoint_b", "=", "False", ",", "a_is_sparse", "=", "False", ",", "b_is_sparse", "=", "False", ",", "name", "=", "None", ")", ":", "# pylint: disable=unused-argument", "if", "a_is_sparse", "or", "b_is_sparse", ":", "raise", "NotImplementedError", "(", "'Numpy backend does not support sparse matmul.'", ")", "if", "transpose_a", "or", "adjoint_a", ":", "a", "=", "_matrix_transpose", "(", "a", ",", "conjugate", "=", "adjoint_a", ")", "if", "transpose_b", "or", "adjoint_b", ":", "b", "=", "_matrix_transpose", "(", "b", ",", "conjugate", "=", "adjoint_b", ")", "return", "np", ".", "matmul", "(", "a", ",", "b", ")"], "docstring": "Numpy matmul wrapper.", "docstring_tokens": ["Numpy", "matmul", "wrapper", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/backend/numpy/linalg.py#L97-L109", "partition": "test"}
{"repo": "praekeltfoundation/seed-control-interface", "path": "ci/utils.py", "func_name": "get_last_value_from_timeseries", "original_string": "def get_last_value_from_timeseries(timeseries):\n    \"\"\"Gets the most recent non-zero value for a .last metric or zero\n    for empty data.\"\"\"\n    if not timeseries:\n        return 0\n    for metric, points in timeseries.items():\n        return next((p['y'] for p in reversed(points) if p['y'] > 0), 0)", "language": "python", "code": "def get_last_value_from_timeseries(timeseries):\n    \"\"\"Gets the most recent non-zero value for a .last metric or zero\n    for empty data.\"\"\"\n    if not timeseries:\n        return 0\n    for metric, points in timeseries.items():\n        return next((p['y'] for p in reversed(points) if p['y'] > 0), 0)", "code_tokens": ["def", "get_last_value_from_timeseries", "(", "timeseries", ")", ":", "if", "not", "timeseries", ":", "return", "0", "for", "metric", ",", "points", "in", "timeseries", ".", "items", "(", ")", ":", "return", "next", "(", "(", "p", "[", "'y'", "]", "for", "p", "in", "reversed", "(", "points", ")", "if", "p", "[", "'y'", "]", ">", "0", ")", ",", "0", ")"], "docstring": "Gets the most recent non-zero value for a .last metric or zero\n    for empty data.", "docstring_tokens": ["Gets", "the", "most", "recent", "non", "-", "zero", "value", "for", "a", ".", "last", "metric", "or", "zero", "for", "empty", "data", "."], "sha": "32ddad88b5bc2f8f4d80b848361899da2e081636", "url": "https://github.com/praekeltfoundation/seed-control-interface/blob/32ddad88b5bc2f8f4d80b848361899da2e081636/ci/utils.py#L32-L38", "partition": "test"}
{"repo": "Valuehorizon/valuehorizon-forex", "path": "forex/models.py", "func_name": "convert_currency", "original_string": "def convert_currency(from_symbol, to_symbol, value, date):\n    \"\"\"\n    Converts an amount of money from one currency to another on a specified date.\n    \"\"\"\n    if from_symbol == to_symbol:\n        return value\n\n    factor = conversion_factor(from_symbol, to_symbol, date)\n\n    if type(value) == float:\n        output = value * float(factor)\n    elif type(value) == Decimal:\n        output = Decimal(format(value * factor, '.%sf' % str(PRICE_PRECISION)))\n    elif type(value) in [np.float16, np.float32, np.float64, np.float128, np.float]:\n        output = float(value) * float(factor)\n    else:\n        output = None\n\n    return output", "language": "python", "code": "def convert_currency(from_symbol, to_symbol, value, date):\n    \"\"\"\n    Converts an amount of money from one currency to another on a specified date.\n    \"\"\"\n    if from_symbol == to_symbol:\n        return value\n\n    factor = conversion_factor(from_symbol, to_symbol, date)\n\n    if type(value) == float:\n        output = value * float(factor)\n    elif type(value) == Decimal:\n        output = Decimal(format(value * factor, '.%sf' % str(PRICE_PRECISION)))\n    elif type(value) in [np.float16, np.float32, np.float64, np.float128, np.float]:\n        output = float(value) * float(factor)\n    else:\n        output = None\n\n    return output", "code_tokens": ["def", "convert_currency", "(", "from_symbol", ",", "to_symbol", ",", "value", ",", "date", ")", ":", "if", "from_symbol", "==", "to_symbol", ":", "return", "value", "factor", "=", "conversion_factor", "(", "from_symbol", ",", "to_symbol", ",", "date", ")", "if", "type", "(", "value", ")", "==", "float", ":", "output", "=", "value", "*", "float", "(", "factor", ")", "elif", "type", "(", "value", ")", "==", "Decimal", ":", "output", "=", "Decimal", "(", "format", "(", "value", "*", "factor", ",", "'.%sf'", "%", "str", "(", "PRICE_PRECISION", ")", ")", ")", "elif", "type", "(", "value", ")", "in", "[", "np", ".", "float16", ",", "np", ".", "float32", ",", "np", ".", "float64", ",", "np", ".", "float128", ",", "np", ".", "float", "]", ":", "output", "=", "float", "(", "value", ")", "*", "float", "(", "factor", ")", "else", ":", "output", "=", "None", "return", "output"], "docstring": "Converts an amount of money from one currency to another on a specified date.", "docstring_tokens": ["Converts", "an", "amount", "of", "money", "from", "one", "currency", "to", "another", "on", "a", "specified", "date", "."], "sha": "e921379ae6c9d07ddad87a1fd3b5bb8fdfc74cb8", "url": "https://github.com/Valuehorizon/valuehorizon-forex/blob/e921379ae6c9d07ddad87a1fd3b5bb8fdfc74cb8/forex/models.py#L232-L250", "partition": "test"}
{"repo": "abau171/highfive", "path": "highfive/master.py", "func_name": "WorkerProtocol.line_received", "original_string": "def line_received(self, line):\n        \"\"\"\n        Called when a complete line is found from the remote worker. Decodes\n        a response object from the line, then passes it to the worker object.\n        \"\"\"\n\n        response = json.loads(line.decode(\"utf-8\"))\n        self._worker.response_received(response)", "language": "python", "code": "def line_received(self, line):\n        \"\"\"\n        Called when a complete line is found from the remote worker. Decodes\n        a response object from the line, then passes it to the worker object.\n        \"\"\"\n\n        response = json.loads(line.decode(\"utf-8\"))\n        self._worker.response_received(response)", "code_tokens": ["def", "line_received", "(", "self", ",", "line", ")", ":", "response", "=", "json", ".", "loads", "(", "line", ".", "decode", "(", "\"utf-8\"", ")", ")", "self", ".", "_worker", ".", "response_received", "(", "response", ")"], "docstring": "Called when a complete line is found from the remote worker. Decodes\n        a response object from the line, then passes it to the worker object.", "docstring_tokens": ["Called", "when", "a", "complete", "line", "is", "found", "from", "the", "remote", "worker", ".", "Decodes", "a", "response", "object", "from", "the", "line", "then", "passes", "it", "to", "the", "worker", "object", "."], "sha": "07b3829331072035ab100d1d66deca3e8f3f372a", "url": "https://github.com/abau171/highfive/blob/07b3829331072035ab100d1d66deca3e8f3f372a/highfive/master.py#L69-L76", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/stats/quantiles.py", "func_name": "find_bins", "original_string": "def find_bins(x,\n              edges,\n              extend_lower_interval=False,\n              extend_upper_interval=False,\n              dtype=None,\n              name=None):\n  \"\"\"Bin values into discrete intervals.\n\n  Given `edges = [c0, ..., cK]`, defining intervals\n  `I0 = [c0, c1)`, `I1 = [c1, c2)`, ..., `I_{K-1} = [c_{K-1}, cK]`,\n  This function returns `bins`, such that:\n  `edges[bins[i]] <= x[i] < edges[bins[i] + 1]`.\n\n  Args:\n    x:  Numeric `N-D` `Tensor` with `N > 0`.\n    edges:  `Tensor` of same `dtype` as `x`.  The first dimension indexes edges\n      of intervals.  Must either be `1-D` or have\n      `x.shape[1:] == edges.shape[1:]`.  If `rank(edges) > 1`, `edges[k]`\n      designates a shape `edges.shape[1:]` `Tensor` of bin edges for the\n      corresponding dimensions of `x`.\n    extend_lower_interval:  Python `bool`.  If `True`, extend the lowest\n      interval `I0` to `(-inf, c1]`.\n    extend_upper_interval:  Python `bool`.  If `True`, extend the upper\n      interval `I_{K-1}` to `[c_{K-1}, +inf)`.\n    dtype: The output type (`int32` or `int64`). `Default value:` `x.dtype`.\n      This effects the output values when `x` is below/above the intervals,\n      which will be `-1/K+1` for `int` types and `NaN` for `float`s.\n      At indices where `x` is `NaN`, the output values will be `0` for `int`\n      types and `NaN` for floats.\n    name:  A Python string name to prepend to created ops. Default: 'find_bins'\n\n  Returns:\n    bins: `Tensor` with same `shape` as `x` and `dtype`.\n      Has whole number values.  `bins[i] = k` means the `x[i]` falls into the\n      `kth` bin, ie, `edges[bins[i]] <= x[i] < edges[bins[i] + 1]`.\n\n  Raises:\n    ValueError:  If `edges.shape[0]` is determined to be less than 2.\n\n  #### Examples\n\n  Cut a `1-D` array\n\n  ```python\n  x = [0., 5., 6., 10., 20.]\n  edges = [0., 5., 10.]\n  tfp.stats.find_bins(x, edges)\n  ==> [0., 0., 1., 1., np.nan]\n  ```\n\n  Cut `x` into its deciles\n\n  ```python\n  x = tf.random_uniform(shape=(100, 200))\n  decile_edges = tfp.stats.quantiles(x, num_quantiles=10)\n  bins = tfp.stats.find_bins(x, edges=decile_edges)\n  bins.shape\n  ==> (100, 200)\n  tf.reduce_mean(bins == 0.)\n  ==> approximately 0.1\n  tf.reduce_mean(bins == 1.)\n  ==> approximately 0.1\n  ```\n\n  \"\"\"\n  # TFP users may be surprised to see the \"action\" in the leftmost dim of\n  # edges, rather than the rightmost (event) dim.  Why?\n  # 1. Most likely you created edges by getting quantiles over samples, and\n  #    quantile/percentile return these edges in the leftmost (sample) dim.\n  # 2. Say you have event_shape = [5], then we expect the bin will be different\n  #    for all 5 events, so the index of the bin should not be in the event dim.\n  with tf.compat.v1.name_scope(\n      name, default_name='find_bins', values=[x, edges]):\n    in_type = dtype_util.common_dtype([x, edges],\n                                      preferred_dtype=tf.float32)\n    edges = tf.convert_to_tensor(value=edges, name='edges', dtype=in_type)\n    x = tf.convert_to_tensor(value=x, name='x', dtype=in_type)\n\n    if (tf.compat.dimension_value(edges.shape[0]) is not None and\n        tf.compat.dimension_value(edges.shape[0]) < 2):\n      raise ValueError(\n          'First dimension of `edges` must have length > 1 to index 1 or '\n          'more bin. Found: {}'.format(edges.shape))\n\n    flattening_x = edges.shape.ndims == 1 and x.shape.ndims > 1\n\n    if flattening_x:\n      x_orig_shape = tf.shape(input=x)\n      x = tf.reshape(x, [-1])\n\n    if dtype is None:\n      dtype = in_type\n    dtype = tf.as_dtype(dtype)\n\n    # Move first dims into the rightmost.\n    x_permed = distribution_util.rotate_transpose(x, shift=-1)\n    edges_permed = distribution_util.rotate_transpose(edges, shift=-1)\n\n    # If...\n    #   x_permed = [0, 1, 6., 10]\n    #   edges = [0, 5, 10.]\n    #   ==> almost_output = [0, 1, 2, 2]\n    searchsorted_type = dtype if dtype in [tf.int32, tf.int64] else None\n    almost_output_permed = tf.searchsorted(\n        sorted_sequence=edges_permed,\n        values=x_permed,\n        side='right',\n        out_type=searchsorted_type)\n    # Move the rightmost dims back to the leftmost.\n    almost_output = tf.cast(\n        distribution_util.rotate_transpose(almost_output_permed, shift=1),\n        dtype)\n\n    # In above example, we want [0, 0, 1, 1], so correct this here.\n    bins = tf.clip_by_value(almost_output - 1, tf.cast(0, dtype),\n                            tf.cast(tf.shape(input=edges)[0] - 2, dtype))\n\n    if not extend_lower_interval:\n      low_fill = np.nan if dtype.is_floating else -1\n      bins = tf.where(x < tf.expand_dims(edges[0], 0),\n                      tf.fill(tf.shape(input=x), tf.cast(low_fill, dtype)),\n                      bins)\n\n    if not extend_upper_interval:\n      up_fill = np.nan if dtype.is_floating else tf.shape(input=edges)[0] - 1\n      bins = tf.where(x > tf.expand_dims(edges[-1], 0),\n                      tf.fill(tf.shape(input=x), tf.cast(up_fill, dtype)), bins)\n\n    if flattening_x:\n      bins = tf.reshape(bins, x_orig_shape)\n\n    return bins", "language": "python", "code": "def find_bins(x,\n              edges,\n              extend_lower_interval=False,\n              extend_upper_interval=False,\n              dtype=None,\n              name=None):\n  \"\"\"Bin values into discrete intervals.\n\n  Given `edges = [c0, ..., cK]`, defining intervals\n  `I0 = [c0, c1)`, `I1 = [c1, c2)`, ..., `I_{K-1} = [c_{K-1}, cK]`,\n  This function returns `bins`, such that:\n  `edges[bins[i]] <= x[i] < edges[bins[i] + 1]`.\n\n  Args:\n    x:  Numeric `N-D` `Tensor` with `N > 0`.\n    edges:  `Tensor` of same `dtype` as `x`.  The first dimension indexes edges\n      of intervals.  Must either be `1-D` or have\n      `x.shape[1:] == edges.shape[1:]`.  If `rank(edges) > 1`, `edges[k]`\n      designates a shape `edges.shape[1:]` `Tensor` of bin edges for the\n      corresponding dimensions of `x`.\n    extend_lower_interval:  Python `bool`.  If `True`, extend the lowest\n      interval `I0` to `(-inf, c1]`.\n    extend_upper_interval:  Python `bool`.  If `True`, extend the upper\n      interval `I_{K-1}` to `[c_{K-1}, +inf)`.\n    dtype: The output type (`int32` or `int64`). `Default value:` `x.dtype`.\n      This effects the output values when `x` is below/above the intervals,\n      which will be `-1/K+1` for `int` types and `NaN` for `float`s.\n      At indices where `x` is `NaN`, the output values will be `0` for `int`\n      types and `NaN` for floats.\n    name:  A Python string name to prepend to created ops. Default: 'find_bins'\n\n  Returns:\n    bins: `Tensor` with same `shape` as `x` and `dtype`.\n      Has whole number values.  `bins[i] = k` means the `x[i]` falls into the\n      `kth` bin, ie, `edges[bins[i]] <= x[i] < edges[bins[i] + 1]`.\n\n  Raises:\n    ValueError:  If `edges.shape[0]` is determined to be less than 2.\n\n  #### Examples\n\n  Cut a `1-D` array\n\n  ```python\n  x = [0., 5., 6., 10., 20.]\n  edges = [0., 5., 10.]\n  tfp.stats.find_bins(x, edges)\n  ==> [0., 0., 1., 1., np.nan]\n  ```\n\n  Cut `x` into its deciles\n\n  ```python\n  x = tf.random_uniform(shape=(100, 200))\n  decile_edges = tfp.stats.quantiles(x, num_quantiles=10)\n  bins = tfp.stats.find_bins(x, edges=decile_edges)\n  bins.shape\n  ==> (100, 200)\n  tf.reduce_mean(bins == 0.)\n  ==> approximately 0.1\n  tf.reduce_mean(bins == 1.)\n  ==> approximately 0.1\n  ```\n\n  \"\"\"\n  # TFP users may be surprised to see the \"action\" in the leftmost dim of\n  # edges, rather than the rightmost (event) dim.  Why?\n  # 1. Most likely you created edges by getting quantiles over samples, and\n  #    quantile/percentile return these edges in the leftmost (sample) dim.\n  # 2. Say you have event_shape = [5], then we expect the bin will be different\n  #    for all 5 events, so the index of the bin should not be in the event dim.\n  with tf.compat.v1.name_scope(\n      name, default_name='find_bins', values=[x, edges]):\n    in_type = dtype_util.common_dtype([x, edges],\n                                      preferred_dtype=tf.float32)\n    edges = tf.convert_to_tensor(value=edges, name='edges', dtype=in_type)\n    x = tf.convert_to_tensor(value=x, name='x', dtype=in_type)\n\n    if (tf.compat.dimension_value(edges.shape[0]) is not None and\n        tf.compat.dimension_value(edges.shape[0]) < 2):\n      raise ValueError(\n          'First dimension of `edges` must have length > 1 to index 1 or '\n          'more bin. Found: {}'.format(edges.shape))\n\n    flattening_x = edges.shape.ndims == 1 and x.shape.ndims > 1\n\n    if flattening_x:\n      x_orig_shape = tf.shape(input=x)\n      x = tf.reshape(x, [-1])\n\n    if dtype is None:\n      dtype = in_type\n    dtype = tf.as_dtype(dtype)\n\n    # Move first dims into the rightmost.\n    x_permed = distribution_util.rotate_transpose(x, shift=-1)\n    edges_permed = distribution_util.rotate_transpose(edges, shift=-1)\n\n    # If...\n    #   x_permed = [0, 1, 6., 10]\n    #   edges = [0, 5, 10.]\n    #   ==> almost_output = [0, 1, 2, 2]\n    searchsorted_type = dtype if dtype in [tf.int32, tf.int64] else None\n    almost_output_permed = tf.searchsorted(\n        sorted_sequence=edges_permed,\n        values=x_permed,\n        side='right',\n        out_type=searchsorted_type)\n    # Move the rightmost dims back to the leftmost.\n    almost_output = tf.cast(\n        distribution_util.rotate_transpose(almost_output_permed, shift=1),\n        dtype)\n\n    # In above example, we want [0, 0, 1, 1], so correct this here.\n    bins = tf.clip_by_value(almost_output - 1, tf.cast(0, dtype),\n                            tf.cast(tf.shape(input=edges)[0] - 2, dtype))\n\n    if not extend_lower_interval:\n      low_fill = np.nan if dtype.is_floating else -1\n      bins = tf.where(x < tf.expand_dims(edges[0], 0),\n                      tf.fill(tf.shape(input=x), tf.cast(low_fill, dtype)),\n                      bins)\n\n    if not extend_upper_interval:\n      up_fill = np.nan if dtype.is_floating else tf.shape(input=edges)[0] - 1\n      bins = tf.where(x > tf.expand_dims(edges[-1], 0),\n                      tf.fill(tf.shape(input=x), tf.cast(up_fill, dtype)), bins)\n\n    if flattening_x:\n      bins = tf.reshape(bins, x_orig_shape)\n\n    return bins", "code_tokens": ["def", "find_bins", "(", "x", ",", "edges", ",", "extend_lower_interval", "=", "False", ",", "extend_upper_interval", "=", "False", ",", "dtype", "=", "None", ",", "name", "=", "None", ")", ":", "# TFP users may be surprised to see the \"action\" in the leftmost dim of", "# edges, rather than the rightmost (event) dim.  Why?", "# 1. Most likely you created edges by getting quantiles over samples, and", "#    quantile/percentile return these edges in the leftmost (sample) dim.", "# 2. Say you have event_shape = [5], then we expect the bin will be different", "#    for all 5 events, so the index of the bin should not be in the event dim.", "with", "tf", ".", "compat", ".", "v1", ".", "name_scope", "(", "name", ",", "default_name", "=", "'find_bins'", ",", "values", "=", "[", "x", ",", "edges", "]", ")", ":", "in_type", "=", "dtype_util", ".", "common_dtype", "(", "[", "x", ",", "edges", "]", ",", "preferred_dtype", "=", "tf", ".", "float32", ")", "edges", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "edges", ",", "name", "=", "'edges'", ",", "dtype", "=", "in_type", ")", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "name", "=", "'x'", ",", "dtype", "=", "in_type", ")", "if", "(", "tf", ".", "compat", ".", "dimension_value", "(", "edges", ".", "shape", "[", "0", "]", ")", "is", "not", "None", "and", "tf", ".", "compat", ".", "dimension_value", "(", "edges", ".", "shape", "[", "0", "]", ")", "<", "2", ")", ":", "raise", "ValueError", "(", "'First dimension of `edges` must have length > 1 to index 1 or '", "'more bin. Found: {}'", ".", "format", "(", "edges", ".", "shape", ")", ")", "flattening_x", "=", "edges", ".", "shape", ".", "ndims", "==", "1", "and", "x", ".", "shape", ".", "ndims", ">", "1", "if", "flattening_x", ":", "x_orig_shape", "=", "tf", ".", "shape", "(", "input", "=", "x", ")", "x", "=", "tf", ".", "reshape", "(", "x", ",", "[", "-", "1", "]", ")", "if", "dtype", "is", "None", ":", "dtype", "=", "in_type", "dtype", "=", "tf", ".", "as_dtype", "(", "dtype", ")", "# Move first dims into the rightmost.", "x_permed", "=", "distribution_util", ".", "rotate_transpose", "(", "x", ",", "shift", "=", "-", "1", ")", "edges_permed", "=", "distribution_util", ".", "rotate_transpose", "(", "edges", ",", "shift", "=", "-", "1", ")", "# If...", "#   x_permed = [0, 1, 6., 10]", "#   edges = [0, 5, 10.]", "#   ==> almost_output = [0, 1, 2, 2]", "searchsorted_type", "=", "dtype", "if", "dtype", "in", "[", "tf", ".", "int32", ",", "tf", ".", "int64", "]", "else", "None", "almost_output_permed", "=", "tf", ".", "searchsorted", "(", "sorted_sequence", "=", "edges_permed", ",", "values", "=", "x_permed", ",", "side", "=", "'right'", ",", "out_type", "=", "searchsorted_type", ")", "# Move the rightmost dims back to the leftmost.", "almost_output", "=", "tf", ".", "cast", "(", "distribution_util", ".", "rotate_transpose", "(", "almost_output_permed", ",", "shift", "=", "1", ")", ",", "dtype", ")", "# In above example, we want [0, 0, 1, 1], so correct this here.", "bins", "=", "tf", ".", "clip_by_value", "(", "almost_output", "-", "1", ",", "tf", ".", "cast", "(", "0", ",", "dtype", ")", ",", "tf", ".", "cast", "(", "tf", ".", "shape", "(", "input", "=", "edges", ")", "[", "0", "]", "-", "2", ",", "dtype", ")", ")", "if", "not", "extend_lower_interval", ":", "low_fill", "=", "np", ".", "nan", "if", "dtype", ".", "is_floating", "else", "-", "1", "bins", "=", "tf", ".", "where", "(", "x", "<", "tf", ".", "expand_dims", "(", "edges", "[", "0", "]", ",", "0", ")", ",", "tf", ".", "fill", "(", "tf", ".", "shape", "(", "input", "=", "x", ")", ",", "tf", ".", "cast", "(", "low_fill", ",", "dtype", ")", ")", ",", "bins", ")", "if", "not", "extend_upper_interval", ":", "up_fill", "=", "np", ".", "nan", "if", "dtype", ".", "is_floating", "else", "tf", ".", "shape", "(", "input", "=", "edges", ")", "[", "0", "]", "-", "1", "bins", "=", "tf", ".", "where", "(", "x", ">", "tf", ".", "expand_dims", "(", "edges", "[", "-", "1", "]", ",", "0", ")", ",", "tf", ".", "fill", "(", "tf", ".", "shape", "(", "input", "=", "x", ")", ",", "tf", ".", "cast", "(", "up_fill", ",", "dtype", ")", ")", ",", "bins", ")", "if", "flattening_x", ":", "bins", "=", "tf", ".", "reshape", "(", "bins", ",", "x_orig_shape", ")", "return", "bins"], "docstring": "Bin values into discrete intervals.\n\n  Given `edges = [c0, ..., cK]`, defining intervals\n  `I0 = [c0, c1)`, `I1 = [c1, c2)`, ..., `I_{K-1} = [c_{K-1}, cK]`,\n  This function returns `bins`, such that:\n  `edges[bins[i]] <= x[i] < edges[bins[i] + 1]`.\n\n  Args:\n    x:  Numeric `N-D` `Tensor` with `N > 0`.\n    edges:  `Tensor` of same `dtype` as `x`.  The first dimension indexes edges\n      of intervals.  Must either be `1-D` or have\n      `x.shape[1:] == edges.shape[1:]`.  If `rank(edges) > 1`, `edges[k]`\n      designates a shape `edges.shape[1:]` `Tensor` of bin edges for the\n      corresponding dimensions of `x`.\n    extend_lower_interval:  Python `bool`.  If `True`, extend the lowest\n      interval `I0` to `(-inf, c1]`.\n    extend_upper_interval:  Python `bool`.  If `True`, extend the upper\n      interval `I_{K-1}` to `[c_{K-1}, +inf)`.\n    dtype: The output type (`int32` or `int64`). `Default value:` `x.dtype`.\n      This effects the output values when `x` is below/above the intervals,\n      which will be `-1/K+1` for `int` types and `NaN` for `float`s.\n      At indices where `x` is `NaN`, the output values will be `0` for `int`\n      types and `NaN` for floats.\n    name:  A Python string name to prepend to created ops. Default: 'find_bins'\n\n  Returns:\n    bins: `Tensor` with same `shape` as `x` and `dtype`.\n      Has whole number values.  `bins[i] = k` means the `x[i]` falls into the\n      `kth` bin, ie, `edges[bins[i]] <= x[i] < edges[bins[i] + 1]`.\n\n  Raises:\n    ValueError:  If `edges.shape[0]` is determined to be less than 2.\n\n  #### Examples\n\n  Cut a `1-D` array\n\n  ```python\n  x = [0., 5., 6., 10., 20.]\n  edges = [0., 5., 10.]\n  tfp.stats.find_bins(x, edges)\n  ==> [0., 0., 1., 1., np.nan]\n  ```\n\n  Cut `x` into its deciles\n\n  ```python\n  x = tf.random_uniform(shape=(100, 200))\n  decile_edges = tfp.stats.quantiles(x, num_quantiles=10)\n  bins = tfp.stats.find_bins(x, edges=decile_edges)\n  bins.shape\n  ==> (100, 200)\n  tf.reduce_mean(bins == 0.)\n  ==> approximately 0.1\n  tf.reduce_mean(bins == 1.)\n  ==> approximately 0.1\n  ```", "docstring_tokens": ["Bin", "values", "into", "discrete", "intervals", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/stats/quantiles.py#L158-L289", "partition": "test"}
{"repo": "funilrys/PyFunceble", "path": "PyFunceble/core.py", "func_name": "Core.url", "original_string": "def url(self, url_to_test=None, last_url=None):\n        \"\"\"\n        Manage the case that we want to test only a given url.\n\n        :param url_to_test: The url to test.\n        :type url_to_test: str\n\n        :param last_url:\n            The last url of the file we are testing\n            (if exist)\n        :type last_url: str\n        \"\"\"\n\n        # We print the header.\n        self._print_header()\n\n        if url_to_test:\n            # An url to test is given.\n\n            # We set the url we are going to test.\n            PyFunceble.INTERN[\"to_test\"] = url_to_test\n        else:\n            # An URL to test is not given.\n\n            # We set the url we are going to test to None.\n            PyFunceble.INTERN[\"to_test\"] = None\n\n        if PyFunceble.INTERN[\"to_test\"]:\n            # An URL to test is given.\n\n            if PyFunceble.CONFIGURATION[\"syntax\"]:\n                # The syntax mode is activated.\n\n                # We get the status from Syntax.\n                status = self.syntax_status.get()\n            else:\n                # The syntax mode is not activated.\n\n                # We get the status from URL.\n                status = self.url_status.get()\n\n            # We run the file decision logic.\n            self._file_decision(PyFunceble.INTERN[\"to_test\"], last_url, status)\n\n            if PyFunceble.CONFIGURATION[\"simple\"]:\n                # The simple mode is activated.\n\n                # We print the URL informations.\n                print(PyFunceble.INTERN[\"to_test\"], status)\n\n            # We return the URL we tested and its status.\n            return PyFunceble.INTERN[\"to_test\"], status\n\n        # We return None, there is nothing to test.\n        return None", "language": "python", "code": "def url(self, url_to_test=None, last_url=None):\n        \"\"\"\n        Manage the case that we want to test only a given url.\n\n        :param url_to_test: The url to test.\n        :type url_to_test: str\n\n        :param last_url:\n            The last url of the file we are testing\n            (if exist)\n        :type last_url: str\n        \"\"\"\n\n        # We print the header.\n        self._print_header()\n\n        if url_to_test:\n            # An url to test is given.\n\n            # We set the url we are going to test.\n            PyFunceble.INTERN[\"to_test\"] = url_to_test\n        else:\n            # An URL to test is not given.\n\n            # We set the url we are going to test to None.\n            PyFunceble.INTERN[\"to_test\"] = None\n\n        if PyFunceble.INTERN[\"to_test\"]:\n            # An URL to test is given.\n\n            if PyFunceble.CONFIGURATION[\"syntax\"]:\n                # The syntax mode is activated.\n\n                # We get the status from Syntax.\n                status = self.syntax_status.get()\n            else:\n                # The syntax mode is not activated.\n\n                # We get the status from URL.\n                status = self.url_status.get()\n\n            # We run the file decision logic.\n            self._file_decision(PyFunceble.INTERN[\"to_test\"], last_url, status)\n\n            if PyFunceble.CONFIGURATION[\"simple\"]:\n                # The simple mode is activated.\n\n                # We print the URL informations.\n                print(PyFunceble.INTERN[\"to_test\"], status)\n\n            # We return the URL we tested and its status.\n            return PyFunceble.INTERN[\"to_test\"], status\n\n        # We return None, there is nothing to test.\n        return None", "code_tokens": ["def", "url", "(", "self", ",", "url_to_test", "=", "None", ",", "last_url", "=", "None", ")", ":", "# We print the header.", "self", ".", "_print_header", "(", ")", "if", "url_to_test", ":", "# An url to test is given.", "# We set the url we are going to test.", "PyFunceble", ".", "INTERN", "[", "\"to_test\"", "]", "=", "url_to_test", "else", ":", "# An URL to test is not given.", "# We set the url we are going to test to None.", "PyFunceble", ".", "INTERN", "[", "\"to_test\"", "]", "=", "None", "if", "PyFunceble", ".", "INTERN", "[", "\"to_test\"", "]", ":", "# An URL to test is given.", "if", "PyFunceble", ".", "CONFIGURATION", "[", "\"syntax\"", "]", ":", "# The syntax mode is activated.", "# We get the status from Syntax.", "status", "=", "self", ".", "syntax_status", ".", "get", "(", ")", "else", ":", "# The syntax mode is not activated.", "# We get the status from URL.", "status", "=", "self", ".", "url_status", ".", "get", "(", ")", "# We run the file decision logic.", "self", ".", "_file_decision", "(", "PyFunceble", ".", "INTERN", "[", "\"to_test\"", "]", ",", "last_url", ",", "status", ")", "if", "PyFunceble", ".", "CONFIGURATION", "[", "\"simple\"", "]", ":", "# The simple mode is activated.", "# We print the URL informations.", "print", "(", "PyFunceble", ".", "INTERN", "[", "\"to_test\"", "]", ",", "status", ")", "# We return the URL we tested and its status.", "return", "PyFunceble", ".", "INTERN", "[", "\"to_test\"", "]", ",", "status", "# We return None, there is nothing to test.", "return", "None"], "docstring": "Manage the case that we want to test only a given url.\n\n        :param url_to_test: The url to test.\n        :type url_to_test: str\n\n        :param last_url:\n            The last url of the file we are testing\n            (if exist)\n        :type last_url: str", "docstring_tokens": ["Manage", "the", "case", "that", "we", "want", "to", "test", "only", "a", "given", "url", "."], "sha": "cdf69cbde120199171f7158e1c33635753e6e2f5", "url": "https://github.com/funilrys/PyFunceble/blob/cdf69cbde120199171f7158e1c33635753e6e2f5/PyFunceble/core.py#L775-L829", "partition": "test"}
{"repo": "Yelp/py_zipkin", "path": "py_zipkin/thrift/__init__.py", "func_name": "create_binary_annotation", "original_string": "def create_binary_annotation(key, value, annotation_type, host):\n    \"\"\"\n    Create a zipkin binary annotation object\n\n    :param key: name of the annotation, such as 'http.uri'\n    :param value: value of the annotation, such as a URI\n    :param annotation_type: type of annotation, such as AnnotationType.I32\n    :param host: zipkin endpoint object\n\n    :returns: zipkin binary annotation object\n    \"\"\"\n    return zipkin_core.BinaryAnnotation(\n        key=key,\n        value=value,\n        annotation_type=annotation_type,\n        host=host,\n    )", "language": "python", "code": "def create_binary_annotation(key, value, annotation_type, host):\n    \"\"\"\n    Create a zipkin binary annotation object\n\n    :param key: name of the annotation, such as 'http.uri'\n    :param value: value of the annotation, such as a URI\n    :param annotation_type: type of annotation, such as AnnotationType.I32\n    :param host: zipkin endpoint object\n\n    :returns: zipkin binary annotation object\n    \"\"\"\n    return zipkin_core.BinaryAnnotation(\n        key=key,\n        value=value,\n        annotation_type=annotation_type,\n        host=host,\n    )", "code_tokens": ["def", "create_binary_annotation", "(", "key", ",", "value", ",", "annotation_type", ",", "host", ")", ":", "return", "zipkin_core", ".", "BinaryAnnotation", "(", "key", "=", "key", ",", "value", "=", "value", ",", "annotation_type", "=", "annotation_type", ",", "host", "=", "host", ",", ")"], "docstring": "Create a zipkin binary annotation object\n\n    :param key: name of the annotation, such as 'http.uri'\n    :param value: value of the annotation, such as a URI\n    :param annotation_type: type of annotation, such as AnnotationType.I32\n    :param host: zipkin endpoint object\n\n    :returns: zipkin binary annotation object", "docstring_tokens": ["Create", "a", "zipkin", "binary", "annotation", "object"], "sha": "0944d9a3fb1f1798dbb276694aeed99f2b4283ba", "url": "https://github.com/Yelp/py_zipkin/blob/0944d9a3fb1f1798dbb276694aeed99f2b4283ba/py_zipkin/thrift/__init__.py#L37-L53", "partition": "test"}
{"repo": "gunyarakun/python-shogi", "path": "shogi/__init__.py", "func_name": "Board.reset", "original_string": "def reset(self):\n        '''Restores the starting position.'''\n        self.piece_bb = [\n                BB_VOID,                       # NONE\n                BB_RANK_C | BB_RANK_G,         # PAWN\n                BB_A1 | BB_I1 | BB_A9 | BB_I9, # LANCE\n                BB_A2 | BB_A8 | BB_I2 | BB_I8, # KNIGHT\n                BB_A3 | BB_A7 | BB_I3 | BB_I7, # SILVER\n                BB_A4 | BB_A6 | BB_I4 | BB_I6, # GOLD\n                BB_B2 | BB_H8,                 # BISHOP\n                BB_B8 | BB_H2,                 # ROOK\n                BB_A5 | BB_I5,                 # KING\n                BB_VOID,                       # PROM_PAWN\n                BB_VOID,                       # PROM_LANCE\n                BB_VOID,                       # PROM_KNIGHT\n                BB_VOID,                       # PROM_SILVER\n                BB_VOID,                       # PROM_BISHOP\n                BB_VOID,                       # PROM_ROOK\n        ]\n\n        self.pieces_in_hand = [collections.Counter(), collections.Counter()]\n\n        self.occupied = Occupied(BB_RANK_G | BB_H2 | BB_H8 | BB_RANK_I, BB_RANK_A | BB_B2 | BB_B8 | BB_RANK_C)\n\n        self.king_squares = [I5, A5]\n        self.pieces = [NONE for i in SQUARES]\n\n        for i in SQUARES:\n            mask = BB_SQUARES[i]\n            for piece_type in PIECE_TYPES:\n                if mask & self.piece_bb[piece_type]:\n                    self.pieces[i] = piece_type\n\n        self.turn = BLACK\n        self.move_number = 1\n        self.captured_piece_stack = collections.deque()\n        self.move_stack = collections.deque()\n        self.incremental_zobrist_hash = self.board_zobrist_hash(DEFAULT_RANDOM_ARRAY)\n        self.transpositions = collections.Counter((self.zobrist_hash(), ))", "language": "python", "code": "def reset(self):\n        '''Restores the starting position.'''\n        self.piece_bb = [\n                BB_VOID,                       # NONE\n                BB_RANK_C | BB_RANK_G,         # PAWN\n                BB_A1 | BB_I1 | BB_A9 | BB_I9, # LANCE\n                BB_A2 | BB_A8 | BB_I2 | BB_I8, # KNIGHT\n                BB_A3 | BB_A7 | BB_I3 | BB_I7, # SILVER\n                BB_A4 | BB_A6 | BB_I4 | BB_I6, # GOLD\n                BB_B2 | BB_H8,                 # BISHOP\n                BB_B8 | BB_H2,                 # ROOK\n                BB_A5 | BB_I5,                 # KING\n                BB_VOID,                       # PROM_PAWN\n                BB_VOID,                       # PROM_LANCE\n                BB_VOID,                       # PROM_KNIGHT\n                BB_VOID,                       # PROM_SILVER\n                BB_VOID,                       # PROM_BISHOP\n                BB_VOID,                       # PROM_ROOK\n        ]\n\n        self.pieces_in_hand = [collections.Counter(), collections.Counter()]\n\n        self.occupied = Occupied(BB_RANK_G | BB_H2 | BB_H8 | BB_RANK_I, BB_RANK_A | BB_B2 | BB_B8 | BB_RANK_C)\n\n        self.king_squares = [I5, A5]\n        self.pieces = [NONE for i in SQUARES]\n\n        for i in SQUARES:\n            mask = BB_SQUARES[i]\n            for piece_type in PIECE_TYPES:\n                if mask & self.piece_bb[piece_type]:\n                    self.pieces[i] = piece_type\n\n        self.turn = BLACK\n        self.move_number = 1\n        self.captured_piece_stack = collections.deque()\n        self.move_stack = collections.deque()\n        self.incremental_zobrist_hash = self.board_zobrist_hash(DEFAULT_RANDOM_ARRAY)\n        self.transpositions = collections.Counter((self.zobrist_hash(), ))", "code_tokens": ["def", "reset", "(", "self", ")", ":", "self", ".", "piece_bb", "=", "[", "BB_VOID", ",", "# NONE", "BB_RANK_C", "|", "BB_RANK_G", ",", "# PAWN", "BB_A1", "|", "BB_I1", "|", "BB_A9", "|", "BB_I9", ",", "# LANCE", "BB_A2", "|", "BB_A8", "|", "BB_I2", "|", "BB_I8", ",", "# KNIGHT", "BB_A3", "|", "BB_A7", "|", "BB_I3", "|", "BB_I7", ",", "# SILVER", "BB_A4", "|", "BB_A6", "|", "BB_I4", "|", "BB_I6", ",", "# GOLD", "BB_B2", "|", "BB_H8", ",", "# BISHOP", "BB_B8", "|", "BB_H2", ",", "# ROOK", "BB_A5", "|", "BB_I5", ",", "# KING", "BB_VOID", ",", "# PROM_PAWN", "BB_VOID", ",", "# PROM_LANCE", "BB_VOID", ",", "# PROM_KNIGHT", "BB_VOID", ",", "# PROM_SILVER", "BB_VOID", ",", "# PROM_BISHOP", "BB_VOID", ",", "# PROM_ROOK", "]", "self", ".", "pieces_in_hand", "=", "[", "collections", ".", "Counter", "(", ")", ",", "collections", ".", "Counter", "(", ")", "]", "self", ".", "occupied", "=", "Occupied", "(", "BB_RANK_G", "|", "BB_H2", "|", "BB_H8", "|", "BB_RANK_I", ",", "BB_RANK_A", "|", "BB_B2", "|", "BB_B8", "|", "BB_RANK_C", ")", "self", ".", "king_squares", "=", "[", "I5", ",", "A5", "]", "self", ".", "pieces", "=", "[", "NONE", "for", "i", "in", "SQUARES", "]", "for", "i", "in", "SQUARES", ":", "mask", "=", "BB_SQUARES", "[", "i", "]", "for", "piece_type", "in", "PIECE_TYPES", ":", "if", "mask", "&", "self", ".", "piece_bb", "[", "piece_type", "]", ":", "self", ".", "pieces", "[", "i", "]", "=", "piece_type", "self", ".", "turn", "=", "BLACK", "self", ".", "move_number", "=", "1", "self", ".", "captured_piece_stack", "=", "collections", ".", "deque", "(", ")", "self", ".", "move_stack", "=", "collections", ".", "deque", "(", ")", "self", ".", "incremental_zobrist_hash", "=", "self", ".", "board_zobrist_hash", "(", "DEFAULT_RANDOM_ARRAY", ")", "self", ".", "transpositions", "=", "collections", ".", "Counter", "(", "(", "self", ".", "zobrist_hash", "(", ")", ",", ")", ")"], "docstring": "Restores the starting position.", "docstring_tokens": ["Restores", "the", "starting", "position", "."], "sha": "137fe5f5e72251e8a97a1dba4a9b44b7c3c79914", "url": "https://github.com/gunyarakun/python-shogi/blob/137fe5f5e72251e8a97a1dba4a9b44b7c3c79914/shogi/__init__.py#L526-L564", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/hidden_markov_model.py", "func_name": "HiddenMarkovModel._marginal_hidden_probs", "original_string": "def _marginal_hidden_probs(self):\n    \"\"\"Compute marginal pdf for each individual observable.\"\"\"\n\n    initial_log_probs = tf.broadcast_to(self._log_init,\n                                        tf.concat([self.batch_shape_tensor(),\n                                                   [self._num_states]],\n                                                  axis=0))\n    # initial_log_probs :: batch_shape num_states\n\n    if self._num_steps > 1:\n      transition_log_probs = self._log_trans\n\n      def forward_step(log_probs, _):\n        return _log_vector_matrix(log_probs, transition_log_probs)\n\n      dummy_index = tf.zeros(self._num_steps - 1, dtype=tf.float32)\n\n      forward_log_probs = tf.scan(forward_step, dummy_index,\n                                  initializer=initial_log_probs,\n                                  name=\"forward_log_probs\")\n\n      forward_log_probs = tf.concat([[initial_log_probs], forward_log_probs],\n                                    axis=0)\n    else:\n      forward_log_probs = initial_log_probs[tf.newaxis, ...]\n\n    # returns :: num_steps batch_shape num_states\n\n    return tf.exp(forward_log_probs)", "language": "python", "code": "def _marginal_hidden_probs(self):\n    \"\"\"Compute marginal pdf for each individual observable.\"\"\"\n\n    initial_log_probs = tf.broadcast_to(self._log_init,\n                                        tf.concat([self.batch_shape_tensor(),\n                                                   [self._num_states]],\n                                                  axis=0))\n    # initial_log_probs :: batch_shape num_states\n\n    if self._num_steps > 1:\n      transition_log_probs = self._log_trans\n\n      def forward_step(log_probs, _):\n        return _log_vector_matrix(log_probs, transition_log_probs)\n\n      dummy_index = tf.zeros(self._num_steps - 1, dtype=tf.float32)\n\n      forward_log_probs = tf.scan(forward_step, dummy_index,\n                                  initializer=initial_log_probs,\n                                  name=\"forward_log_probs\")\n\n      forward_log_probs = tf.concat([[initial_log_probs], forward_log_probs],\n                                    axis=0)\n    else:\n      forward_log_probs = initial_log_probs[tf.newaxis, ...]\n\n    # returns :: num_steps batch_shape num_states\n\n    return tf.exp(forward_log_probs)", "code_tokens": ["def", "_marginal_hidden_probs", "(", "self", ")", ":", "initial_log_probs", "=", "tf", ".", "broadcast_to", "(", "self", ".", "_log_init", ",", "tf", ".", "concat", "(", "[", "self", ".", "batch_shape_tensor", "(", ")", ",", "[", "self", ".", "_num_states", "]", "]", ",", "axis", "=", "0", ")", ")", "# initial_log_probs :: batch_shape num_states", "if", "self", ".", "_num_steps", ">", "1", ":", "transition_log_probs", "=", "self", ".", "_log_trans", "def", "forward_step", "(", "log_probs", ",", "_", ")", ":", "return", "_log_vector_matrix", "(", "log_probs", ",", "transition_log_probs", ")", "dummy_index", "=", "tf", ".", "zeros", "(", "self", ".", "_num_steps", "-", "1", ",", "dtype", "=", "tf", ".", "float32", ")", "forward_log_probs", "=", "tf", ".", "scan", "(", "forward_step", ",", "dummy_index", ",", "initializer", "=", "initial_log_probs", ",", "name", "=", "\"forward_log_probs\"", ")", "forward_log_probs", "=", "tf", ".", "concat", "(", "[", "[", "initial_log_probs", "]", ",", "forward_log_probs", "]", ",", "axis", "=", "0", ")", "else", ":", "forward_log_probs", "=", "initial_log_probs", "[", "tf", ".", "newaxis", ",", "...", "]", "# returns :: num_steps batch_shape num_states", "return", "tf", ".", "exp", "(", "forward_log_probs", ")"], "docstring": "Compute marginal pdf for each individual observable.", "docstring_tokens": ["Compute", "marginal", "pdf", "for", "each", "individual", "observable", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/hidden_markov_model.py#L489-L517", "partition": "test"}
{"repo": "kgiusti/pyngus", "path": "pyngus/connection.py", "func_name": "Connection._not_reentrant", "original_string": "def _not_reentrant(func):\n        \"\"\"Decorator that prevents callbacks from calling into methods that are\n        not reentrant\n        \"\"\"\n        def wrap(self, *args, **kws):\n            if self._callback_lock and self._callback_lock.in_callback:\n                m = \"Connection %s cannot be invoked from a callback!\" % func\n                raise RuntimeError(m)\n            return func(self, *args, **kws)\n        return wrap", "language": "python", "code": "def _not_reentrant(func):\n        \"\"\"Decorator that prevents callbacks from calling into methods that are\n        not reentrant\n        \"\"\"\n        def wrap(self, *args, **kws):\n            if self._callback_lock and self._callback_lock.in_callback:\n                m = \"Connection %s cannot be invoked from a callback!\" % func\n                raise RuntimeError(m)\n            return func(self, *args, **kws)\n        return wrap", "code_tokens": ["def", "_not_reentrant", "(", "func", ")", ":", "def", "wrap", "(", "self", ",", "*", "args", ",", "*", "*", "kws", ")", ":", "if", "self", ".", "_callback_lock", "and", "self", ".", "_callback_lock", ".", "in_callback", ":", "m", "=", "\"Connection %s cannot be invoked from a callback!\"", "%", "func", "raise", "RuntimeError", "(", "m", ")", "return", "func", "(", "self", ",", "*", "args", ",", "*", "*", "kws", ")", "return", "wrap"], "docstring": "Decorator that prevents callbacks from calling into methods that are\n        not reentrant", "docstring_tokens": ["Decorator", "that", "prevents", "callbacks", "from", "calling", "into", "methods", "that", "are", "not", "reentrant"], "sha": "5392392046989f1bb84ba938c30e4d48311075f1", "url": "https://github.com/kgiusti/pyngus/blob/5392392046989f1bb84ba938c30e4d48311075f1/pyngus/connection.py#L122-L131", "partition": "test"}
{"repo": "nkgilley/python-ecobee-api", "path": "pyecobee/__init__.py", "func_name": "Ecobee.request_pin", "original_string": "def request_pin(self):\n        ''' Method to request a PIN from ecobee for authorization '''\n        url = 'https://api.ecobee.com/authorize'\n        params = {'response_type': 'ecobeePin',\n                  'client_id': self.api_key, 'scope': 'smartWrite'}\n        try:\n            request = requests.get(url, params=params)\n        except RequestException:\n            logger.warn(\"Error connecting to Ecobee.  Possible connectivity outage.\"\n                        \"Could not request pin.\")\n            return\n        self.authorization_code = request.json()['code']\n        self.pin = request.json()['ecobeePin']\n        logger.error('Please authorize your ecobee developer app with PIN code '\n              + self.pin + '\\nGoto https://www.ecobee.com/consumerportal'\n              '/index.html, click\\nMy Apps, Add application, Enter Pin'\n              ' and click Authorize.\\nAfter authorizing, call request_'\n              'tokens() method.')", "language": "python", "code": "def request_pin(self):\n        ''' Method to request a PIN from ecobee for authorization '''\n        url = 'https://api.ecobee.com/authorize'\n        params = {'response_type': 'ecobeePin',\n                  'client_id': self.api_key, 'scope': 'smartWrite'}\n        try:\n            request = requests.get(url, params=params)\n        except RequestException:\n            logger.warn(\"Error connecting to Ecobee.  Possible connectivity outage.\"\n                        \"Could not request pin.\")\n            return\n        self.authorization_code = request.json()['code']\n        self.pin = request.json()['ecobeePin']\n        logger.error('Please authorize your ecobee developer app with PIN code '\n              + self.pin + '\\nGoto https://www.ecobee.com/consumerportal'\n              '/index.html, click\\nMy Apps, Add application, Enter Pin'\n              ' and click Authorize.\\nAfter authorizing, call request_'\n              'tokens() method.')", "code_tokens": ["def", "request_pin", "(", "self", ")", ":", "url", "=", "'https://api.ecobee.com/authorize'", "params", "=", "{", "'response_type'", ":", "'ecobeePin'", ",", "'client_id'", ":", "self", ".", "api_key", ",", "'scope'", ":", "'smartWrite'", "}", "try", ":", "request", "=", "requests", ".", "get", "(", "url", ",", "params", "=", "params", ")", "except", "RequestException", ":", "logger", ".", "warn", "(", "\"Error connecting to Ecobee.  Possible connectivity outage.\"", "\"Could not request pin.\"", ")", "return", "self", ".", "authorization_code", "=", "request", ".", "json", "(", ")", "[", "'code'", "]", "self", ".", "pin", "=", "request", ".", "json", "(", ")", "[", "'ecobeePin'", "]", "logger", ".", "error", "(", "'Please authorize your ecobee developer app with PIN code '", "+", "self", ".", "pin", "+", "'\\nGoto https://www.ecobee.com/consumerportal'", "'/index.html, click\\nMy Apps, Add application, Enter Pin'", "' and click Authorize.\\nAfter authorizing, call request_'", "'tokens() method.'", ")"], "docstring": "Method to request a PIN from ecobee for authorization", "docstring_tokens": ["Method", "to", "request", "a", "PIN", "from", "ecobee", "for", "authorization"], "sha": "cc8d90d20abcb9ef5b66ec9cb035bae2f06ba174", "url": "https://github.com/nkgilley/python-ecobee-api/blob/cc8d90d20abcb9ef5b66ec9cb035bae2f06ba174/pyecobee/__init__.py#L77-L94", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/dagcircuit/dagcircuit.py", "func_name": "DAGCircuit.twoQ_gates", "original_string": "def twoQ_gates(self):\n        \"\"\"Get list of 2-qubit gates. Ignore snapshot, barriers, and the like.\"\"\"\n        two_q_gates = []\n        for node in self.gate_nodes():\n            if len(node.qargs) == 2:\n                two_q_gates.append(node)\n        return two_q_gates", "language": "python", "code": "def twoQ_gates(self):\n        \"\"\"Get list of 2-qubit gates. Ignore snapshot, barriers, and the like.\"\"\"\n        two_q_gates = []\n        for node in self.gate_nodes():\n            if len(node.qargs) == 2:\n                two_q_gates.append(node)\n        return two_q_gates", "code_tokens": ["def", "twoQ_gates", "(", "self", ")", ":", "two_q_gates", "=", "[", "]", "for", "node", "in", "self", ".", "gate_nodes", "(", ")", ":", "if", "len", "(", "node", ".", "qargs", ")", "==", "2", ":", "two_q_gates", ".", "append", "(", "node", ")", "return", "two_q_gates"], "docstring": "Get list of 2-qubit gates. Ignore snapshot, barriers, and the like.", "docstring_tokens": ["Get", "list", "of", "2", "-", "qubit", "gates", ".", "Ignore", "snapshot", "barriers", "and", "the", "like", "."], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/dagcircuit/dagcircuit.py#L1042-L1048", "partition": "test"}
{"repo": "lmjohns3/theanets", "path": "theanets/graph.py", "func_name": "Network.find", "original_string": "def find(self, which, param):\n        '''Get a parameter from a layer in the network.\n\n        Parameters\n        ----------\n        which : int or str\n            The layer that owns the parameter to return.\n\n            If this is an integer, then 0 refers to the input layer, 1 refers\n            to the first hidden layer, 2 to the second, and so on.\n\n            If this is a string, the layer with the corresponding name, if any,\n            will be used.\n\n        param : int or str\n            Name of the parameter to retrieve from the specified layer, or its\n            index in the parameter list of the layer.\n\n        Raises\n        ------\n        KeyError\n            If there is no such layer, or if there is no such parameter in the\n            specified layer.\n\n        Returns\n        -------\n        param : Theano shared variable\n            A shared parameter variable from the indicated layer.\n        '''\n        for i, layer in enumerate(self.layers):\n            if which == i or which == layer.name:\n                return layer.find(param)\n        raise KeyError(which)", "language": "python", "code": "def find(self, which, param):\n        '''Get a parameter from a layer in the network.\n\n        Parameters\n        ----------\n        which : int or str\n            The layer that owns the parameter to return.\n\n            If this is an integer, then 0 refers to the input layer, 1 refers\n            to the first hidden layer, 2 to the second, and so on.\n\n            If this is a string, the layer with the corresponding name, if any,\n            will be used.\n\n        param : int or str\n            Name of the parameter to retrieve from the specified layer, or its\n            index in the parameter list of the layer.\n\n        Raises\n        ------\n        KeyError\n            If there is no such layer, or if there is no such parameter in the\n            specified layer.\n\n        Returns\n        -------\n        param : Theano shared variable\n            A shared parameter variable from the indicated layer.\n        '''\n        for i, layer in enumerate(self.layers):\n            if which == i or which == layer.name:\n                return layer.find(param)\n        raise KeyError(which)", "code_tokens": ["def", "find", "(", "self", ",", "which", ",", "param", ")", ":", "for", "i", ",", "layer", "in", "enumerate", "(", "self", ".", "layers", ")", ":", "if", "which", "==", "i", "or", "which", "==", "layer", ".", "name", ":", "return", "layer", ".", "find", "(", "param", ")", "raise", "KeyError", "(", "which", ")"], "docstring": "Get a parameter from a layer in the network.\n\n        Parameters\n        ----------\n        which : int or str\n            The layer that owns the parameter to return.\n\n            If this is an integer, then 0 refers to the input layer, 1 refers\n            to the first hidden layer, 2 to the second, and so on.\n\n            If this is a string, the layer with the corresponding name, if any,\n            will be used.\n\n        param : int or str\n            Name of the parameter to retrieve from the specified layer, or its\n            index in the parameter list of the layer.\n\n        Raises\n        ------\n        KeyError\n            If there is no such layer, or if there is no such parameter in the\n            specified layer.\n\n        Returns\n        -------\n        param : Theano shared variable\n            A shared parameter variable from the indicated layer.", "docstring_tokens": ["Get", "a", "parameter", "from", "a", "layer", "in", "the", "network", "."], "sha": "79db9f878ef2071f2f576a1cf5d43a752a55894a", "url": "https://github.com/lmjohns3/theanets/blob/79db9f878ef2071f2f576a1cf5d43a752a55894a/theanets/graph.py#L467-L499", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "setup.py", "func_name": "MyInstallLib.run", "original_string": "def run(self):\n        \"\"\"overridden from install_lib class\"\"\"\n        install_lib.install_lib.run(self)\n        # manually install included directories if any\n        if include_dirs:\n            for directory in include_dirs:\n                dest = join(self.install_dir, directory)\n                if sys.version_info >= (3, 0):\n                    exclude = {\"invalid_encoded_data*\", \"unknown_encoding*\"}\n                else:\n                    exclude = set()\n                shutil.rmtree(dest, ignore_errors=True)\n                shutil.copytree(\n                    directory, dest, ignore=shutil.ignore_patterns(*exclude)\n                )", "language": "python", "code": "def run(self):\n        \"\"\"overridden from install_lib class\"\"\"\n        install_lib.install_lib.run(self)\n        # manually install included directories if any\n        if include_dirs:\n            for directory in include_dirs:\n                dest = join(self.install_dir, directory)\n                if sys.version_info >= (3, 0):\n                    exclude = {\"invalid_encoded_data*\", \"unknown_encoding*\"}\n                else:\n                    exclude = set()\n                shutil.rmtree(dest, ignore_errors=True)\n                shutil.copytree(\n                    directory, dest, ignore=shutil.ignore_patterns(*exclude)\n                )", "code_tokens": ["def", "run", "(", "self", ")", ":", "install_lib", ".", "install_lib", ".", "run", "(", "self", ")", "# manually install included directories if any", "if", "include_dirs", ":", "for", "directory", "in", "include_dirs", ":", "dest", "=", "join", "(", "self", ".", "install_dir", ",", "directory", ")", "if", "sys", ".", "version_info", ">=", "(", "3", ",", "0", ")", ":", "exclude", "=", "{", "\"invalid_encoded_data*\"", ",", "\"unknown_encoding*\"", "}", "else", ":", "exclude", "=", "set", "(", ")", "shutil", ".", "rmtree", "(", "dest", ",", "ignore_errors", "=", "True", ")", "shutil", ".", "copytree", "(", "directory", ",", "dest", ",", "ignore", "=", "shutil", ".", "ignore_patterns", "(", "*", "exclude", ")", ")"], "docstring": "overridden from install_lib class", "docstring_tokens": ["overridden", "from", "install_lib", "class"], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/setup.py#L107-L121", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/core/page.py", "func_name": "page", "original_string": "def page(strng, start=0, screen_lines=0, pager_cmd=None):\n    \"\"\"Print a string, piping through a pager after a certain length.\n\n    The screen_lines parameter specifies the number of *usable* lines of your\n    terminal screen (total lines minus lines you need to reserve to show other\n    information).\n\n    If you set screen_lines to a number <=0, page() will try to auto-determine\n    your screen size and will only use up to (screen_size+screen_lines) for\n    printing, paging after that. That is, if you want auto-detection but need\n    to reserve the bottom 3 lines of the screen, use screen_lines = -3, and for\n    auto-detection without any lines reserved simply use screen_lines = 0.\n\n    If a string won't fit in the allowed lines, it is sent through the\n    specified pager command. If none given, look for PAGER in the environment,\n    and ultimately default to less.\n\n    If no system pager works, the string is sent through a 'dumb pager'\n    written in python, very simplistic.\n    \"\"\"\n\n    # Some routines may auto-compute start offsets incorrectly and pass a\n    # negative value.  Offset to 0 for robustness.\n    start = max(0, start)\n\n    # first, try the hook\n    ip = ipapi.get()\n    if ip:\n        try:\n            ip.hooks.show_in_pager(strng)\n            return\n        except TryNext:\n            pass\n\n    # Ugly kludge, but calling curses.initscr() flat out crashes in emacs\n    TERM = os.environ.get('TERM','dumb')\n    if TERM in ['dumb','emacs'] and os.name != 'nt':\n        print strng\n        return\n    # chop off the topmost part of the string we don't want to see\n    str_lines = strng.splitlines()[start:]\n    str_toprint = os.linesep.join(str_lines)\n    num_newlines = len(str_lines)\n    len_str = len(str_toprint)\n\n    # Dumb heuristics to guesstimate number of on-screen lines the string\n    # takes.  Very basic, but good enough for docstrings in reasonable\n    # terminals. If someone later feels like refining it, it's not hard.\n    numlines = max(num_newlines,int(len_str/80)+1)\n\n    screen_lines_def = get_terminal_size()[1]\n\n    # auto-determine screen size\n    if screen_lines <= 0:\n        try:\n            screen_lines += _detect_screen_size(use_curses, screen_lines_def)\n        except (TypeError, UnsupportedOperation):\n            print >>io.stdout, str_toprint\n            return\n\n    #print 'numlines',numlines,'screenlines',screen_lines  # dbg\n    if numlines <= screen_lines :\n        #print '*** normal print'  # dbg\n        print >>io.stdout, str_toprint\n    else:\n        # Try to open pager and default to internal one if that fails.\n        # All failure modes are tagged as 'retval=1', to match the return\n        # value of a failed system command.  If any intermediate attempt\n        # sets retval to 1, at the end we resort to our own page_dumb() pager.\n        pager_cmd = get_pager_cmd(pager_cmd)\n        pager_cmd += ' ' + get_pager_start(pager_cmd,start)\n        if os.name == 'nt':\n            if pager_cmd.startswith('type'):\n                # The default WinXP 'type' command is failing on complex strings.\n                retval = 1\n            else:\n                tmpname = tempfile.mktemp('.txt')\n                tmpfile = open(tmpname,'wt')\n                tmpfile.write(strng)\n                tmpfile.close()\n                cmd = \"%s < %s\" % (pager_cmd,tmpname)\n                if os.system(cmd):\n                  retval = 1\n                else:\n                  retval = None\n                os.remove(tmpname)\n        else:\n            try:\n                retval = None\n                # if I use popen4, things hang. No idea why.\n                #pager,shell_out = os.popen4(pager_cmd)\n                pager = os.popen(pager_cmd,'w')\n                pager.write(strng)\n                pager.close()\n                retval = pager.close()  # success returns None\n            except IOError,msg:  # broken pipe when user quits\n                if msg.args == (32,'Broken pipe'):\n                    retval = None\n                else:\n                    retval = 1\n            except OSError:\n                # Other strange problems, sometimes seen in Win2k/cygwin\n                retval = 1\n        if retval is not None:\n            page_dumb(strng,screen_lines=screen_lines)", "language": "python", "code": "def page(strng, start=0, screen_lines=0, pager_cmd=None):\n    \"\"\"Print a string, piping through a pager after a certain length.\n\n    The screen_lines parameter specifies the number of *usable* lines of your\n    terminal screen (total lines minus lines you need to reserve to show other\n    information).\n\n    If you set screen_lines to a number <=0, page() will try to auto-determine\n    your screen size and will only use up to (screen_size+screen_lines) for\n    printing, paging after that. That is, if you want auto-detection but need\n    to reserve the bottom 3 lines of the screen, use screen_lines = -3, and for\n    auto-detection without any lines reserved simply use screen_lines = 0.\n\n    If a string won't fit in the allowed lines, it is sent through the\n    specified pager command. If none given, look for PAGER in the environment,\n    and ultimately default to less.\n\n    If no system pager works, the string is sent through a 'dumb pager'\n    written in python, very simplistic.\n    \"\"\"\n\n    # Some routines may auto-compute start offsets incorrectly and pass a\n    # negative value.  Offset to 0 for robustness.\n    start = max(0, start)\n\n    # first, try the hook\n    ip = ipapi.get()\n    if ip:\n        try:\n            ip.hooks.show_in_pager(strng)\n            return\n        except TryNext:\n            pass\n\n    # Ugly kludge, but calling curses.initscr() flat out crashes in emacs\n    TERM = os.environ.get('TERM','dumb')\n    if TERM in ['dumb','emacs'] and os.name != 'nt':\n        print strng\n        return\n    # chop off the topmost part of the string we don't want to see\n    str_lines = strng.splitlines()[start:]\n    str_toprint = os.linesep.join(str_lines)\n    num_newlines = len(str_lines)\n    len_str = len(str_toprint)\n\n    # Dumb heuristics to guesstimate number of on-screen lines the string\n    # takes.  Very basic, but good enough for docstrings in reasonable\n    # terminals. If someone later feels like refining it, it's not hard.\n    numlines = max(num_newlines,int(len_str/80)+1)\n\n    screen_lines_def = get_terminal_size()[1]\n\n    # auto-determine screen size\n    if screen_lines <= 0:\n        try:\n            screen_lines += _detect_screen_size(use_curses, screen_lines_def)\n        except (TypeError, UnsupportedOperation):\n            print >>io.stdout, str_toprint\n            return\n\n    #print 'numlines',numlines,'screenlines',screen_lines  # dbg\n    if numlines <= screen_lines :\n        #print '*** normal print'  # dbg\n        print >>io.stdout, str_toprint\n    else:\n        # Try to open pager and default to internal one if that fails.\n        # All failure modes are tagged as 'retval=1', to match the return\n        # value of a failed system command.  If any intermediate attempt\n        # sets retval to 1, at the end we resort to our own page_dumb() pager.\n        pager_cmd = get_pager_cmd(pager_cmd)\n        pager_cmd += ' ' + get_pager_start(pager_cmd,start)\n        if os.name == 'nt':\n            if pager_cmd.startswith('type'):\n                # The default WinXP 'type' command is failing on complex strings.\n                retval = 1\n            else:\n                tmpname = tempfile.mktemp('.txt')\n                tmpfile = open(tmpname,'wt')\n                tmpfile.write(strng)\n                tmpfile.close()\n                cmd = \"%s < %s\" % (pager_cmd,tmpname)\n                if os.system(cmd):\n                  retval = 1\n                else:\n                  retval = None\n                os.remove(tmpname)\n        else:\n            try:\n                retval = None\n                # if I use popen4, things hang. No idea why.\n                #pager,shell_out = os.popen4(pager_cmd)\n                pager = os.popen(pager_cmd,'w')\n                pager.write(strng)\n                pager.close()\n                retval = pager.close()  # success returns None\n            except IOError,msg:  # broken pipe when user quits\n                if msg.args == (32,'Broken pipe'):\n                    retval = None\n                else:\n                    retval = 1\n            except OSError:\n                # Other strange problems, sometimes seen in Win2k/cygwin\n                retval = 1\n        if retval is not None:\n            page_dumb(strng,screen_lines=screen_lines)", "code_tokens": ["def", "page", "(", "strng", ",", "start", "=", "0", ",", "screen_lines", "=", "0", ",", "pager_cmd", "=", "None", ")", ":", "# Some routines may auto-compute start offsets incorrectly and pass a", "# negative value.  Offset to 0 for robustness.", "start", "=", "max", "(", "0", ",", "start", ")", "# first, try the hook", "ip", "=", "ipapi", ".", "get", "(", ")", "if", "ip", ":", "try", ":", "ip", ".", "hooks", ".", "show_in_pager", "(", "strng", ")", "return", "except", "TryNext", ":", "pass", "# Ugly kludge, but calling curses.initscr() flat out crashes in emacs", "TERM", "=", "os", ".", "environ", ".", "get", "(", "'TERM'", ",", "'dumb'", ")", "if", "TERM", "in", "[", "'dumb'", ",", "'emacs'", "]", "and", "os", ".", "name", "!=", "'nt'", ":", "print", "strng", "return", "# chop off the topmost part of the string we don't want to see", "str_lines", "=", "strng", ".", "splitlines", "(", ")", "[", "start", ":", "]", "str_toprint", "=", "os", ".", "linesep", ".", "join", "(", "str_lines", ")", "num_newlines", "=", "len", "(", "str_lines", ")", "len_str", "=", "len", "(", "str_toprint", ")", "# Dumb heuristics to guesstimate number of on-screen lines the string", "# takes.  Very basic, but good enough for docstrings in reasonable", "# terminals. If someone later feels like refining it, it's not hard.", "numlines", "=", "max", "(", "num_newlines", ",", "int", "(", "len_str", "/", "80", ")", "+", "1", ")", "screen_lines_def", "=", "get_terminal_size", "(", ")", "[", "1", "]", "# auto-determine screen size", "if", "screen_lines", "<=", "0", ":", "try", ":", "screen_lines", "+=", "_detect_screen_size", "(", "use_curses", ",", "screen_lines_def", ")", "except", "(", "TypeError", ",", "UnsupportedOperation", ")", ":", "print", ">>", "io", ".", "stdout", ",", "str_toprint", "return", "#print 'numlines',numlines,'screenlines',screen_lines  # dbg", "if", "numlines", "<=", "screen_lines", ":", "#print '*** normal print'  # dbg", "print", ">>", "io", ".", "stdout", ",", "str_toprint", "else", ":", "# Try to open pager and default to internal one if that fails.", "# All failure modes are tagged as 'retval=1', to match the return", "# value of a failed system command.  If any intermediate attempt", "# sets retval to 1, at the end we resort to our own page_dumb() pager.", "pager_cmd", "=", "get_pager_cmd", "(", "pager_cmd", ")", "pager_cmd", "+=", "' '", "+", "get_pager_start", "(", "pager_cmd", ",", "start", ")", "if", "os", ".", "name", "==", "'nt'", ":", "if", "pager_cmd", ".", "startswith", "(", "'type'", ")", ":", "# The default WinXP 'type' command is failing on complex strings.", "retval", "=", "1", "else", ":", "tmpname", "=", "tempfile", ".", "mktemp", "(", "'.txt'", ")", "tmpfile", "=", "open", "(", "tmpname", ",", "'wt'", ")", "tmpfile", ".", "write", "(", "strng", ")", "tmpfile", ".", "close", "(", ")", "cmd", "=", "\"%s < %s\"", "%", "(", "pager_cmd", ",", "tmpname", ")", "if", "os", ".", "system", "(", "cmd", ")", ":", "retval", "=", "1", "else", ":", "retval", "=", "None", "os", ".", "remove", "(", "tmpname", ")", "else", ":", "try", ":", "retval", "=", "None", "# if I use popen4, things hang. No idea why.", "#pager,shell_out = os.popen4(pager_cmd)", "pager", "=", "os", ".", "popen", "(", "pager_cmd", ",", "'w'", ")", "pager", ".", "write", "(", "strng", ")", "pager", ".", "close", "(", ")", "retval", "=", "pager", ".", "close", "(", ")", "# success returns None", "except", "IOError", ",", "msg", ":", "# broken pipe when user quits", "if", "msg", ".", "args", "==", "(", "32", ",", "'Broken pipe'", ")", ":", "retval", "=", "None", "else", ":", "retval", "=", "1", "except", "OSError", ":", "# Other strange problems, sometimes seen in Win2k/cygwin", "retval", "=", "1", "if", "retval", "is", "not", "None", ":", "page_dumb", "(", "strng", ",", "screen_lines", "=", "screen_lines", ")"], "docstring": "Print a string, piping through a pager after a certain length.\n\n    The screen_lines parameter specifies the number of *usable* lines of your\n    terminal screen (total lines minus lines you need to reserve to show other\n    information).\n\n    If you set screen_lines to a number <=0, page() will try to auto-determine\n    your screen size and will only use up to (screen_size+screen_lines) for\n    printing, paging after that. That is, if you want auto-detection but need\n    to reserve the bottom 3 lines of the screen, use screen_lines = -3, and for\n    auto-detection without any lines reserved simply use screen_lines = 0.\n\n    If a string won't fit in the allowed lines, it is sent through the\n    specified pager command. If none given, look for PAGER in the environment,\n    and ultimately default to less.\n\n    If no system pager works, the string is sent through a 'dumb pager'\n    written in python, very simplistic.", "docstring_tokens": ["Print", "a", "string", "piping", "through", "a", "pager", "after", "a", "certain", "length", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/core/page.py#L129-L233", "partition": "test"}
{"repo": "chrisjrn/registrasion", "path": "registrasion/controllers/discount.py", "func_name": "DiscountController._annotate_with_past_uses", "original_string": "def _annotate_with_past_uses(cls, queryset, user):\n        ''' Annotates the queryset with a usage count for that discount claus\n        by the given user. '''\n\n        if queryset.model == conditions.DiscountForCategory:\n            matches = (\n                Q(category=F('discount__discountitem__product__category'))\n            )\n        elif queryset.model == conditions.DiscountForProduct:\n            matches = (\n                Q(product=F('discount__discountitem__product'))\n            )\n\n        in_carts = (\n            Q(discount__discountitem__cart__user=user) &\n            Q(discount__discountitem__cart__status=commerce.Cart.STATUS_PAID)\n        )\n\n        past_use_quantity = When(\n            in_carts & matches,\n            then=\"discount__discountitem__quantity\",\n        )\n\n        past_use_quantity_or_zero = Case(\n            past_use_quantity,\n            default=Value(0),\n        )\n\n        queryset = queryset.annotate(\n            past_use_count=Sum(past_use_quantity_or_zero)\n        )\n        return queryset", "language": "python", "code": "def _annotate_with_past_uses(cls, queryset, user):\n        ''' Annotates the queryset with a usage count for that discount claus\n        by the given user. '''\n\n        if queryset.model == conditions.DiscountForCategory:\n            matches = (\n                Q(category=F('discount__discountitem__product__category'))\n            )\n        elif queryset.model == conditions.DiscountForProduct:\n            matches = (\n                Q(product=F('discount__discountitem__product'))\n            )\n\n        in_carts = (\n            Q(discount__discountitem__cart__user=user) &\n            Q(discount__discountitem__cart__status=commerce.Cart.STATUS_PAID)\n        )\n\n        past_use_quantity = When(\n            in_carts & matches,\n            then=\"discount__discountitem__quantity\",\n        )\n\n        past_use_quantity_or_zero = Case(\n            past_use_quantity,\n            default=Value(0),\n        )\n\n        queryset = queryset.annotate(\n            past_use_count=Sum(past_use_quantity_or_zero)\n        )\n        return queryset", "code_tokens": ["def", "_annotate_with_past_uses", "(", "cls", ",", "queryset", ",", "user", ")", ":", "if", "queryset", ".", "model", "==", "conditions", ".", "DiscountForCategory", ":", "matches", "=", "(", "Q", "(", "category", "=", "F", "(", "'discount__discountitem__product__category'", ")", ")", ")", "elif", "queryset", ".", "model", "==", "conditions", ".", "DiscountForProduct", ":", "matches", "=", "(", "Q", "(", "product", "=", "F", "(", "'discount__discountitem__product'", ")", ")", ")", "in_carts", "=", "(", "Q", "(", "discount__discountitem__cart__user", "=", "user", ")", "&", "Q", "(", "discount__discountitem__cart__status", "=", "commerce", ".", "Cart", ".", "STATUS_PAID", ")", ")", "past_use_quantity", "=", "When", "(", "in_carts", "&", "matches", ",", "then", "=", "\"discount__discountitem__quantity\"", ",", ")", "past_use_quantity_or_zero", "=", "Case", "(", "past_use_quantity", ",", "default", "=", "Value", "(", "0", ")", ",", ")", "queryset", "=", "queryset", ".", "annotate", "(", "past_use_count", "=", "Sum", "(", "past_use_quantity_or_zero", ")", ")", "return", "queryset"], "docstring": "Annotates the queryset with a usage count for that discount claus\n        by the given user.", "docstring_tokens": ["Annotates", "the", "queryset", "with", "a", "usage", "count", "for", "that", "discount", "claus", "by", "the", "given", "user", "."], "sha": "461d5846c6f9f3b7099322a94f5d9911564448e4", "url": "https://github.com/chrisjrn/registrasion/blob/461d5846c6f9f3b7099322a94f5d9911564448e4/registrasion/controllers/discount.py#L166-L197", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/share/doc/ipython/examples/parallel/pi/pidigits.py", "func_name": "txt_file_to_digits", "original_string": "def txt_file_to_digits(filename, the_type=str):\n    \"\"\"\n    Yield the digits of pi read from a .txt file.\n    \"\"\"\n    with open(filename, 'r') as f:\n        for line in f.readlines():\n            for c in line:\n                if c != '\\n' and c!= ' ':\n                    yield the_type(c)", "language": "python", "code": "def txt_file_to_digits(filename, the_type=str):\n    \"\"\"\n    Yield the digits of pi read from a .txt file.\n    \"\"\"\n    with open(filename, 'r') as f:\n        for line in f.readlines():\n            for c in line:\n                if c != '\\n' and c!= ' ':\n                    yield the_type(c)", "code_tokens": ["def", "txt_file_to_digits", "(", "filename", ",", "the_type", "=", "str", ")", ":", "with", "open", "(", "filename", ",", "'r'", ")", "as", "f", ":", "for", "line", "in", "f", ".", "readlines", "(", ")", ":", "for", "c", "in", "line", ":", "if", "c", "!=", "'\\n'", "and", "c", "!=", "' '", ":", "yield", "the_type", "(", "c", ")"], "docstring": "Yield the digits of pi read from a .txt file.", "docstring_tokens": ["Yield", "the", "digits", "of", "pi", "read", "from", "a", ".", "txt", "file", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/share/doc/ipython/examples/parallel/pi/pidigits.py#L74-L82", "partition": "test"}
{"repo": "rollbar/pyrollbar", "path": "rollbar/__init__.py", "func_name": "_check_add_locals", "original_string": "def _check_add_locals(frame, frame_num, total_frames):\n    \"\"\"\n    Returns True if we should record local variables for the given frame.\n    \"\"\"\n    # Include the last frames locals\n    # Include any frame locals that came from a file in the project's root\n    return any(((frame_num == total_frames - 1),\n                ('root' in SETTINGS and (frame.get('filename') or '').lower().startswith((SETTINGS['root'] or '').lower()))))", "language": "python", "code": "def _check_add_locals(frame, frame_num, total_frames):\n    \"\"\"\n    Returns True if we should record local variables for the given frame.\n    \"\"\"\n    # Include the last frames locals\n    # Include any frame locals that came from a file in the project's root\n    return any(((frame_num == total_frames - 1),\n                ('root' in SETTINGS and (frame.get('filename') or '').lower().startswith((SETTINGS['root'] or '').lower()))))", "code_tokens": ["def", "_check_add_locals", "(", "frame", ",", "frame_num", ",", "total_frames", ")", ":", "# Include the last frames locals", "# Include any frame locals that came from a file in the project's root", "return", "any", "(", "(", "(", "frame_num", "==", "total_frames", "-", "1", ")", ",", "(", "'root'", "in", "SETTINGS", "and", "(", "frame", ".", "get", "(", "'filename'", ")", "or", "''", ")", ".", "lower", "(", ")", ".", "startswith", "(", "(", "SETTINGS", "[", "'root'", "]", "or", "''", ")", ".", "lower", "(", ")", ")", ")", ")", ")"], "docstring": "Returns True if we should record local variables for the given frame.", "docstring_tokens": ["Returns", "True", "if", "we", "should", "record", "local", "variables", "for", "the", "given", "frame", "."], "sha": "33ef2e723a33d09dd6302f978f4a3908be95b9d2", "url": "https://github.com/rollbar/pyrollbar/blob/33ef2e723a33d09dd6302f978f4a3908be95b9d2/rollbar/__init__.py#L1029-L1036", "partition": "test"}
{"repo": "Qiskit/qiskit-terra", "path": "qiskit/providers/basicaer/unitary_simulator.py", "func_name": "UnitarySimulatorPy._validate_initial_unitary", "original_string": "def _validate_initial_unitary(self):\n        \"\"\"Validate an initial unitary matrix\"\"\"\n        # If initial unitary isn't set we don't need to validate\n        if self._initial_unitary is None:\n            return\n        # Check unitary is correct length for number of qubits\n        shape = np.shape(self._initial_unitary)\n        required_shape = (2 ** self._number_of_qubits,\n                          2 ** self._number_of_qubits)\n        if shape != required_shape:\n            raise BasicAerError('initial unitary is incorrect shape: ' +\n                                '{} != 2 ** {}'.format(shape, required_shape))", "language": "python", "code": "def _validate_initial_unitary(self):\n        \"\"\"Validate an initial unitary matrix\"\"\"\n        # If initial unitary isn't set we don't need to validate\n        if self._initial_unitary is None:\n            return\n        # Check unitary is correct length for number of qubits\n        shape = np.shape(self._initial_unitary)\n        required_shape = (2 ** self._number_of_qubits,\n                          2 ** self._number_of_qubits)\n        if shape != required_shape:\n            raise BasicAerError('initial unitary is incorrect shape: ' +\n                                '{} != 2 ** {}'.format(shape, required_shape))", "code_tokens": ["def", "_validate_initial_unitary", "(", "self", ")", ":", "# If initial unitary isn't set we don't need to validate", "if", "self", ".", "_initial_unitary", "is", "None", ":", "return", "# Check unitary is correct length for number of qubits", "shape", "=", "np", ".", "shape", "(", "self", ".", "_initial_unitary", ")", "required_shape", "=", "(", "2", "**", "self", ".", "_number_of_qubits", ",", "2", "**", "self", ".", "_number_of_qubits", ")", "if", "shape", "!=", "required_shape", ":", "raise", "BasicAerError", "(", "'initial unitary is incorrect shape: '", "+", "'{} != 2 ** {}'", ".", "format", "(", "shape", ",", "required_shape", ")", ")"], "docstring": "Validate an initial unitary matrix", "docstring_tokens": ["Validate", "an", "initial", "unitary", "matrix"], "sha": "d4f58d903bc96341b816f7c35df936d6421267d1", "url": "https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/providers/basicaer/unitary_simulator.py#L144-L155", "partition": "test"}
{"repo": "5j9/wikitextparser", "path": "wikitextparser/_parser_function.py", "func_name": "SubWikiTextWithArgs.lists", "original_string": "def lists(self, pattern: str = None) -> List[WikiList]:\n        \"\"\"Return the lists in all arguments.\n\n        For performance reasons it is usually preferred to get a specific\n        Argument and use the `lists` method of that argument instead.\n        \"\"\"\n        return [\n            lst for arg in self.arguments for lst in arg.lists(pattern) if lst]", "language": "python", "code": "def lists(self, pattern: str = None) -> List[WikiList]:\n        \"\"\"Return the lists in all arguments.\n\n        For performance reasons it is usually preferred to get a specific\n        Argument and use the `lists` method of that argument instead.\n        \"\"\"\n        return [\n            lst for arg in self.arguments for lst in arg.lists(pattern) if lst]", "code_tokens": ["def", "lists", "(", "self", ",", "pattern", ":", "str", "=", "None", ")", "->", "List", "[", "WikiList", "]", ":", "return", "[", "lst", "for", "arg", "in", "self", ".", "arguments", "for", "lst", "in", "arg", ".", "lists", "(", "pattern", ")", "if", "lst", "]"], "docstring": "Return the lists in all arguments.\n\n        For performance reasons it is usually preferred to get a specific\n        Argument and use the `lists` method of that argument instead.", "docstring_tokens": ["Return", "the", "lists", "in", "all", "arguments", "."], "sha": "1347425814361d7955342c53212edbb27f0ff4b5", "url": "https://github.com/5j9/wikitextparser/blob/1347425814361d7955342c53212edbb27f0ff4b5/wikitextparser/_parser_function.py#L56-L63", "partition": "test"}
{"repo": "LionelAuroux/pyrser", "path": "pyrser/parsing/base.py", "func_name": "BasicParser.parsed_stream", "original_string": "def parsed_stream(self, content: str, name: str=None):\n        \"\"\"Push a new Stream into the parser.\n        All subsequent called functions will parse this new stream,\n        until the 'popStream' function is called.\n        \"\"\"\n        self._streams.append(Stream(content, name))", "language": "python", "code": "def parsed_stream(self, content: str, name: str=None):\n        \"\"\"Push a new Stream into the parser.\n        All subsequent called functions will parse this new stream,\n        until the 'popStream' function is called.\n        \"\"\"\n        self._streams.append(Stream(content, name))", "code_tokens": ["def", "parsed_stream", "(", "self", ",", "content", ":", "str", ",", "name", ":", "str", "=", "None", ")", ":", "self", ".", "_streams", ".", "append", "(", "Stream", "(", "content", ",", "name", ")", ")"], "docstring": "Push a new Stream into the parser.\n        All subsequent called functions will parse this new stream,\n        until the 'popStream' function is called.", "docstring_tokens": ["Push", "a", "new", "Stream", "into", "the", "parser", ".", "All", "subsequent", "called", "functions", "will", "parse", "this", "new", "stream", "until", "the", "popStream", "function", "is", "called", "."], "sha": "f153a97ef2b6bf915a1ed468c0252a9a59b754d5", "url": "https://github.com/LionelAuroux/pyrser/blob/f153a97ef2b6bf915a1ed468c0252a9a59b754d5/pyrser/parsing/base.py#L137-L142", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/utils/text.py", "func_name": "SList.fields", "original_string": "def fields(self, *fields):\n        \"\"\" Collect whitespace-separated fields from string list\n\n        Allows quick awk-like usage of string lists.\n\n        Example data (in var a, created by 'a = !ls -l')::\n            -rwxrwxrwx  1 ville None      18 Dec 14  2006 ChangeLog\n            drwxrwxrwx+ 6 ville None       0 Oct 24 18:05 IPython\n\n        a.fields(0) is ['-rwxrwxrwx', 'drwxrwxrwx+']\n        a.fields(1,0) is ['1 -rwxrwxrwx', '6 drwxrwxrwx+']\n        (note the joining by space).\n        a.fields(-1) is ['ChangeLog', 'IPython']\n\n        IndexErrors are ignored.\n\n        Without args, fields() just split()'s the strings.\n        \"\"\"\n        if len(fields) == 0:\n            return [el.split() for el in self]\n\n        res = SList()\n        for el in [f.split() for f in self]:\n            lineparts = []\n\n            for fd in fields:\n                try:\n                    lineparts.append(el[fd])\n                except IndexError:\n                    pass\n            if lineparts:\n                res.append(\" \".join(lineparts))\n\n        return res", "language": "python", "code": "def fields(self, *fields):\n        \"\"\" Collect whitespace-separated fields from string list\n\n        Allows quick awk-like usage of string lists.\n\n        Example data (in var a, created by 'a = !ls -l')::\n            -rwxrwxrwx  1 ville None      18 Dec 14  2006 ChangeLog\n            drwxrwxrwx+ 6 ville None       0 Oct 24 18:05 IPython\n\n        a.fields(0) is ['-rwxrwxrwx', 'drwxrwxrwx+']\n        a.fields(1,0) is ['1 -rwxrwxrwx', '6 drwxrwxrwx+']\n        (note the joining by space).\n        a.fields(-1) is ['ChangeLog', 'IPython']\n\n        IndexErrors are ignored.\n\n        Without args, fields() just split()'s the strings.\n        \"\"\"\n        if len(fields) == 0:\n            return [el.split() for el in self]\n\n        res = SList()\n        for el in [f.split() for f in self]:\n            lineparts = []\n\n            for fd in fields:\n                try:\n                    lineparts.append(el[fd])\n                except IndexError:\n                    pass\n            if lineparts:\n                res.append(\" \".join(lineparts))\n\n        return res", "code_tokens": ["def", "fields", "(", "self", ",", "*", "fields", ")", ":", "if", "len", "(", "fields", ")", "==", "0", ":", "return", "[", "el", ".", "split", "(", ")", "for", "el", "in", "self", "]", "res", "=", "SList", "(", ")", "for", "el", "in", "[", "f", ".", "split", "(", ")", "for", "f", "in", "self", "]", ":", "lineparts", "=", "[", "]", "for", "fd", "in", "fields", ":", "try", ":", "lineparts", ".", "append", "(", "el", "[", "fd", "]", ")", "except", "IndexError", ":", "pass", "if", "lineparts", ":", "res", ".", "append", "(", "\" \"", ".", "join", "(", "lineparts", ")", ")", "return", "res"], "docstring": "Collect whitespace-separated fields from string list\n\n        Allows quick awk-like usage of string lists.\n\n        Example data (in var a, created by 'a = !ls -l')::\n            -rwxrwxrwx  1 ville None      18 Dec 14  2006 ChangeLog\n            drwxrwxrwx+ 6 ville None       0 Oct 24 18:05 IPython\n\n        a.fields(0) is ['-rwxrwxrwx', 'drwxrwxrwx+']\n        a.fields(1,0) is ['1 -rwxrwxrwx', '6 drwxrwxrwx+']\n        (note the joining by space).\n        a.fields(-1) is ['ChangeLog', 'IPython']\n\n        IndexErrors are ignored.\n\n        Without args, fields() just split()'s the strings.", "docstring_tokens": ["Collect", "whitespace", "-", "separated", "fields", "from", "string", "list"], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/utils/text.py#L189-L222", "partition": "test"}
{"repo": "iotile/typedargs", "path": "typedargs/typeinfo.py", "func_name": "TypeSystem.convert_from_binary", "original_string": "def convert_from_binary(self, binvalue, type, **kwargs):\n        \"\"\"\n        Convert binary data to type 'type'.\n\n        'type' must have a convert_binary function.  If 'type'\n        supports size checking, the size function is called to ensure\n        that binvalue is the correct size for deserialization\n        \"\"\"\n\n        size = self.get_type_size(type)\n        if size > 0 and len(binvalue) != size:\n            raise ArgumentError(\"Could not convert type from binary since the data was not the correct size\", required_size=size, actual_size=len(binvalue), type=type)\n\n        typeobj = self.get_type(type)\n\n        if not hasattr(typeobj, 'convert_binary'):\n            raise ArgumentError(\"Type does not support conversion from binary\", type=type)\n\n        return typeobj.convert_binary(binvalue, **kwargs)", "language": "python", "code": "def convert_from_binary(self, binvalue, type, **kwargs):\n        \"\"\"\n        Convert binary data to type 'type'.\n\n        'type' must have a convert_binary function.  If 'type'\n        supports size checking, the size function is called to ensure\n        that binvalue is the correct size for deserialization\n        \"\"\"\n\n        size = self.get_type_size(type)\n        if size > 0 and len(binvalue) != size:\n            raise ArgumentError(\"Could not convert type from binary since the data was not the correct size\", required_size=size, actual_size=len(binvalue), type=type)\n\n        typeobj = self.get_type(type)\n\n        if not hasattr(typeobj, 'convert_binary'):\n            raise ArgumentError(\"Type does not support conversion from binary\", type=type)\n\n        return typeobj.convert_binary(binvalue, **kwargs)", "code_tokens": ["def", "convert_from_binary", "(", "self", ",", "binvalue", ",", "type", ",", "*", "*", "kwargs", ")", ":", "size", "=", "self", ".", "get_type_size", "(", "type", ")", "if", "size", ">", "0", "and", "len", "(", "binvalue", ")", "!=", "size", ":", "raise", "ArgumentError", "(", "\"Could not convert type from binary since the data was not the correct size\"", ",", "required_size", "=", "size", ",", "actual_size", "=", "len", "(", "binvalue", ")", ",", "type", "=", "type", ")", "typeobj", "=", "self", ".", "get_type", "(", "type", ")", "if", "not", "hasattr", "(", "typeobj", ",", "'convert_binary'", ")", ":", "raise", "ArgumentError", "(", "\"Type does not support conversion from binary\"", ",", "type", "=", "type", ")", "return", "typeobj", ".", "convert_binary", "(", "binvalue", ",", "*", "*", "kwargs", ")"], "docstring": "Convert binary data to type 'type'.\n\n        'type' must have a convert_binary function.  If 'type'\n        supports size checking, the size function is called to ensure\n        that binvalue is the correct size for deserialization", "docstring_tokens": ["Convert", "binary", "data", "to", "type", "type", "."], "sha": "0a5091a664b9b4d836e091e9ba583e944f438fd8", "url": "https://github.com/iotile/typedargs/blob/0a5091a664b9b4d836e091e9ba583e944f438fd8/typedargs/typeinfo.py#L95-L113", "partition": "test"}
{"repo": "RRZE-HPC/kerncraft", "path": "kerncraft/kernel.py", "func_name": "KernelCode.get_kernel_loop_nest", "original_string": "def get_kernel_loop_nest(self):\n        \"\"\"Return kernel loop nest including any preceding pragmas and following swaps.\"\"\"\n        loop_nest = [s for s in self.kernel_ast.block_items\n                     if type(s) in [c_ast.For, c_ast.Pragma, c_ast.FuncCall]]\n        assert len(loop_nest) >= 1, \"Found to few for statements in kernel\"\n        return loop_nest", "language": "python", "code": "def get_kernel_loop_nest(self):\n        \"\"\"Return kernel loop nest including any preceding pragmas and following swaps.\"\"\"\n        loop_nest = [s for s in self.kernel_ast.block_items\n                     if type(s) in [c_ast.For, c_ast.Pragma, c_ast.FuncCall]]\n        assert len(loop_nest) >= 1, \"Found to few for statements in kernel\"\n        return loop_nest", "code_tokens": ["def", "get_kernel_loop_nest", "(", "self", ")", ":", "loop_nest", "=", "[", "s", "for", "s", "in", "self", ".", "kernel_ast", ".", "block_items", "if", "type", "(", "s", ")", "in", "[", "c_ast", ".", "For", ",", "c_ast", ".", "Pragma", ",", "c_ast", ".", "FuncCall", "]", "]", "assert", "len", "(", "loop_nest", ")", ">=", "1", ",", "\"Found to few for statements in kernel\"", "return", "loop_nest"], "docstring": "Return kernel loop nest including any preceding pragmas and following swaps.", "docstring_tokens": ["Return", "kernel", "loop", "nest", "including", "any", "preceding", "pragmas", "and", "following", "swaps", "."], "sha": "c60baf8043e4da8d8d66da7575021c2f4c6c78af", "url": "https://github.com/RRZE-HPC/kerncraft/blob/c60baf8043e4da8d8d66da7575021c2f4c6c78af/kerncraft/kernel.py#L1110-L1115", "partition": "test"}
{"repo": "calston/tensor", "path": "tensor/utils.py", "func_name": "PersistentCache.contains", "original_string": "def contains(self, k):\n        \"\"\"Return True if key `k` exists\"\"\"\n        if self._changed():\n            self._read()\n        return k in self.store.keys()", "language": "python", "code": "def contains(self, k):\n        \"\"\"Return True if key `k` exists\"\"\"\n        if self._changed():\n            self._read()\n        return k in self.store.keys()", "code_tokens": ["def", "contains", "(", "self", ",", "k", ")", ":", "if", "self", ".", "_changed", "(", ")", ":", "self", ".", "_read", "(", ")", "return", "k", "in", "self", ".", "store", ".", "keys", "(", ")"], "docstring": "Return True if key `k` exists", "docstring_tokens": ["Return", "True", "if", "key", "k", "exists"], "sha": "7c0c99708b5dbff97f3895f705e11996b608549d", "url": "https://github.com/calston/tensor/blob/7c0c99708b5dbff97f3895f705e11996b608549d/tensor/utils.py#L359-L363", "partition": "test"}
{"repo": "urinieto/msaf", "path": "msaf/algorithms/foote/segmenter.py", "func_name": "compute_gaussian_krnl", "original_string": "def compute_gaussian_krnl(M):\n    \"\"\"Creates a gaussian kernel following Foote's paper.\"\"\"\n    g = signal.gaussian(M, M // 3., sym=True)\n    G = np.dot(g.reshape(-1, 1), g.reshape(1, -1))\n    G[M // 2:, :M // 2] = -G[M // 2:, :M // 2]\n    G[:M // 2, M // 2:] = -G[:M // 2, M // 2:]\n    return G", "language": "python", "code": "def compute_gaussian_krnl(M):\n    \"\"\"Creates a gaussian kernel following Foote's paper.\"\"\"\n    g = signal.gaussian(M, M // 3., sym=True)\n    G = np.dot(g.reshape(-1, 1), g.reshape(1, -1))\n    G[M // 2:, :M // 2] = -G[M // 2:, :M // 2]\n    G[:M // 2, M // 2:] = -G[:M // 2, M // 2:]\n    return G", "code_tokens": ["def", "compute_gaussian_krnl", "(", "M", ")", ":", "g", "=", "signal", ".", "gaussian", "(", "M", ",", "M", "//", "3.", ",", "sym", "=", "True", ")", "G", "=", "np", ".", "dot", "(", "g", ".", "reshape", "(", "-", "1", ",", "1", ")", ",", "g", ".", "reshape", "(", "1", ",", "-", "1", ")", ")", "G", "[", "M", "//", "2", ":", ",", ":", "M", "//", "2", "]", "=", "-", "G", "[", "M", "//", "2", ":", ",", ":", "M", "//", "2", "]", "G", "[", ":", "M", "//", "2", ",", "M", "//", "2", ":", "]", "=", "-", "G", "[", ":", "M", "//", "2", ",", "M", "//", "2", ":", "]", "return", "G"], "docstring": "Creates a gaussian kernel following Foote's paper.", "docstring_tokens": ["Creates", "a", "gaussian", "kernel", "following", "Foote", "s", "paper", "."], "sha": "9dbb57d77a1310465a65cc40f1641d083ca74385", "url": "https://github.com/urinieto/msaf/blob/9dbb57d77a1310465a65cc40f1641d083ca74385/msaf/algorithms/foote/segmenter.py#L22-L28", "partition": "test"}
{"repo": "h2oai/h2o-3", "path": "h2o-py/h2o/frame.py", "func_name": "H2OFrame.idxmax", "original_string": "def idxmax(self,skipna=True, axis=0):\n        \"\"\"\n        Get the index of the max value in a column or row\n\n        :param bool skipna: If True (default), then NAs are ignored during the search. Otherwise presence\n            of NAs renders the entire result NA.\n        :param int axis: Direction of finding the max index. If 0 (default), then the max index is searched columnwise, and the\n            result is a frame with 1 row and number of columns as in the original frame. If 1, then the max index is searched\n            rowwise and the result is a frame with 1 column, and number of rows equal to the number of rows in the original frame.\n        :returns: either a list of max index values per-column or an H2OFrame containing max index values\n                  per-row from the original frame.\n        \"\"\"\n        return H2OFrame._expr(expr=ExprNode(\"which.max\", self, skipna, axis))", "language": "python", "code": "def idxmax(self,skipna=True, axis=0):\n        \"\"\"\n        Get the index of the max value in a column or row\n\n        :param bool skipna: If True (default), then NAs are ignored during the search. Otherwise presence\n            of NAs renders the entire result NA.\n        :param int axis: Direction of finding the max index. If 0 (default), then the max index is searched columnwise, and the\n            result is a frame with 1 row and number of columns as in the original frame. If 1, then the max index is searched\n            rowwise and the result is a frame with 1 column, and number of rows equal to the number of rows in the original frame.\n        :returns: either a list of max index values per-column or an H2OFrame containing max index values\n                  per-row from the original frame.\n        \"\"\"\n        return H2OFrame._expr(expr=ExprNode(\"which.max\", self, skipna, axis))", "code_tokens": ["def", "idxmax", "(", "self", ",", "skipna", "=", "True", ",", "axis", "=", "0", ")", ":", "return", "H2OFrame", ".", "_expr", "(", "expr", "=", "ExprNode", "(", "\"which.max\"", ",", "self", ",", "skipna", ",", "axis", ")", ")"], "docstring": "Get the index of the max value in a column or row\n\n        :param bool skipna: If True (default), then NAs are ignored during the search. Otherwise presence\n            of NAs renders the entire result NA.\n        :param int axis: Direction of finding the max index. If 0 (default), then the max index is searched columnwise, and the\n            result is a frame with 1 row and number of columns as in the original frame. If 1, then the max index is searched\n            rowwise and the result is a frame with 1 column, and number of rows equal to the number of rows in the original frame.\n        :returns: either a list of max index values per-column or an H2OFrame containing max index values\n                  per-row from the original frame.", "docstring_tokens": ["Get", "the", "index", "of", "the", "max", "value", "in", "a", "column", "or", "row"], "sha": "dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8", "url": "https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/h2o-py/h2o/frame.py#L3256-L3268", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/mcmc/slice_sampler_kernel.py", "func_name": "_maybe_call_fn", "original_string": "def _maybe_call_fn(fn,\n                   fn_arg_list,\n                   fn_result=None,\n                   description='target_log_prob'):\n  \"\"\"Helper which computes `fn_result` if needed.\"\"\"\n  fn_arg_list = (list(fn_arg_list) if mcmc_util.is_list_like(fn_arg_list)\n                 else [fn_arg_list])\n  if fn_result is None:\n    fn_result = fn(*fn_arg_list)\n  if not fn_result.dtype.is_floating:\n    raise TypeError('`{}` must be a `Tensor` with `float` `dtype`.'.format(\n        description))\n  return fn_result", "language": "python", "code": "def _maybe_call_fn(fn,\n                   fn_arg_list,\n                   fn_result=None,\n                   description='target_log_prob'):\n  \"\"\"Helper which computes `fn_result` if needed.\"\"\"\n  fn_arg_list = (list(fn_arg_list) if mcmc_util.is_list_like(fn_arg_list)\n                 else [fn_arg_list])\n  if fn_result is None:\n    fn_result = fn(*fn_arg_list)\n  if not fn_result.dtype.is_floating:\n    raise TypeError('`{}` must be a `Tensor` with `float` `dtype`.'.format(\n        description))\n  return fn_result", "code_tokens": ["def", "_maybe_call_fn", "(", "fn", ",", "fn_arg_list", ",", "fn_result", "=", "None", ",", "description", "=", "'target_log_prob'", ")", ":", "fn_arg_list", "=", "(", "list", "(", "fn_arg_list", ")", "if", "mcmc_util", ".", "is_list_like", "(", "fn_arg_list", ")", "else", "[", "fn_arg_list", "]", ")", "if", "fn_result", "is", "None", ":", "fn_result", "=", "fn", "(", "*", "fn_arg_list", ")", "if", "not", "fn_result", ".", "dtype", ".", "is_floating", ":", "raise", "TypeError", "(", "'`{}` must be a `Tensor` with `float` `dtype`.'", ".", "format", "(", "description", ")", ")", "return", "fn_result"], "docstring": "Helper which computes `fn_result` if needed.", "docstring_tokens": ["Helper", "which", "computes", "fn_result", "if", "needed", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/mcmc/slice_sampler_kernel.py#L540-L552", "partition": "test"}
{"repo": "zqfang/GSEApy", "path": "gseapy/algorithm.py", "func_name": "gsea_pval", "original_string": "def gsea_pval(es, esnull):\n    \"\"\"Compute nominal p-value.\n\n    From article (PNAS):\n    estimate nominal p-value for S from esnull by using the positive\n    or negative portion of the distribution corresponding to the sign\n    of the observed ES(S).\n    \"\"\"\n\n    # to speed up, using numpy function to compute pval in parallel.\n    condlist = [ es < 0, es >=0]\n    choicelist = [np.sum(esnull < es.reshape(len(es),1), axis=1)/ np.sum(esnull < 0, axis=1),\n                  np.sum(esnull >= es.reshape(len(es),1), axis=1)/ np.sum(esnull >= 0, axis=1)]\n    pval = np.select(condlist, choicelist)\n\n    return pval", "language": "python", "code": "def gsea_pval(es, esnull):\n    \"\"\"Compute nominal p-value.\n\n    From article (PNAS):\n    estimate nominal p-value for S from esnull by using the positive\n    or negative portion of the distribution corresponding to the sign\n    of the observed ES(S).\n    \"\"\"\n\n    # to speed up, using numpy function to compute pval in parallel.\n    condlist = [ es < 0, es >=0]\n    choicelist = [np.sum(esnull < es.reshape(len(es),1), axis=1)/ np.sum(esnull < 0, axis=1),\n                  np.sum(esnull >= es.reshape(len(es),1), axis=1)/ np.sum(esnull >= 0, axis=1)]\n    pval = np.select(condlist, choicelist)\n\n    return pval", "code_tokens": ["def", "gsea_pval", "(", "es", ",", "esnull", ")", ":", "# to speed up, using numpy function to compute pval in parallel.", "condlist", "=", "[", "es", "<", "0", ",", "es", ">=", "0", "]", "choicelist", "=", "[", "np", ".", "sum", "(", "esnull", "<", "es", ".", "reshape", "(", "len", "(", "es", ")", ",", "1", ")", ",", "axis", "=", "1", ")", "/", "np", ".", "sum", "(", "esnull", "<", "0", ",", "axis", "=", "1", ")", ",", "np", ".", "sum", "(", "esnull", ">=", "es", ".", "reshape", "(", "len", "(", "es", ")", ",", "1", ")", ",", "axis", "=", "1", ")", "/", "np", ".", "sum", "(", "esnull", ">=", "0", ",", "axis", "=", "1", ")", "]", "pval", "=", "np", ".", "select", "(", "condlist", ",", "choicelist", ")", "return", "pval"], "docstring": "Compute nominal p-value.\n\n    From article (PNAS):\n    estimate nominal p-value for S from esnull by using the positive\n    or negative portion of the distribution corresponding to the sign\n    of the observed ES(S).", "docstring_tokens": ["Compute", "nominal", "p", "-", "value", "."], "sha": "673e9ec1391e3b14d3e8a4353117151fd2cb9345", "url": "https://github.com/zqfang/GSEApy/blob/673e9ec1391e3b14d3e8a4353117151fd2cb9345/gseapy/algorithm.py#L509-L524", "partition": "test"}
{"repo": "bloomreach/s4cmd", "path": "s4cmd.py", "func_name": "S3Handler.del_files", "original_string": "def del_files(self, source):\n    '''Delete files on S3'''\n    src_files = []\n    for obj in self.s3walk(source):\n      if not obj['is_dir']: # ignore directories\n        src_files.append(obj['name'])\n\n    pool = ThreadPool(ThreadUtil, self.opt)\n    pool.batch_delete(src_files)\n    pool.join()", "language": "python", "code": "def del_files(self, source):\n    '''Delete files on S3'''\n    src_files = []\n    for obj in self.s3walk(source):\n      if not obj['is_dir']: # ignore directories\n        src_files.append(obj['name'])\n\n    pool = ThreadPool(ThreadUtil, self.opt)\n    pool.batch_delete(src_files)\n    pool.join()", "code_tokens": ["def", "del_files", "(", "self", ",", "source", ")", ":", "src_files", "=", "[", "]", "for", "obj", "in", "self", ".", "s3walk", "(", "source", ")", ":", "if", "not", "obj", "[", "'is_dir'", "]", ":", "# ignore directories", "src_files", ".", "append", "(", "obj", "[", "'name'", "]", ")", "pool", "=", "ThreadPool", "(", "ThreadUtil", ",", "self", ".", "opt", ")", "pool", ".", "batch_delete", "(", "src_files", ")", "pool", ".", "join", "(", ")"], "docstring": "Delete files on S3", "docstring_tokens": ["Delete", "files", "on", "S3"], "sha": "bb51075bf43703e7cd95aa39288cf7732ec13a6d", "url": "https://github.com/bloomreach/s4cmd/blob/bb51075bf43703e7cd95aa39288cf7732ec13a6d/s4cmd.py#L952-L961", "partition": "test"}
{"repo": "Nic30/hwt", "path": "hwt/hdl/transTmpl.py", "func_name": "TransTmpl._loadFromArray", "original_string": "def _loadFromArray(self, dtype: HdlType, bitAddr: int) -> int:\n        \"\"\"\n        Parse HArray type to this transaction template instance\n\n        :return: address of it's end\n        \"\"\"\n        self.itemCnt = evalParam(dtype.size).val\n        self.children = TransTmpl(\n            dtype.elmType, 0, parent=self, origin=self.origin)\n        return bitAddr + self.itemCnt * self.children.bitAddrEnd", "language": "python", "code": "def _loadFromArray(self, dtype: HdlType, bitAddr: int) -> int:\n        \"\"\"\n        Parse HArray type to this transaction template instance\n\n        :return: address of it's end\n        \"\"\"\n        self.itemCnt = evalParam(dtype.size).val\n        self.children = TransTmpl(\n            dtype.elmType, 0, parent=self, origin=self.origin)\n        return bitAddr + self.itemCnt * self.children.bitAddrEnd", "code_tokens": ["def", "_loadFromArray", "(", "self", ",", "dtype", ":", "HdlType", ",", "bitAddr", ":", "int", ")", "->", "int", ":", "self", ".", "itemCnt", "=", "evalParam", "(", "dtype", ".", "size", ")", ".", "val", "self", ".", "children", "=", "TransTmpl", "(", "dtype", ".", "elmType", ",", "0", ",", "parent", "=", "self", ",", "origin", "=", "self", ".", "origin", ")", "return", "bitAddr", "+", "self", ".", "itemCnt", "*", "self", ".", "children", ".", "bitAddrEnd"], "docstring": "Parse HArray type to this transaction template instance\n\n        :return: address of it's end", "docstring_tokens": ["Parse", "HArray", "type", "to", "this", "transaction", "template", "instance"], "sha": "8cbb399e326da3b22c233b98188a9d08dec057e6", "url": "https://github.com/Nic30/hwt/blob/8cbb399e326da3b22c233b98188a9d08dec057e6/hwt/hdl/transTmpl.py#L119-L128", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/utils/jsonutil.py", "func_name": "rekey", "original_string": "def rekey(dikt):\n    \"\"\"Rekey a dict that has been forced to use str keys where there should be\n    ints by json.\"\"\"\n    for k in dikt.iterkeys():\n        if isinstance(k, basestring):\n            ik=fk=None\n            try:\n                ik = int(k)\n            except ValueError:\n                try:\n                    fk = float(k)\n                except ValueError:\n                    continue\n            if ik is not None:\n                nk = ik\n            else:\n                nk = fk\n            if nk in dikt:\n                raise KeyError(\"already have key %r\"%nk)\n            dikt[nk] = dikt.pop(k)\n    return dikt", "language": "python", "code": "def rekey(dikt):\n    \"\"\"Rekey a dict that has been forced to use str keys where there should be\n    ints by json.\"\"\"\n    for k in dikt.iterkeys():\n        if isinstance(k, basestring):\n            ik=fk=None\n            try:\n                ik = int(k)\n            except ValueError:\n                try:\n                    fk = float(k)\n                except ValueError:\n                    continue\n            if ik is not None:\n                nk = ik\n            else:\n                nk = fk\n            if nk in dikt:\n                raise KeyError(\"already have key %r\"%nk)\n            dikt[nk] = dikt.pop(k)\n    return dikt", "code_tokens": ["def", "rekey", "(", "dikt", ")", ":", "for", "k", "in", "dikt", ".", "iterkeys", "(", ")", ":", "if", "isinstance", "(", "k", ",", "basestring", ")", ":", "ik", "=", "fk", "=", "None", "try", ":", "ik", "=", "int", "(", "k", ")", "except", "ValueError", ":", "try", ":", "fk", "=", "float", "(", "k", ")", "except", "ValueError", ":", "continue", "if", "ik", "is", "not", "None", ":", "nk", "=", "ik", "else", ":", "nk", "=", "fk", "if", "nk", "in", "dikt", ":", "raise", "KeyError", "(", "\"already have key %r\"", "%", "nk", ")", "dikt", "[", "nk", "]", "=", "dikt", ".", "pop", "(", "k", ")", "return", "dikt"], "docstring": "Rekey a dict that has been forced to use str keys where there should be\n    ints by json.", "docstring_tokens": ["Rekey", "a", "dict", "that", "has", "been", "forced", "to", "use", "str", "keys", "where", "there", "should", "be", "ints", "by", "json", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/utils/jsonutil.py#L38-L58", "partition": "test"}
{"repo": "praekeltfoundation/seed-control-interface", "path": "ci/views.py", "func_name": "tokens_required", "original_string": "def tokens_required(service_list):\n    \"\"\"\n    Ensure the user has the necessary tokens for the specified services\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        def inner(request, *args, **kwargs):\n            for service in service_list:\n                if service not in request.session[\"user_tokens\"]:\n                    return redirect('denied')\n            return func(request, *args, **kwargs)\n        return inner\n    return decorator", "language": "python", "code": "def tokens_required(service_list):\n    \"\"\"\n    Ensure the user has the necessary tokens for the specified services\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        def inner(request, *args, **kwargs):\n            for service in service_list:\n                if service not in request.session[\"user_tokens\"]:\n                    return redirect('denied')\n            return func(request, *args, **kwargs)\n        return inner\n    return decorator", "code_tokens": ["def", "tokens_required", "(", "service_list", ")", ":", "def", "decorator", "(", "func", ")", ":", "@", "wraps", "(", "func", ")", "def", "inner", "(", "request", ",", "*", "args", ",", "*", "*", "kwargs", ")", ":", "for", "service", "in", "service_list", ":", "if", "service", "not", "in", "request", ".", "session", "[", "\"user_tokens\"", "]", ":", "return", "redirect", "(", "'denied'", ")", "return", "func", "(", "request", ",", "*", "args", ",", "*", "*", "kwargs", ")", "return", "inner", "return", "decorator"], "docstring": "Ensure the user has the necessary tokens for the specified services", "docstring_tokens": ["Ensure", "the", "user", "has", "the", "necessary", "tokens", "for", "the", "specified", "services"], "sha": "32ddad88b5bc2f8f4d80b848361899da2e081636", "url": "https://github.com/praekeltfoundation/seed-control-interface/blob/32ddad88b5bc2f8f4d80b848361899da2e081636/ci/views.py#L153-L165", "partition": "test"}
{"repo": "ndrlslz/ternya", "path": "ternya/annotation.py", "func_name": "glance", "original_string": "def glance(*arg):\n    \"\"\"\n    Glance annotation for adding function to process glance notification.\n\n    if event_type include wildcard, will put {pattern: function} into process_wildcard dict\n    else will put {event_type: function} into process dict\n\n    :param arg: event_type of notification\n    \"\"\"\n    check_event_type(Openstack.Glance, *arg)\n    event_type = arg[0]\n\n    def decorator(func):\n        if event_type.find(\"*\") != -1:\n            event_type_pattern = pre_compile(event_type)\n            glance_customer_process_wildcard[event_type_pattern] = func\n        else:\n            glance_customer_process[event_type] = func\n        log.info(\"add function {0} to process event_type:{1}\".format(func.__name__, event_type))\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            func(*args, **kwargs)\n\n        return wrapper\n\n    return decorator", "language": "python", "code": "def glance(*arg):\n    \"\"\"\n    Glance annotation for adding function to process glance notification.\n\n    if event_type include wildcard, will put {pattern: function} into process_wildcard dict\n    else will put {event_type: function} into process dict\n\n    :param arg: event_type of notification\n    \"\"\"\n    check_event_type(Openstack.Glance, *arg)\n    event_type = arg[0]\n\n    def decorator(func):\n        if event_type.find(\"*\") != -1:\n            event_type_pattern = pre_compile(event_type)\n            glance_customer_process_wildcard[event_type_pattern] = func\n        else:\n            glance_customer_process[event_type] = func\n        log.info(\"add function {0} to process event_type:{1}\".format(func.__name__, event_type))\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            func(*args, **kwargs)\n\n        return wrapper\n\n    return decorator", "code_tokens": ["def", "glance", "(", "*", "arg", ")", ":", "check_event_type", "(", "Openstack", ".", "Glance", ",", "*", "arg", ")", "event_type", "=", "arg", "[", "0", "]", "def", "decorator", "(", "func", ")", ":", "if", "event_type", ".", "find", "(", "\"*\"", ")", "!=", "-", "1", ":", "event_type_pattern", "=", "pre_compile", "(", "event_type", ")", "glance_customer_process_wildcard", "[", "event_type_pattern", "]", "=", "func", "else", ":", "glance_customer_process", "[", "event_type", "]", "=", "func", "log", ".", "info", "(", "\"add function {0} to process event_type:{1}\"", ".", "format", "(", "func", ".", "__name__", ",", "event_type", ")", ")", "@", "functools", ".", "wraps", "(", "func", ")", "def", "wrapper", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "func", "(", "*", "args", ",", "*", "*", "kwargs", ")", "return", "wrapper", "return", "decorator"], "docstring": "Glance annotation for adding function to process glance notification.\n\n    if event_type include wildcard, will put {pattern: function} into process_wildcard dict\n    else will put {event_type: function} into process dict\n\n    :param arg: event_type of notification", "docstring_tokens": ["Glance", "annotation", "for", "adding", "function", "to", "process", "glance", "notification", "."], "sha": "c05aec10029e645d63ff04313dbcf2644743481f", "url": "https://github.com/ndrlslz/ternya/blob/c05aec10029e645d63ff04313dbcf2644743481f/ternya/annotation.py#L140-L166", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval", "path": "perceval/backends/core/launchpad.py", "func_name": "LaunchpadClient.user", "original_string": "def user(self, user_name):\n        \"\"\"Get the user data by URL\"\"\"\n\n        user = None\n\n        if user_name in self._users:\n            return self._users[user_name]\n\n        url_user = self.__get_url(\"~\" + user_name)\n\n        logger.info(\"Getting info for %s\" % (url_user))\n\n        try:\n            raw_user = self.__send_request(url_user)\n            user = raw_user\n        except requests.exceptions.HTTPError as e:\n            if e.response.status_code in [404, 410]:\n                logger.warning(\"Data is not available - %s\", url_user)\n                user = '{}'\n            else:\n                raise e\n\n        self._users[user_name] = user\n\n        return user", "language": "python", "code": "def user(self, user_name):\n        \"\"\"Get the user data by URL\"\"\"\n\n        user = None\n\n        if user_name in self._users:\n            return self._users[user_name]\n\n        url_user = self.__get_url(\"~\" + user_name)\n\n        logger.info(\"Getting info for %s\" % (url_user))\n\n        try:\n            raw_user = self.__send_request(url_user)\n            user = raw_user\n        except requests.exceptions.HTTPError as e:\n            if e.response.status_code in [404, 410]:\n                logger.warning(\"Data is not available - %s\", url_user)\n                user = '{}'\n            else:\n                raise e\n\n        self._users[user_name] = user\n\n        return user", "code_tokens": ["def", "user", "(", "self", ",", "user_name", ")", ":", "user", "=", "None", "if", "user_name", "in", "self", ".", "_users", ":", "return", "self", ".", "_users", "[", "user_name", "]", "url_user", "=", "self", ".", "__get_url", "(", "\"~\"", "+", "user_name", ")", "logger", ".", "info", "(", "\"Getting info for %s\"", "%", "(", "url_user", ")", ")", "try", ":", "raw_user", "=", "self", ".", "__send_request", "(", "url_user", ")", "user", "=", "raw_user", "except", "requests", ".", "exceptions", ".", "HTTPError", "as", "e", ":", "if", "e", ".", "response", ".", "status_code", "in", "[", "404", ",", "410", "]", ":", "logger", ".", "warning", "(", "\"Data is not available - %s\"", ",", "url_user", ")", "user", "=", "'{}'", "else", ":", "raise", "e", "self", ".", "_users", "[", "user_name", "]", "=", "user", "return", "user"], "docstring": "Get the user data by URL", "docstring_tokens": ["Get", "the", "user", "data", "by", "URL"], "sha": "41c908605e88b7ebc3a536c643fa0f212eaf9e0e", "url": "https://github.com/chaoss/grimoirelab-perceval/blob/41c908605e88b7ebc3a536c643fa0f212eaf9e0e/perceval/backends/core/launchpad.py#L307-L331", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/nbformat/v2/nbbase.py", "func_name": "new_worksheet", "original_string": "def new_worksheet(name=None, cells=None):\n    \"\"\"Create a worksheet by name with with a list of cells.\"\"\"\n    ws = NotebookNode()\n    if name is not None:\n        ws.name = unicode(name)\n    if cells is None:\n        ws.cells = []\n    else:\n        ws.cells = list(cells)\n    return ws", "language": "python", "code": "def new_worksheet(name=None, cells=None):\n    \"\"\"Create a worksheet by name with with a list of cells.\"\"\"\n    ws = NotebookNode()\n    if name is not None:\n        ws.name = unicode(name)\n    if cells is None:\n        ws.cells = []\n    else:\n        ws.cells = list(cells)\n    return ws", "code_tokens": ["def", "new_worksheet", "(", "name", "=", "None", ",", "cells", "=", "None", ")", ":", "ws", "=", "NotebookNode", "(", ")", "if", "name", "is", "not", "None", ":", "ws", ".", "name", "=", "unicode", "(", "name", ")", "if", "cells", "is", "None", ":", "ws", ".", "cells", "=", "[", "]", "else", ":", "ws", ".", "cells", "=", "list", "(", "cells", ")", "return", "ws"], "docstring": "Create a worksheet by name with with a list of cells.", "docstring_tokens": ["Create", "a", "worksheet", "by", "name", "with", "with", "a", "list", "of", "cells", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/nbformat/v2/nbbase.py#L122-L131", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/checkers/utils.py", "func_name": "class_is_abstract", "original_string": "def class_is_abstract(node: astroid.ClassDef) -> bool:\n    \"\"\"return true if the given class node should be considered as an abstract\n    class\n    \"\"\"\n    for method in node.methods():\n        if method.parent.frame() is node:\n            if method.is_abstract(pass_is_abstract=False):\n                return True\n    return False", "language": "python", "code": "def class_is_abstract(node: astroid.ClassDef) -> bool:\n    \"\"\"return true if the given class node should be considered as an abstract\n    class\n    \"\"\"\n    for method in node.methods():\n        if method.parent.frame() is node:\n            if method.is_abstract(pass_is_abstract=False):\n                return True\n    return False", "code_tokens": ["def", "class_is_abstract", "(", "node", ":", "astroid", ".", "ClassDef", ")", "->", "bool", ":", "for", "method", "in", "node", ".", "methods", "(", ")", ":", "if", "method", ".", "parent", ".", "frame", "(", ")", "is", "node", ":", "if", "method", ".", "is_abstract", "(", "pass_is_abstract", "=", "False", ")", ":", "return", "True", "return", "False"], "docstring": "return true if the given class node should be considered as an abstract\n    class", "docstring_tokens": ["return", "true", "if", "the", "given", "class", "node", "should", "be", "considered", "as", "an", "abstract", "class"], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/checkers/utils.py#L905-L913", "partition": "test"}
{"repo": "Nekmo/amazon-dash", "path": "amazon_dash/execute.py", "func_name": "ExecuteUrl.validate", "original_string": "def validate(self):\n        \"\"\"Check self.data. Raise InvalidConfig on error\n\n        :return: None\n        \"\"\"\n        if (self.data.get('content-type') or self.data.get('body')) and \\\n                self.data.get('method', '').lower() not in CONTENT_TYPE_METHODS:\n            raise InvalidConfig(\n                extra_body='The body/content-type option only can be used with the {} methods. The device is {}. '\n                           'Check the configuration file.'.format(', '.join(CONTENT_TYPE_METHODS), self.name)\n            )\n        self.data['content-type'] = CONTENT_TYPE_ALIASES.get(self.data.get('content-type'),\n                                                             self.data.get('content-type'))\n        form_type = CONTENT_TYPE_ALIASES['form']\n        if self.data.get('body') and (self.data.get('content-type') or form_type) == form_type:\n            try:\n                self.data['body'] = json.loads(self.data['body'])\n            except JSONDecodeError:\n                raise InvalidConfig(\n                    extra_body='Invalid JSON body on {} device.'.format(self.name)\n                )", "language": "python", "code": "def validate(self):\n        \"\"\"Check self.data. Raise InvalidConfig on error\n\n        :return: None\n        \"\"\"\n        if (self.data.get('content-type') or self.data.get('body')) and \\\n                self.data.get('method', '').lower() not in CONTENT_TYPE_METHODS:\n            raise InvalidConfig(\n                extra_body='The body/content-type option only can be used with the {} methods. The device is {}. '\n                           'Check the configuration file.'.format(', '.join(CONTENT_TYPE_METHODS), self.name)\n            )\n        self.data['content-type'] = CONTENT_TYPE_ALIASES.get(self.data.get('content-type'),\n                                                             self.data.get('content-type'))\n        form_type = CONTENT_TYPE_ALIASES['form']\n        if self.data.get('body') and (self.data.get('content-type') or form_type) == form_type:\n            try:\n                self.data['body'] = json.loads(self.data['body'])\n            except JSONDecodeError:\n                raise InvalidConfig(\n                    extra_body='Invalid JSON body on {} device.'.format(self.name)\n                )", "code_tokens": ["def", "validate", "(", "self", ")", ":", "if", "(", "self", ".", "data", ".", "get", "(", "'content-type'", ")", "or", "self", ".", "data", ".", "get", "(", "'body'", ")", ")", "and", "self", ".", "data", ".", "get", "(", "'method'", ",", "''", ")", ".", "lower", "(", ")", "not", "in", "CONTENT_TYPE_METHODS", ":", "raise", "InvalidConfig", "(", "extra_body", "=", "'The body/content-type option only can be used with the {} methods. The device is {}. '", "'Check the configuration file.'", ".", "format", "(", "', '", ".", "join", "(", "CONTENT_TYPE_METHODS", ")", ",", "self", ".", "name", ")", ")", "self", ".", "data", "[", "'content-type'", "]", "=", "CONTENT_TYPE_ALIASES", ".", "get", "(", "self", ".", "data", ".", "get", "(", "'content-type'", ")", ",", "self", ".", "data", ".", "get", "(", "'content-type'", ")", ")", "form_type", "=", "CONTENT_TYPE_ALIASES", "[", "'form'", "]", "if", "self", ".", "data", ".", "get", "(", "'body'", ")", "and", "(", "self", ".", "data", ".", "get", "(", "'content-type'", ")", "or", "form_type", ")", "==", "form_type", ":", "try", ":", "self", ".", "data", "[", "'body'", "]", "=", "json", ".", "loads", "(", "self", ".", "data", "[", "'body'", "]", ")", "except", "JSONDecodeError", ":", "raise", "InvalidConfig", "(", "extra_body", "=", "'Invalid JSON body on {} device.'", ".", "format", "(", "self", ".", "name", ")", ")"], "docstring": "Check self.data. Raise InvalidConfig on error\n\n        :return: None", "docstring_tokens": ["Check", "self", ".", "data", ".", "Raise", "InvalidConfig", "on", "error"], "sha": "0e2bdc24ff8ea32cecb2f5f54f5cc1c0f99c197b", "url": "https://github.com/Nekmo/amazon-dash/blob/0e2bdc24ff8ea32cecb2f5f54f5cc1c0f99c197b/amazon_dash/execute.py#L179-L199", "partition": "test"}
{"repo": "BlueDragonX/detach", "path": "detach.py", "func_name": "Detach._close_fd", "original_string": "def _close_fd(self, fd):\n        \"\"\"Close a file descriptor if it is open.\"\"\"\n        try:\n            os.close(fd)\n        except OSError, exc:\n            if exc.errno != errno.EBADF:\n                msg = \"Failed to close file descriptor {}: {}\".format(fd, exc)\n                raise Error(msg)", "language": "python", "code": "def _close_fd(self, fd):\n        \"\"\"Close a file descriptor if it is open.\"\"\"\n        try:\n            os.close(fd)\n        except OSError, exc:\n            if exc.errno != errno.EBADF:\n                msg = \"Failed to close file descriptor {}: {}\".format(fd, exc)\n                raise Error(msg)", "code_tokens": ["def", "_close_fd", "(", "self", ",", "fd", ")", ":", "try", ":", "os", ".", "close", "(", "fd", ")", "except", "OSError", ",", "exc", ":", "if", "exc", ".", "errno", "!=", "errno", ".", "EBADF", ":", "msg", "=", "\"Failed to close file descriptor {}: {}\"", ".", "format", "(", "fd", ",", "exc", ")", "raise", "Error", "(", "msg", ")"], "docstring": "Close a file descriptor if it is open.", "docstring_tokens": ["Close", "a", "file", "descriptor", "if", "it", "is", "open", "."], "sha": "e2e5a1076e19f508baf3ffb2b586a75934fbae28", "url": "https://github.com/BlueDragonX/detach/blob/e2e5a1076e19f508baf3ffb2b586a75934fbae28/detach.py#L50-L57", "partition": "test"}
{"repo": "lvh/txampext", "path": "docs/_exts/ditaa.py", "func_name": "render_ditaa", "original_string": "def render_ditaa(self, code, options, prefix='ditaa'):\n    \"\"\"Render ditaa code into a PNG output file.\"\"\"\n    hashkey = code.encode('utf-8') + str(options) + \\\n              str(self.builder.config.ditaa) + \\\n              str(self.builder.config.ditaa_args)\n    infname = '%s-%s.%s' % (prefix, sha(hashkey).hexdigest(), \"ditaa\")\n    outfname = '%s-%s.%s' % (prefix, sha(hashkey).hexdigest(), \"png\")\n\n    inrelfn = posixpath.join(self.builder.imgpath, infname)\n    infullfn = path.join(self.builder.outdir, '_images', infname)\n    outrelfn = posixpath.join(self.builder.imgpath, outfname)\n    outfullfn = path.join(self.builder.outdir, '_images', outfname)\n\n    if path.isfile(outfullfn):\n        return outrelfn, outfullfn\n\n    ensuredir(path.dirname(outfullfn))\n\n    # ditaa expects UTF-8 by default\n    if isinstance(code, unicode):\n        code = code.encode('utf-8')\n\n    ditaa_args = [self.builder.config.ditaa]\n    ditaa_args.extend(self.builder.config.ditaa_args)\n    ditaa_args.extend(options)\n    ditaa_args.extend( [infullfn] )\n    ditaa_args.extend( [outfullfn] )\n\n    f = open(infullfn, 'w')\n    f.write(code)\n    f.close()\n\n    try:\n        self.builder.warn(ditaa_args)\n        p = Popen(ditaa_args, stdout=PIPE, stdin=PIPE, stderr=PIPE)\n    except OSError, err:\n        if err.errno != ENOENT:   # No such file or directory\n            raise\n        self.builder.warn('ditaa command %r cannot be run (needed for ditaa '\n                          'output), check the ditaa setting' %\n                          self.builder.config.ditaa)\n        self.builder._ditaa_warned_dot = True\n        return None, None\n    wentWrong = False\n    try:\n        # Ditaa may close standard input when an error occurs,\n        # resulting in a broken pipe on communicate()\n        stdout, stderr = p.communicate(code)\n    except OSError, err:\n        if err.errno != EPIPE:\n            raise\n        wentWrong = True\n    except IOError, err:\n        if err.errno != EINVAL:\n            raise\n        wentWrong = True\n    if wentWrong:\n        # in this case, read the standard output and standard error streams\n        # directly, to get the error message(s)\n        stdout, stderr = p.stdout.read(), p.stderr.read()\n        p.wait()\n    if p.returncode != 0:\n        raise DitaaError('ditaa exited with error:\\n[stderr]\\n%s\\n'\n                            '[stdout]\\n%s' % (stderr, stdout))\n    return outrelfn, outfullfn", "language": "python", "code": "def render_ditaa(self, code, options, prefix='ditaa'):\n    \"\"\"Render ditaa code into a PNG output file.\"\"\"\n    hashkey = code.encode('utf-8') + str(options) + \\\n              str(self.builder.config.ditaa) + \\\n              str(self.builder.config.ditaa_args)\n    infname = '%s-%s.%s' % (prefix, sha(hashkey).hexdigest(), \"ditaa\")\n    outfname = '%s-%s.%s' % (prefix, sha(hashkey).hexdigest(), \"png\")\n\n    inrelfn = posixpath.join(self.builder.imgpath, infname)\n    infullfn = path.join(self.builder.outdir, '_images', infname)\n    outrelfn = posixpath.join(self.builder.imgpath, outfname)\n    outfullfn = path.join(self.builder.outdir, '_images', outfname)\n\n    if path.isfile(outfullfn):\n        return outrelfn, outfullfn\n\n    ensuredir(path.dirname(outfullfn))\n\n    # ditaa expects UTF-8 by default\n    if isinstance(code, unicode):\n        code = code.encode('utf-8')\n\n    ditaa_args = [self.builder.config.ditaa]\n    ditaa_args.extend(self.builder.config.ditaa_args)\n    ditaa_args.extend(options)\n    ditaa_args.extend( [infullfn] )\n    ditaa_args.extend( [outfullfn] )\n\n    f = open(infullfn, 'w')\n    f.write(code)\n    f.close()\n\n    try:\n        self.builder.warn(ditaa_args)\n        p = Popen(ditaa_args, stdout=PIPE, stdin=PIPE, stderr=PIPE)\n    except OSError, err:\n        if err.errno != ENOENT:   # No such file or directory\n            raise\n        self.builder.warn('ditaa command %r cannot be run (needed for ditaa '\n                          'output), check the ditaa setting' %\n                          self.builder.config.ditaa)\n        self.builder._ditaa_warned_dot = True\n        return None, None\n    wentWrong = False\n    try:\n        # Ditaa may close standard input when an error occurs,\n        # resulting in a broken pipe on communicate()\n        stdout, stderr = p.communicate(code)\n    except OSError, err:\n        if err.errno != EPIPE:\n            raise\n        wentWrong = True\n    except IOError, err:\n        if err.errno != EINVAL:\n            raise\n        wentWrong = True\n    if wentWrong:\n        # in this case, read the standard output and standard error streams\n        # directly, to get the error message(s)\n        stdout, stderr = p.stdout.read(), p.stderr.read()\n        p.wait()\n    if p.returncode != 0:\n        raise DitaaError('ditaa exited with error:\\n[stderr]\\n%s\\n'\n                            '[stdout]\\n%s' % (stderr, stdout))\n    return outrelfn, outfullfn", "code_tokens": ["def", "render_ditaa", "(", "self", ",", "code", ",", "options", ",", "prefix", "=", "'ditaa'", ")", ":", "hashkey", "=", "code", ".", "encode", "(", "'utf-8'", ")", "+", "str", "(", "options", ")", "+", "str", "(", "self", ".", "builder", ".", "config", ".", "ditaa", ")", "+", "str", "(", "self", ".", "builder", ".", "config", ".", "ditaa_args", ")", "infname", "=", "'%s-%s.%s'", "%", "(", "prefix", ",", "sha", "(", "hashkey", ")", ".", "hexdigest", "(", ")", ",", "\"ditaa\"", ")", "outfname", "=", "'%s-%s.%s'", "%", "(", "prefix", ",", "sha", "(", "hashkey", ")", ".", "hexdigest", "(", ")", ",", "\"png\"", ")", "inrelfn", "=", "posixpath", ".", "join", "(", "self", ".", "builder", ".", "imgpath", ",", "infname", ")", "infullfn", "=", "path", ".", "join", "(", "self", ".", "builder", ".", "outdir", ",", "'_images'", ",", "infname", ")", "outrelfn", "=", "posixpath", ".", "join", "(", "self", ".", "builder", ".", "imgpath", ",", "outfname", ")", "outfullfn", "=", "path", ".", "join", "(", "self", ".", "builder", ".", "outdir", ",", "'_images'", ",", "outfname", ")", "if", "path", ".", "isfile", "(", "outfullfn", ")", ":", "return", "outrelfn", ",", "outfullfn", "ensuredir", "(", "path", ".", "dirname", "(", "outfullfn", ")", ")", "# ditaa expects UTF-8 by default", "if", "isinstance", "(", "code", ",", "unicode", ")", ":", "code", "=", "code", ".", "encode", "(", "'utf-8'", ")", "ditaa_args", "=", "[", "self", ".", "builder", ".", "config", ".", "ditaa", "]", "ditaa_args", ".", "extend", "(", "self", ".", "builder", ".", "config", ".", "ditaa_args", ")", "ditaa_args", ".", "extend", "(", "options", ")", "ditaa_args", ".", "extend", "(", "[", "infullfn", "]", ")", "ditaa_args", ".", "extend", "(", "[", "outfullfn", "]", ")", "f", "=", "open", "(", "infullfn", ",", "'w'", ")", "f", ".", "write", "(", "code", ")", "f", ".", "close", "(", ")", "try", ":", "self", ".", "builder", ".", "warn", "(", "ditaa_args", ")", "p", "=", "Popen", "(", "ditaa_args", ",", "stdout", "=", "PIPE", ",", "stdin", "=", "PIPE", ",", "stderr", "=", "PIPE", ")", "except", "OSError", ",", "err", ":", "if", "err", ".", "errno", "!=", "ENOENT", ":", "# No such file or directory", "raise", "self", ".", "builder", ".", "warn", "(", "'ditaa command %r cannot be run (needed for ditaa '", "'output), check the ditaa setting'", "%", "self", ".", "builder", ".", "config", ".", "ditaa", ")", "self", ".", "builder", ".", "_ditaa_warned_dot", "=", "True", "return", "None", ",", "None", "wentWrong", "=", "False", "try", ":", "# Ditaa may close standard input when an error occurs,", "# resulting in a broken pipe on communicate()", "stdout", ",", "stderr", "=", "p", ".", "communicate", "(", "code", ")", "except", "OSError", ",", "err", ":", "if", "err", ".", "errno", "!=", "EPIPE", ":", "raise", "wentWrong", "=", "True", "except", "IOError", ",", "err", ":", "if", "err", ".", "errno", "!=", "EINVAL", ":", "raise", "wentWrong", "=", "True", "if", "wentWrong", ":", "# in this case, read the standard output and standard error streams", "# directly, to get the error message(s)", "stdout", ",", "stderr", "=", "p", ".", "stdout", ".", "read", "(", ")", ",", "p", ".", "stderr", ".", "read", "(", ")", "p", ".", "wait", "(", ")", "if", "p", ".", "returncode", "!=", "0", ":", "raise", "DitaaError", "(", "'ditaa exited with error:\\n[stderr]\\n%s\\n'", "'[stdout]\\n%s'", "%", "(", "stderr", ",", "stdout", ")", ")", "return", "outrelfn", ",", "outfullfn"], "docstring": "Render ditaa code into a PNG output file.", "docstring_tokens": ["Render", "ditaa", "code", "into", "a", "PNG", "output", "file", "."], "sha": "a7d6cb9f1e9200dba597378cd40eb6a2096d4fd9", "url": "https://github.com/lvh/txampext/blob/a7d6cb9f1e9200dba597378cd40eb6a2096d4fd9/docs/_exts/ditaa.py#L96-L160", "partition": "test"}
{"repo": "respondcreate/django-versatileimagefield", "path": "versatileimagefield/fields.py", "func_name": "VersatileImageField.pre_save", "original_string": "def pre_save(self, model_instance, add):\n        \"\"\"Return field's value just before saving.\"\"\"\n        file = super(VersatileImageField, self).pre_save(model_instance, add)\n        self.update_ppoi_field(model_instance)\n        return file", "language": "python", "code": "def pre_save(self, model_instance, add):\n        \"\"\"Return field's value just before saving.\"\"\"\n        file = super(VersatileImageField, self).pre_save(model_instance, add)\n        self.update_ppoi_field(model_instance)\n        return file", "code_tokens": ["def", "pre_save", "(", "self", ",", "model_instance", ",", "add", ")", ":", "file", "=", "super", "(", "VersatileImageField", ",", "self", ")", ".", "pre_save", "(", "model_instance", ",", "add", ")", "self", ".", "update_ppoi_field", "(", "model_instance", ")", "return", "file"], "docstring": "Return field's value just before saving.", "docstring_tokens": ["Return", "field", "s", "value", "just", "before", "saving", "."], "sha": "d41e279c39cccffafbe876c67596184704ae8877", "url": "https://github.com/respondcreate/django-versatileimagefield/blob/d41e279c39cccffafbe876c67596184704ae8877/versatileimagefield/fields.py#L81-L85", "partition": "test"}
{"repo": "ioam/parambokeh", "path": "parambokeh/view.py", "func_name": "render_function", "original_string": "def render_function(obj, view):\n    \"\"\"\n    The default Renderer function which handles HoloViews objects.\n    \"\"\"\n    try:\n        import holoviews as hv\n    except:\n        hv = None\n\n    if hv and isinstance(obj, hv.core.Dimensioned):\n        renderer = hv.renderer('bokeh')\n        if not view._notebook:\n            renderer = renderer.instance(mode='server')\n        plot = renderer.get_plot(obj, doc=view._document)\n        if view._notebook:\n            plot.comm = view._comm\n        plot.document = view._document\n        return plot.state\n    return obj", "language": "python", "code": "def render_function(obj, view):\n    \"\"\"\n    The default Renderer function which handles HoloViews objects.\n    \"\"\"\n    try:\n        import holoviews as hv\n    except:\n        hv = None\n\n    if hv and isinstance(obj, hv.core.Dimensioned):\n        renderer = hv.renderer('bokeh')\n        if not view._notebook:\n            renderer = renderer.instance(mode='server')\n        plot = renderer.get_plot(obj, doc=view._document)\n        if view._notebook:\n            plot.comm = view._comm\n        plot.document = view._document\n        return plot.state\n    return obj", "code_tokens": ["def", "render_function", "(", "obj", ",", "view", ")", ":", "try", ":", "import", "holoviews", "as", "hv", "except", ":", "hv", "=", "None", "if", "hv", "and", "isinstance", "(", "obj", ",", "hv", ".", "core", ".", "Dimensioned", ")", ":", "renderer", "=", "hv", ".", "renderer", "(", "'bokeh'", ")", "if", "not", "view", ".", "_notebook", ":", "renderer", "=", "renderer", ".", "instance", "(", "mode", "=", "'server'", ")", "plot", "=", "renderer", ".", "get_plot", "(", "obj", ",", "doc", "=", "view", ".", "_document", ")", "if", "view", ".", "_notebook", ":", "plot", ".", "comm", "=", "view", ".", "_comm", "plot", ".", "document", "=", "view", ".", "_document", "return", "plot", ".", "state", "return", "obj"], "docstring": "The default Renderer function which handles HoloViews objects.", "docstring_tokens": ["The", "default", "Renderer", "function", "which", "handles", "HoloViews", "objects", "."], "sha": "fb9744f216273c7b24e65d037b1d621c08d7fde6", "url": "https://github.com/ioam/parambokeh/blob/fb9744f216273c7b24e65d037b1d621c08d7fde6/parambokeh/view.py#L3-L21", "partition": "test"}
{"repo": "abau171/highfive", "path": "highfive/jobs.py", "func_name": "JobManager.close", "original_string": "def close(self):\n        \"\"\"\n        Closes the job manager. No more jobs will be assigned, no more job sets\n        will be added, and any queued or active job sets will be cancelled.\n        \"\"\"\n\n        if self._closed:\n            return\n\n        self._closed = True\n        if self._active_js is not None:\n            self._active_js.cancel()\n        for js in self._js_queue:\n            js.cancel()", "language": "python", "code": "def close(self):\n        \"\"\"\n        Closes the job manager. No more jobs will be assigned, no more job sets\n        will be added, and any queued or active job sets will be cancelled.\n        \"\"\"\n\n        if self._closed:\n            return\n\n        self._closed = True\n        if self._active_js is not None:\n            self._active_js.cancel()\n        for js in self._js_queue:\n            js.cancel()", "code_tokens": ["def", "close", "(", "self", ")", ":", "if", "self", ".", "_closed", ":", "return", "self", ".", "_closed", "=", "True", "if", "self", ".", "_active_js", "is", "not", "None", ":", "self", ".", "_active_js", ".", "cancel", "(", ")", "for", "js", "in", "self", ".", "_js_queue", ":", "js", ".", "cancel", "(", ")"], "docstring": "Closes the job manager. No more jobs will be assigned, no more job sets\n        will be added, and any queued or active job sets will be cancelled.", "docstring_tokens": ["Closes", "the", "job", "manager", ".", "No", "more", "jobs", "will", "be", "assigned", "no", "more", "job", "sets", "will", "be", "added", "and", "any", "queued", "or", "active", "job", "sets", "will", "be", "cancelled", "."], "sha": "07b3829331072035ab100d1d66deca3e8f3f372a", "url": "https://github.com/abau171/highfive/blob/07b3829331072035ab100d1d66deca3e8f3f372a/highfive/jobs.py#L466-L479", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/core/oinspect.py", "func_name": "object_info", "original_string": "def object_info(**kw):\n    \"\"\"Make an object info dict with all fields present.\"\"\"\n    infodict = dict(izip_longest(info_fields, [None]))\n    infodict.update(kw)\n    return infodict", "language": "python", "code": "def object_info(**kw):\n    \"\"\"Make an object info dict with all fields present.\"\"\"\n    infodict = dict(izip_longest(info_fields, [None]))\n    infodict.update(kw)\n    return infodict", "code_tokens": ["def", "object_info", "(", "*", "*", "kw", ")", ":", "infodict", "=", "dict", "(", "izip_longest", "(", "info_fields", ",", "[", "None", "]", ")", ")", "infodict", ".", "update", "(", "kw", ")", "return", "infodict"], "docstring": "Make an object info dict with all fields present.", "docstring_tokens": ["Make", "an", "object", "info", "dict", "with", "all", "fields", "present", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/core/oinspect.py#L85-L89", "partition": "test"}
{"repo": "assemblerflow/flowcraft", "path": "flowcraft/generator/engine.py", "func_name": "NextflowGenerator._set_channels", "original_string": "def _set_channels(self):\n        \"\"\"Sets the main channels for the pipeline\n\n        This method will parse de the :attr:`~Process.processes` attribute\n        and perform the following tasks for each process:\n\n            - Sets the input/output channels and main input forks and adds\n              them to the process's\n              :attr:`flowcraft.process.Process._context`\n              attribute (See\n              :func:`~NextflowGenerator.set_channels`).\n            - Automatically updates the main input channel of the first\n              process of each lane so that they fork from the user provide\n              parameters (See\n              :func:`~NextflowGenerator._update_raw_input`).\n            - Check for the presence of secondary channels and adds them to the\n              :attr:`~NextflowGenerator.secondary_channels` attribute.\n\n        Notes\n        -----\n        **On the secondary channel setup**: With this approach, there can only\n        be one secondary link start for each type of secondary link. For\n        instance, If there are two processes that start a secondary channel\n        for the ``SIDE_max_len`` channel, only the last one will be recorded,\n        and all receiving processes will get the channel from the latest\n        process. Secondary channels can only link if the source process if\n        downstream of the sink process in its \"forking\" path.\n        \"\"\"\n\n        logger.debug(\"=====================\")\n        logger.debug(\"Setting main channels\")\n        logger.debug(\"=====================\")\n\n        for i, p in enumerate(self.processes):\n\n            # Set main channels for the process\n            logger.debug(\"[{}] Setting main channels with pid: {}\".format(\n                p.template, i))\n            p.set_channels(pid=i)\n\n            # If there is no parent lane, set the raw input channel from user\n            logger.debug(\"{} {} {}\".format(p.parent_lane, p.input_type, p.template))\n            if not p.parent_lane and p.input_type:\n                self._update_raw_input(p)\n\n            self._update_extra_inputs(p)\n\n            self._update_secondary_channels(p)\n\n            logger.info(colored_print(\n                \"\\tChannels set for {} \\u2713\".format(p.template)))", "language": "python", "code": "def _set_channels(self):\n        \"\"\"Sets the main channels for the pipeline\n\n        This method will parse de the :attr:`~Process.processes` attribute\n        and perform the following tasks for each process:\n\n            - Sets the input/output channels and main input forks and adds\n              them to the process's\n              :attr:`flowcraft.process.Process._context`\n              attribute (See\n              :func:`~NextflowGenerator.set_channels`).\n            - Automatically updates the main input channel of the first\n              process of each lane so that they fork from the user provide\n              parameters (See\n              :func:`~NextflowGenerator._update_raw_input`).\n            - Check for the presence of secondary channels and adds them to the\n              :attr:`~NextflowGenerator.secondary_channels` attribute.\n\n        Notes\n        -----\n        **On the secondary channel setup**: With this approach, there can only\n        be one secondary link start for each type of secondary link. For\n        instance, If there are two processes that start a secondary channel\n        for the ``SIDE_max_len`` channel, only the last one will be recorded,\n        and all receiving processes will get the channel from the latest\n        process. Secondary channels can only link if the source process if\n        downstream of the sink process in its \"forking\" path.\n        \"\"\"\n\n        logger.debug(\"=====================\")\n        logger.debug(\"Setting main channels\")\n        logger.debug(\"=====================\")\n\n        for i, p in enumerate(self.processes):\n\n            # Set main channels for the process\n            logger.debug(\"[{}] Setting main channels with pid: {}\".format(\n                p.template, i))\n            p.set_channels(pid=i)\n\n            # If there is no parent lane, set the raw input channel from user\n            logger.debug(\"{} {} {}\".format(p.parent_lane, p.input_type, p.template))\n            if not p.parent_lane and p.input_type:\n                self._update_raw_input(p)\n\n            self._update_extra_inputs(p)\n\n            self._update_secondary_channels(p)\n\n            logger.info(colored_print(\n                \"\\tChannels set for {} \\u2713\".format(p.template)))", "code_tokens": ["def", "_set_channels", "(", "self", ")", ":", "logger", ".", "debug", "(", "\"=====================\"", ")", "logger", ".", "debug", "(", "\"Setting main channels\"", ")", "logger", ".", "debug", "(", "\"=====================\"", ")", "for", "i", ",", "p", "in", "enumerate", "(", "self", ".", "processes", ")", ":", "# Set main channels for the process", "logger", ".", "debug", "(", "\"[{}] Setting main channels with pid: {}\"", ".", "format", "(", "p", ".", "template", ",", "i", ")", ")", "p", ".", "set_channels", "(", "pid", "=", "i", ")", "# If there is no parent lane, set the raw input channel from user", "logger", ".", "debug", "(", "\"{} {} {}\"", ".", "format", "(", "p", ".", "parent_lane", ",", "p", ".", "input_type", ",", "p", ".", "template", ")", ")", "if", "not", "p", ".", "parent_lane", "and", "p", ".", "input_type", ":", "self", ".", "_update_raw_input", "(", "p", ")", "self", ".", "_update_extra_inputs", "(", "p", ")", "self", ".", "_update_secondary_channels", "(", "p", ")", "logger", ".", "info", "(", "colored_print", "(", "\"\\tChannels set for {} \\u2713\"", ".", "format", "(", "p", ".", "template", ")", ")", ")"], "docstring": "Sets the main channels for the pipeline\n\n        This method will parse de the :attr:`~Process.processes` attribute\n        and perform the following tasks for each process:\n\n            - Sets the input/output channels and main input forks and adds\n              them to the process's\n              :attr:`flowcraft.process.Process._context`\n              attribute (See\n              :func:`~NextflowGenerator.set_channels`).\n            - Automatically updates the main input channel of the first\n              process of each lane so that they fork from the user provide\n              parameters (See\n              :func:`~NextflowGenerator._update_raw_input`).\n            - Check for the presence of secondary channels and adds them to the\n              :attr:`~NextflowGenerator.secondary_channels` attribute.\n\n        Notes\n        -----\n        **On the secondary channel setup**: With this approach, there can only\n        be one secondary link start for each type of secondary link. For\n        instance, If there are two processes that start a secondary channel\n        for the ``SIDE_max_len`` channel, only the last one will be recorded,\n        and all receiving processes will get the channel from the latest\n        process. Secondary channels can only link if the source process if\n        downstream of the sink process in its \"forking\" path.", "docstring_tokens": ["Sets", "the", "main", "channels", "for", "the", "pipeline"], "sha": "fc3f4bddded1efc76006600016dc71a06dd908c0", "url": "https://github.com/assemblerflow/flowcraft/blob/fc3f4bddded1efc76006600016dc71a06dd908c0/flowcraft/generator/engine.py#L759-L809", "partition": "test"}
{"repo": "h2oai/h2o-3", "path": "h2o-py/h2o/demos.py", "func_name": "gbm", "original_string": "def gbm(interactive=True, echo=True, testing=False):\n    \"\"\"GBM model demo.\"\"\"\n\n    def demo_body(go):\n        \"\"\"\n        Demo of H2O's Gradient Boosting estimator.\n\n        This demo uploads a dataset to h2o, parses it, and shows a description.\n        Then it divides the dataset into training and test sets, builds a GLM\n        from the training set, and makes predictions for the test set.\n        Finally, default performance metrics are displayed.\n        \"\"\"\n        go()\n        # Connect to H2O\n        h2o.init()\n\n        go()\n        # Upload the prostate dataset that comes included in the h2o python package\n        prostate = h2o.load_dataset(\"prostate\")\n\n        go()\n        # Print a description of the prostate data\n        prostate.describe()\n\n        go()\n        # Randomly split the dataset into ~70/30, training/test sets\n        train, test = prostate.split_frame(ratios=[0.70])\n\n        go()\n        # Convert the response columns to factors (for binary classification problems)\n        train[\"CAPSULE\"] = train[\"CAPSULE\"].asfactor()\n        test[\"CAPSULE\"] = test[\"CAPSULE\"].asfactor()\n\n        go()\n        # Build a (classification) GLM\n        from h2o.estimators import H2OGradientBoostingEstimator\n        prostate_gbm = H2OGradientBoostingEstimator(distribution=\"bernoulli\", ntrees=10, max_depth=8,\n                                                    min_rows=10, learn_rate=0.2)\n        prostate_gbm.train(x=[\"AGE\", \"RACE\", \"PSA\", \"VOL\", \"GLEASON\"],\n                           y=\"CAPSULE\", training_frame=train)\n\n        go()\n        # Show the model\n        prostate_gbm.show()\n\n        go()\n        # Predict on the test set and show the first ten predictions\n        predictions = prostate_gbm.predict(test)\n        predictions.show()\n\n        go()\n        # Fetch a tree, print number of tree nodes, show root node description\n        from h2o.tree import H2OTree, H2ONode\n        tree = H2OTree(prostate_gbm, 0, \"0\")\n        len(tree)\n        tree.left_children\n        tree.right_children\n        tree.root_node.show()\n\n        go()\n        # Show default performance metrics\n        performance = prostate_gbm.model_performance(test)\n        performance.show()\n\n    # Execute:\n    _run_demo(demo_body, interactive, echo, testing)", "language": "python", "code": "def gbm(interactive=True, echo=True, testing=False):\n    \"\"\"GBM model demo.\"\"\"\n\n    def demo_body(go):\n        \"\"\"\n        Demo of H2O's Gradient Boosting estimator.\n\n        This demo uploads a dataset to h2o, parses it, and shows a description.\n        Then it divides the dataset into training and test sets, builds a GLM\n        from the training set, and makes predictions for the test set.\n        Finally, default performance metrics are displayed.\n        \"\"\"\n        go()\n        # Connect to H2O\n        h2o.init()\n\n        go()\n        # Upload the prostate dataset that comes included in the h2o python package\n        prostate = h2o.load_dataset(\"prostate\")\n\n        go()\n        # Print a description of the prostate data\n        prostate.describe()\n\n        go()\n        # Randomly split the dataset into ~70/30, training/test sets\n        train, test = prostate.split_frame(ratios=[0.70])\n\n        go()\n        # Convert the response columns to factors (for binary classification problems)\n        train[\"CAPSULE\"] = train[\"CAPSULE\"].asfactor()\n        test[\"CAPSULE\"] = test[\"CAPSULE\"].asfactor()\n\n        go()\n        # Build a (classification) GLM\n        from h2o.estimators import H2OGradientBoostingEstimator\n        prostate_gbm = H2OGradientBoostingEstimator(distribution=\"bernoulli\", ntrees=10, max_depth=8,\n                                                    min_rows=10, learn_rate=0.2)\n        prostate_gbm.train(x=[\"AGE\", \"RACE\", \"PSA\", \"VOL\", \"GLEASON\"],\n                           y=\"CAPSULE\", training_frame=train)\n\n        go()\n        # Show the model\n        prostate_gbm.show()\n\n        go()\n        # Predict on the test set and show the first ten predictions\n        predictions = prostate_gbm.predict(test)\n        predictions.show()\n\n        go()\n        # Fetch a tree, print number of tree nodes, show root node description\n        from h2o.tree import H2OTree, H2ONode\n        tree = H2OTree(prostate_gbm, 0, \"0\")\n        len(tree)\n        tree.left_children\n        tree.right_children\n        tree.root_node.show()\n\n        go()\n        # Show default performance metrics\n        performance = prostate_gbm.model_performance(test)\n        performance.show()\n\n    # Execute:\n    _run_demo(demo_body, interactive, echo, testing)", "code_tokens": ["def", "gbm", "(", "interactive", "=", "True", ",", "echo", "=", "True", ",", "testing", "=", "False", ")", ":", "def", "demo_body", "(", "go", ")", ":", "\"\"\"\n        Demo of H2O's Gradient Boosting estimator.\n\n        This demo uploads a dataset to h2o, parses it, and shows a description.\n        Then it divides the dataset into training and test sets, builds a GLM\n        from the training set, and makes predictions for the test set.\n        Finally, default performance metrics are displayed.\n        \"\"\"", "go", "(", ")", "# Connect to H2O", "h2o", ".", "init", "(", ")", "go", "(", ")", "# Upload the prostate dataset that comes included in the h2o python package", "prostate", "=", "h2o", ".", "load_dataset", "(", "\"prostate\"", ")", "go", "(", ")", "# Print a description of the prostate data", "prostate", ".", "describe", "(", ")", "go", "(", ")", "# Randomly split the dataset into ~70/30, training/test sets", "train", ",", "test", "=", "prostate", ".", "split_frame", "(", "ratios", "=", "[", "0.70", "]", ")", "go", "(", ")", "# Convert the response columns to factors (for binary classification problems)", "train", "[", "\"CAPSULE\"", "]", "=", "train", "[", "\"CAPSULE\"", "]", ".", "asfactor", "(", ")", "test", "[", "\"CAPSULE\"", "]", "=", "test", "[", "\"CAPSULE\"", "]", ".", "asfactor", "(", ")", "go", "(", ")", "# Build a (classification) GLM", "from", "h2o", ".", "estimators", "import", "H2OGradientBoostingEstimator", "prostate_gbm", "=", "H2OGradientBoostingEstimator", "(", "distribution", "=", "\"bernoulli\"", ",", "ntrees", "=", "10", ",", "max_depth", "=", "8", ",", "min_rows", "=", "10", ",", "learn_rate", "=", "0.2", ")", "prostate_gbm", ".", "train", "(", "x", "=", "[", "\"AGE\"", ",", "\"RACE\"", ",", "\"PSA\"", ",", "\"VOL\"", ",", "\"GLEASON\"", "]", ",", "y", "=", "\"CAPSULE\"", ",", "training_frame", "=", "train", ")", "go", "(", ")", "# Show the model", "prostate_gbm", ".", "show", "(", ")", "go", "(", ")", "# Predict on the test set and show the first ten predictions", "predictions", "=", "prostate_gbm", ".", "predict", "(", "test", ")", "predictions", ".", "show", "(", ")", "go", "(", ")", "# Fetch a tree, print number of tree nodes, show root node description", "from", "h2o", ".", "tree", "import", "H2OTree", ",", "H2ONode", "tree", "=", "H2OTree", "(", "prostate_gbm", ",", "0", ",", "\"0\"", ")", "len", "(", "tree", ")", "tree", ".", "left_children", "tree", ".", "right_children", "tree", ".", "root_node", ".", "show", "(", ")", "go", "(", ")", "# Show default performance metrics", "performance", "=", "prostate_gbm", ".", "model_performance", "(", "test", ")", "performance", ".", "show", "(", ")", "# Execute:", "_run_demo", "(", "demo_body", ",", "interactive", ",", "echo", ",", "testing", ")"], "docstring": "GBM model demo.", "docstring_tokens": ["GBM", "model", "demo", "."], "sha": "dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8", "url": "https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/h2o-py/h2o/demos.py#L20-L85", "partition": "test"}
{"repo": "PolicyStat/docx2html", "path": "docx2html/core.py", "func_name": "get_relationship_info", "original_string": "def get_relationship_info(tree, media, image_sizes):\n    \"\"\"\n    There is a separate file holds the targets to links as well as the targets\n    for images. Return a dictionary based on the relationship id and the\n    target.\n    \"\"\"\n    if tree is None:\n        return {}\n    result = {}\n    # Loop through each relationship.\n    for el in tree.iter():\n        el_id = el.get('Id')\n        if el_id is None:\n            continue\n        # Store the target in the result dict.\n        target = el.get('Target')\n        if any(\n                target.lower().endswith(ext) for\n                ext in IMAGE_EXTENSIONS_TO_SKIP):\n            continue\n        if target in media:\n            image_size = image_sizes.get(el_id)\n            target = convert_image(media[target], image_size)\n        # cgi will replace things like & < > with &amp; &lt; &gt;\n        result[el_id] = cgi.escape(target)\n\n    return result", "language": "python", "code": "def get_relationship_info(tree, media, image_sizes):\n    \"\"\"\n    There is a separate file holds the targets to links as well as the targets\n    for images. Return a dictionary based on the relationship id and the\n    target.\n    \"\"\"\n    if tree is None:\n        return {}\n    result = {}\n    # Loop through each relationship.\n    for el in tree.iter():\n        el_id = el.get('Id')\n        if el_id is None:\n            continue\n        # Store the target in the result dict.\n        target = el.get('Target')\n        if any(\n                target.lower().endswith(ext) for\n                ext in IMAGE_EXTENSIONS_TO_SKIP):\n            continue\n        if target in media:\n            image_size = image_sizes.get(el_id)\n            target = convert_image(media[target], image_size)\n        # cgi will replace things like & < > with &amp; &lt; &gt;\n        result[el_id] = cgi.escape(target)\n\n    return result", "code_tokens": ["def", "get_relationship_info", "(", "tree", ",", "media", ",", "image_sizes", ")", ":", "if", "tree", "is", "None", ":", "return", "{", "}", "result", "=", "{", "}", "# Loop through each relationship.", "for", "el", "in", "tree", ".", "iter", "(", ")", ":", "el_id", "=", "el", ".", "get", "(", "'Id'", ")", "if", "el_id", "is", "None", ":", "continue", "# Store the target in the result dict.", "target", "=", "el", ".", "get", "(", "'Target'", ")", "if", "any", "(", "target", ".", "lower", "(", ")", ".", "endswith", "(", "ext", ")", "for", "ext", "in", "IMAGE_EXTENSIONS_TO_SKIP", ")", ":", "continue", "if", "target", "in", "media", ":", "image_size", "=", "image_sizes", ".", "get", "(", "el_id", ")", "target", "=", "convert_image", "(", "media", "[", "target", "]", ",", "image_size", ")", "# cgi will replace things like & < > with &amp; &lt; &gt;", "result", "[", "el_id", "]", "=", "cgi", ".", "escape", "(", "target", ")", "return", "result"], "docstring": "There is a separate file holds the targets to links as well as the targets\n    for images. Return a dictionary based on the relationship id and the\n    target.", "docstring_tokens": ["There", "is", "a", "separate", "file", "holds", "the", "targets", "to", "links", "as", "well", "as", "the", "targets", "for", "images", ".", "Return", "a", "dictionary", "based", "on", "the", "relationship", "id", "and", "the", "target", "."], "sha": "2dc4afd1e3a3f2f0b357d0bff903eb58bcc94429", "url": "https://github.com/PolicyStat/docx2html/blob/2dc4afd1e3a3f2f0b357d0bff903eb58bcc94429/docx2html/core.py#L752-L778", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/frontend/qt/console/console_widget.py", "func_name": "ConsoleWidget._insert_html_fetching_plain_text", "original_string": "def _insert_html_fetching_plain_text(self, cursor, html):\n        \"\"\" Inserts HTML using the specified cursor, then returns its plain text\n            version.\n        \"\"\"\n        cursor.beginEditBlock()\n        cursor.removeSelectedText()\n\n        start = cursor.position()\n        self._insert_html(cursor, html)\n        end = cursor.position()\n        cursor.setPosition(start, QtGui.QTextCursor.KeepAnchor)\n        text = cursor.selection().toPlainText()\n\n        cursor.setPosition(end)\n        cursor.endEditBlock()\n        return text", "language": "python", "code": "def _insert_html_fetching_plain_text(self, cursor, html):\n        \"\"\" Inserts HTML using the specified cursor, then returns its plain text\n            version.\n        \"\"\"\n        cursor.beginEditBlock()\n        cursor.removeSelectedText()\n\n        start = cursor.position()\n        self._insert_html(cursor, html)\n        end = cursor.position()\n        cursor.setPosition(start, QtGui.QTextCursor.KeepAnchor)\n        text = cursor.selection().toPlainText()\n\n        cursor.setPosition(end)\n        cursor.endEditBlock()\n        return text", "code_tokens": ["def", "_insert_html_fetching_plain_text", "(", "self", ",", "cursor", ",", "html", ")", ":", "cursor", ".", "beginEditBlock", "(", ")", "cursor", ".", "removeSelectedText", "(", ")", "start", "=", "cursor", ".", "position", "(", ")", "self", ".", "_insert_html", "(", "cursor", ",", "html", ")", "end", "=", "cursor", ".", "position", "(", ")", "cursor", ".", "setPosition", "(", "start", ",", "QtGui", ".", "QTextCursor", ".", "KeepAnchor", ")", "text", "=", "cursor", ".", "selection", "(", ")", ".", "toPlainText", "(", ")", "cursor", ".", "setPosition", "(", "end", ")", "cursor", ".", "endEditBlock", "(", ")", "return", "text"], "docstring": "Inserts HTML using the specified cursor, then returns its plain text\n            version.", "docstring_tokens": ["Inserts", "HTML", "using", "the", "specified", "cursor", "then", "returns", "its", "plain", "text", "version", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/frontend/qt/console/console_widget.py#L1569-L1584", "partition": "test"}
{"repo": "SmokinCaterpillar/pypet", "path": "pypet/storageservice.py", "func_name": "HDF5StorageService._all_extract_insert_dict", "original_string": "def _all_extract_insert_dict(self, item, colnames, additional_info=None):\n        \"\"\"Extracts information from a given item to be stored into a pytable row.\n\n        Items can be a variety of things here, trajectories, single runs, group node,\n        parameters, results.\n\n        :param item: Item from which data should be extracted\n\n        :param colnames: Names of the columns in the pytable\n\n        :param additional_info: (dict)\n\n            Additional information that should be stored into the pytable row that cannot be\n            read out from `item`.\n\n        :return: Dictionary containing the data to be inserted into a row\n\n        \"\"\"\n        insert_dict = {}\n\n        if 'length' in colnames:\n            insert_dict['length'] = len(item)\n\n        if 'comment' in colnames:\n            comment = self._all_cut_string(item.v_comment.encode('utf-8'),\n                                           pypetconstants.HDF5_STRCOL_MAX_COMMENT_LENGTH,\n                                           self._logger)\n\n            insert_dict['comment'] = comment\n\n        if 'location' in colnames:\n            insert_dict['location'] = item.v_location.encode('utf-8')\n\n        if 'name' in colnames:\n            name = item._name if (not item.v_is_root or not item.v_is_run) else item._crun\n            insert_dict['name'] = name.encode('utf-8')\n\n        if 'class_name' in colnames:\n            insert_dict['class_name'] = item.f_get_class_name().encode('utf-8')\n\n        if 'value' in colnames:\n            insert_dict['value'] = self._all_cut_string(\n                item.f_val_to_str().encode('utf-8'),\n                pypetconstants.HDF5_STRCOL_MAX_VALUE_LENGTH,\n                self._logger)\n\n        if 'hexdigest' in colnames:\n            insert_dict['hexdigest'] = additional_info['hexdigest']\n\n        if 'idx' in colnames:\n            insert_dict['idx'] = item.v_idx\n\n        if 'time' in colnames:\n            time_ = item._time\n            insert_dict['time'] = time_.encode('utf-8')\n\n        if 'timestamp' in colnames:\n            timestamp = item._timestamp\n            insert_dict['timestamp'] = timestamp\n\n        if 'range' in colnames:\n            third_length = pypetconstants.HDF5_STRCOL_MAX_RANGE_LENGTH // 3 + 10\n            item_range = itools.islice(item.f_get_range(copy=False), 0, third_length)\n            range_string = ', '.join([repr(x) for x in item_range])\n            insert_dict['range'] = self._all_cut_string(\n                range_string.encode('utf-8'),\n                pypetconstants.HDF5_STRCOL_MAX_RANGE_LENGTH,\n                self._logger)\n\n        # To allow backwards compatibility\n        if 'array' in colnames:\n            third_length = pypetconstants.HDF5_STRCOL_MAX_RANGE_LENGTH // 3 + 10\n            item_range = itools.islice(item.f_get_range(copy=False), 0, third_length)\n            range_string = ', '.join([repr(x) for x in item_range])\n            insert_dict['array'] = self._all_cut_string(\n                range_string.encode('utf-8'),\n                pypetconstants.HDF5_STRCOL_MAX_RANGE_LENGTH,\n                self._logger)\n\n        if 'version' in colnames:\n            insert_dict['version'] = item.v_version.encode('utf-8')\n\n        if 'python' in colnames:\n            insert_dict['python'] = item.v_python.encode('utf-8')\n\n        if 'finish_timestamp' in colnames:\n            insert_dict['finish_timestamp'] = item._finish_timestamp_run\n\n        return insert_dict", "language": "python", "code": "def _all_extract_insert_dict(self, item, colnames, additional_info=None):\n        \"\"\"Extracts information from a given item to be stored into a pytable row.\n\n        Items can be a variety of things here, trajectories, single runs, group node,\n        parameters, results.\n\n        :param item: Item from which data should be extracted\n\n        :param colnames: Names of the columns in the pytable\n\n        :param additional_info: (dict)\n\n            Additional information that should be stored into the pytable row that cannot be\n            read out from `item`.\n\n        :return: Dictionary containing the data to be inserted into a row\n\n        \"\"\"\n        insert_dict = {}\n\n        if 'length' in colnames:\n            insert_dict['length'] = len(item)\n\n        if 'comment' in colnames:\n            comment = self._all_cut_string(item.v_comment.encode('utf-8'),\n                                           pypetconstants.HDF5_STRCOL_MAX_COMMENT_LENGTH,\n                                           self._logger)\n\n            insert_dict['comment'] = comment\n\n        if 'location' in colnames:\n            insert_dict['location'] = item.v_location.encode('utf-8')\n\n        if 'name' in colnames:\n            name = item._name if (not item.v_is_root or not item.v_is_run) else item._crun\n            insert_dict['name'] = name.encode('utf-8')\n\n        if 'class_name' in colnames:\n            insert_dict['class_name'] = item.f_get_class_name().encode('utf-8')\n\n        if 'value' in colnames:\n            insert_dict['value'] = self._all_cut_string(\n                item.f_val_to_str().encode('utf-8'),\n                pypetconstants.HDF5_STRCOL_MAX_VALUE_LENGTH,\n                self._logger)\n\n        if 'hexdigest' in colnames:\n            insert_dict['hexdigest'] = additional_info['hexdigest']\n\n        if 'idx' in colnames:\n            insert_dict['idx'] = item.v_idx\n\n        if 'time' in colnames:\n            time_ = item._time\n            insert_dict['time'] = time_.encode('utf-8')\n\n        if 'timestamp' in colnames:\n            timestamp = item._timestamp\n            insert_dict['timestamp'] = timestamp\n\n        if 'range' in colnames:\n            third_length = pypetconstants.HDF5_STRCOL_MAX_RANGE_LENGTH // 3 + 10\n            item_range = itools.islice(item.f_get_range(copy=False), 0, third_length)\n            range_string = ', '.join([repr(x) for x in item_range])\n            insert_dict['range'] = self._all_cut_string(\n                range_string.encode('utf-8'),\n                pypetconstants.HDF5_STRCOL_MAX_RANGE_LENGTH,\n                self._logger)\n\n        # To allow backwards compatibility\n        if 'array' in colnames:\n            third_length = pypetconstants.HDF5_STRCOL_MAX_RANGE_LENGTH // 3 + 10\n            item_range = itools.islice(item.f_get_range(copy=False), 0, third_length)\n            range_string = ', '.join([repr(x) for x in item_range])\n            insert_dict['array'] = self._all_cut_string(\n                range_string.encode('utf-8'),\n                pypetconstants.HDF5_STRCOL_MAX_RANGE_LENGTH,\n                self._logger)\n\n        if 'version' in colnames:\n            insert_dict['version'] = item.v_version.encode('utf-8')\n\n        if 'python' in colnames:\n            insert_dict['python'] = item.v_python.encode('utf-8')\n\n        if 'finish_timestamp' in colnames:\n            insert_dict['finish_timestamp'] = item._finish_timestamp_run\n\n        return insert_dict", "code_tokens": ["def", "_all_extract_insert_dict", "(", "self", ",", "item", ",", "colnames", ",", "additional_info", "=", "None", ")", ":", "insert_dict", "=", "{", "}", "if", "'length'", "in", "colnames", ":", "insert_dict", "[", "'length'", "]", "=", "len", "(", "item", ")", "if", "'comment'", "in", "colnames", ":", "comment", "=", "self", ".", "_all_cut_string", "(", "item", ".", "v_comment", ".", "encode", "(", "'utf-8'", ")", ",", "pypetconstants", ".", "HDF5_STRCOL_MAX_COMMENT_LENGTH", ",", "self", ".", "_logger", ")", "insert_dict", "[", "'comment'", "]", "=", "comment", "if", "'location'", "in", "colnames", ":", "insert_dict", "[", "'location'", "]", "=", "item", ".", "v_location", ".", "encode", "(", "'utf-8'", ")", "if", "'name'", "in", "colnames", ":", "name", "=", "item", ".", "_name", "if", "(", "not", "item", ".", "v_is_root", "or", "not", "item", ".", "v_is_run", ")", "else", "item", ".", "_crun", "insert_dict", "[", "'name'", "]", "=", "name", ".", "encode", "(", "'utf-8'", ")", "if", "'class_name'", "in", "colnames", ":", "insert_dict", "[", "'class_name'", "]", "=", "item", ".", "f_get_class_name", "(", ")", ".", "encode", "(", "'utf-8'", ")", "if", "'value'", "in", "colnames", ":", "insert_dict", "[", "'value'", "]", "=", "self", ".", "_all_cut_string", "(", "item", ".", "f_val_to_str", "(", ")", ".", "encode", "(", "'utf-8'", ")", ",", "pypetconstants", ".", "HDF5_STRCOL_MAX_VALUE_LENGTH", ",", "self", ".", "_logger", ")", "if", "'hexdigest'", "in", "colnames", ":", "insert_dict", "[", "'hexdigest'", "]", "=", "additional_info", "[", "'hexdigest'", "]", "if", "'idx'", "in", "colnames", ":", "insert_dict", "[", "'idx'", "]", "=", "item", ".", "v_idx", "if", "'time'", "in", "colnames", ":", "time_", "=", "item", ".", "_time", "insert_dict", "[", "'time'", "]", "=", "time_", ".", "encode", "(", "'utf-8'", ")", "if", "'timestamp'", "in", "colnames", ":", "timestamp", "=", "item", ".", "_timestamp", "insert_dict", "[", "'timestamp'", "]", "=", "timestamp", "if", "'range'", "in", "colnames", ":", "third_length", "=", "pypetconstants", ".", "HDF5_STRCOL_MAX_RANGE_LENGTH", "//", "3", "+", "10", "item_range", "=", "itools", ".", "islice", "(", "item", ".", "f_get_range", "(", "copy", "=", "False", ")", ",", "0", ",", "third_length", ")", "range_string", "=", "', '", ".", "join", "(", "[", "repr", "(", "x", ")", "for", "x", "in", "item_range", "]", ")", "insert_dict", "[", "'range'", "]", "=", "self", ".", "_all_cut_string", "(", "range_string", ".", "encode", "(", "'utf-8'", ")", ",", "pypetconstants", ".", "HDF5_STRCOL_MAX_RANGE_LENGTH", ",", "self", ".", "_logger", ")", "# To allow backwards compatibility", "if", "'array'", "in", "colnames", ":", "third_length", "=", "pypetconstants", ".", "HDF5_STRCOL_MAX_RANGE_LENGTH", "//", "3", "+", "10", "item_range", "=", "itools", ".", "islice", "(", "item", ".", "f_get_range", "(", "copy", "=", "False", ")", ",", "0", ",", "third_length", ")", "range_string", "=", "', '", ".", "join", "(", "[", "repr", "(", "x", ")", "for", "x", "in", "item_range", "]", ")", "insert_dict", "[", "'array'", "]", "=", "self", ".", "_all_cut_string", "(", "range_string", ".", "encode", "(", "'utf-8'", ")", ",", "pypetconstants", ".", "HDF5_STRCOL_MAX_RANGE_LENGTH", ",", "self", ".", "_logger", ")", "if", "'version'", "in", "colnames", ":", "insert_dict", "[", "'version'", "]", "=", "item", ".", "v_version", ".", "encode", "(", "'utf-8'", ")", "if", "'python'", "in", "colnames", ":", "insert_dict", "[", "'python'", "]", "=", "item", ".", "v_python", ".", "encode", "(", "'utf-8'", ")", "if", "'finish_timestamp'", "in", "colnames", ":", "insert_dict", "[", "'finish_timestamp'", "]", "=", "item", ".", "_finish_timestamp_run", "return", "insert_dict"], "docstring": "Extracts information from a given item to be stored into a pytable row.\n\n        Items can be a variety of things here, trajectories, single runs, group node,\n        parameters, results.\n\n        :param item: Item from which data should be extracted\n\n        :param colnames: Names of the columns in the pytable\n\n        :param additional_info: (dict)\n\n            Additional information that should be stored into the pytable row that cannot be\n            read out from `item`.\n\n        :return: Dictionary containing the data to be inserted into a row", "docstring_tokens": ["Extracts", "information", "from", "a", "given", "item", "to", "be", "stored", "into", "a", "pytable", "row", "."], "sha": "97ad3e80d46dbdea02deeb98ea41f05a19565826", "url": "https://github.com/SmokinCaterpillar/pypet/blob/97ad3e80d46dbdea02deeb98ea41f05a19565826/pypet/storageservice.py#L3326-L3414", "partition": "test"}
{"repo": "PolicyStat/docx2html", "path": "docx2html/core.py", "func_name": "_get_document_data", "original_string": "def _get_document_data(f, image_handler=None):\n    '''\n    ``f`` is a ``ZipFile`` that is open\n    Extract out the document data, numbering data and the relationship data.\n    '''\n    if image_handler is None:\n        def image_handler(image_id, relationship_dict):\n            return relationship_dict.get(image_id)\n\n    document_xml = None\n    numbering_xml = None\n    relationship_xml = None\n    styles_xml = None\n    parser = etree.XMLParser(strip_cdata=False)\n    path, _ = os.path.split(f.filename)\n    media = {}\n    image_sizes = {}\n    # Loop through the files in the zip file.\n    for item in f.infolist():\n        # This file holds all the content of the document.\n        if item.filename == 'word/document.xml':\n            xml = f.read(item.filename)\n            document_xml = etree.fromstring(xml, parser)\n        # This file tells document.xml how lists should look.\n        elif item.filename == 'word/numbering.xml':\n            xml = f.read(item.filename)\n            numbering_xml = etree.fromstring(xml, parser)\n        elif item.filename == 'word/styles.xml':\n            xml = f.read(item.filename)\n            styles_xml = etree.fromstring(xml, parser)\n        # This file holds the targets for hyperlinks and images.\n        elif item.filename == 'word/_rels/document.xml.rels':\n            xml = f.read(item.filename)\n            try:\n                relationship_xml = etree.fromstring(xml, parser)\n            except XMLSyntaxError:\n                relationship_xml = etree.fromstring('<xml></xml>', parser)\n        if item.filename.startswith('word/media/'):\n            # Strip off the leading word/\n            media[item.filename[len('word/'):]] = f.extract(\n                item.filename,\n                path,\n            )\n    # Close the file pointer.\n    f.close()\n\n    # Get dictionaries for the numbering and the relationships.\n    numbering_dict = get_numbering_info(numbering_xml)\n    image_sizes = get_image_sizes(document_xml)\n    relationship_dict = get_relationship_info(\n        relationship_xml,\n        media,\n        image_sizes\n    )\n    styles_dict = get_style_dict(styles_xml)\n    font_sizes_dict = defaultdict(int)\n    if DETECT_FONT_SIZE:\n        font_sizes_dict = get_font_sizes_dict(document_xml, styles_dict)\n    meta_data = MetaData(\n        numbering_dict=numbering_dict,\n        relationship_dict=relationship_dict,\n        styles_dict=styles_dict,\n        font_sizes_dict=font_sizes_dict,\n        image_handler=image_handler,\n        image_sizes=image_sizes,\n    )\n    return document_xml, meta_data", "language": "python", "code": "def _get_document_data(f, image_handler=None):\n    '''\n    ``f`` is a ``ZipFile`` that is open\n    Extract out the document data, numbering data and the relationship data.\n    '''\n    if image_handler is None:\n        def image_handler(image_id, relationship_dict):\n            return relationship_dict.get(image_id)\n\n    document_xml = None\n    numbering_xml = None\n    relationship_xml = None\n    styles_xml = None\n    parser = etree.XMLParser(strip_cdata=False)\n    path, _ = os.path.split(f.filename)\n    media = {}\n    image_sizes = {}\n    # Loop through the files in the zip file.\n    for item in f.infolist():\n        # This file holds all the content of the document.\n        if item.filename == 'word/document.xml':\n            xml = f.read(item.filename)\n            document_xml = etree.fromstring(xml, parser)\n        # This file tells document.xml how lists should look.\n        elif item.filename == 'word/numbering.xml':\n            xml = f.read(item.filename)\n            numbering_xml = etree.fromstring(xml, parser)\n        elif item.filename == 'word/styles.xml':\n            xml = f.read(item.filename)\n            styles_xml = etree.fromstring(xml, parser)\n        # This file holds the targets for hyperlinks and images.\n        elif item.filename == 'word/_rels/document.xml.rels':\n            xml = f.read(item.filename)\n            try:\n                relationship_xml = etree.fromstring(xml, parser)\n            except XMLSyntaxError:\n                relationship_xml = etree.fromstring('<xml></xml>', parser)\n        if item.filename.startswith('word/media/'):\n            # Strip off the leading word/\n            media[item.filename[len('word/'):]] = f.extract(\n                item.filename,\n                path,\n            )\n    # Close the file pointer.\n    f.close()\n\n    # Get dictionaries for the numbering and the relationships.\n    numbering_dict = get_numbering_info(numbering_xml)\n    image_sizes = get_image_sizes(document_xml)\n    relationship_dict = get_relationship_info(\n        relationship_xml,\n        media,\n        image_sizes\n    )\n    styles_dict = get_style_dict(styles_xml)\n    font_sizes_dict = defaultdict(int)\n    if DETECT_FONT_SIZE:\n        font_sizes_dict = get_font_sizes_dict(document_xml, styles_dict)\n    meta_data = MetaData(\n        numbering_dict=numbering_dict,\n        relationship_dict=relationship_dict,\n        styles_dict=styles_dict,\n        font_sizes_dict=font_sizes_dict,\n        image_handler=image_handler,\n        image_sizes=image_sizes,\n    )\n    return document_xml, meta_data", "code_tokens": ["def", "_get_document_data", "(", "f", ",", "image_handler", "=", "None", ")", ":", "if", "image_handler", "is", "None", ":", "def", "image_handler", "(", "image_id", ",", "relationship_dict", ")", ":", "return", "relationship_dict", ".", "get", "(", "image_id", ")", "document_xml", "=", "None", "numbering_xml", "=", "None", "relationship_xml", "=", "None", "styles_xml", "=", "None", "parser", "=", "etree", ".", "XMLParser", "(", "strip_cdata", "=", "False", ")", "path", ",", "_", "=", "os", ".", "path", ".", "split", "(", "f", ".", "filename", ")", "media", "=", "{", "}", "image_sizes", "=", "{", "}", "# Loop through the files in the zip file.", "for", "item", "in", "f", ".", "infolist", "(", ")", ":", "# This file holds all the content of the document.", "if", "item", ".", "filename", "==", "'word/document.xml'", ":", "xml", "=", "f", ".", "read", "(", "item", ".", "filename", ")", "document_xml", "=", "etree", ".", "fromstring", "(", "xml", ",", "parser", ")", "# This file tells document.xml how lists should look.", "elif", "item", ".", "filename", "==", "'word/numbering.xml'", ":", "xml", "=", "f", ".", "read", "(", "item", ".", "filename", ")", "numbering_xml", "=", "etree", ".", "fromstring", "(", "xml", ",", "parser", ")", "elif", "item", ".", "filename", "==", "'word/styles.xml'", ":", "xml", "=", "f", ".", "read", "(", "item", ".", "filename", ")", "styles_xml", "=", "etree", ".", "fromstring", "(", "xml", ",", "parser", ")", "# This file holds the targets for hyperlinks and images.", "elif", "item", ".", "filename", "==", "'word/_rels/document.xml.rels'", ":", "xml", "=", "f", ".", "read", "(", "item", ".", "filename", ")", "try", ":", "relationship_xml", "=", "etree", ".", "fromstring", "(", "xml", ",", "parser", ")", "except", "XMLSyntaxError", ":", "relationship_xml", "=", "etree", ".", "fromstring", "(", "'<xml></xml>'", ",", "parser", ")", "if", "item", ".", "filename", ".", "startswith", "(", "'word/media/'", ")", ":", "# Strip off the leading word/", "media", "[", "item", ".", "filename", "[", "len", "(", "'word/'", ")", ":", "]", "]", "=", "f", ".", "extract", "(", "item", ".", "filename", ",", "path", ",", ")", "# Close the file pointer.", "f", ".", "close", "(", ")", "# Get dictionaries for the numbering and the relationships.", "numbering_dict", "=", "get_numbering_info", "(", "numbering_xml", ")", "image_sizes", "=", "get_image_sizes", "(", "document_xml", ")", "relationship_dict", "=", "get_relationship_info", "(", "relationship_xml", ",", "media", ",", "image_sizes", ")", "styles_dict", "=", "get_style_dict", "(", "styles_xml", ")", "font_sizes_dict", "=", "defaultdict", "(", "int", ")", "if", "DETECT_FONT_SIZE", ":", "font_sizes_dict", "=", "get_font_sizes_dict", "(", "document_xml", ",", "styles_dict", ")", "meta_data", "=", "MetaData", "(", "numbering_dict", "=", "numbering_dict", ",", "relationship_dict", "=", "relationship_dict", ",", "styles_dict", "=", "styles_dict", ",", "font_sizes_dict", "=", "font_sizes_dict", ",", "image_handler", "=", "image_handler", ",", "image_sizes", "=", "image_sizes", ",", ")", "return", "document_xml", ",", "meta_data"], "docstring": "``f`` is a ``ZipFile`` that is open\n    Extract out the document data, numbering data and the relationship data.", "docstring_tokens": ["f", "is", "a", "ZipFile", "that", "is", "open", "Extract", "out", "the", "document", "data", "numbering", "data", "and", "the", "relationship", "data", "."], "sha": "2dc4afd1e3a3f2f0b357d0bff903eb58bcc94429", "url": "https://github.com/PolicyStat/docx2html/blob/2dc4afd1e3a3f2f0b357d0bff903eb58bcc94429/docx2html/core.py#L816-L882", "partition": "test"}
{"repo": "pyca/pyopenssl", "path": "src/OpenSSL/SSL.py", "func_name": "Connection.get_servername", "original_string": "def get_servername(self):\n        \"\"\"\n        Retrieve the servername extension value if provided in the client hello\n        message, or None if there wasn't one.\n\n        :return: A byte string giving the server name or :data:`None`.\n\n        .. versionadded:: 0.13\n        \"\"\"\n        name = _lib.SSL_get_servername(\n            self._ssl, _lib.TLSEXT_NAMETYPE_host_name\n        )\n        if name == _ffi.NULL:\n            return None\n\n        return _ffi.string(name)", "language": "python", "code": "def get_servername(self):\n        \"\"\"\n        Retrieve the servername extension value if provided in the client hello\n        message, or None if there wasn't one.\n\n        :return: A byte string giving the server name or :data:`None`.\n\n        .. versionadded:: 0.13\n        \"\"\"\n        name = _lib.SSL_get_servername(\n            self._ssl, _lib.TLSEXT_NAMETYPE_host_name\n        )\n        if name == _ffi.NULL:\n            return None\n\n        return _ffi.string(name)", "code_tokens": ["def", "get_servername", "(", "self", ")", ":", "name", "=", "_lib", ".", "SSL_get_servername", "(", "self", ".", "_ssl", ",", "_lib", ".", "TLSEXT_NAMETYPE_host_name", ")", "if", "name", "==", "_ffi", ".", "NULL", ":", "return", "None", "return", "_ffi", ".", "string", "(", "name", ")"], "docstring": "Retrieve the servername extension value if provided in the client hello\n        message, or None if there wasn't one.\n\n        :return: A byte string giving the server name or :data:`None`.\n\n        .. versionadded:: 0.13", "docstring_tokens": ["Retrieve", "the", "servername", "extension", "value", "if", "provided", "in", "the", "client", "hello", "message", "or", "None", "if", "there", "wasn", "t", "one", "."], "sha": "1fbe064c50fd030948141d7d630673761525b0d0", "url": "https://github.com/pyca/pyopenssl/blob/1fbe064c50fd030948141d7d630673761525b0d0/src/OpenSSL/SSL.py#L1672-L1687", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/lib/inputhook.py", "func_name": "InputHookManager.enable_gtk", "original_string": "def enable_gtk(self, app=None):\n        \"\"\"Enable event loop integration with PyGTK.\n\n        Parameters\n        ----------\n        app : ignored\n           Ignored, it's only a placeholder to keep the call signature of all\n           gui activation methods consistent, which simplifies the logic of\n           supporting magics.\n\n        Notes\n        -----\n        This methods sets the PyOS_InputHook for PyGTK, which allows\n        the PyGTK to integrate with terminal based applications like\n        IPython.\n        \"\"\"\n        import gtk\n        try:\n            gtk.set_interactive(True)\n            self._current_gui = GUI_GTK\n        except AttributeError:\n            # For older versions of gtk, use our own ctypes version\n            from IPython.lib.inputhookgtk import inputhook_gtk\n            self.set_inputhook(inputhook_gtk)\n            self._current_gui = GUI_GTK", "language": "python", "code": "def enable_gtk(self, app=None):\n        \"\"\"Enable event loop integration with PyGTK.\n\n        Parameters\n        ----------\n        app : ignored\n           Ignored, it's only a placeholder to keep the call signature of all\n           gui activation methods consistent, which simplifies the logic of\n           supporting magics.\n\n        Notes\n        -----\n        This methods sets the PyOS_InputHook for PyGTK, which allows\n        the PyGTK to integrate with terminal based applications like\n        IPython.\n        \"\"\"\n        import gtk\n        try:\n            gtk.set_interactive(True)\n            self._current_gui = GUI_GTK\n        except AttributeError:\n            # For older versions of gtk, use our own ctypes version\n            from IPython.lib.inputhookgtk import inputhook_gtk\n            self.set_inputhook(inputhook_gtk)\n            self._current_gui = GUI_GTK", "code_tokens": ["def", "enable_gtk", "(", "self", ",", "app", "=", "None", ")", ":", "import", "gtk", "try", ":", "gtk", ".", "set_interactive", "(", "True", ")", "self", ".", "_current_gui", "=", "GUI_GTK", "except", "AttributeError", ":", "# For older versions of gtk, use our own ctypes version", "from", "IPython", ".", "lib", ".", "inputhookgtk", "import", "inputhook_gtk", "self", ".", "set_inputhook", "(", "inputhook_gtk", ")", "self", ".", "_current_gui", "=", "GUI_GTK"], "docstring": "Enable event loop integration with PyGTK.\n\n        Parameters\n        ----------\n        app : ignored\n           Ignored, it's only a placeholder to keep the call signature of all\n           gui activation methods consistent, which simplifies the logic of\n           supporting magics.\n\n        Notes\n        -----\n        This methods sets the PyOS_InputHook for PyGTK, which allows\n        the PyGTK to integrate with terminal based applications like\n        IPython.", "docstring_tokens": ["Enable", "event", "loop", "integration", "with", "PyGTK", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/lib/inputhook.py#L272-L296", "partition": "test"}
{"repo": "RRZE-HPC/kerncraft", "path": "kerncraft/kernel.py", "func_name": "Kernel.compile_sympy_accesses", "original_string": "def compile_sympy_accesses(self, sources=True, destinations=True):\n        \"\"\"\n        Return a dictionary of lists of sympy accesses, for each variable.\n\n        Use *source* and *destination* to filter output\n        \"\"\"\n        sympy_accesses = defaultdict(list)\n        # Compile sympy accesses\n        for var_name in self.variables:\n            if sources:\n                for r in self.sources.get(var_name, []):\n                    if r is None:\n                        continue\n                    sympy_accesses[var_name].append(self.access_to_sympy(var_name, r))\n            if destinations:\n                for w in self.destinations.get(var_name, []):\n                    if w is None:\n                        continue\n                    sympy_accesses[var_name].append(self.access_to_sympy(var_name, w))\n\n        return sympy_accesses", "language": "python", "code": "def compile_sympy_accesses(self, sources=True, destinations=True):\n        \"\"\"\n        Return a dictionary of lists of sympy accesses, for each variable.\n\n        Use *source* and *destination* to filter output\n        \"\"\"\n        sympy_accesses = defaultdict(list)\n        # Compile sympy accesses\n        for var_name in self.variables:\n            if sources:\n                for r in self.sources.get(var_name, []):\n                    if r is None:\n                        continue\n                    sympy_accesses[var_name].append(self.access_to_sympy(var_name, r))\n            if destinations:\n                for w in self.destinations.get(var_name, []):\n                    if w is None:\n                        continue\n                    sympy_accesses[var_name].append(self.access_to_sympy(var_name, w))\n\n        return sympy_accesses", "code_tokens": ["def", "compile_sympy_accesses", "(", "self", ",", "sources", "=", "True", ",", "destinations", "=", "True", ")", ":", "sympy_accesses", "=", "defaultdict", "(", "list", ")", "# Compile sympy accesses", "for", "var_name", "in", "self", ".", "variables", ":", "if", "sources", ":", "for", "r", "in", "self", ".", "sources", ".", "get", "(", "var_name", ",", "[", "]", ")", ":", "if", "r", "is", "None", ":", "continue", "sympy_accesses", "[", "var_name", "]", ".", "append", "(", "self", ".", "access_to_sympy", "(", "var_name", ",", "r", ")", ")", "if", "destinations", ":", "for", "w", "in", "self", ".", "destinations", ".", "get", "(", "var_name", ",", "[", "]", ")", ":", "if", "w", "is", "None", ":", "continue", "sympy_accesses", "[", "var_name", "]", ".", "append", "(", "self", ".", "access_to_sympy", "(", "var_name", ",", "w", ")", ")", "return", "sympy_accesses"], "docstring": "Return a dictionary of lists of sympy accesses, for each variable.\n\n        Use *source* and *destination* to filter output", "docstring_tokens": ["Return", "a", "dictionary", "of", "lists", "of", "sympy", "accesses", "for", "each", "variable", "."], "sha": "c60baf8043e4da8d8d66da7575021c2f4c6c78af", "url": "https://github.com/RRZE-HPC/kerncraft/blob/c60baf8043e4da8d8d66da7575021c2f4c6c78af/kerncraft/kernel.py#L377-L397", "partition": "test"}
{"repo": "ekmmetering/ekmmeters", "path": "ekmmeters.py", "func_name": "Meter.setSeasonSchedules", "original_string": "def setSeasonSchedules(self, cmd_dict=None, password=\"00000000\"):\n        \"\"\" Serial command to set seasons table.\n\n        If no dictionary is passed, the meter object buffer is used.\n\n        Args:\n            cmd_dict (dict): Optional dictionary of season schedules.\n            password (str): Optional password\n\n        Returns:\n            bool: True on completion and ACK.\n        \"\"\"\n        result = False\n        self.setContext(\"setSeasonSchedules\")\n\n        if not cmd_dict:\n            cmd_dict = self.m_seasons_sched_params\n\n        try:\n            if not self.request(False):\n                self.writeCmdMsg(\"Bad read CRC on setting\")\n            else:\n                if not self.serialCmdPwdAuth(password):\n                    self.writeCmdMsg(\"Password failure\")\n                else:\n                    req_table = \"\"\n                    req_table += binascii.hexlify(str(cmd_dict[\"Season_1_Start_Month\"]).zfill(2))\n                    req_table += binascii.hexlify(str(cmd_dict[\"Season_1_Start_Day\"]).zfill(2))\n                    req_table += binascii.hexlify(str(cmd_dict[\"Season_1_Schedule\"]).zfill(2))\n                    req_table += binascii.hexlify(str(cmd_dict[\"Season_2_Start_Month\"]).zfill(2))\n                    req_table += binascii.hexlify(str(cmd_dict[\"Season_2_Start_Day\"]).zfill(2))\n                    req_table += binascii.hexlify(str(cmd_dict[\"Season_2_Schedule\"]).zfill(2))\n                    req_table += binascii.hexlify(str(cmd_dict[\"Season_3_Start_Month\"]).zfill(2))\n                    req_table += binascii.hexlify(str(cmd_dict[\"Season_3_Start_Day\"]).zfill(2))\n                    req_table += binascii.hexlify(str(cmd_dict[\"Season_3_Schedule\"]).zfill(2))\n                    req_table += binascii.hexlify(str(cmd_dict[\"Season_4_Start_Month\"]).zfill(2))\n                    req_table += binascii.hexlify(str(cmd_dict[\"Season_4_Start_Day\"]).zfill(2))\n                    req_table += binascii.hexlify(str(cmd_dict[\"Season_4_Schedule\"]).zfill(2))\n                    req_table += binascii.hexlify(str(0).zfill(24))\n                    req_str = \"015731023030383028\" + req_table + \"2903\"\n                    req_str += self.calc_crc16(req_str[2:].decode(\"hex\"))\n                    self.m_serial_port.write(req_str.decode(\"hex\"))\n                    if self.m_serial_port.getResponse(self.getContext()).encode(\"hex\") == \"06\":\n                        self.writeCmdMsg(\"Success(setSeasonSchedules): 06 returned.\")\n                        result = True\n            self.serialPostEnd()\n        except:\n            ekm_log(traceback.format_exc(sys.exc_info()))\n\n        self.setContext(\"\")\n        return result", "language": "python", "code": "def setSeasonSchedules(self, cmd_dict=None, password=\"00000000\"):\n        \"\"\" Serial command to set seasons table.\n\n        If no dictionary is passed, the meter object buffer is used.\n\n        Args:\n            cmd_dict (dict): Optional dictionary of season schedules.\n            password (str): Optional password\n\n        Returns:\n            bool: True on completion and ACK.\n        \"\"\"\n        result = False\n        self.setContext(\"setSeasonSchedules\")\n\n        if not cmd_dict:\n            cmd_dict = self.m_seasons_sched_params\n\n        try:\n            if not self.request(False):\n                self.writeCmdMsg(\"Bad read CRC on setting\")\n            else:\n                if not self.serialCmdPwdAuth(password):\n                    self.writeCmdMsg(\"Password failure\")\n                else:\n                    req_table = \"\"\n                    req_table += binascii.hexlify(str(cmd_dict[\"Season_1_Start_Month\"]).zfill(2))\n                    req_table += binascii.hexlify(str(cmd_dict[\"Season_1_Start_Day\"]).zfill(2))\n                    req_table += binascii.hexlify(str(cmd_dict[\"Season_1_Schedule\"]).zfill(2))\n                    req_table += binascii.hexlify(str(cmd_dict[\"Season_2_Start_Month\"]).zfill(2))\n                    req_table += binascii.hexlify(str(cmd_dict[\"Season_2_Start_Day\"]).zfill(2))\n                    req_table += binascii.hexlify(str(cmd_dict[\"Season_2_Schedule\"]).zfill(2))\n                    req_table += binascii.hexlify(str(cmd_dict[\"Season_3_Start_Month\"]).zfill(2))\n                    req_table += binascii.hexlify(str(cmd_dict[\"Season_3_Start_Day\"]).zfill(2))\n                    req_table += binascii.hexlify(str(cmd_dict[\"Season_3_Schedule\"]).zfill(2))\n                    req_table += binascii.hexlify(str(cmd_dict[\"Season_4_Start_Month\"]).zfill(2))\n                    req_table += binascii.hexlify(str(cmd_dict[\"Season_4_Start_Day\"]).zfill(2))\n                    req_table += binascii.hexlify(str(cmd_dict[\"Season_4_Schedule\"]).zfill(2))\n                    req_table += binascii.hexlify(str(0).zfill(24))\n                    req_str = \"015731023030383028\" + req_table + \"2903\"\n                    req_str += self.calc_crc16(req_str[2:].decode(\"hex\"))\n                    self.m_serial_port.write(req_str.decode(\"hex\"))\n                    if self.m_serial_port.getResponse(self.getContext()).encode(\"hex\") == \"06\":\n                        self.writeCmdMsg(\"Success(setSeasonSchedules): 06 returned.\")\n                        result = True\n            self.serialPostEnd()\n        except:\n            ekm_log(traceback.format_exc(sys.exc_info()))\n\n        self.setContext(\"\")\n        return result", "code_tokens": ["def", "setSeasonSchedules", "(", "self", ",", "cmd_dict", "=", "None", ",", "password", "=", "\"00000000\"", ")", ":", "result", "=", "False", "self", ".", "setContext", "(", "\"setSeasonSchedules\"", ")", "if", "not", "cmd_dict", ":", "cmd_dict", "=", "self", ".", "m_seasons_sched_params", "try", ":", "if", "not", "self", ".", "request", "(", "False", ")", ":", "self", ".", "writeCmdMsg", "(", "\"Bad read CRC on setting\"", ")", "else", ":", "if", "not", "self", ".", "serialCmdPwdAuth", "(", "password", ")", ":", "self", ".", "writeCmdMsg", "(", "\"Password failure\"", ")", "else", ":", "req_table", "=", "\"\"", "req_table", "+=", "binascii", ".", "hexlify", "(", "str", "(", "cmd_dict", "[", "\"Season_1_Start_Month\"", "]", ")", ".", "zfill", "(", "2", ")", ")", "req_table", "+=", "binascii", ".", "hexlify", "(", "str", "(", "cmd_dict", "[", "\"Season_1_Start_Day\"", "]", ")", ".", "zfill", "(", "2", ")", ")", "req_table", "+=", "binascii", ".", "hexlify", "(", "str", "(", "cmd_dict", "[", "\"Season_1_Schedule\"", "]", ")", ".", "zfill", "(", "2", ")", ")", "req_table", "+=", "binascii", ".", "hexlify", "(", "str", "(", "cmd_dict", "[", "\"Season_2_Start_Month\"", "]", ")", ".", "zfill", "(", "2", ")", ")", "req_table", "+=", "binascii", ".", "hexlify", "(", "str", "(", "cmd_dict", "[", "\"Season_2_Start_Day\"", "]", ")", ".", "zfill", "(", "2", ")", ")", "req_table", "+=", "binascii", ".", "hexlify", "(", "str", "(", "cmd_dict", "[", "\"Season_2_Schedule\"", "]", ")", ".", "zfill", "(", "2", ")", ")", "req_table", "+=", "binascii", ".", "hexlify", "(", "str", "(", "cmd_dict", "[", "\"Season_3_Start_Month\"", "]", ")", ".", "zfill", "(", "2", ")", ")", "req_table", "+=", "binascii", ".", "hexlify", "(", "str", "(", "cmd_dict", "[", "\"Season_3_Start_Day\"", "]", ")", ".", "zfill", "(", "2", ")", ")", "req_table", "+=", "binascii", ".", "hexlify", "(", "str", "(", "cmd_dict", "[", "\"Season_3_Schedule\"", "]", ")", ".", "zfill", "(", "2", ")", ")", "req_table", "+=", "binascii", ".", "hexlify", "(", "str", "(", "cmd_dict", "[", "\"Season_4_Start_Month\"", "]", ")", ".", "zfill", "(", "2", ")", ")", "req_table", "+=", "binascii", ".", "hexlify", "(", "str", "(", "cmd_dict", "[", "\"Season_4_Start_Day\"", "]", ")", ".", "zfill", "(", "2", ")", ")", "req_table", "+=", "binascii", ".", "hexlify", "(", "str", "(", "cmd_dict", "[", "\"Season_4_Schedule\"", "]", ")", ".", "zfill", "(", "2", ")", ")", "req_table", "+=", "binascii", ".", "hexlify", "(", "str", "(", "0", ")", ".", "zfill", "(", "24", ")", ")", "req_str", "=", "\"015731023030383028\"", "+", "req_table", "+", "\"2903\"", "req_str", "+=", "self", ".", "calc_crc16", "(", "req_str", "[", "2", ":", "]", ".", "decode", "(", "\"hex\"", ")", ")", "self", ".", "m_serial_port", ".", "write", "(", "req_str", ".", "decode", "(", "\"hex\"", ")", ")", "if", "self", ".", "m_serial_port", ".", "getResponse", "(", "self", ".", "getContext", "(", ")", ")", ".", "encode", "(", "\"hex\"", ")", "==", "\"06\"", ":", "self", ".", "writeCmdMsg", "(", "\"Success(setSeasonSchedules): 06 returned.\"", ")", "result", "=", "True", "self", ".", "serialPostEnd", "(", ")", "except", ":", "ekm_log", "(", "traceback", ".", "format_exc", "(", "sys", ".", "exc_info", "(", ")", ")", ")", "self", ".", "setContext", "(", "\"\"", ")", "return", "result"], "docstring": "Serial command to set seasons table.\n\n        If no dictionary is passed, the meter object buffer is used.\n\n        Args:\n            cmd_dict (dict): Optional dictionary of season schedules.\n            password (str): Optional password\n\n        Returns:\n            bool: True on completion and ACK.", "docstring_tokens": ["Serial", "command", "to", "set", "seasons", "table", "."], "sha": "b3748bdf30263bfa46ea40157bdf8df2522e1904", "url": "https://github.com/ekmmetering/ekmmeters/blob/b3748bdf30263bfa46ea40157bdf8df2522e1904/ekmmeters.py#L2494-L2544", "partition": "test"}
{"repo": "funilrys/PyFunceble", "path": "PyFunceble/expiration_date.py", "func_name": "ExpirationDate.get", "original_string": "def get(self):  # pragma: no cover\n        \"\"\"\n        Execute the logic behind the meaning of ExpirationDate + return the matched status.\n\n        :return:\n            The status of the tested domain.\n            Can be one of the official status.\n        :rtype: str\n        \"\"\"\n\n        # We get the status of the domain validation.\n        domain_validation = self.checker.is_domain_valid()\n        # We get the status of the IPv4 validation.\n        ip_validation = self.checker.is_ip_valid()\n\n        if \"current_test_data\" in PyFunceble.INTERN:\n            # The end-user want more information whith his test.\n\n            # We update some index.\n            PyFunceble.INTERN[\"current_test_data\"].update(\n                {\n                    \"domain_syntax_validation\": domain_validation,\n                    \"ip4_syntax_validation\": ip_validation,\n                }\n            )\n\n        if (\n            domain_validation\n            and not ip_validation\n            or domain_validation\n            or PyFunceble.CONFIGURATION[\"local\"]\n        ):\n            # * The element is a valid domain.\n            # and\n            # * The element is not ahe valid IPv4.\n            # or\n            # * The element is a valid domain.\n\n            # * We get the HTTP status code of the currently tested element.\n            # and\n            # * We try to get the element status from the IANA database.\n            PyFunceble.INTERN.update(\n                {\"http_code\": HTTPCode().get(), \"referer\": Referer().get()}\n            )\n\n            if not PyFunceble.INTERN[\"referer\"]:\n                # We could not get the referer.\n\n                # We parse the referer status into the upstream call.\n                return PyFunceble.INTERN[\"referer\"]\n\n            # The WHOIS record status is not into our list of official status.\n\n            if PyFunceble.INTERN[\"referer\"] and not self.checker.is_subdomain():\n                # * The iana database comparison status is not None.\n                # and\n                # * The domain we are testing is not a subdomain.\n\n                # We try to extract the expiration date from the WHOIS record.\n                # And we return the matched status.\n                return self._extract()\n\n            # The iana database comparison status is None.\n\n            # We log our whois record if the debug mode is activated.\n            Logs().whois(self.whois_record)\n\n            # And we return None, we could not extract the expiration date.\n            return None\n\n        if (\n            ip_validation\n            and not domain_validation\n            or ip_validation\n            or PyFunceble.CONFIGURATION[\"local\"]\n        ):\n            # * The element is a valid IPv4.\n            # and\n            # * The element is not a valid domain.\n            # or\n            # * The element is a valid IPv4.\n\n            # We get the HTTP status code.\n            PyFunceble.INTERN[\"http_code\"] = HTTPCode().get()\n\n            # We log our whois record if the debug mode is activated.\n            Logs().whois(self.whois_record)\n\n            # And we return None, there is no expiration date to look for.\n            return None\n\n        # The validation was not passed.\n\n        # We log our whois record if the debug mode is activated.\n        Logs().whois(self.whois_record)\n\n        # And we return False, the domain could not pass the IP and domains syntax validation.\n        return False", "language": "python", "code": "def get(self):  # pragma: no cover\n        \"\"\"\n        Execute the logic behind the meaning of ExpirationDate + return the matched status.\n\n        :return:\n            The status of the tested domain.\n            Can be one of the official status.\n        :rtype: str\n        \"\"\"\n\n        # We get the status of the domain validation.\n        domain_validation = self.checker.is_domain_valid()\n        # We get the status of the IPv4 validation.\n        ip_validation = self.checker.is_ip_valid()\n\n        if \"current_test_data\" in PyFunceble.INTERN:\n            # The end-user want more information whith his test.\n\n            # We update some index.\n            PyFunceble.INTERN[\"current_test_data\"].update(\n                {\n                    \"domain_syntax_validation\": domain_validation,\n                    \"ip4_syntax_validation\": ip_validation,\n                }\n            )\n\n        if (\n            domain_validation\n            and not ip_validation\n            or domain_validation\n            or PyFunceble.CONFIGURATION[\"local\"]\n        ):\n            # * The element is a valid domain.\n            # and\n            # * The element is not ahe valid IPv4.\n            # or\n            # * The element is a valid domain.\n\n            # * We get the HTTP status code of the currently tested element.\n            # and\n            # * We try to get the element status from the IANA database.\n            PyFunceble.INTERN.update(\n                {\"http_code\": HTTPCode().get(), \"referer\": Referer().get()}\n            )\n\n            if not PyFunceble.INTERN[\"referer\"]:\n                # We could not get the referer.\n\n                # We parse the referer status into the upstream call.\n                return PyFunceble.INTERN[\"referer\"]\n\n            # The WHOIS record status is not into our list of official status.\n\n            if PyFunceble.INTERN[\"referer\"] and not self.checker.is_subdomain():\n                # * The iana database comparison status is not None.\n                # and\n                # * The domain we are testing is not a subdomain.\n\n                # We try to extract the expiration date from the WHOIS record.\n                # And we return the matched status.\n                return self._extract()\n\n            # The iana database comparison status is None.\n\n            # We log our whois record if the debug mode is activated.\n            Logs().whois(self.whois_record)\n\n            # And we return None, we could not extract the expiration date.\n            return None\n\n        if (\n            ip_validation\n            and not domain_validation\n            or ip_validation\n            or PyFunceble.CONFIGURATION[\"local\"]\n        ):\n            # * The element is a valid IPv4.\n            # and\n            # * The element is not a valid domain.\n            # or\n            # * The element is a valid IPv4.\n\n            # We get the HTTP status code.\n            PyFunceble.INTERN[\"http_code\"] = HTTPCode().get()\n\n            # We log our whois record if the debug mode is activated.\n            Logs().whois(self.whois_record)\n\n            # And we return None, there is no expiration date to look for.\n            return None\n\n        # The validation was not passed.\n\n        # We log our whois record if the debug mode is activated.\n        Logs().whois(self.whois_record)\n\n        # And we return False, the domain could not pass the IP and domains syntax validation.\n        return False", "code_tokens": ["def", "get", "(", "self", ")", ":", "# pragma: no cover", "# We get the status of the domain validation.", "domain_validation", "=", "self", ".", "checker", ".", "is_domain_valid", "(", ")", "# We get the status of the IPv4 validation.", "ip_validation", "=", "self", ".", "checker", ".", "is_ip_valid", "(", ")", "if", "\"current_test_data\"", "in", "PyFunceble", ".", "INTERN", ":", "# The end-user want more information whith his test.", "# We update some index.", "PyFunceble", ".", "INTERN", "[", "\"current_test_data\"", "]", ".", "update", "(", "{", "\"domain_syntax_validation\"", ":", "domain_validation", ",", "\"ip4_syntax_validation\"", ":", "ip_validation", ",", "}", ")", "if", "(", "domain_validation", "and", "not", "ip_validation", "or", "domain_validation", "or", "PyFunceble", ".", "CONFIGURATION", "[", "\"local\"", "]", ")", ":", "# * The element is a valid domain.", "# and", "# * The element is not ahe valid IPv4.", "# or", "# * The element is a valid domain.", "# * We get the HTTP status code of the currently tested element.", "# and", "# * We try to get the element status from the IANA database.", "PyFunceble", ".", "INTERN", ".", "update", "(", "{", "\"http_code\"", ":", "HTTPCode", "(", ")", ".", "get", "(", ")", ",", "\"referer\"", ":", "Referer", "(", ")", ".", "get", "(", ")", "}", ")", "if", "not", "PyFunceble", ".", "INTERN", "[", "\"referer\"", "]", ":", "# We could not get the referer.", "# We parse the referer status into the upstream call.", "return", "PyFunceble", ".", "INTERN", "[", "\"referer\"", "]", "# The WHOIS record status is not into our list of official status.", "if", "PyFunceble", ".", "INTERN", "[", "\"referer\"", "]", "and", "not", "self", ".", "checker", ".", "is_subdomain", "(", ")", ":", "# * The iana database comparison status is not None.", "# and", "# * The domain we are testing is not a subdomain.", "# We try to extract the expiration date from the WHOIS record.", "# And we return the matched status.", "return", "self", ".", "_extract", "(", ")", "# The iana database comparison status is None.", "# We log our whois record if the debug mode is activated.", "Logs", "(", ")", ".", "whois", "(", "self", ".", "whois_record", ")", "# And we return None, we could not extract the expiration date.", "return", "None", "if", "(", "ip_validation", "and", "not", "domain_validation", "or", "ip_validation", "or", "PyFunceble", ".", "CONFIGURATION", "[", "\"local\"", "]", ")", ":", "# * The element is a valid IPv4.", "# and", "# * The element is not a valid domain.", "# or", "# * The element is a valid IPv4.", "# We get the HTTP status code.", "PyFunceble", ".", "INTERN", "[", "\"http_code\"", "]", "=", "HTTPCode", "(", ")", ".", "get", "(", ")", "# We log our whois record if the debug mode is activated.", "Logs", "(", ")", ".", "whois", "(", "self", ".", "whois_record", ")", "# And we return None, there is no expiration date to look for.", "return", "None", "# The validation was not passed.", "# We log our whois record if the debug mode is activated.", "Logs", "(", ")", ".", "whois", "(", "self", ".", "whois_record", ")", "# And we return False, the domain could not pass the IP and domains syntax validation.", "return", "False"], "docstring": "Execute the logic behind the meaning of ExpirationDate + return the matched status.\n\n        :return:\n            The status of the tested domain.\n            Can be one of the official status.\n        :rtype: str", "docstring_tokens": ["Execute", "the", "logic", "behind", "the", "meaning", "of", "ExpirationDate", "+", "return", "the", "matched", "status", "."], "sha": "cdf69cbde120199171f7158e1c33635753e6e2f5", "url": "https://github.com/funilrys/PyFunceble/blob/cdf69cbde120199171f7158e1c33635753e6e2f5/PyFunceble/expiration_date.py#L94-L191", "partition": "test"}
{"repo": "hivetech/dna", "path": "python/dna/apy/auth.py", "func_name": "requires_token_auth", "original_string": "def requires_token_auth(resource):\n    '''\n    Flask decorator protecting ressources using token scheme\n    '''\n    @functools.wraps(resource)\n    def decorated(*args, **kwargs):\n        ''' Check provided token '''\n        token = flask.request.headers.get('Authorization')\n        user = check_token(token)\n\n        if not token or user is None:\n            log.warn('authentification failed', token=token)\n            return auth_failed()\n\n        flask.g.user = user\n        log.info('authentification succeeded', token=token, user=flask.g.user)\n        return resource(*args, **kwargs)\n    return decorated", "language": "python", "code": "def requires_token_auth(resource):\n    '''\n    Flask decorator protecting ressources using token scheme\n    '''\n    @functools.wraps(resource)\n    def decorated(*args, **kwargs):\n        ''' Check provided token '''\n        token = flask.request.headers.get('Authorization')\n        user = check_token(token)\n\n        if not token or user is None:\n            log.warn('authentification failed', token=token)\n            return auth_failed()\n\n        flask.g.user = user\n        log.info('authentification succeeded', token=token, user=flask.g.user)\n        return resource(*args, **kwargs)\n    return decorated", "code_tokens": ["def", "requires_token_auth", "(", "resource", ")", ":", "@", "functools", ".", "wraps", "(", "resource", ")", "def", "decorated", "(", "*", "args", ",", "*", "*", "kwargs", ")", ":", "''' Check provided token '''", "token", "=", "flask", ".", "request", ".", "headers", ".", "get", "(", "'Authorization'", ")", "user", "=", "check_token", "(", "token", ")", "if", "not", "token", "or", "user", "is", "None", ":", "log", ".", "warn", "(", "'authentification failed'", ",", "token", "=", "token", ")", "return", "auth_failed", "(", ")", "flask", ".", "g", ".", "user", "=", "user", "log", ".", "info", "(", "'authentification succeeded'", ",", "token", "=", "token", ",", "user", "=", "flask", ".", "g", ".", "user", ")", "return", "resource", "(", "*", "args", ",", "*", "*", "kwargs", ")", "return", "decorated"], "docstring": "Flask decorator protecting ressources using token scheme", "docstring_tokens": ["Flask", "decorator", "protecting", "ressources", "using", "token", "scheme"], "sha": "50ad00031be29765b2576fa407d35a36e0608de9", "url": "https://github.com/hivetech/dna/blob/50ad00031be29765b2576fa407d35a36e0608de9/python/dna/apy/auth.py#L64-L81", "partition": "test"}
{"repo": "hivetech/dna", "path": "python/dna/apy/worker.py", "func_name": "RestfulWorker.get", "original_string": "def get(self, worker_id):\n        ''' Return status report '''\n        code = 200\n\n        if worker_id == 'all':\n            report = {'workers': [{\n                'id': job,\n                'report': self._inspect_worker(job)}\n                for job in self.jobs]\n            }\n\n        elif worker_id in self.jobs:\n            report = {\n                'id': worker_id,\n                'report': self._inspect_worker(worker_id)\n            }\n\n        else:\n            report = {'error': 'job {} unknown'.format(worker_id)}\n            code = 404\n\n        return flask.jsonify(report), code", "language": "python", "code": "def get(self, worker_id):\n        ''' Return status report '''\n        code = 200\n\n        if worker_id == 'all':\n            report = {'workers': [{\n                'id': job,\n                'report': self._inspect_worker(job)}\n                for job in self.jobs]\n            }\n\n        elif worker_id in self.jobs:\n            report = {\n                'id': worker_id,\n                'report': self._inspect_worker(worker_id)\n            }\n\n        else:\n            report = {'error': 'job {} unknown'.format(worker_id)}\n            code = 404\n\n        return flask.jsonify(report), code", "code_tokens": ["def", "get", "(", "self", ",", "worker_id", ")", ":", "code", "=", "200", "if", "worker_id", "==", "'all'", ":", "report", "=", "{", "'workers'", ":", "[", "{", "'id'", ":", "job", ",", "'report'", ":", "self", ".", "_inspect_worker", "(", "job", ")", "}", "for", "job", "in", "self", ".", "jobs", "]", "}", "elif", "worker_id", "in", "self", ".", "jobs", ":", "report", "=", "{", "'id'", ":", "worker_id", ",", "'report'", ":", "self", ".", "_inspect_worker", "(", "worker_id", ")", "}", "else", ":", "report", "=", "{", "'error'", ":", "'job {} unknown'", ".", "format", "(", "worker_id", ")", "}", "code", "=", "404", "return", "flask", ".", "jsonify", "(", "report", ")", ",", "code"], "docstring": "Return status report", "docstring_tokens": ["Return", "status", "report"], "sha": "50ad00031be29765b2576fa407d35a36e0608de9", "url": "https://github.com/hivetech/dna/blob/50ad00031be29765b2576fa407d35a36e0608de9/python/dna/apy/worker.py#L93-L114", "partition": "test"}
{"repo": "SmokinCaterpillar/pypet", "path": "examples/example_24_large_scale_brian2_simulation/clusternet.py", "func_name": "CNNeuronGroup.pre_build", "original_string": "def pre_build(self, traj, brian_list, network_dict):\n        \"\"\"Pre-builds the neuron groups.\n\n        Pre-build is only performed if none of the\n        relevant parameters is explored.\n\n        :param traj: Trajectory container\n\n        :param brian_list:\n\n            List of objects passed to BRIAN network constructor.\n\n            Adds:\n\n            Inhibitory neuron group\n\n            Excitatory neuron group\n\n        :param network_dict:\n\n            Dictionary of elements shared among the components\n\n            Adds:\n\n            'neurons_i': Inhibitory neuron group\n\n            'neurons_e': Excitatory neuron group\n\n        \"\"\"\n        self._pre_build = not _explored_parameters_in_group(traj, traj.parameters.model)\n\n        if self._pre_build:\n            self._build_model(traj, brian_list, network_dict)", "language": "python", "code": "def pre_build(self, traj, brian_list, network_dict):\n        \"\"\"Pre-builds the neuron groups.\n\n        Pre-build is only performed if none of the\n        relevant parameters is explored.\n\n        :param traj: Trajectory container\n\n        :param brian_list:\n\n            List of objects passed to BRIAN network constructor.\n\n            Adds:\n\n            Inhibitory neuron group\n\n            Excitatory neuron group\n\n        :param network_dict:\n\n            Dictionary of elements shared among the components\n\n            Adds:\n\n            'neurons_i': Inhibitory neuron group\n\n            'neurons_e': Excitatory neuron group\n\n        \"\"\"\n        self._pre_build = not _explored_parameters_in_group(traj, traj.parameters.model)\n\n        if self._pre_build:\n            self._build_model(traj, brian_list, network_dict)", "code_tokens": ["def", "pre_build", "(", "self", ",", "traj", ",", "brian_list", ",", "network_dict", ")", ":", "self", ".", "_pre_build", "=", "not", "_explored_parameters_in_group", "(", "traj", ",", "traj", ".", "parameters", ".", "model", ")", "if", "self", ".", "_pre_build", ":", "self", ".", "_build_model", "(", "traj", ",", "brian_list", ",", "network_dict", ")"], "docstring": "Pre-builds the neuron groups.\n\n        Pre-build is only performed if none of the\n        relevant parameters is explored.\n\n        :param traj: Trajectory container\n\n        :param brian_list:\n\n            List of objects passed to BRIAN network constructor.\n\n            Adds:\n\n            Inhibitory neuron group\n\n            Excitatory neuron group\n\n        :param network_dict:\n\n            Dictionary of elements shared among the components\n\n            Adds:\n\n            'neurons_i': Inhibitory neuron group\n\n            'neurons_e': Excitatory neuron group", "docstring_tokens": ["Pre", "-", "builds", "the", "neuron", "groups", "."], "sha": "97ad3e80d46dbdea02deeb98ea41f05a19565826", "url": "https://github.com/SmokinCaterpillar/pypet/blob/97ad3e80d46dbdea02deeb98ea41f05a19565826/examples/example_24_large_scale_brian2_simulation/clusternet.py#L129-L161", "partition": "test"}
{"repo": "open-mmlab/mmcv", "path": "mmcv/runner/log_buffer.py", "func_name": "LogBuffer.average", "original_string": "def average(self, n=0):\n        \"\"\"Average latest n values or all values\"\"\"\n        assert n >= 0\n        for key in self.val_history:\n            values = np.array(self.val_history[key][-n:])\n            nums = np.array(self.n_history[key][-n:])\n            avg = np.sum(values * nums) / np.sum(nums)\n            self.output[key] = avg\n        self.ready = True", "language": "python", "code": "def average(self, n=0):\n        \"\"\"Average latest n values or all values\"\"\"\n        assert n >= 0\n        for key in self.val_history:\n            values = np.array(self.val_history[key][-n:])\n            nums = np.array(self.n_history[key][-n:])\n            avg = np.sum(values * nums) / np.sum(nums)\n            self.output[key] = avg\n        self.ready = True", "code_tokens": ["def", "average", "(", "self", ",", "n", "=", "0", ")", ":", "assert", "n", ">=", "0", "for", "key", "in", "self", ".", "val_history", ":", "values", "=", "np", ".", "array", "(", "self", ".", "val_history", "[", "key", "]", "[", "-", "n", ":", "]", ")", "nums", "=", "np", ".", "array", "(", "self", ".", "n_history", "[", "key", "]", "[", "-", "n", ":", "]", ")", "avg", "=", "np", ".", "sum", "(", "values", "*", "nums", ")", "/", "np", ".", "sum", "(", "nums", ")", "self", ".", "output", "[", "key", "]", "=", "avg", "self", ".", "ready", "=", "True"], "docstring": "Average latest n values or all values", "docstring_tokens": ["Average", "latest", "n", "values", "or", "all", "values"], "sha": "0d77f61450aab4dde8b8585a577cc496acb95d7f", "url": "https://github.com/open-mmlab/mmcv/blob/0d77f61450aab4dde8b8585a577cc496acb95d7f/mmcv/runner/log_buffer.py#L32-L40", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/aws_glue_catalog_hook.py", "func_name": "AwsGlueCatalogHook.get_table_location", "original_string": "def get_table_location(self, database_name, table_name):\n        \"\"\"\n        Get the physical location of the table\n\n        :param database_name: Name of hive database (schema) @table belongs to\n        :type database_name: str\n        :param table_name: Name of hive table\n        :type table_name: str\n        :return: str\n        \"\"\"\n\n        table = self.get_table(database_name, table_name)\n\n        return table['StorageDescriptor']['Location']", "language": "python", "code": "def get_table_location(self, database_name, table_name):\n        \"\"\"\n        Get the physical location of the table\n\n        :param database_name: Name of hive database (schema) @table belongs to\n        :type database_name: str\n        :param table_name: Name of hive table\n        :type table_name: str\n        :return: str\n        \"\"\"\n\n        table = self.get_table(database_name, table_name)\n\n        return table['StorageDescriptor']['Location']", "code_tokens": ["def", "get_table_location", "(", "self", ",", "database_name", ",", "table_name", ")", ":", "table", "=", "self", ".", "get_table", "(", "database_name", ",", "table_name", ")", "return", "table", "[", "'StorageDescriptor'", "]", "[", "'Location'", "]"], "docstring": "Get the physical location of the table\n\n        :param database_name: Name of hive database (schema) @table belongs to\n        :type database_name: str\n        :param table_name: Name of hive table\n        :type table_name: str\n        :return: str", "docstring_tokens": ["Get", "the", "physical", "location", "of", "the", "table"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_glue_catalog_hook.py#L139-L152", "partition": "test"}
{"repo": "assemblerflow/flowcraft", "path": "flowcraft/generator/process.py", "func_name": "Compiler.set_compiler_channels", "original_string": "def set_compiler_channels(self, channel_list, operator=\"mix\"):\n        \"\"\"General method for setting the input channels for the status process\n\n        Given a list of status channels that are gathered during the pipeline\n        construction, this method will automatically set the input channel\n        for the status process. This makes use of the ``mix`` channel operator\n        of nextflow for multiple channels::\n\n            STATUS_1.mix(STATUS_2,STATUS_3,...)\n\n        This will set the ``status_channels`` key for the ``_context``\n        attribute of the process.\n\n        Parameters\n        ----------\n        channel_list : list\n            List of strings with the final name of the status channels\n        operator : str\n            Specifies the operator used to join the compiler channels.\n            Available options are 'mix'and 'join'.\n        \"\"\"\n\n        if not channel_list:\n            raise eh.ProcessError(\"At least one status channel must be \"\n                                  \"provided to include this process in the \"\n                                  \"pipeline\")\n\n        if len(channel_list) == 1:\n            logger.debug(\"Setting only one status channel: {}\".format(\n                channel_list[0]))\n            self._context = {\"compile_channels\": channel_list[0]}\n\n        else:\n\n            first_status = channel_list[0]\n\n            if operator == \"mix\":\n                lst = \",\".join(channel_list[1:])\n\n                s = \"{}.mix({})\".format(first_status, lst)\n\n            elif operator == \"join\":\n\n                s = first_status\n                for ch in channel_list[1:]:\n                    s += \".join({})\".format(ch)\n\n                s += \".map{ ot -> [ ot[0], ot[1..-1] ] }\"\n\n            logger.debug(\"Status channel string: {}\".format(s))\n\n            self._context = {\"compile_channels\": s}", "language": "python", "code": "def set_compiler_channels(self, channel_list, operator=\"mix\"):\n        \"\"\"General method for setting the input channels for the status process\n\n        Given a list of status channels that are gathered during the pipeline\n        construction, this method will automatically set the input channel\n        for the status process. This makes use of the ``mix`` channel operator\n        of nextflow for multiple channels::\n\n            STATUS_1.mix(STATUS_2,STATUS_3,...)\n\n        This will set the ``status_channels`` key for the ``_context``\n        attribute of the process.\n\n        Parameters\n        ----------\n        channel_list : list\n            List of strings with the final name of the status channels\n        operator : str\n            Specifies the operator used to join the compiler channels.\n            Available options are 'mix'and 'join'.\n        \"\"\"\n\n        if not channel_list:\n            raise eh.ProcessError(\"At least one status channel must be \"\n                                  \"provided to include this process in the \"\n                                  \"pipeline\")\n\n        if len(channel_list) == 1:\n            logger.debug(\"Setting only one status channel: {}\".format(\n                channel_list[0]))\n            self._context = {\"compile_channels\": channel_list[0]}\n\n        else:\n\n            first_status = channel_list[0]\n\n            if operator == \"mix\":\n                lst = \",\".join(channel_list[1:])\n\n                s = \"{}.mix({})\".format(first_status, lst)\n\n            elif operator == \"join\":\n\n                s = first_status\n                for ch in channel_list[1:]:\n                    s += \".join({})\".format(ch)\n\n                s += \".map{ ot -> [ ot[0], ot[1..-1] ] }\"\n\n            logger.debug(\"Status channel string: {}\".format(s))\n\n            self._context = {\"compile_channels\": s}", "code_tokens": ["def", "set_compiler_channels", "(", "self", ",", "channel_list", ",", "operator", "=", "\"mix\"", ")", ":", "if", "not", "channel_list", ":", "raise", "eh", ".", "ProcessError", "(", "\"At least one status channel must be \"", "\"provided to include this process in the \"", "\"pipeline\"", ")", "if", "len", "(", "channel_list", ")", "==", "1", ":", "logger", ".", "debug", "(", "\"Setting only one status channel: {}\"", ".", "format", "(", "channel_list", "[", "0", "]", ")", ")", "self", ".", "_context", "=", "{", "\"compile_channels\"", ":", "channel_list", "[", "0", "]", "}", "else", ":", "first_status", "=", "channel_list", "[", "0", "]", "if", "operator", "==", "\"mix\"", ":", "lst", "=", "\",\"", ".", "join", "(", "channel_list", "[", "1", ":", "]", ")", "s", "=", "\"{}.mix({})\"", ".", "format", "(", "first_status", ",", "lst", ")", "elif", "operator", "==", "\"join\"", ":", "s", "=", "first_status", "for", "ch", "in", "channel_list", "[", "1", ":", "]", ":", "s", "+=", "\".join({})\"", ".", "format", "(", "ch", ")", "s", "+=", "\".map{ ot -> [ ot[0], ot[1..-1] ] }\"", "logger", ".", "debug", "(", "\"Status channel string: {}\"", ".", "format", "(", "s", ")", ")", "self", ".", "_context", "=", "{", "\"compile_channels\"", ":", "s", "}"], "docstring": "General method for setting the input channels for the status process\n\n        Given a list of status channels that are gathered during the pipeline\n        construction, this method will automatically set the input channel\n        for the status process. This makes use of the ``mix`` channel operator\n        of nextflow for multiple channels::\n\n            STATUS_1.mix(STATUS_2,STATUS_3,...)\n\n        This will set the ``status_channels`` key for the ``_context``\n        attribute of the process.\n\n        Parameters\n        ----------\n        channel_list : list\n            List of strings with the final name of the status channels\n        operator : str\n            Specifies the operator used to join the compiler channels.\n            Available options are 'mix'and 'join'.", "docstring_tokens": ["General", "method", "for", "setting", "the", "input", "channels", "for", "the", "status", "process"], "sha": "fc3f4bddded1efc76006600016dc71a06dd908c0", "url": "https://github.com/assemblerflow/flowcraft/blob/fc3f4bddded1efc76006600016dc71a06dd908c0/flowcraft/generator/process.py#L622-L673", "partition": "test"}
{"repo": "urda/nistbeacon", "path": "nistbeacon/nistbeacon.py", "func_name": "NistBeacon.chain_check", "original_string": "def chain_check(cls, timestamp: int) -> bool:\n        \"\"\"\n        Given a record timestamp, verify the chain integrity.\n\n        :param timestamp: UNIX time / POSIX time / Epoch time\n        :return: 'True' if the timestamp fits the chain. 'False' otherwise.\n        \"\"\"\n\n        # Creation is messy.\n        # You want genius, you get madness; two sides of the same coin.\n        # ... I'm sure this can be cleaned up. However, let's test it first.\n\n        record = cls.get_record(timestamp)\n\n        if isinstance(record, NistBeaconValue) is False:\n            # Don't you dare try to play me\n            return False\n\n        prev_record = cls.get_previous(record.timestamp)\n        next_record = cls.get_next(record.timestamp)\n\n        if prev_record is None and next_record is None:\n            # Uh, how did you manage to do this?\n            # I'm not even mad, that's amazing.\n            return False\n\n        if (\n                isinstance(prev_record, NistBeaconValue) and\n                isinstance(next_record, NistBeaconValue)\n        ):\n            # Majority case, somewhere in the middle of the chain\n            # True if:\n            #   - All three records have proper signatures\n            #   - The requested record's previous output equals previous\n            #   - The next possible record's previous output equals the record\n            return (\n                record.valid_signature and\n                prev_record.valid_signature and\n                next_record.valid_signature and\n                record.previous_output_value == prev_record.output_value and\n                next_record.previous_output_value == record.output_value\n            )\n\n        if (\n                prev_record is None and\n                isinstance(next_record, NistBeaconValue)\n        ):\n            # Edge case, this was potentially the first record of all time\n            return (\n                record.valid_signature and\n                next_record.valid_signature and\n                cls._INIT_RECORD == record and\n                next_record.previous_output_value == record.output_value\n            )\n\n        if (\n                isinstance(prev_record, NistBeaconValue) and\n                next_record is None\n        ):\n            # Edge case, this was potentially the latest and greatest\n            return (\n                record.valid_signature and\n                prev_record.valid_signature and\n                record.previous_output_value == prev_record.output_value\n            )", "language": "python", "code": "def chain_check(cls, timestamp: int) -> bool:\n        \"\"\"\n        Given a record timestamp, verify the chain integrity.\n\n        :param timestamp: UNIX time / POSIX time / Epoch time\n        :return: 'True' if the timestamp fits the chain. 'False' otherwise.\n        \"\"\"\n\n        # Creation is messy.\n        # You want genius, you get madness; two sides of the same coin.\n        # ... I'm sure this can be cleaned up. However, let's test it first.\n\n        record = cls.get_record(timestamp)\n\n        if isinstance(record, NistBeaconValue) is False:\n            # Don't you dare try to play me\n            return False\n\n        prev_record = cls.get_previous(record.timestamp)\n        next_record = cls.get_next(record.timestamp)\n\n        if prev_record is None and next_record is None:\n            # Uh, how did you manage to do this?\n            # I'm not even mad, that's amazing.\n            return False\n\n        if (\n                isinstance(prev_record, NistBeaconValue) and\n                isinstance(next_record, NistBeaconValue)\n        ):\n            # Majority case, somewhere in the middle of the chain\n            # True if:\n            #   - All three records have proper signatures\n            #   - The requested record's previous output equals previous\n            #   - The next possible record's previous output equals the record\n            return (\n                record.valid_signature and\n                prev_record.valid_signature and\n                next_record.valid_signature and\n                record.previous_output_value == prev_record.output_value and\n                next_record.previous_output_value == record.output_value\n            )\n\n        if (\n                prev_record is None and\n                isinstance(next_record, NistBeaconValue)\n        ):\n            # Edge case, this was potentially the first record of all time\n            return (\n                record.valid_signature and\n                next_record.valid_signature and\n                cls._INIT_RECORD == record and\n                next_record.previous_output_value == record.output_value\n            )\n\n        if (\n                isinstance(prev_record, NistBeaconValue) and\n                next_record is None\n        ):\n            # Edge case, this was potentially the latest and greatest\n            return (\n                record.valid_signature and\n                prev_record.valid_signature and\n                record.previous_output_value == prev_record.output_value\n            )", "code_tokens": ["def", "chain_check", "(", "cls", ",", "timestamp", ":", "int", ")", "->", "bool", ":", "# Creation is messy.", "# You want genius, you get madness; two sides of the same coin.", "# ... I'm sure this can be cleaned up. However, let's test it first.", "record", "=", "cls", ".", "get_record", "(", "timestamp", ")", "if", "isinstance", "(", "record", ",", "NistBeaconValue", ")", "is", "False", ":", "# Don't you dare try to play me", "return", "False", "prev_record", "=", "cls", ".", "get_previous", "(", "record", ".", "timestamp", ")", "next_record", "=", "cls", ".", "get_next", "(", "record", ".", "timestamp", ")", "if", "prev_record", "is", "None", "and", "next_record", "is", "None", ":", "# Uh, how did you manage to do this?", "# I'm not even mad, that's amazing.", "return", "False", "if", "(", "isinstance", "(", "prev_record", ",", "NistBeaconValue", ")", "and", "isinstance", "(", "next_record", ",", "NistBeaconValue", ")", ")", ":", "# Majority case, somewhere in the middle of the chain", "# True if:", "#   - All three records have proper signatures", "#   - The requested record's previous output equals previous", "#   - The next possible record's previous output equals the record", "return", "(", "record", ".", "valid_signature", "and", "prev_record", ".", "valid_signature", "and", "next_record", ".", "valid_signature", "and", "record", ".", "previous_output_value", "==", "prev_record", ".", "output_value", "and", "next_record", ".", "previous_output_value", "==", "record", ".", "output_value", ")", "if", "(", "prev_record", "is", "None", "and", "isinstance", "(", "next_record", ",", "NistBeaconValue", ")", ")", ":", "# Edge case, this was potentially the first record of all time", "return", "(", "record", ".", "valid_signature", "and", "next_record", ".", "valid_signature", "and", "cls", ".", "_INIT_RECORD", "==", "record", "and", "next_record", ".", "previous_output_value", "==", "record", ".", "output_value", ")", "if", "(", "isinstance", "(", "prev_record", ",", "NistBeaconValue", ")", "and", "next_record", "is", "None", ")", ":", "# Edge case, this was potentially the latest and greatest", "return", "(", "record", ".", "valid_signature", "and", "prev_record", ".", "valid_signature", "and", "record", ".", "previous_output_value", "==", "prev_record", ".", "output_value", ")"], "docstring": "Given a record timestamp, verify the chain integrity.\n\n        :param timestamp: UNIX time / POSIX time / Epoch time\n        :return: 'True' if the timestamp fits the chain. 'False' otherwise.", "docstring_tokens": ["Given", "a", "record", "timestamp", "verify", "the", "chain", "integrity", "."], "sha": "43e0c3d1e186e71387f072daf98911abb14469dd", "url": "https://github.com/urda/nistbeacon/blob/43e0c3d1e186e71387f072daf98911abb14469dd/nistbeacon/nistbeacon.py#L80-L144", "partition": "test"}
{"repo": "Nic30/hwt", "path": "hwt/hdl/operator.py", "func_name": "Operator.withRes", "original_string": "def withRes(opDef, operands, resT, outputs=[]):\n        \"\"\"\n        Create operator with result signal\n\n        :ivar resT: data type of result signal\n        :ivar outputs: iterable of singnals which are outputs\n            from this operator\n        \"\"\"\n        op = Operator(opDef, operands)\n        out = RtlSignal(getCtxFromOps(operands), None, resT)\n        out._const = arr_all(op.operands, isConst)\n        out.drivers.append(op)\n        out.origin = op\n        op.result = out\n        op.registerSignals(outputs)\n        if out._const:\n            out.staticEval()\n        return out", "language": "python", "code": "def withRes(opDef, operands, resT, outputs=[]):\n        \"\"\"\n        Create operator with result signal\n\n        :ivar resT: data type of result signal\n        :ivar outputs: iterable of singnals which are outputs\n            from this operator\n        \"\"\"\n        op = Operator(opDef, operands)\n        out = RtlSignal(getCtxFromOps(operands), None, resT)\n        out._const = arr_all(op.operands, isConst)\n        out.drivers.append(op)\n        out.origin = op\n        op.result = out\n        op.registerSignals(outputs)\n        if out._const:\n            out.staticEval()\n        return out", "code_tokens": ["def", "withRes", "(", "opDef", ",", "operands", ",", "resT", ",", "outputs", "=", "[", "]", ")", ":", "op", "=", "Operator", "(", "opDef", ",", "operands", ")", "out", "=", "RtlSignal", "(", "getCtxFromOps", "(", "operands", ")", ",", "None", ",", "resT", ")", "out", ".", "_const", "=", "arr_all", "(", "op", ".", "operands", ",", "isConst", ")", "out", ".", "drivers", ".", "append", "(", "op", ")", "out", ".", "origin", "=", "op", "op", ".", "result", "=", "out", "op", ".", "registerSignals", "(", "outputs", ")", "if", "out", ".", "_const", ":", "out", ".", "staticEval", "(", ")", "return", "out"], "docstring": "Create operator with result signal\n\n        :ivar resT: data type of result signal\n        :ivar outputs: iterable of singnals which are outputs\n            from this operator", "docstring_tokens": ["Create", "operator", "with", "result", "signal"], "sha": "8cbb399e326da3b22c233b98188a9d08dec057e6", "url": "https://github.com/Nic30/hwt/blob/8cbb399e326da3b22c233b98188a9d08dec057e6/hwt/hdl/operator.py#L110-L127", "partition": "test"}
{"repo": "mdgoldberg/sportsref", "path": "sportsref/decorators.py", "func_name": "cache", "original_string": "def cache(func):\n    \"\"\"Caches the HTML returned by the specified function `func`. Caches it in\n    the user cache determined by the appdirs package.\n    \"\"\"\n\n    CACHE_DIR = appdirs.user_cache_dir('sportsref', getpass.getuser())\n    if not os.path.isdir(CACHE_DIR):\n        os.makedirs(CACHE_DIR)\n\n    @funcutils.wraps(func)\n    def wrapper(url):\n        # hash based on the URL\n        file_hash = hashlib.md5()\n        encoded_url = url.encode(errors='replace')\n        file_hash.update(encoded_url)\n        file_hash = file_hash.hexdigest()\n        filename = '{}/{}'.format(CACHE_DIR, file_hash)\n\n        sport_id = None\n        for a_base_url, a_sport_id in sportsref.SITE_ABBREV.items():\n            if url.startswith(a_base_url):\n                sport_id = a_sport_id\n                break\n        else:\n            print('No sport ID found for {}, not able to check cache'.format(url))\n\n        # check whether cache is valid or stale\n        file_exists = os.path.isfile(filename)\n        if sport_id and file_exists:\n            cur_time = int(time.time())\n            mod_time = int(os.path.getmtime(filename))\n            days_since_mod = datetime.timedelta(seconds=(cur_time - mod_time)).days\n            days_cache_valid = globals()['_days_valid_{}'.format(sport_id)](url)\n            cache_is_valid = days_since_mod < days_cache_valid\n        else:\n            cache_is_valid = False\n\n        # if file found and cache is valid, read from file\n        allow_caching = sportsref.get_option('cache')\n        if file_exists and cache_is_valid and allow_caching:\n            with codecs.open(filename, 'r', encoding='utf-8', errors='replace') as f:\n                text = f.read()\n        # otherwise, execute function and cache results\n        else:\n            text = func(url)\n            with codecs.open(filename, 'w+', encoding='utf-8') as f:\n                f.write(text)\n        return text\n\n    return wrapper", "language": "python", "code": "def cache(func):\n    \"\"\"Caches the HTML returned by the specified function `func`. Caches it in\n    the user cache determined by the appdirs package.\n    \"\"\"\n\n    CACHE_DIR = appdirs.user_cache_dir('sportsref', getpass.getuser())\n    if not os.path.isdir(CACHE_DIR):\n        os.makedirs(CACHE_DIR)\n\n    @funcutils.wraps(func)\n    def wrapper(url):\n        # hash based on the URL\n        file_hash = hashlib.md5()\n        encoded_url = url.encode(errors='replace')\n        file_hash.update(encoded_url)\n        file_hash = file_hash.hexdigest()\n        filename = '{}/{}'.format(CACHE_DIR, file_hash)\n\n        sport_id = None\n        for a_base_url, a_sport_id in sportsref.SITE_ABBREV.items():\n            if url.startswith(a_base_url):\n                sport_id = a_sport_id\n                break\n        else:\n            print('No sport ID found for {}, not able to check cache'.format(url))\n\n        # check whether cache is valid or stale\n        file_exists = os.path.isfile(filename)\n        if sport_id and file_exists:\n            cur_time = int(time.time())\n            mod_time = int(os.path.getmtime(filename))\n            days_since_mod = datetime.timedelta(seconds=(cur_time - mod_time)).days\n            days_cache_valid = globals()['_days_valid_{}'.format(sport_id)](url)\n            cache_is_valid = days_since_mod < days_cache_valid\n        else:\n            cache_is_valid = False\n\n        # if file found and cache is valid, read from file\n        allow_caching = sportsref.get_option('cache')\n        if file_exists and cache_is_valid and allow_caching:\n            with codecs.open(filename, 'r', encoding='utf-8', errors='replace') as f:\n                text = f.read()\n        # otherwise, execute function and cache results\n        else:\n            text = func(url)\n            with codecs.open(filename, 'w+', encoding='utf-8') as f:\n                f.write(text)\n        return text\n\n    return wrapper", "code_tokens": ["def", "cache", "(", "func", ")", ":", "CACHE_DIR", "=", "appdirs", ".", "user_cache_dir", "(", "'sportsref'", ",", "getpass", ".", "getuser", "(", ")", ")", "if", "not", "os", ".", "path", ".", "isdir", "(", "CACHE_DIR", ")", ":", "os", ".", "makedirs", "(", "CACHE_DIR", ")", "@", "funcutils", ".", "wraps", "(", "func", ")", "def", "wrapper", "(", "url", ")", ":", "# hash based on the URL", "file_hash", "=", "hashlib", ".", "md5", "(", ")", "encoded_url", "=", "url", ".", "encode", "(", "errors", "=", "'replace'", ")", "file_hash", ".", "update", "(", "encoded_url", ")", "file_hash", "=", "file_hash", ".", "hexdigest", "(", ")", "filename", "=", "'{}/{}'", ".", "format", "(", "CACHE_DIR", ",", "file_hash", ")", "sport_id", "=", "None", "for", "a_base_url", ",", "a_sport_id", "in", "sportsref", ".", "SITE_ABBREV", ".", "items", "(", ")", ":", "if", "url", ".", "startswith", "(", "a_base_url", ")", ":", "sport_id", "=", "a_sport_id", "break", "else", ":", "print", "(", "'No sport ID found for {}, not able to check cache'", ".", "format", "(", "url", ")", ")", "# check whether cache is valid or stale", "file_exists", "=", "os", ".", "path", ".", "isfile", "(", "filename", ")", "if", "sport_id", "and", "file_exists", ":", "cur_time", "=", "int", "(", "time", ".", "time", "(", ")", ")", "mod_time", "=", "int", "(", "os", ".", "path", ".", "getmtime", "(", "filename", ")", ")", "days_since_mod", "=", "datetime", ".", "timedelta", "(", "seconds", "=", "(", "cur_time", "-", "mod_time", ")", ")", ".", "days", "days_cache_valid", "=", "globals", "(", ")", "[", "'_days_valid_{}'", ".", "format", "(", "sport_id", ")", "]", "(", "url", ")", "cache_is_valid", "=", "days_since_mod", "<", "days_cache_valid", "else", ":", "cache_is_valid", "=", "False", "# if file found and cache is valid, read from file", "allow_caching", "=", "sportsref", ".", "get_option", "(", "'cache'", ")", "if", "file_exists", "and", "cache_is_valid", "and", "allow_caching", ":", "with", "codecs", ".", "open", "(", "filename", ",", "'r'", ",", "encoding", "=", "'utf-8'", ",", "errors", "=", "'replace'", ")", "as", "f", ":", "text", "=", "f", ".", "read", "(", ")", "# otherwise, execute function and cache results", "else", ":", "text", "=", "func", "(", "url", ")", "with", "codecs", ".", "open", "(", "filename", ",", "'w+'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "f", ".", "write", "(", "text", ")", "return", "text", "return", "wrapper"], "docstring": "Caches the HTML returned by the specified function `func`. Caches it in\n    the user cache determined by the appdirs package.", "docstring_tokens": ["Caches", "the", "HTML", "returned", "by", "the", "specified", "function", "func", ".", "Caches", "it", "in", "the", "user", "cache", "determined", "by", "the", "appdirs", "package", "."], "sha": "09f11ac856a23c96d666d1d510bb35d6f050b5c3", "url": "https://github.com/mdgoldberg/sportsref/blob/09f11ac856a23c96d666d1d510bb35d6f050b5c3/sportsref/decorators.py#L95-L144", "partition": "test"}
{"repo": "rwl/godot", "path": "godot/node.py", "func_name": "Node._on_position_change", "original_string": "def _on_position_change(self, new):\n        \"\"\" Handles the poition of the component changing.\n        \"\"\"\n        w, h = self.component.bounds\n        self.pos = tuple([ new[0] + (w/2), new[1] + (h/2) ])", "language": "python", "code": "def _on_position_change(self, new):\n        \"\"\" Handles the poition of the component changing.\n        \"\"\"\n        w, h = self.component.bounds\n        self.pos = tuple([ new[0] + (w/2), new[1] + (h/2) ])", "code_tokens": ["def", "_on_position_change", "(", "self", ",", "new", ")", ":", "w", ",", "h", "=", "self", ".", "component", ".", "bounds", "self", ".", "pos", "=", "tuple", "(", "[", "new", "[", "0", "]", "+", "(", "w", "/", "2", ")", ",", "new", "[", "1", "]", "+", "(", "h", "/", "2", ")", "]", ")"], "docstring": "Handles the poition of the component changing.", "docstring_tokens": ["Handles", "the", "poition", "of", "the", "component", "changing", "."], "sha": "013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f", "url": "https://github.com/rwl/godot/blob/013687c9e8983d2aa2ceebb8a76c5c4f1e37c90f/godot/node.py#L709-L713", "partition": "test"}
{"repo": "3DLIRIOUS/MeshLabXML", "path": "meshlabxml/layers.py", "func_name": "delete_lower", "original_string": "def delete_lower(script, layer_num=None):\n    \"\"\" Delete all layers below the specified one.\n\n    Useful for MeshLab ver 2016.12, whcih will only output layer 0.\n    \"\"\"\n    if layer_num is None:\n        layer_num = script.current_layer()\n    if layer_num != 0:\n        change(script, 0)\n    for i in range(layer_num):\n        delete(script, 0)\n    return None", "language": "python", "code": "def delete_lower(script, layer_num=None):\n    \"\"\" Delete all layers below the specified one.\n\n    Useful for MeshLab ver 2016.12, whcih will only output layer 0.\n    \"\"\"\n    if layer_num is None:\n        layer_num = script.current_layer()\n    if layer_num != 0:\n        change(script, 0)\n    for i in range(layer_num):\n        delete(script, 0)\n    return None", "code_tokens": ["def", "delete_lower", "(", "script", ",", "layer_num", "=", "None", ")", ":", "if", "layer_num", "is", "None", ":", "layer_num", "=", "script", ".", "current_layer", "(", ")", "if", "layer_num", "!=", "0", ":", "change", "(", "script", ",", "0", ")", "for", "i", "in", "range", "(", "layer_num", ")", ":", "delete", "(", "script", ",", "0", ")", "return", "None"], "docstring": "Delete all layers below the specified one.\n\n    Useful for MeshLab ver 2016.12, whcih will only output layer 0.", "docstring_tokens": ["Delete", "all", "layers", "below", "the", "specified", "one", "."], "sha": "177cce21e92baca500f56a932d66bd9a33257af8", "url": "https://github.com/3DLIRIOUS/MeshLabXML/blob/177cce21e92baca500f56a932d66bd9a33257af8/meshlabxml/layers.py#L273-L284", "partition": "test"}
{"repo": "MaxStrange/AudioSegment", "path": "algorithms/asa.py", "func_name": "_remove_overlaps", "original_string": "def _remove_overlaps(segmentation_mask, fronts):\n    \"\"\"\n    Removes all points in the fronts that overlap with the segmentation mask.\n    \"\"\"\n    fidxs, sidxs = np.where((segmentation_mask != fronts) & (segmentation_mask != 0) & (fronts != 0))\n    fronts[fidxs, sidxs] = 0", "language": "python", "code": "def _remove_overlaps(segmentation_mask, fronts):\n    \"\"\"\n    Removes all points in the fronts that overlap with the segmentation mask.\n    \"\"\"\n    fidxs, sidxs = np.where((segmentation_mask != fronts) & (segmentation_mask != 0) & (fronts != 0))\n    fronts[fidxs, sidxs] = 0", "code_tokens": ["def", "_remove_overlaps", "(", "segmentation_mask", ",", "fronts", ")", ":", "fidxs", ",", "sidxs", "=", "np", ".", "where", "(", "(", "segmentation_mask", "!=", "fronts", ")", "&", "(", "segmentation_mask", "!=", "0", ")", "&", "(", "fronts", "!=", "0", ")", ")", "fronts", "[", "fidxs", ",", "sidxs", "]", "=", "0"], "docstring": "Removes all points in the fronts that overlap with the segmentation mask.", "docstring_tokens": ["Removes", "all", "points", "in", "the", "fronts", "that", "overlap", "with", "the", "segmentation", "mask", "."], "sha": "1daefb8de626ddff3ff7016697c3ad31d262ecd6", "url": "https://github.com/MaxStrange/AudioSegment/blob/1daefb8de626ddff3ff7016697c3ad31d262ecd6/algorithms/asa.py#L632-L637", "partition": "test"}
{"repo": "zomux/deepy", "path": "deepy/layers/layer.py", "func_name": "NeuralLayer.register_training_updates", "original_string": "def register_training_updates(self, *updates):\n        \"\"\"\n        Register updates that will only be executed in training phase.\n        \"\"\"\n        for key, node in updates:\n            if key not in self._registered_training_updates:\n                self.training_updates.append((key, node))\n                self._registered_training_updates.add(key)", "language": "python", "code": "def register_training_updates(self, *updates):\n        \"\"\"\n        Register updates that will only be executed in training phase.\n        \"\"\"\n        for key, node in updates:\n            if key not in self._registered_training_updates:\n                self.training_updates.append((key, node))\n                self._registered_training_updates.add(key)", "code_tokens": ["def", "register_training_updates", "(", "self", ",", "*", "updates", ")", ":", "for", "key", ",", "node", "in", "updates", ":", "if", "key", "not", "in", "self", ".", "_registered_training_updates", ":", "self", ".", "training_updates", ".", "append", "(", "(", "key", ",", "node", ")", ")", "self", ".", "_registered_training_updates", ".", "add", "(", "key", ")"], "docstring": "Register updates that will only be executed in training phase.", "docstring_tokens": ["Register", "updates", "that", "will", "only", "be", "executed", "in", "training", "phase", "."], "sha": "090fbad22a08a809b12951cd0d4984f5bd432698", "url": "https://github.com/zomux/deepy/blob/090fbad22a08a809b12951cd0d4984f5bd432698/deepy/layers/layer.py#L160-L167", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/utils/operator_helpers.py", "func_name": "context_to_airflow_vars", "original_string": "def context_to_airflow_vars(context, in_env_var_format=False):\n    \"\"\"\n    Given a context, this function provides a dictionary of values that can be used to\n    externally reconstruct relations between dags, dag_runs, tasks and task_instances.\n    Default to abc.def.ghi format and can be made to ABC_DEF_GHI format if\n    in_env_var_format is set to True.\n\n    :param context: The context for the task_instance of interest.\n    :type context: dict\n    :param in_env_var_format: If returned vars should be in ABC_DEF_GHI format.\n    :type in_env_var_format: bool\n    :return: task_instance context as dict.\n    \"\"\"\n    params = dict()\n    if in_env_var_format:\n        name_format = 'env_var_format'\n    else:\n        name_format = 'default'\n    task_instance = context.get('task_instance')\n    if task_instance and task_instance.dag_id:\n        params[AIRFLOW_VAR_NAME_FORMAT_MAPPING['AIRFLOW_CONTEXT_DAG_ID'][\n            name_format]] = task_instance.dag_id\n    if task_instance and task_instance.task_id:\n        params[AIRFLOW_VAR_NAME_FORMAT_MAPPING['AIRFLOW_CONTEXT_TASK_ID'][\n            name_format]] = task_instance.task_id\n    if task_instance and task_instance.execution_date:\n        params[\n            AIRFLOW_VAR_NAME_FORMAT_MAPPING['AIRFLOW_CONTEXT_EXECUTION_DATE'][\n                name_format]] = task_instance.execution_date.isoformat()\n    dag_run = context.get('dag_run')\n    if dag_run and dag_run.run_id:\n        params[AIRFLOW_VAR_NAME_FORMAT_MAPPING['AIRFLOW_CONTEXT_DAG_RUN_ID'][\n            name_format]] = dag_run.run_id\n    return params", "language": "python", "code": "def context_to_airflow_vars(context, in_env_var_format=False):\n    \"\"\"\n    Given a context, this function provides a dictionary of values that can be used to\n    externally reconstruct relations between dags, dag_runs, tasks and task_instances.\n    Default to abc.def.ghi format and can be made to ABC_DEF_GHI format if\n    in_env_var_format is set to True.\n\n    :param context: The context for the task_instance of interest.\n    :type context: dict\n    :param in_env_var_format: If returned vars should be in ABC_DEF_GHI format.\n    :type in_env_var_format: bool\n    :return: task_instance context as dict.\n    \"\"\"\n    params = dict()\n    if in_env_var_format:\n        name_format = 'env_var_format'\n    else:\n        name_format = 'default'\n    task_instance = context.get('task_instance')\n    if task_instance and task_instance.dag_id:\n        params[AIRFLOW_VAR_NAME_FORMAT_MAPPING['AIRFLOW_CONTEXT_DAG_ID'][\n            name_format]] = task_instance.dag_id\n    if task_instance and task_instance.task_id:\n        params[AIRFLOW_VAR_NAME_FORMAT_MAPPING['AIRFLOW_CONTEXT_TASK_ID'][\n            name_format]] = task_instance.task_id\n    if task_instance and task_instance.execution_date:\n        params[\n            AIRFLOW_VAR_NAME_FORMAT_MAPPING['AIRFLOW_CONTEXT_EXECUTION_DATE'][\n                name_format]] = task_instance.execution_date.isoformat()\n    dag_run = context.get('dag_run')\n    if dag_run and dag_run.run_id:\n        params[AIRFLOW_VAR_NAME_FORMAT_MAPPING['AIRFLOW_CONTEXT_DAG_RUN_ID'][\n            name_format]] = dag_run.run_id\n    return params", "code_tokens": ["def", "context_to_airflow_vars", "(", "context", ",", "in_env_var_format", "=", "False", ")", ":", "params", "=", "dict", "(", ")", "if", "in_env_var_format", ":", "name_format", "=", "'env_var_format'", "else", ":", "name_format", "=", "'default'", "task_instance", "=", "context", ".", "get", "(", "'task_instance'", ")", "if", "task_instance", "and", "task_instance", ".", "dag_id", ":", "params", "[", "AIRFLOW_VAR_NAME_FORMAT_MAPPING", "[", "'AIRFLOW_CONTEXT_DAG_ID'", "]", "[", "name_format", "]", "]", "=", "task_instance", ".", "dag_id", "if", "task_instance", "and", "task_instance", ".", "task_id", ":", "params", "[", "AIRFLOW_VAR_NAME_FORMAT_MAPPING", "[", "'AIRFLOW_CONTEXT_TASK_ID'", "]", "[", "name_format", "]", "]", "=", "task_instance", ".", "task_id", "if", "task_instance", "and", "task_instance", ".", "execution_date", ":", "params", "[", "AIRFLOW_VAR_NAME_FORMAT_MAPPING", "[", "'AIRFLOW_CONTEXT_EXECUTION_DATE'", "]", "[", "name_format", "]", "]", "=", "task_instance", ".", "execution_date", ".", "isoformat", "(", ")", "dag_run", "=", "context", ".", "get", "(", "'dag_run'", ")", "if", "dag_run", "and", "dag_run", ".", "run_id", ":", "params", "[", "AIRFLOW_VAR_NAME_FORMAT_MAPPING", "[", "'AIRFLOW_CONTEXT_DAG_RUN_ID'", "]", "[", "name_format", "]", "]", "=", "dag_run", ".", "run_id", "return", "params"], "docstring": "Given a context, this function provides a dictionary of values that can be used to\n    externally reconstruct relations between dags, dag_runs, tasks and task_instances.\n    Default to abc.def.ghi format and can be made to ABC_DEF_GHI format if\n    in_env_var_format is set to True.\n\n    :param context: The context for the task_instance of interest.\n    :type context: dict\n    :param in_env_var_format: If returned vars should be in ABC_DEF_GHI format.\n    :type in_env_var_format: bool\n    :return: task_instance context as dict.", "docstring_tokens": ["Given", "a", "context", "this", "function", "provides", "a", "dictionary", "of", "values", "that", "can", "be", "used", "to", "externally", "reconstruct", "relations", "between", "dags", "dag_runs", "tasks", "and", "task_instances", ".", "Default", "to", "abc", ".", "def", ".", "ghi", "format", "and", "can", "be", "made", "to", "ABC_DEF_GHI", "format", "if", "in_env_var_format", "is", "set", "to", "True", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/operator_helpers.py#L33-L66", "partition": "test"}
{"repo": "buzzfeed/phonon", "path": "phonon/reference.py", "func_name": "Reference.dereference", "original_string": "def dereference(self, callback=None, args=None, kwargs=None):\n        \"\"\"\n        This method should only be called while the reference is locked.\n\n        Decrements the reference count for the resource. If this process holds\n        the only reference at the time we finish dereferencing it; True is\n        returned. Operating on the resource after it has been dereferenced is\n        undefined behavior.\n\n        Dereference queries the value stored in the backend, if any, iff (if\n        and only if) this instance is the last reference to that resource. e.g.\n        self.count() == 0\n\n        :param function callback: A function to execute iff it's determined\n            this process holds the only reference to the resource. When there\n            is a failure communicating with the backend in the cleanup step the\n            callback function will be called an additional time for that\n            failure and each subsequent one thereafter. Ensure your callback\n            handles this properly.\n        :param tuple args: Positional arguments to pass your callback.\n        :param dict kwargs: keyword arguments to pass your callback.\n\n        :returns: Whether or not there are no more references among all\n            processes. True if this was the last reference. False otherwise.\n        :rtype: bool\n        \"\"\"\n        if args is None:\n            args = tuple()\n        if kwargs is None:\n            kwargs = {}\n\n        client = self.conn.client\n\n        should_execute = False\n        if self.force_expiry:\n            should_execute = True\n\n        if not should_execute:\n            self.nodelist.remove_node(self.conn.id)\n            self.nodelist.remove_expired_nodes()\n\n            updated_refcount = client.incr(self.refcount_key, -1)\n            should_execute = (updated_refcount <= 0)  # When we force expiry this will be -1\n\n        try:\n            if callable(callback) and should_execute:\n                callback(*args, **kwargs)\n        finally:\n            if should_execute:\n                client.delete(self.resource_key,\n                              self.nodelist.nodelist_key,\n                              self.times_modified_key,\n                              self.refcount_key)\n\n            self.conn.remove_from_registry(self.resource_key)\n        return should_execute", "language": "python", "code": "def dereference(self, callback=None, args=None, kwargs=None):\n        \"\"\"\n        This method should only be called while the reference is locked.\n\n        Decrements the reference count for the resource. If this process holds\n        the only reference at the time we finish dereferencing it; True is\n        returned. Operating on the resource after it has been dereferenced is\n        undefined behavior.\n\n        Dereference queries the value stored in the backend, if any, iff (if\n        and only if) this instance is the last reference to that resource. e.g.\n        self.count() == 0\n\n        :param function callback: A function to execute iff it's determined\n            this process holds the only reference to the resource. When there\n            is a failure communicating with the backend in the cleanup step the\n            callback function will be called an additional time for that\n            failure and each subsequent one thereafter. Ensure your callback\n            handles this properly.\n        :param tuple args: Positional arguments to pass your callback.\n        :param dict kwargs: keyword arguments to pass your callback.\n\n        :returns: Whether or not there are no more references among all\n            processes. True if this was the last reference. False otherwise.\n        :rtype: bool\n        \"\"\"\n        if args is None:\n            args = tuple()\n        if kwargs is None:\n            kwargs = {}\n\n        client = self.conn.client\n\n        should_execute = False\n        if self.force_expiry:\n            should_execute = True\n\n        if not should_execute:\n            self.nodelist.remove_node(self.conn.id)\n            self.nodelist.remove_expired_nodes()\n\n            updated_refcount = client.incr(self.refcount_key, -1)\n            should_execute = (updated_refcount <= 0)  # When we force expiry this will be -1\n\n        try:\n            if callable(callback) and should_execute:\n                callback(*args, **kwargs)\n        finally:\n            if should_execute:\n                client.delete(self.resource_key,\n                              self.nodelist.nodelist_key,\n                              self.times_modified_key,\n                              self.refcount_key)\n\n            self.conn.remove_from_registry(self.resource_key)\n        return should_execute", "code_tokens": ["def", "dereference", "(", "self", ",", "callback", "=", "None", ",", "args", "=", "None", ",", "kwargs", "=", "None", ")", ":", "if", "args", "is", "None", ":", "args", "=", "tuple", "(", ")", "if", "kwargs", "is", "None", ":", "kwargs", "=", "{", "}", "client", "=", "self", ".", "conn", ".", "client", "should_execute", "=", "False", "if", "self", ".", "force_expiry", ":", "should_execute", "=", "True", "if", "not", "should_execute", ":", "self", ".", "nodelist", ".", "remove_node", "(", "self", ".", "conn", ".", "id", ")", "self", ".", "nodelist", ".", "remove_expired_nodes", "(", ")", "updated_refcount", "=", "client", ".", "incr", "(", "self", ".", "refcount_key", ",", "-", "1", ")", "should_execute", "=", "(", "updated_refcount", "<=", "0", ")", "# When we force expiry this will be -1", "try", ":", "if", "callable", "(", "callback", ")", "and", "should_execute", ":", "callback", "(", "*", "args", ",", "*", "*", "kwargs", ")", "finally", ":", "if", "should_execute", ":", "client", ".", "delete", "(", "self", ".", "resource_key", ",", "self", ".", "nodelist", ".", "nodelist_key", ",", "self", ".", "times_modified_key", ",", "self", ".", "refcount_key", ")", "self", ".", "conn", ".", "remove_from_registry", "(", "self", ".", "resource_key", ")", "return", "should_execute"], "docstring": "This method should only be called while the reference is locked.\n\n        Decrements the reference count for the resource. If this process holds\n        the only reference at the time we finish dereferencing it; True is\n        returned. Operating on the resource after it has been dereferenced is\n        undefined behavior.\n\n        Dereference queries the value stored in the backend, if any, iff (if\n        and only if) this instance is the last reference to that resource. e.g.\n        self.count() == 0\n\n        :param function callback: A function to execute iff it's determined\n            this process holds the only reference to the resource. When there\n            is a failure communicating with the backend in the cleanup step the\n            callback function will be called an additional time for that\n            failure and each subsequent one thereafter. Ensure your callback\n            handles this properly.\n        :param tuple args: Positional arguments to pass your callback.\n        :param dict kwargs: keyword arguments to pass your callback.\n\n        :returns: Whether or not there are no more references among all\n            processes. True if this was the last reference. False otherwise.\n        :rtype: bool", "docstring_tokens": ["This", "method", "should", "only", "be", "called", "while", "the", "reference", "is", "locked", "."], "sha": "32fd036d64fab19c554e841f162466f6eb28b50f", "url": "https://github.com/buzzfeed/phonon/blob/32fd036d64fab19c554e841f162466f6eb28b50f/phonon/reference.py#L115-L170", "partition": "test"}
{"repo": "Azure/azure-sdk-for-python", "path": "azure-servicemanagement-legacy/azure/servicemanagement/servicemanagementclient.py", "func_name": "_ServiceManagementClient.get_operation_status", "original_string": "def get_operation_status(self, request_id):\n        '''\n        Returns the status of the specified operation. After calling an\n        asynchronous operation, you can call Get Operation Status to determine\n        whether the operation has succeeded, failed, or is still in progress.\n\n        request_id:\n            The request ID for the request you wish to track.\n        '''\n        _validate_not_none('request_id', request_id)\n        return self._perform_get(\n            '/' + self.subscription_id + '/operations/' + _str(request_id),\n            Operation)", "language": "python", "code": "def get_operation_status(self, request_id):\n        '''\n        Returns the status of the specified operation. After calling an\n        asynchronous operation, you can call Get Operation Status to determine\n        whether the operation has succeeded, failed, or is still in progress.\n\n        request_id:\n            The request ID for the request you wish to track.\n        '''\n        _validate_not_none('request_id', request_id)\n        return self._perform_get(\n            '/' + self.subscription_id + '/operations/' + _str(request_id),\n            Operation)", "code_tokens": ["def", "get_operation_status", "(", "self", ",", "request_id", ")", ":", "_validate_not_none", "(", "'request_id'", ",", "request_id", ")", "return", "self", ".", "_perform_get", "(", "'/'", "+", "self", ".", "subscription_id", "+", "'/operations/'", "+", "_str", "(", "request_id", ")", ",", "Operation", ")"], "docstring": "Returns the status of the specified operation. After calling an\n        asynchronous operation, you can call Get Operation Status to determine\n        whether the operation has succeeded, failed, or is still in progress.\n\n        request_id:\n            The request ID for the request you wish to track.", "docstring_tokens": ["Returns", "the", "status", "of", "the", "specified", "operation", ".", "After", "calling", "an", "asynchronous", "operation", "you", "can", "call", "Get", "Operation", "Status", "to", "determine", "whether", "the", "operation", "has", "succeeded", "failed", "or", "is", "still", "in", "progress", "."], "sha": "d7306fde32f60a293a7567678692bdad31e4b667", "url": "https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicemanagement-legacy/azure/servicemanagement/servicemanagementclient.py#L320-L332", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/utils/utils.py", "func_name": "register_plugins", "original_string": "def register_plugins(linter, directory):\n    \"\"\"load all module and package in the given directory, looking for a\n    'register' function in each one, used to register pylint checkers\n    \"\"\"\n    imported = {}\n    for filename in listdir(directory):\n        base, extension = splitext(filename)\n        if base in imported or base == \"__pycache__\":\n            continue\n        if (\n            extension in PY_EXTS\n            and base != \"__init__\"\n            or (not extension and isdir(join(directory, base)))\n        ):\n            try:\n                module = modutils.load_module_from_file(join(directory, filename))\n            except ValueError:\n                # empty module name (usually emacs auto-save files)\n                continue\n            except ImportError as exc:\n                print(\n                    \"Problem importing module %s: %s\" % (filename, exc), file=sys.stderr\n                )\n            else:\n                if hasattr(module, \"register\"):\n                    module.register(linter)\n                    imported[base] = 1", "language": "python", "code": "def register_plugins(linter, directory):\n    \"\"\"load all module and package in the given directory, looking for a\n    'register' function in each one, used to register pylint checkers\n    \"\"\"\n    imported = {}\n    for filename in listdir(directory):\n        base, extension = splitext(filename)\n        if base in imported or base == \"__pycache__\":\n            continue\n        if (\n            extension in PY_EXTS\n            and base != \"__init__\"\n            or (not extension and isdir(join(directory, base)))\n        ):\n            try:\n                module = modutils.load_module_from_file(join(directory, filename))\n            except ValueError:\n                # empty module name (usually emacs auto-save files)\n                continue\n            except ImportError as exc:\n                print(\n                    \"Problem importing module %s: %s\" % (filename, exc), file=sys.stderr\n                )\n            else:\n                if hasattr(module, \"register\"):\n                    module.register(linter)\n                    imported[base] = 1", "code_tokens": ["def", "register_plugins", "(", "linter", ",", "directory", ")", ":", "imported", "=", "{", "}", "for", "filename", "in", "listdir", "(", "directory", ")", ":", "base", ",", "extension", "=", "splitext", "(", "filename", ")", "if", "base", "in", "imported", "or", "base", "==", "\"__pycache__\"", ":", "continue", "if", "(", "extension", "in", "PY_EXTS", "and", "base", "!=", "\"__init__\"", "or", "(", "not", "extension", "and", "isdir", "(", "join", "(", "directory", ",", "base", ")", ")", ")", ")", ":", "try", ":", "module", "=", "modutils", ".", "load_module_from_file", "(", "join", "(", "directory", ",", "filename", ")", ")", "except", "ValueError", ":", "# empty module name (usually emacs auto-save files)", "continue", "except", "ImportError", "as", "exc", ":", "print", "(", "\"Problem importing module %s: %s\"", "%", "(", "filename", ",", "exc", ")", ",", "file", "=", "sys", ".", "stderr", ")", "else", ":", "if", "hasattr", "(", "module", ",", "\"register\"", ")", ":", "module", ".", "register", "(", "linter", ")", "imported", "[", "base", "]", "=", "1"], "docstring": "load all module and package in the given directory, looking for a\n    'register' function in each one, used to register pylint checkers", "docstring_tokens": ["load", "all", "module", "and", "package", "in", "the", "given", "directory", "looking", "for", "a", "register", "function", "in", "each", "one", "used", "to", "register", "pylint", "checkers"], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/utils/utils.py#L187-L213", "partition": "test"}
{"repo": "idlesign/django-sitetree", "path": "sitetree/admin.py", "func_name": "TreeItemAdmin.response_change", "original_string": "def response_change(self, request, obj, **kwargs):\n        \"\"\"Redirects to the appropriate items' 'add' page on item change.\n\n        As we administer tree items within tree itself, we\n        should make some changes to redirection process.\n\n        \"\"\"\n        return self._redirect(request, super(TreeItemAdmin, self).response_change(request, obj))", "language": "python", "code": "def response_change(self, request, obj, **kwargs):\n        \"\"\"Redirects to the appropriate items' 'add' page on item change.\n\n        As we administer tree items within tree itself, we\n        should make some changes to redirection process.\n\n        \"\"\"\n        return self._redirect(request, super(TreeItemAdmin, self).response_change(request, obj))", "code_tokens": ["def", "response_change", "(", "self", ",", "request", ",", "obj", ",", "*", "*", "kwargs", ")", ":", "return", "self", ".", "_redirect", "(", "request", ",", "super", "(", "TreeItemAdmin", ",", "self", ")", ".", "response_change", "(", "request", ",", "obj", ")", ")"], "docstring": "Redirects to the appropriate items' 'add' page on item change.\n\n        As we administer tree items within tree itself, we\n        should make some changes to redirection process.", "docstring_tokens": ["Redirects", "to", "the", "appropriate", "items", "add", "page", "on", "item", "change", "."], "sha": "61de4608e6e415247c75fe8691027d7c4ed0d1e7", "url": "https://github.com/idlesign/django-sitetree/blob/61de4608e6e415247c75fe8691027d7c4ed0d1e7/sitetree/admin.py#L153-L160", "partition": "test"}
{"repo": "deepmipt/DeepPavlov", "path": "deeppavlov/settings.py", "func_name": "main", "original_string": "def main():\n    \"\"\"DeepPavlov console configuration utility.\"\"\"\n    args = parser.parse_args()\n    path = get_settings_path()\n\n    if args.default:\n        if populate_settings_dir(force=True):\n            print(f'Populated {path} with default settings files')\n        else:\n            print(f'{path} is already a default settings directory')\n    else:\n        print(f'Current DeepPavlov settings path: {path}')", "language": "python", "code": "def main():\n    \"\"\"DeepPavlov console configuration utility.\"\"\"\n    args = parser.parse_args()\n    path = get_settings_path()\n\n    if args.default:\n        if populate_settings_dir(force=True):\n            print(f'Populated {path} with default settings files')\n        else:\n            print(f'{path} is already a default settings directory')\n    else:\n        print(f'Current DeepPavlov settings path: {path}')", "code_tokens": ["def", "main", "(", ")", ":", "args", "=", "parser", ".", "parse_args", "(", ")", "path", "=", "get_settings_path", "(", ")", "if", "args", ".", "default", ":", "if", "populate_settings_dir", "(", "force", "=", "True", ")", ":", "print", "(", "f'Populated {path} with default settings files'", ")", "else", ":", "print", "(", "f'{path} is already a default settings directory'", ")", "else", ":", "print", "(", "f'Current DeepPavlov settings path: {path}'", ")"], "docstring": "DeepPavlov console configuration utility.", "docstring_tokens": ["DeepPavlov", "console", "configuration", "utility", "."], "sha": "f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c", "url": "https://github.com/deepmipt/DeepPavlov/blob/f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c/deeppavlov/settings.py#L24-L35", "partition": "test"}
{"repo": "PythonSanSebastian/docstamp", "path": "docstamp/template.py", "func_name": "TextDocument.save_content", "original_string": "def save_content(self, file_path, encoding='utf-8'):\n        \"\"\" Save the content of the .txt file in a text file.\n\n        Parameters\n        ----------\n        file_path: str\n            Path to the output file.\n        \"\"\"\n        if self.file_content_ is None:\n            msg = 'Template content has not been updated. \\\n                   Please fill the template before rendering it.'\n            log.exception(msg)\n            raise ValueError(msg)\n\n        try:\n            write_to_file(file_path, content=self.file_content_,\n                          encoding=encoding)\n        except Exception as exc:\n            msg = 'Document of type {} got an error when \\\n                   writing content.'.format(self.__class__)\n            log.exception(msg)\n            raise Exception(msg) from exc", "language": "python", "code": "def save_content(self, file_path, encoding='utf-8'):\n        \"\"\" Save the content of the .txt file in a text file.\n\n        Parameters\n        ----------\n        file_path: str\n            Path to the output file.\n        \"\"\"\n        if self.file_content_ is None:\n            msg = 'Template content has not been updated. \\\n                   Please fill the template before rendering it.'\n            log.exception(msg)\n            raise ValueError(msg)\n\n        try:\n            write_to_file(file_path, content=self.file_content_,\n                          encoding=encoding)\n        except Exception as exc:\n            msg = 'Document of type {} got an error when \\\n                   writing content.'.format(self.__class__)\n            log.exception(msg)\n            raise Exception(msg) from exc", "code_tokens": ["def", "save_content", "(", "self", ",", "file_path", ",", "encoding", "=", "'utf-8'", ")", ":", "if", "self", ".", "file_content_", "is", "None", ":", "msg", "=", "'Template content has not been updated. \\\n                   Please fill the template before rendering it.'", "log", ".", "exception", "(", "msg", ")", "raise", "ValueError", "(", "msg", ")", "try", ":", "write_to_file", "(", "file_path", ",", "content", "=", "self", ".", "file_content_", ",", "encoding", "=", "encoding", ")", "except", "Exception", "as", "exc", ":", "msg", "=", "'Document of type {} got an error when \\\n                   writing content.'", ".", "format", "(", "self", ".", "__class__", ")", "log", ".", "exception", "(", "msg", ")", "raise", "Exception", "(", "msg", ")", "from", "exc"], "docstring": "Save the content of the .txt file in a text file.\n\n        Parameters\n        ----------\n        file_path: str\n            Path to the output file.", "docstring_tokens": ["Save", "the", "content", "of", "the", ".", "txt", "file", "in", "a", "text", "file", "."], "sha": "b43808f2e15351b0b2f0b7eade9c7ef319c9e646", "url": "https://github.com/PythonSanSebastian/docstamp/blob/b43808f2e15351b0b2f0b7eade9c7ef319c9e646/docstamp/template.py#L142-L163", "partition": "test"}
{"repo": "domainaware/parsedmarc", "path": "parsedmarc/splunk.py", "func_name": "HECClient.save_forensic_reports_to_splunk", "original_string": "def save_forensic_reports_to_splunk(self, forensic_reports):\n        \"\"\"\n        Saves forensic DMARC reports to Splunk\n\n        Args:\n            forensic_reports (list):  A list of forensic report dictionaries\n            to save in Splunk\n        \"\"\"\n        logger.debug(\"Saving forensic reports to Splunk\")\n        if type(forensic_reports) == dict:\n            forensic_reports = [forensic_reports]\n\n        if len(forensic_reports) < 1:\n            return\n\n        json_str = \"\"\n        for report in forensic_reports:\n            data = self._common_data.copy()\n            data[\"sourcetype\"] = \"dmarc:forensic\"\n            timestamp = human_timestamp_to_timestamp(\n                report[\"arrival_date_utc\"])\n            data[\"time\"] = timestamp\n            data[\"event\"] = report.copy()\n            json_str += \"{0}\\n\".format(json.dumps(data))\n\n        if not self.session.verify:\n            logger.debug(\"Skipping certificate verification for Splunk HEC\")\n        try:\n            response = self.session.post(self.url, data=json_str,\n                                         timeout=self.timeout)\n            response = response.json()\n        except Exception as e:\n            raise SplunkError(e.__str__())\n        if response[\"code\"] != 0:\n            raise SplunkError(response[\"text\"])", "language": "python", "code": "def save_forensic_reports_to_splunk(self, forensic_reports):\n        \"\"\"\n        Saves forensic DMARC reports to Splunk\n\n        Args:\n            forensic_reports (list):  A list of forensic report dictionaries\n            to save in Splunk\n        \"\"\"\n        logger.debug(\"Saving forensic reports to Splunk\")\n        if type(forensic_reports) == dict:\n            forensic_reports = [forensic_reports]\n\n        if len(forensic_reports) < 1:\n            return\n\n        json_str = \"\"\n        for report in forensic_reports:\n            data = self._common_data.copy()\n            data[\"sourcetype\"] = \"dmarc:forensic\"\n            timestamp = human_timestamp_to_timestamp(\n                report[\"arrival_date_utc\"])\n            data[\"time\"] = timestamp\n            data[\"event\"] = report.copy()\n            json_str += \"{0}\\n\".format(json.dumps(data))\n\n        if not self.session.verify:\n            logger.debug(\"Skipping certificate verification for Splunk HEC\")\n        try:\n            response = self.session.post(self.url, data=json_str,\n                                         timeout=self.timeout)\n            response = response.json()\n        except Exception as e:\n            raise SplunkError(e.__str__())\n        if response[\"code\"] != 0:\n            raise SplunkError(response[\"text\"])", "code_tokens": ["def", "save_forensic_reports_to_splunk", "(", "self", ",", "forensic_reports", ")", ":", "logger", ".", "debug", "(", "\"Saving forensic reports to Splunk\"", ")", "if", "type", "(", "forensic_reports", ")", "==", "dict", ":", "forensic_reports", "=", "[", "forensic_reports", "]", "if", "len", "(", "forensic_reports", ")", "<", "1", ":", "return", "json_str", "=", "\"\"", "for", "report", "in", "forensic_reports", ":", "data", "=", "self", ".", "_common_data", ".", "copy", "(", ")", "data", "[", "\"sourcetype\"", "]", "=", "\"dmarc:forensic\"", "timestamp", "=", "human_timestamp_to_timestamp", "(", "report", "[", "\"arrival_date_utc\"", "]", ")", "data", "[", "\"time\"", "]", "=", "timestamp", "data", "[", "\"event\"", "]", "=", "report", ".", "copy", "(", ")", "json_str", "+=", "\"{0}\\n\"", ".", "format", "(", "json", ".", "dumps", "(", "data", ")", ")", "if", "not", "self", ".", "session", ".", "verify", ":", "logger", ".", "debug", "(", "\"Skipping certificate verification for Splunk HEC\"", ")", "try", ":", "response", "=", "self", ".", "session", ".", "post", "(", "self", ".", "url", ",", "data", "=", "json_str", ",", "timeout", "=", "self", ".", "timeout", ")", "response", "=", "response", ".", "json", "(", ")", "except", "Exception", "as", "e", ":", "raise", "SplunkError", "(", "e", ".", "__str__", "(", ")", ")", "if", "response", "[", "\"code\"", "]", "!=", "0", ":", "raise", "SplunkError", "(", "response", "[", "\"text\"", "]", ")"], "docstring": "Saves forensic DMARC reports to Splunk\n\n        Args:\n            forensic_reports (list):  A list of forensic report dictionaries\n            to save in Splunk", "docstring_tokens": ["Saves", "forensic", "DMARC", "reports", "to", "Splunk"], "sha": "ecc9fd434c23d896ccd1f35795ccc047f946ed05", "url": "https://github.com/domainaware/parsedmarc/blob/ecc9fd434c23d896ccd1f35795ccc047f946ed05/parsedmarc/splunk.py#L125-L159", "partition": "test"}
{"repo": "gholt/swiftly", "path": "swiftly/cli/auth.py", "func_name": "cli_auth", "original_string": "def cli_auth(context):\n    \"\"\"\n    Authenticates and then outputs the resulting information.\n\n    See :py:mod:`swiftly.cli.auth` for context usage information.\n\n    See :py:class:`CLIAuth` for more information.\n    \"\"\"\n    with context.io_manager.with_stdout() as fp:\n        with context.client_manager.with_client() as client:\n            info = []\n            client.auth()\n            if getattr(client, 'auth_cache_path', None):\n                info.append(('Auth Cache', client.auth_cache_path))\n            if getattr(client, 'auth_url', None):\n                info.append(('Auth URL', client.auth_url))\n            if getattr(client, 'auth_user', None):\n                info.append(('Auth User', client.auth_user))\n            if getattr(client, 'auth_key', None):\n                info.append(('Auth Key', client.auth_key))\n            if getattr(client, 'auth_tenant', None):\n                info.append(('Auth Tenant', client.auth_tenant))\n            if getattr(client, 'auth_methods', None):\n                info.append(('Auth Methods', client.auth_methods))\n            if getattr(client, 'storage_path', None):\n                info.append(('Direct Storage Path', client.storage_path))\n            if getattr(client, 'cdn_path', None):\n                info.append(('Direct CDN Path', client.cdn_path))\n            if getattr(client, 'local_path', None):\n                info.append(('Local Path', client.local_path))\n            if getattr(client, 'regions', None):\n                info.append(('Regions', ' '.join(client.regions)))\n            if getattr(client, 'default_region', None):\n                info.append(('Default Region', client.default_region))\n            if getattr(client, 'region', None):\n                info.append(('Selected Region', client.region))\n            if getattr(client, 'snet', None):\n                info.append(('SNet', client.snet))\n            if getattr(client, 'storage_url', None):\n                info.append(('Storage URL', client.storage_url))\n            if getattr(client, 'cdn_url', None):\n                info.append(('CDN URL', client.cdn_url))\n            if getattr(client, 'auth_token', None):\n                info.append(('Auth Token', client.auth_token))\n            if not info:\n                info.append((\n                    'No auth information available',\n                    'Maybe no credentials were provided?'))\n            fmt = '%%-%ds %%s\\n' % (max(len(t) for t, v in info) + 1)\n            for t, v in info:\n                fp.write(fmt % (t + ':', v))\n            fp.flush()", "language": "python", "code": "def cli_auth(context):\n    \"\"\"\n    Authenticates and then outputs the resulting information.\n\n    See :py:mod:`swiftly.cli.auth` for context usage information.\n\n    See :py:class:`CLIAuth` for more information.\n    \"\"\"\n    with context.io_manager.with_stdout() as fp:\n        with context.client_manager.with_client() as client:\n            info = []\n            client.auth()\n            if getattr(client, 'auth_cache_path', None):\n                info.append(('Auth Cache', client.auth_cache_path))\n            if getattr(client, 'auth_url', None):\n                info.append(('Auth URL', client.auth_url))\n            if getattr(client, 'auth_user', None):\n                info.append(('Auth User', client.auth_user))\n            if getattr(client, 'auth_key', None):\n                info.append(('Auth Key', client.auth_key))\n            if getattr(client, 'auth_tenant', None):\n                info.append(('Auth Tenant', client.auth_tenant))\n            if getattr(client, 'auth_methods', None):\n                info.append(('Auth Methods', client.auth_methods))\n            if getattr(client, 'storage_path', None):\n                info.append(('Direct Storage Path', client.storage_path))\n            if getattr(client, 'cdn_path', None):\n                info.append(('Direct CDN Path', client.cdn_path))\n            if getattr(client, 'local_path', None):\n                info.append(('Local Path', client.local_path))\n            if getattr(client, 'regions', None):\n                info.append(('Regions', ' '.join(client.regions)))\n            if getattr(client, 'default_region', None):\n                info.append(('Default Region', client.default_region))\n            if getattr(client, 'region', None):\n                info.append(('Selected Region', client.region))\n            if getattr(client, 'snet', None):\n                info.append(('SNet', client.snet))\n            if getattr(client, 'storage_url', None):\n                info.append(('Storage URL', client.storage_url))\n            if getattr(client, 'cdn_url', None):\n                info.append(('CDN URL', client.cdn_url))\n            if getattr(client, 'auth_token', None):\n                info.append(('Auth Token', client.auth_token))\n            if not info:\n                info.append((\n                    'No auth information available',\n                    'Maybe no credentials were provided?'))\n            fmt = '%%-%ds %%s\\n' % (max(len(t) for t, v in info) + 1)\n            for t, v in info:\n                fp.write(fmt % (t + ':', v))\n            fp.flush()", "code_tokens": ["def", "cli_auth", "(", "context", ")", ":", "with", "context", ".", "io_manager", ".", "with_stdout", "(", ")", "as", "fp", ":", "with", "context", ".", "client_manager", ".", "with_client", "(", ")", "as", "client", ":", "info", "=", "[", "]", "client", ".", "auth", "(", ")", "if", "getattr", "(", "client", ",", "'auth_cache_path'", ",", "None", ")", ":", "info", ".", "append", "(", "(", "'Auth Cache'", ",", "client", ".", "auth_cache_path", ")", ")", "if", "getattr", "(", "client", ",", "'auth_url'", ",", "None", ")", ":", "info", ".", "append", "(", "(", "'Auth URL'", ",", "client", ".", "auth_url", ")", ")", "if", "getattr", "(", "client", ",", "'auth_user'", ",", "None", ")", ":", "info", ".", "append", "(", "(", "'Auth User'", ",", "client", ".", "auth_user", ")", ")", "if", "getattr", "(", "client", ",", "'auth_key'", ",", "None", ")", ":", "info", ".", "append", "(", "(", "'Auth Key'", ",", "client", ".", "auth_key", ")", ")", "if", "getattr", "(", "client", ",", "'auth_tenant'", ",", "None", ")", ":", "info", ".", "append", "(", "(", "'Auth Tenant'", ",", "client", ".", "auth_tenant", ")", ")", "if", "getattr", "(", "client", ",", "'auth_methods'", ",", "None", ")", ":", "info", ".", "append", "(", "(", "'Auth Methods'", ",", "client", ".", "auth_methods", ")", ")", "if", "getattr", "(", "client", ",", "'storage_path'", ",", "None", ")", ":", "info", ".", "append", "(", "(", "'Direct Storage Path'", ",", "client", ".", "storage_path", ")", ")", "if", "getattr", "(", "client", ",", "'cdn_path'", ",", "None", ")", ":", "info", ".", "append", "(", "(", "'Direct CDN Path'", ",", "client", ".", "cdn_path", ")", ")", "if", "getattr", "(", "client", ",", "'local_path'", ",", "None", ")", ":", "info", ".", "append", "(", "(", "'Local Path'", ",", "client", ".", "local_path", ")", ")", "if", "getattr", "(", "client", ",", "'regions'", ",", "None", ")", ":", "info", ".", "append", "(", "(", "'Regions'", ",", "' '", ".", "join", "(", "client", ".", "regions", ")", ")", ")", "if", "getattr", "(", "client", ",", "'default_region'", ",", "None", ")", ":", "info", ".", "append", "(", "(", "'Default Region'", ",", "client", ".", "default_region", ")", ")", "if", "getattr", "(", "client", ",", "'region'", ",", "None", ")", ":", "info", ".", "append", "(", "(", "'Selected Region'", ",", "client", ".", "region", ")", ")", "if", "getattr", "(", "client", ",", "'snet'", ",", "None", ")", ":", "info", ".", "append", "(", "(", "'SNet'", ",", "client", ".", "snet", ")", ")", "if", "getattr", "(", "client", ",", "'storage_url'", ",", "None", ")", ":", "info", ".", "append", "(", "(", "'Storage URL'", ",", "client", ".", "storage_url", ")", ")", "if", "getattr", "(", "client", ",", "'cdn_url'", ",", "None", ")", ":", "info", ".", "append", "(", "(", "'CDN URL'", ",", "client", ".", "cdn_url", ")", ")", "if", "getattr", "(", "client", ",", "'auth_token'", ",", "None", ")", ":", "info", ".", "append", "(", "(", "'Auth Token'", ",", "client", ".", "auth_token", ")", ")", "if", "not", "info", ":", "info", ".", "append", "(", "(", "'No auth information available'", ",", "'Maybe no credentials were provided?'", ")", ")", "fmt", "=", "'%%-%ds %%s\\n'", "%", "(", "max", "(", "len", "(", "t", ")", "for", "t", ",", "v", "in", "info", ")", "+", "1", ")", "for", "t", ",", "v", "in", "info", ":", "fp", ".", "write", "(", "fmt", "%", "(", "t", "+", "':'", ",", "v", ")", ")", "fp", ".", "flush", "(", ")"], "docstring": "Authenticates and then outputs the resulting information.\n\n    See :py:mod:`swiftly.cli.auth` for context usage information.\n\n    See :py:class:`CLIAuth` for more information.", "docstring_tokens": ["Authenticates", "and", "then", "outputs", "the", "resulting", "information", "."], "sha": "5bcc1c65323b1caf1f85adbefd9fc4988c072149", "url": "https://github.com/gholt/swiftly/blob/5bcc1c65323b1caf1f85adbefd9fc4988c072149/swiftly/cli/auth.py#L32-L83", "partition": "test"}
{"repo": "lepture/flask-oauthlib", "path": "flask_oauthlib/client.py", "func_name": "OAuthRemoteApp.authorize", "original_string": "def authorize(self, callback=None, state=None, **kwargs):\n        \"\"\"\n        Returns a redirect response to the remote authorization URL with\n        the signed callback given.\n\n        :param callback: a redirect url for the callback\n        :param state: an optional value to embed in the OAuth request.\n                      Use this if you want to pass around application\n                      state (e.g. CSRF tokens).\n        :param kwargs: add optional key/value pairs to the query string\n        \"\"\"\n        params = dict(self.request_token_params) or {}\n        params.update(**kwargs)\n\n        if self.request_token_url:\n            token = self.generate_request_token(callback)[0]\n            url = '%s?oauth_token=%s' % (\n                self.expand_url(self.authorize_url), url_quote(token)\n            )\n            if params:\n                url += '&' + url_encode(params)\n        else:\n            assert callback is not None, 'Callback is required for OAuth2'\n\n            client = self.make_client()\n\n            if 'scope' in params:\n                scope = params.pop('scope')\n            else:\n                scope = None\n\n            if isinstance(scope, str):\n                # oauthlib need unicode\n                scope = _encode(scope, self.encoding)\n\n            if 'state' in params:\n                if not state:\n                    state = params.pop('state')\n                else:\n                    # remove state in params\n                    params.pop('state')\n\n            if callable(state):\n                # state can be function for generate a random string\n                state = state()\n\n            session['%s_oauthredir' % self.name] = callback\n            url = client.prepare_request_uri(\n                self.expand_url(self.authorize_url),\n                redirect_uri=callback,\n                scope=scope,\n                state=state,\n                **params\n            )\n        return redirect(url)", "language": "python", "code": "def authorize(self, callback=None, state=None, **kwargs):\n        \"\"\"\n        Returns a redirect response to the remote authorization URL with\n        the signed callback given.\n\n        :param callback: a redirect url for the callback\n        :param state: an optional value to embed in the OAuth request.\n                      Use this if you want to pass around application\n                      state (e.g. CSRF tokens).\n        :param kwargs: add optional key/value pairs to the query string\n        \"\"\"\n        params = dict(self.request_token_params) or {}\n        params.update(**kwargs)\n\n        if self.request_token_url:\n            token = self.generate_request_token(callback)[0]\n            url = '%s?oauth_token=%s' % (\n                self.expand_url(self.authorize_url), url_quote(token)\n            )\n            if params:\n                url += '&' + url_encode(params)\n        else:\n            assert callback is not None, 'Callback is required for OAuth2'\n\n            client = self.make_client()\n\n            if 'scope' in params:\n                scope = params.pop('scope')\n            else:\n                scope = None\n\n            if isinstance(scope, str):\n                # oauthlib need unicode\n                scope = _encode(scope, self.encoding)\n\n            if 'state' in params:\n                if not state:\n                    state = params.pop('state')\n                else:\n                    # remove state in params\n                    params.pop('state')\n\n            if callable(state):\n                # state can be function for generate a random string\n                state = state()\n\n            session['%s_oauthredir' % self.name] = callback\n            url = client.prepare_request_uri(\n                self.expand_url(self.authorize_url),\n                redirect_uri=callback,\n                scope=scope,\n                state=state,\n                **params\n            )\n        return redirect(url)", "code_tokens": ["def", "authorize", "(", "self", ",", "callback", "=", "None", ",", "state", "=", "None", ",", "*", "*", "kwargs", ")", ":", "params", "=", "dict", "(", "self", ".", "request_token_params", ")", "or", "{", "}", "params", ".", "update", "(", "*", "*", "kwargs", ")", "if", "self", ".", "request_token_url", ":", "token", "=", "self", ".", "generate_request_token", "(", "callback", ")", "[", "0", "]", "url", "=", "'%s?oauth_token=%s'", "%", "(", "self", ".", "expand_url", "(", "self", ".", "authorize_url", ")", ",", "url_quote", "(", "token", ")", ")", "if", "params", ":", "url", "+=", "'&'", "+", "url_encode", "(", "params", ")", "else", ":", "assert", "callback", "is", "not", "None", ",", "'Callback is required for OAuth2'", "client", "=", "self", ".", "make_client", "(", ")", "if", "'scope'", "in", "params", ":", "scope", "=", "params", ".", "pop", "(", "'scope'", ")", "else", ":", "scope", "=", "None", "if", "isinstance", "(", "scope", ",", "str", ")", ":", "# oauthlib need unicode", "scope", "=", "_encode", "(", "scope", ",", "self", ".", "encoding", ")", "if", "'state'", "in", "params", ":", "if", "not", "state", ":", "state", "=", "params", ".", "pop", "(", "'state'", ")", "else", ":", "# remove state in params", "params", ".", "pop", "(", "'state'", ")", "if", "callable", "(", "state", ")", ":", "# state can be function for generate a random string", "state", "=", "state", "(", ")", "session", "[", "'%s_oauthredir'", "%", "self", ".", "name", "]", "=", "callback", "url", "=", "client", ".", "prepare_request_uri", "(", "self", ".", "expand_url", "(", "self", ".", "authorize_url", ")", ",", "redirect_uri", "=", "callback", ",", "scope", "=", "scope", ",", "state", "=", "state", ",", "*", "*", "params", ")", "return", "redirect", "(", "url", ")"], "docstring": "Returns a redirect response to the remote authorization URL with\n        the signed callback given.\n\n        :param callback: a redirect url for the callback\n        :param state: an optional value to embed in the OAuth request.\n                      Use this if you want to pass around application\n                      state (e.g. CSRF tokens).\n        :param kwargs: add optional key/value pairs to the query string", "docstring_tokens": ["Returns", "a", "redirect", "response", "to", "the", "remote", "authorization", "URL", "with", "the", "signed", "callback", "given", "."], "sha": "9e6f152a5bb360e7496210da21561c3e6d41b0e1", "url": "https://github.com/lepture/flask-oauthlib/blob/9e6f152a5bb360e7496210da21561c3e6d41b0e1/flask_oauthlib/client.py#L508-L562", "partition": "test"}
{"repo": "librosa/librosa", "path": "librosa/core/pitch.py", "func_name": "piptrack", "original_string": "def piptrack(y=None, sr=22050, S=None, n_fft=2048, hop_length=None,\n             fmin=150.0, fmax=4000.0, threshold=0.1,\n             win_length=None, window='hann', center=True, pad_mode='reflect',\n             ref=None):\n    '''Pitch tracking on thresholded parabolically-interpolated STFT.\n\n    This implementation uses the parabolic interpolation method described by [1]_.\n\n    .. [1] https://ccrma.stanford.edu/~jos/sasp/Sinusoidal_Peak_Interpolation.html\n\n    Parameters\n    ----------\n    y: np.ndarray [shape=(n,)] or None\n        audio signal\n\n    sr : number > 0 [scalar]\n        audio sampling rate of `y`\n\n    S: np.ndarray [shape=(d, t)] or None\n        magnitude or power spectrogram\n\n    n_fft : int > 0 [scalar] or None\n        number of FFT bins to use, if `y` is provided.\n\n    hop_length : int > 0 [scalar] or None\n        number of samples to hop\n\n    threshold : float in `(0, 1)`\n        A bin in spectrum `S` is considered a pitch when it is greater than\n        `threshold*ref(S)`.\n\n        By default, `ref(S)` is taken to be `max(S, axis=0)` (the maximum value in\n        each column).\n\n    fmin : float > 0 [scalar]\n        lower frequency cutoff.\n\n    fmax : float > 0 [scalar]\n        upper frequency cutoff.\n\n    win_length : int <= n_fft [scalar]\n        Each frame of audio is windowed by `window()`.\n        The window will be of length `win_length` and then padded\n        with zeros to match `n_fft`.\n\n        If unspecified, defaults to ``win_length = n_fft``.\n\n    window : string, tuple, number, function, or np.ndarray [shape=(n_fft,)]\n        - a window specification (string, tuple, or number);\n          see `scipy.signal.get_window`\n        - a window function, such as `scipy.signal.hanning`\n        - a vector or array of length `n_fft`\n\n        .. see also:: `filters.get_window`\n\n    center : boolean\n        - If `True`, the signal `y` is padded so that frame\n          `t` is centered at `y[t * hop_length]`.\n        - If `False`, then frame `t` begins at `y[t * hop_length]`\n\n    pad_mode : string\n        If `center=True`, the padding mode to use at the edges of the signal.\n        By default, STFT uses reflection padding.\n\n    ref : scalar or callable [default=np.max]\n        If scalar, the reference value against which `S` is compared for determining\n        pitches.\n\n        If callable, the reference value is computed as `ref(S, axis=0)`.\n\n    .. note::\n        One of `S` or `y` must be provided.\n\n        If `S` is not given, it is computed from `y` using\n        the default parameters of `librosa.core.stft`.\n\n    Returns\n    -------\n    pitches : np.ndarray [shape=(d, t)]\n    magnitudes : np.ndarray [shape=(d,t)]\n        Where `d` is the subset of FFT bins within `fmin` and `fmax`.\n\n        `pitches[f, t]` contains instantaneous frequency at bin\n        `f`, time `t`\n\n        `magnitudes[f, t]` contains the corresponding magnitudes.\n\n        Both `pitches` and `magnitudes` take value 0 at bins\n        of non-maximal magnitude.\n\n    Notes\n    -----\n    This function caches at level 30.\n\n    Examples\n    --------\n    Computing pitches from a waveform input\n\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n\n    Or from a spectrogram input\n\n    >>> S = np.abs(librosa.stft(y))\n    >>> pitches, magnitudes = librosa.piptrack(S=S, sr=sr)\n\n    Or with an alternate reference value for pitch detection, where\n    values above the mean spectral energy in each frame are counted as pitches\n\n    >>> pitches, magnitudes = librosa.piptrack(S=S, sr=sr, threshold=1,\n    ...                                        ref=np.mean)\n\n    '''\n\n    # Check that we received an audio time series or STFT\n    S, n_fft = _spectrogram(y=y, S=S, n_fft=n_fft, hop_length=hop_length,\n                            win_length=win_length, window=window,\n                            center=center, pad_mode=pad_mode)\n\n    # Make sure we're dealing with magnitudes\n    S = np.abs(S)\n\n    # Truncate to feasible region\n    fmin = np.maximum(fmin, 0)\n    fmax = np.minimum(fmax, float(sr) / 2)\n\n    fft_freqs = time_frequency.fft_frequencies(sr=sr, n_fft=n_fft)\n\n    # Do the parabolic interpolation everywhere,\n    # then figure out where the peaks are\n    # then restrict to the feasible range (fmin:fmax)\n    avg = 0.5 * (S[2:] - S[:-2])\n\n    shift = 2 * S[1:-1] - S[2:] - S[:-2]\n\n    # Suppress divide-by-zeros.\n    # Points where shift == 0 will never be selected by localmax anyway\n    shift = avg / (shift + (np.abs(shift) < util.tiny(shift)))\n\n    # Pad back up to the same shape as S\n    avg = np.pad(avg, ([1, 1], [0, 0]), mode='constant')\n    shift = np.pad(shift, ([1, 1], [0, 0]), mode='constant')\n\n    dskew = 0.5 * avg * shift\n\n    # Pre-allocate output\n    pitches = np.zeros_like(S)\n    mags = np.zeros_like(S)\n\n    # Clip to the viable frequency range\n    freq_mask = ((fmin <= fft_freqs) & (fft_freqs < fmax)).reshape((-1, 1))\n\n    # Compute the column-wise local max of S after thresholding\n    # Find the argmax coordinates\n    if ref is None:\n        ref = np.max\n\n    if six.callable(ref):\n        ref_value = threshold * ref(S, axis=0)\n    else:\n        ref_value = np.abs(ref)\n\n    idx = np.argwhere(freq_mask & util.localmax(S * (S > ref_value)))\n\n    # Store pitch and magnitude\n    pitches[idx[:, 0], idx[:, 1]] = ((idx[:, 0] + shift[idx[:, 0], idx[:, 1]])\n                                     * float(sr) / n_fft)\n\n    mags[idx[:, 0], idx[:, 1]] = (S[idx[:, 0], idx[:, 1]]\n                                  + dskew[idx[:, 0], idx[:, 1]])\n\n    return pitches, mags", "language": "python", "code": "def piptrack(y=None, sr=22050, S=None, n_fft=2048, hop_length=None,\n             fmin=150.0, fmax=4000.0, threshold=0.1,\n             win_length=None, window='hann', center=True, pad_mode='reflect',\n             ref=None):\n    '''Pitch tracking on thresholded parabolically-interpolated STFT.\n\n    This implementation uses the parabolic interpolation method described by [1]_.\n\n    .. [1] https://ccrma.stanford.edu/~jos/sasp/Sinusoidal_Peak_Interpolation.html\n\n    Parameters\n    ----------\n    y: np.ndarray [shape=(n,)] or None\n        audio signal\n\n    sr : number > 0 [scalar]\n        audio sampling rate of `y`\n\n    S: np.ndarray [shape=(d, t)] or None\n        magnitude or power spectrogram\n\n    n_fft : int > 0 [scalar] or None\n        number of FFT bins to use, if `y` is provided.\n\n    hop_length : int > 0 [scalar] or None\n        number of samples to hop\n\n    threshold : float in `(0, 1)`\n        A bin in spectrum `S` is considered a pitch when it is greater than\n        `threshold*ref(S)`.\n\n        By default, `ref(S)` is taken to be `max(S, axis=0)` (the maximum value in\n        each column).\n\n    fmin : float > 0 [scalar]\n        lower frequency cutoff.\n\n    fmax : float > 0 [scalar]\n        upper frequency cutoff.\n\n    win_length : int <= n_fft [scalar]\n        Each frame of audio is windowed by `window()`.\n        The window will be of length `win_length` and then padded\n        with zeros to match `n_fft`.\n\n        If unspecified, defaults to ``win_length = n_fft``.\n\n    window : string, tuple, number, function, or np.ndarray [shape=(n_fft,)]\n        - a window specification (string, tuple, or number);\n          see `scipy.signal.get_window`\n        - a window function, such as `scipy.signal.hanning`\n        - a vector or array of length `n_fft`\n\n        .. see also:: `filters.get_window`\n\n    center : boolean\n        - If `True`, the signal `y` is padded so that frame\n          `t` is centered at `y[t * hop_length]`.\n        - If `False`, then frame `t` begins at `y[t * hop_length]`\n\n    pad_mode : string\n        If `center=True`, the padding mode to use at the edges of the signal.\n        By default, STFT uses reflection padding.\n\n    ref : scalar or callable [default=np.max]\n        If scalar, the reference value against which `S` is compared for determining\n        pitches.\n\n        If callable, the reference value is computed as `ref(S, axis=0)`.\n\n    .. note::\n        One of `S` or `y` must be provided.\n\n        If `S` is not given, it is computed from `y` using\n        the default parameters of `librosa.core.stft`.\n\n    Returns\n    -------\n    pitches : np.ndarray [shape=(d, t)]\n    magnitudes : np.ndarray [shape=(d,t)]\n        Where `d` is the subset of FFT bins within `fmin` and `fmax`.\n\n        `pitches[f, t]` contains instantaneous frequency at bin\n        `f`, time `t`\n\n        `magnitudes[f, t]` contains the corresponding magnitudes.\n\n        Both `pitches` and `magnitudes` take value 0 at bins\n        of non-maximal magnitude.\n\n    Notes\n    -----\n    This function caches at level 30.\n\n    Examples\n    --------\n    Computing pitches from a waveform input\n\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n\n    Or from a spectrogram input\n\n    >>> S = np.abs(librosa.stft(y))\n    >>> pitches, magnitudes = librosa.piptrack(S=S, sr=sr)\n\n    Or with an alternate reference value for pitch detection, where\n    values above the mean spectral energy in each frame are counted as pitches\n\n    >>> pitches, magnitudes = librosa.piptrack(S=S, sr=sr, threshold=1,\n    ...                                        ref=np.mean)\n\n    '''\n\n    # Check that we received an audio time series or STFT\n    S, n_fft = _spectrogram(y=y, S=S, n_fft=n_fft, hop_length=hop_length,\n                            win_length=win_length, window=window,\n                            center=center, pad_mode=pad_mode)\n\n    # Make sure we're dealing with magnitudes\n    S = np.abs(S)\n\n    # Truncate to feasible region\n    fmin = np.maximum(fmin, 0)\n    fmax = np.minimum(fmax, float(sr) / 2)\n\n    fft_freqs = time_frequency.fft_frequencies(sr=sr, n_fft=n_fft)\n\n    # Do the parabolic interpolation everywhere,\n    # then figure out where the peaks are\n    # then restrict to the feasible range (fmin:fmax)\n    avg = 0.5 * (S[2:] - S[:-2])\n\n    shift = 2 * S[1:-1] - S[2:] - S[:-2]\n\n    # Suppress divide-by-zeros.\n    # Points where shift == 0 will never be selected by localmax anyway\n    shift = avg / (shift + (np.abs(shift) < util.tiny(shift)))\n\n    # Pad back up to the same shape as S\n    avg = np.pad(avg, ([1, 1], [0, 0]), mode='constant')\n    shift = np.pad(shift, ([1, 1], [0, 0]), mode='constant')\n\n    dskew = 0.5 * avg * shift\n\n    # Pre-allocate output\n    pitches = np.zeros_like(S)\n    mags = np.zeros_like(S)\n\n    # Clip to the viable frequency range\n    freq_mask = ((fmin <= fft_freqs) & (fft_freqs < fmax)).reshape((-1, 1))\n\n    # Compute the column-wise local max of S after thresholding\n    # Find the argmax coordinates\n    if ref is None:\n        ref = np.max\n\n    if six.callable(ref):\n        ref_value = threshold * ref(S, axis=0)\n    else:\n        ref_value = np.abs(ref)\n\n    idx = np.argwhere(freq_mask & util.localmax(S * (S > ref_value)))\n\n    # Store pitch and magnitude\n    pitches[idx[:, 0], idx[:, 1]] = ((idx[:, 0] + shift[idx[:, 0], idx[:, 1]])\n                                     * float(sr) / n_fft)\n\n    mags[idx[:, 0], idx[:, 1]] = (S[idx[:, 0], idx[:, 1]]\n                                  + dskew[idx[:, 0], idx[:, 1]])\n\n    return pitches, mags", "code_tokens": ["def", "piptrack", "(", "y", "=", "None", ",", "sr", "=", "22050", ",", "S", "=", "None", ",", "n_fft", "=", "2048", ",", "hop_length", "=", "None", ",", "fmin", "=", "150.0", ",", "fmax", "=", "4000.0", ",", "threshold", "=", "0.1", ",", "win_length", "=", "None", ",", "window", "=", "'hann'", ",", "center", "=", "True", ",", "pad_mode", "=", "'reflect'", ",", "ref", "=", "None", ")", ":", "# Check that we received an audio time series or STFT", "S", ",", "n_fft", "=", "_spectrogram", "(", "y", "=", "y", ",", "S", "=", "S", ",", "n_fft", "=", "n_fft", ",", "hop_length", "=", "hop_length", ",", "win_length", "=", "win_length", ",", "window", "=", "window", ",", "center", "=", "center", ",", "pad_mode", "=", "pad_mode", ")", "# Make sure we're dealing with magnitudes", "S", "=", "np", ".", "abs", "(", "S", ")", "# Truncate to feasible region", "fmin", "=", "np", ".", "maximum", "(", "fmin", ",", "0", ")", "fmax", "=", "np", ".", "minimum", "(", "fmax", ",", "float", "(", "sr", ")", "/", "2", ")", "fft_freqs", "=", "time_frequency", ".", "fft_frequencies", "(", "sr", "=", "sr", ",", "n_fft", "=", "n_fft", ")", "# Do the parabolic interpolation everywhere,", "# then figure out where the peaks are", "# then restrict to the feasible range (fmin:fmax)", "avg", "=", "0.5", "*", "(", "S", "[", "2", ":", "]", "-", "S", "[", ":", "-", "2", "]", ")", "shift", "=", "2", "*", "S", "[", "1", ":", "-", "1", "]", "-", "S", "[", "2", ":", "]", "-", "S", "[", ":", "-", "2", "]", "# Suppress divide-by-zeros.", "# Points where shift == 0 will never be selected by localmax anyway", "shift", "=", "avg", "/", "(", "shift", "+", "(", "np", ".", "abs", "(", "shift", ")", "<", "util", ".", "tiny", "(", "shift", ")", ")", ")", "# Pad back up to the same shape as S", "avg", "=", "np", ".", "pad", "(", "avg", ",", "(", "[", "1", ",", "1", "]", ",", "[", "0", ",", "0", "]", ")", ",", "mode", "=", "'constant'", ")", "shift", "=", "np", ".", "pad", "(", "shift", ",", "(", "[", "1", ",", "1", "]", ",", "[", "0", ",", "0", "]", ")", ",", "mode", "=", "'constant'", ")", "dskew", "=", "0.5", "*", "avg", "*", "shift", "# Pre-allocate output", "pitches", "=", "np", ".", "zeros_like", "(", "S", ")", "mags", "=", "np", ".", "zeros_like", "(", "S", ")", "# Clip to the viable frequency range", "freq_mask", "=", "(", "(", "fmin", "<=", "fft_freqs", ")", "&", "(", "fft_freqs", "<", "fmax", ")", ")", ".", "reshape", "(", "(", "-", "1", ",", "1", ")", ")", "# Compute the column-wise local max of S after thresholding", "# Find the argmax coordinates", "if", "ref", "is", "None", ":", "ref", "=", "np", ".", "max", "if", "six", ".", "callable", "(", "ref", ")", ":", "ref_value", "=", "threshold", "*", "ref", "(", "S", ",", "axis", "=", "0", ")", "else", ":", "ref_value", "=", "np", ".", "abs", "(", "ref", ")", "idx", "=", "np", ".", "argwhere", "(", "freq_mask", "&", "util", ".", "localmax", "(", "S", "*", "(", "S", ">", "ref_value", ")", ")", ")", "# Store pitch and magnitude", "pitches", "[", "idx", "[", ":", ",", "0", "]", ",", "idx", "[", ":", ",", "1", "]", "]", "=", "(", "(", "idx", "[", ":", ",", "0", "]", "+", "shift", "[", "idx", "[", ":", ",", "0", "]", ",", "idx", "[", ":", ",", "1", "]", "]", ")", "*", "float", "(", "sr", ")", "/", "n_fft", ")", "mags", "[", "idx", "[", ":", ",", "0", "]", ",", "idx", "[", ":", ",", "1", "]", "]", "=", "(", "S", "[", "idx", "[", ":", ",", "0", "]", ",", "idx", "[", ":", ",", "1", "]", "]", "+", "dskew", "[", "idx", "[", ":", ",", "0", "]", ",", "idx", "[", ":", ",", "1", "]", "]", ")", "return", "pitches", ",", "mags"], "docstring": "Pitch tracking on thresholded parabolically-interpolated STFT.\n\n    This implementation uses the parabolic interpolation method described by [1]_.\n\n    .. [1] https://ccrma.stanford.edu/~jos/sasp/Sinusoidal_Peak_Interpolation.html\n\n    Parameters\n    ----------\n    y: np.ndarray [shape=(n,)] or None\n        audio signal\n\n    sr : number > 0 [scalar]\n        audio sampling rate of `y`\n\n    S: np.ndarray [shape=(d, t)] or None\n        magnitude or power spectrogram\n\n    n_fft : int > 0 [scalar] or None\n        number of FFT bins to use, if `y` is provided.\n\n    hop_length : int > 0 [scalar] or None\n        number of samples to hop\n\n    threshold : float in `(0, 1)`\n        A bin in spectrum `S` is considered a pitch when it is greater than\n        `threshold*ref(S)`.\n\n        By default, `ref(S)` is taken to be `max(S, axis=0)` (the maximum value in\n        each column).\n\n    fmin : float > 0 [scalar]\n        lower frequency cutoff.\n\n    fmax : float > 0 [scalar]\n        upper frequency cutoff.\n\n    win_length : int <= n_fft [scalar]\n        Each frame of audio is windowed by `window()`.\n        The window will be of length `win_length` and then padded\n        with zeros to match `n_fft`.\n\n        If unspecified, defaults to ``win_length = n_fft``.\n\n    window : string, tuple, number, function, or np.ndarray [shape=(n_fft,)]\n        - a window specification (string, tuple, or number);\n          see `scipy.signal.get_window`\n        - a window function, such as `scipy.signal.hanning`\n        - a vector or array of length `n_fft`\n\n        .. see also:: `filters.get_window`\n\n    center : boolean\n        - If `True`, the signal `y` is padded so that frame\n          `t` is centered at `y[t * hop_length]`.\n        - If `False`, then frame `t` begins at `y[t * hop_length]`\n\n    pad_mode : string\n        If `center=True`, the padding mode to use at the edges of the signal.\n        By default, STFT uses reflection padding.\n\n    ref : scalar or callable [default=np.max]\n        If scalar, the reference value against which `S` is compared for determining\n        pitches.\n\n        If callable, the reference value is computed as `ref(S, axis=0)`.\n\n    .. note::\n        One of `S` or `y` must be provided.\n\n        If `S` is not given, it is computed from `y` using\n        the default parameters of `librosa.core.stft`.\n\n    Returns\n    -------\n    pitches : np.ndarray [shape=(d, t)]\n    magnitudes : np.ndarray [shape=(d,t)]\n        Where `d` is the subset of FFT bins within `fmin` and `fmax`.\n\n        `pitches[f, t]` contains instantaneous frequency at bin\n        `f`, time `t`\n\n        `magnitudes[f, t]` contains the corresponding magnitudes.\n\n        Both `pitches` and `magnitudes` take value 0 at bins\n        of non-maximal magnitude.\n\n    Notes\n    -----\n    This function caches at level 30.\n\n    Examples\n    --------\n    Computing pitches from a waveform input\n\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n\n    Or from a spectrogram input\n\n    >>> S = np.abs(librosa.stft(y))\n    >>> pitches, magnitudes = librosa.piptrack(S=S, sr=sr)\n\n    Or with an alternate reference value for pitch detection, where\n    values above the mean spectral energy in each frame are counted as pitches\n\n    >>> pitches, magnitudes = librosa.piptrack(S=S, sr=sr, threshold=1,\n    ...                                        ref=np.mean)", "docstring_tokens": ["Pitch", "tracking", "on", "thresholded", "parabolically", "-", "interpolated", "STFT", "."], "sha": "180e8e6eb8f958fa6b20b8cba389f7945d508247", "url": "https://github.com/librosa/librosa/blob/180e8e6eb8f958fa6b20b8cba389f7945d508247/librosa/core/pitch.py#L167-L338", "partition": "test"}
{"repo": "joopert/nad_receiver", "path": "nad_receiver/__init__.py", "func_name": "NADReceiverTCP._send", "original_string": "def _send(self, message, read_reply=False):\n        \"\"\"Send a command string to the amplifier.\"\"\"\n        sock = None\n        for tries in range(0, 3):\n            try:\n                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n                sock.connect((self._host, self.PORT))\n                break\n            except (ConnectionError, BrokenPipeError):\n                if tries == 3:\n                    print(\"socket connect failed.\")\n                    return\n                sleep(0.1)\n        sock.send(codecs.decode(message, 'hex_codec'))\n        if read_reply:\n            sleep(0.1)\n            reply = ''\n            tries = 0\n            max_tries = 20\n            while len(reply) < len(message) and tries < max_tries:\n                try:\n                    reply += codecs.encode(sock.recv(self.BUFFERSIZE), 'hex')\\\n                        .decode(\"utf-8\")\n                except (ConnectionError, BrokenPipeError):\n                    pass\n                tries += 1\n            sock.close()\n            if tries >= max_tries:\n                return\n            return reply\n        sock.close()", "language": "python", "code": "def _send(self, message, read_reply=False):\n        \"\"\"Send a command string to the amplifier.\"\"\"\n        sock = None\n        for tries in range(0, 3):\n            try:\n                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n                sock.connect((self._host, self.PORT))\n                break\n            except (ConnectionError, BrokenPipeError):\n                if tries == 3:\n                    print(\"socket connect failed.\")\n                    return\n                sleep(0.1)\n        sock.send(codecs.decode(message, 'hex_codec'))\n        if read_reply:\n            sleep(0.1)\n            reply = ''\n            tries = 0\n            max_tries = 20\n            while len(reply) < len(message) and tries < max_tries:\n                try:\n                    reply += codecs.encode(sock.recv(self.BUFFERSIZE), 'hex')\\\n                        .decode(\"utf-8\")\n                except (ConnectionError, BrokenPipeError):\n                    pass\n                tries += 1\n            sock.close()\n            if tries >= max_tries:\n                return\n            return reply\n        sock.close()", "code_tokens": ["def", "_send", "(", "self", ",", "message", ",", "read_reply", "=", "False", ")", ":", "sock", "=", "None", "for", "tries", "in", "range", "(", "0", ",", "3", ")", ":", "try", ":", "sock", "=", "socket", ".", "socket", "(", "socket", ".", "AF_INET", ",", "socket", ".", "SOCK_STREAM", ")", "sock", ".", "connect", "(", "(", "self", ".", "_host", ",", "self", ".", "PORT", ")", ")", "break", "except", "(", "ConnectionError", ",", "BrokenPipeError", ")", ":", "if", "tries", "==", "3", ":", "print", "(", "\"socket connect failed.\"", ")", "return", "sleep", "(", "0.1", ")", "sock", ".", "send", "(", "codecs", ".", "decode", "(", "message", ",", "'hex_codec'", ")", ")", "if", "read_reply", ":", "sleep", "(", "0.1", ")", "reply", "=", "''", "tries", "=", "0", "max_tries", "=", "20", "while", "len", "(", "reply", ")", "<", "len", "(", "message", ")", "and", "tries", "<", "max_tries", ":", "try", ":", "reply", "+=", "codecs", ".", "encode", "(", "sock", ".", "recv", "(", "self", ".", "BUFFERSIZE", ")", ",", "'hex'", ")", ".", "decode", "(", "\"utf-8\"", ")", "except", "(", "ConnectionError", ",", "BrokenPipeError", ")", ":", "pass", "tries", "+=", "1", "sock", ".", "close", "(", ")", "if", "tries", ">=", "max_tries", ":", "return", "return", "reply", "sock", ".", "close", "(", ")"], "docstring": "Send a command string to the amplifier.", "docstring_tokens": ["Send", "a", "command", "string", "to", "the", "amplifier", "."], "sha": "416de0173a330c75cc73f9c90b0c5df32e5e0ba3", "url": "https://github.com/joopert/nad_receiver/blob/416de0173a330c75cc73f9c90b0c5df32e5e0ba3/nad_receiver/__init__.py#L183-L213", "partition": "test"}
{"repo": "ubyssey/dispatch", "path": "dispatch/admin/views.py", "func_name": "signup", "original_string": "def signup(request, uuid=None):\n    \"\"\"Handles requests to the user signup page.\"\"\"\n\n    invite = get_object_or_404(Invite.objects.all(), id=uuid)\n\n    if invite.expiration_date < timezone.now():\n        invite.delete()\n        raise Http404('This page does not exist.')\n\n    if request.method == 'POST':\n        form = SignUpForm(request.POST)\n        if form.is_valid():\n            user = form.save(commit=False)\n\n            user.email = invite.email\n            user.person = invite.person\n\n            user.save()\n\n            if invite.permissions == 'admin':\n                group = Group.objects.get(name='Admin')\n                user.groups.add(group)\n\n            invite.delete()\n\n            return redirect('dispatch-admin')\n        else:\n            return render(\n                request,\n                'registration/signup.html',\n                {\n                    'form': form,\n                    'email': invite.email\n                }\n            )\n\n    else:\n        form = SignUpForm()\n\n    return render(\n        request,\n        'registration/signup.html',\n        {\n            'form': form,\n            'email': invite.email\n        }\n    )", "language": "python", "code": "def signup(request, uuid=None):\n    \"\"\"Handles requests to the user signup page.\"\"\"\n\n    invite = get_object_or_404(Invite.objects.all(), id=uuid)\n\n    if invite.expiration_date < timezone.now():\n        invite.delete()\n        raise Http404('This page does not exist.')\n\n    if request.method == 'POST':\n        form = SignUpForm(request.POST)\n        if form.is_valid():\n            user = form.save(commit=False)\n\n            user.email = invite.email\n            user.person = invite.person\n\n            user.save()\n\n            if invite.permissions == 'admin':\n                group = Group.objects.get(name='Admin')\n                user.groups.add(group)\n\n            invite.delete()\n\n            return redirect('dispatch-admin')\n        else:\n            return render(\n                request,\n                'registration/signup.html',\n                {\n                    'form': form,\n                    'email': invite.email\n                }\n            )\n\n    else:\n        form = SignUpForm()\n\n    return render(\n        request,\n        'registration/signup.html',\n        {\n            'form': form,\n            'email': invite.email\n        }\n    )", "code_tokens": ["def", "signup", "(", "request", ",", "uuid", "=", "None", ")", ":", "invite", "=", "get_object_or_404", "(", "Invite", ".", "objects", ".", "all", "(", ")", ",", "id", "=", "uuid", ")", "if", "invite", ".", "expiration_date", "<", "timezone", ".", "now", "(", ")", ":", "invite", ".", "delete", "(", ")", "raise", "Http404", "(", "'This page does not exist.'", ")", "if", "request", ".", "method", "==", "'POST'", ":", "form", "=", "SignUpForm", "(", "request", ".", "POST", ")", "if", "form", ".", "is_valid", "(", ")", ":", "user", "=", "form", ".", "save", "(", "commit", "=", "False", ")", "user", ".", "email", "=", "invite", ".", "email", "user", ".", "person", "=", "invite", ".", "person", "user", ".", "save", "(", ")", "if", "invite", ".", "permissions", "==", "'admin'", ":", "group", "=", "Group", ".", "objects", ".", "get", "(", "name", "=", "'Admin'", ")", "user", ".", "groups", ".", "add", "(", "group", ")", "invite", ".", "delete", "(", ")", "return", "redirect", "(", "'dispatch-admin'", ")", "else", ":", "return", "render", "(", "request", ",", "'registration/signup.html'", ",", "{", "'form'", ":", "form", ",", "'email'", ":", "invite", ".", "email", "}", ")", "else", ":", "form", "=", "SignUpForm", "(", ")", "return", "render", "(", "request", ",", "'registration/signup.html'", ",", "{", "'form'", ":", "form", ",", "'email'", ":", "invite", ".", "email", "}", ")"], "docstring": "Handles requests to the user signup page.", "docstring_tokens": ["Handles", "requests", "to", "the", "user", "signup", "page", "."], "sha": "8da6084fe61726f20e9cf675190480cfc45ee764", "url": "https://github.com/ubyssey/dispatch/blob/8da6084fe61726f20e9cf675190480cfc45ee764/dispatch/admin/views.py#L14-L60", "partition": "test"}
{"repo": "RRZE-HPC/kerncraft", "path": "kerncraft/models/ecm.py", "func_name": "blocking", "original_string": "def blocking(indices, block_size, initial_boundary=0):\n    \"\"\"\n    Split list of integers into blocks of block_size and return block indices.\n\n    First block element will be located at initial_boundary (default 0).\n\n    >>> blocking([0, -1, -2, -3, -4, -5, -6, -7, -8, -9], 8)\n    [0,-1]\n    >>> blocking([0], 8)\n    [0]\n    >>> blocking([0], 8, initial_boundary=32)\n    [-4]\n    \"\"\"\n    blocks = []\n\n    for idx in indices:\n        bl_idx = (idx-initial_boundary)//float(block_size)\n        if bl_idx not in blocks:\n            blocks.append(bl_idx)\n    blocks.sort()\n\n    return blocks", "language": "python", "code": "def blocking(indices, block_size, initial_boundary=0):\n    \"\"\"\n    Split list of integers into blocks of block_size and return block indices.\n\n    First block element will be located at initial_boundary (default 0).\n\n    >>> blocking([0, -1, -2, -3, -4, -5, -6, -7, -8, -9], 8)\n    [0,-1]\n    >>> blocking([0], 8)\n    [0]\n    >>> blocking([0], 8, initial_boundary=32)\n    [-4]\n    \"\"\"\n    blocks = []\n\n    for idx in indices:\n        bl_idx = (idx-initial_boundary)//float(block_size)\n        if bl_idx not in blocks:\n            blocks.append(bl_idx)\n    blocks.sort()\n\n    return blocks", "code_tokens": ["def", "blocking", "(", "indices", ",", "block_size", ",", "initial_boundary", "=", "0", ")", ":", "blocks", "=", "[", "]", "for", "idx", "in", "indices", ":", "bl_idx", "=", "(", "idx", "-", "initial_boundary", ")", "//", "float", "(", "block_size", ")", "if", "bl_idx", "not", "in", "blocks", ":", "blocks", ".", "append", "(", "bl_idx", ")", "blocks", ".", "sort", "(", ")", "return", "blocks"], "docstring": "Split list of integers into blocks of block_size and return block indices.\n\n    First block element will be located at initial_boundary (default 0).\n\n    >>> blocking([0, -1, -2, -3, -4, -5, -6, -7, -8, -9], 8)\n    [0,-1]\n    >>> blocking([0], 8)\n    [0]\n    >>> blocking([0], 8, initial_boundary=32)\n    [-4]", "docstring_tokens": ["Split", "list", "of", "integers", "into", "blocks", "of", "block_size", "and", "return", "block", "indices", "."], "sha": "c60baf8043e4da8d8d66da7575021c2f4c6c78af", "url": "https://github.com/RRZE-HPC/kerncraft/blob/c60baf8043e4da8d8d66da7575021c2f4c6c78af/kerncraft/models/ecm.py#L28-L49", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/core/completer.py", "func_name": "expand_user", "original_string": "def expand_user(path):\n    \"\"\"Expand '~'-style usernames in strings.\n\n    This is similar to :func:`os.path.expanduser`, but it computes and returns\n    extra information that will be useful if the input was being used in\n    computing completions, and you wish to return the completions with the\n    original '~' instead of its expanded value.\n\n    Parameters\n    ----------\n    path : str\n      String to be expanded.  If no ~ is present, the output is the same as the\n      input.\n\n    Returns\n    -------\n    newpath : str\n      Result of ~ expansion in the input path.\n    tilde_expand : bool\n      Whether any expansion was performed or not.\n    tilde_val : str\n      The value that ~ was replaced with.\n    \"\"\"\n    # Default values\n    tilde_expand = False\n    tilde_val = ''\n    newpath = path\n\n    if path.startswith('~'):\n        tilde_expand = True\n        rest = len(path)-1\n        newpath = os.path.expanduser(path)\n        if rest:\n            tilde_val = newpath[:-rest]\n        else:\n            tilde_val = newpath\n\n    return newpath, tilde_expand, tilde_val", "language": "python", "code": "def expand_user(path):\n    \"\"\"Expand '~'-style usernames in strings.\n\n    This is similar to :func:`os.path.expanduser`, but it computes and returns\n    extra information that will be useful if the input was being used in\n    computing completions, and you wish to return the completions with the\n    original '~' instead of its expanded value.\n\n    Parameters\n    ----------\n    path : str\n      String to be expanded.  If no ~ is present, the output is the same as the\n      input.\n\n    Returns\n    -------\n    newpath : str\n      Result of ~ expansion in the input path.\n    tilde_expand : bool\n      Whether any expansion was performed or not.\n    tilde_val : str\n      The value that ~ was replaced with.\n    \"\"\"\n    # Default values\n    tilde_expand = False\n    tilde_val = ''\n    newpath = path\n\n    if path.startswith('~'):\n        tilde_expand = True\n        rest = len(path)-1\n        newpath = os.path.expanduser(path)\n        if rest:\n            tilde_val = newpath[:-rest]\n        else:\n            tilde_val = newpath\n\n    return newpath, tilde_expand, tilde_val", "code_tokens": ["def", "expand_user", "(", "path", ")", ":", "# Default values", "tilde_expand", "=", "False", "tilde_val", "=", "''", "newpath", "=", "path", "if", "path", ".", "startswith", "(", "'~'", ")", ":", "tilde_expand", "=", "True", "rest", "=", "len", "(", "path", ")", "-", "1", "newpath", "=", "os", ".", "path", ".", "expanduser", "(", "path", ")", "if", "rest", ":", "tilde_val", "=", "newpath", "[", ":", "-", "rest", "]", "else", ":", "tilde_val", "=", "newpath", "return", "newpath", ",", "tilde_expand", ",", "tilde_val"], "docstring": "Expand '~'-style usernames in strings.\n\n    This is similar to :func:`os.path.expanduser`, but it computes and returns\n    extra information that will be useful if the input was being used in\n    computing completions, and you wish to return the completions with the\n    original '~' instead of its expanded value.\n\n    Parameters\n    ----------\n    path : str\n      String to be expanded.  If no ~ is present, the output is the same as the\n      input.\n\n    Returns\n    -------\n    newpath : str\n      Result of ~ expansion in the input path.\n    tilde_expand : bool\n      Whether any expansion was performed or not.\n    tilde_val : str\n      The value that ~ was replaced with.", "docstring_tokens": ["Expand", "~", "-", "style", "usernames", "in", "strings", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/core/completer.py#L132-L169", "partition": "test"}
{"repo": "xmikos/soapy_power", "path": "soapypower/power.py", "func_name": "SoapyPower.create_buffer", "original_string": "def create_buffer(self, bins, repeats, base_buffer_size, max_buffer_size=0):\n        \"\"\"Create buffer for reading samples\"\"\"\n        samples = bins * repeats\n        buffer_repeats = 1\n        buffer_size = math.ceil(samples / base_buffer_size) * base_buffer_size\n\n        if not max_buffer_size:\n            # Max buffer size about 100 MB\n            max_buffer_size = (100 * 1024**2) / 8\n\n        if max_buffer_size > 0:\n            max_buffer_size = math.ceil(max_buffer_size / base_buffer_size) * base_buffer_size\n            if buffer_size > max_buffer_size:\n                logger.warning('Required buffer size ({}) will be shrinked to max_buffer_size ({})!'.format(\n                    buffer_size, max_buffer_size\n                ))\n                buffer_repeats = math.ceil(buffer_size / max_buffer_size)\n                buffer_size = max_buffer_size\n\n        logger.info('repeats: {}'.format(repeats))\n        logger.info('samples: {} (time: {:.5f} s)'.format(samples, samples / self.device.sample_rate))\n        if max_buffer_size > 0:\n            logger.info('max_buffer_size (samples): {} (repeats: {:.2f}, time: {:.5f} s)'.format(\n                max_buffer_size, max_buffer_size / bins, max_buffer_size / self.device.sample_rate\n            ))\n        else:\n            logger.info('max_buffer_size (samples): UNLIMITED')\n        logger.info('buffer_size (samples): {} (repeats: {:.2f}, time: {:.5f} s)'.format(\n            buffer_size, buffer_size / bins, buffer_size / self.device.sample_rate\n        ))\n        logger.info('buffer_repeats: {}'.format(buffer_repeats))\n\n        return (buffer_repeats, zeros(buffer_size, numpy.complex64))", "language": "python", "code": "def create_buffer(self, bins, repeats, base_buffer_size, max_buffer_size=0):\n        \"\"\"Create buffer for reading samples\"\"\"\n        samples = bins * repeats\n        buffer_repeats = 1\n        buffer_size = math.ceil(samples / base_buffer_size) * base_buffer_size\n\n        if not max_buffer_size:\n            # Max buffer size about 100 MB\n            max_buffer_size = (100 * 1024**2) / 8\n\n        if max_buffer_size > 0:\n            max_buffer_size = math.ceil(max_buffer_size / base_buffer_size) * base_buffer_size\n            if buffer_size > max_buffer_size:\n                logger.warning('Required buffer size ({}) will be shrinked to max_buffer_size ({})!'.format(\n                    buffer_size, max_buffer_size\n                ))\n                buffer_repeats = math.ceil(buffer_size / max_buffer_size)\n                buffer_size = max_buffer_size\n\n        logger.info('repeats: {}'.format(repeats))\n        logger.info('samples: {} (time: {:.5f} s)'.format(samples, samples / self.device.sample_rate))\n        if max_buffer_size > 0:\n            logger.info('max_buffer_size (samples): {} (repeats: {:.2f}, time: {:.5f} s)'.format(\n                max_buffer_size, max_buffer_size / bins, max_buffer_size / self.device.sample_rate\n            ))\n        else:\n            logger.info('max_buffer_size (samples): UNLIMITED')\n        logger.info('buffer_size (samples): {} (repeats: {:.2f}, time: {:.5f} s)'.format(\n            buffer_size, buffer_size / bins, buffer_size / self.device.sample_rate\n        ))\n        logger.info('buffer_repeats: {}'.format(buffer_repeats))\n\n        return (buffer_repeats, zeros(buffer_size, numpy.complex64))", "code_tokens": ["def", "create_buffer", "(", "self", ",", "bins", ",", "repeats", ",", "base_buffer_size", ",", "max_buffer_size", "=", "0", ")", ":", "samples", "=", "bins", "*", "repeats", "buffer_repeats", "=", "1", "buffer_size", "=", "math", ".", "ceil", "(", "samples", "/", "base_buffer_size", ")", "*", "base_buffer_size", "if", "not", "max_buffer_size", ":", "# Max buffer size about 100 MB", "max_buffer_size", "=", "(", "100", "*", "1024", "**", "2", ")", "/", "8", "if", "max_buffer_size", ">", "0", ":", "max_buffer_size", "=", "math", ".", "ceil", "(", "max_buffer_size", "/", "base_buffer_size", ")", "*", "base_buffer_size", "if", "buffer_size", ">", "max_buffer_size", ":", "logger", ".", "warning", "(", "'Required buffer size ({}) will be shrinked to max_buffer_size ({})!'", ".", "format", "(", "buffer_size", ",", "max_buffer_size", ")", ")", "buffer_repeats", "=", "math", ".", "ceil", "(", "buffer_size", "/", "max_buffer_size", ")", "buffer_size", "=", "max_buffer_size", "logger", ".", "info", "(", "'repeats: {}'", ".", "format", "(", "repeats", ")", ")", "logger", ".", "info", "(", "'samples: {} (time: {:.5f} s)'", ".", "format", "(", "samples", ",", "samples", "/", "self", ".", "device", ".", "sample_rate", ")", ")", "if", "max_buffer_size", ">", "0", ":", "logger", ".", "info", "(", "'max_buffer_size (samples): {} (repeats: {:.2f}, time: {:.5f} s)'", ".", "format", "(", "max_buffer_size", ",", "max_buffer_size", "/", "bins", ",", "max_buffer_size", "/", "self", ".", "device", ".", "sample_rate", ")", ")", "else", ":", "logger", ".", "info", "(", "'max_buffer_size (samples): UNLIMITED'", ")", "logger", ".", "info", "(", "'buffer_size (samples): {} (repeats: {:.2f}, time: {:.5f} s)'", ".", "format", "(", "buffer_size", ",", "buffer_size", "/", "bins", ",", "buffer_size", "/", "self", ".", "device", ".", "sample_rate", ")", ")", "logger", ".", "info", "(", "'buffer_repeats: {}'", ".", "format", "(", "buffer_repeats", ")", ")", "return", "(", "buffer_repeats", ",", "zeros", "(", "buffer_size", ",", "numpy", ".", "complex64", ")", ")"], "docstring": "Create buffer for reading samples", "docstring_tokens": ["Create", "buffer", "for", "reading", "samples"], "sha": "46e12659b8d08af764dc09a1f31b0e85a68f808f", "url": "https://github.com/xmikos/soapy_power/blob/46e12659b8d08af764dc09a1f31b0e85a68f808f/soapypower/power.py#L141-L173", "partition": "test"}
{"repo": "streamlink/streamlink", "path": "src/streamlink_cli/main.py", "func_name": "output_stream_passthrough", "original_string": "def output_stream_passthrough(plugin, stream):\n    \"\"\"Prepares a filename to be passed to the player.\"\"\"\n    global output\n\n    title = create_title(plugin)\n    filename = '\"{0}\"'.format(stream_to_url(stream))\n    output = PlayerOutput(args.player, args=args.player_args,\n                          filename=filename, call=True,\n                          quiet=not args.verbose_player,\n                          title=title)\n\n    try:\n        log.info(\"Starting player: {0}\", args.player)\n        output.open()\n    except OSError as err:\n        console.exit(\"Failed to start player: {0} ({1})\", args.player, err)\n        return False\n\n    return True", "language": "python", "code": "def output_stream_passthrough(plugin, stream):\n    \"\"\"Prepares a filename to be passed to the player.\"\"\"\n    global output\n\n    title = create_title(plugin)\n    filename = '\"{0}\"'.format(stream_to_url(stream))\n    output = PlayerOutput(args.player, args=args.player_args,\n                          filename=filename, call=True,\n                          quiet=not args.verbose_player,\n                          title=title)\n\n    try:\n        log.info(\"Starting player: {0}\", args.player)\n        output.open()\n    except OSError as err:\n        console.exit(\"Failed to start player: {0} ({1})\", args.player, err)\n        return False\n\n    return True", "code_tokens": ["def", "output_stream_passthrough", "(", "plugin", ",", "stream", ")", ":", "global", "output", "title", "=", "create_title", "(", "plugin", ")", "filename", "=", "'\"{0}\"'", ".", "format", "(", "stream_to_url", "(", "stream", ")", ")", "output", "=", "PlayerOutput", "(", "args", ".", "player", ",", "args", "=", "args", ".", "player_args", ",", "filename", "=", "filename", ",", "call", "=", "True", ",", "quiet", "=", "not", "args", ".", "verbose_player", ",", "title", "=", "title", ")", "try", ":", "log", ".", "info", "(", "\"Starting player: {0}\"", ",", "args", ".", "player", ")", "output", ".", "open", "(", ")", "except", "OSError", "as", "err", ":", "console", ".", "exit", "(", "\"Failed to start player: {0} ({1})\"", ",", "args", ".", "player", ",", "err", ")", "return", "False", "return", "True"], "docstring": "Prepares a filename to be passed to the player.", "docstring_tokens": ["Prepares", "a", "filename", "to", "be", "passed", "to", "the", "player", "."], "sha": "c8ed1daff14ac03195870238b9b900c1109dd5c1", "url": "https://github.com/streamlink/streamlink/blob/c8ed1daff14ac03195870238b9b900c1109dd5c1/src/streamlink_cli/main.py#L249-L267", "partition": "test"}
{"repo": "ingolemo/python-lenses", "path": "examples/robots.py", "func_name": "Vector.step_towards", "original_string": "def step_towards(self, other):\n        '''returns the vector moved one step in the direction of the\n        other, potentially diagonally.'''\n\n        return self + Vector(\n            (\n                (self[0] < other[0]) - (self[0] > other[0]),\n                (self[1] < other[1]) - (self[1] > other[1]),\n            )\n        )", "language": "python", "code": "def step_towards(self, other):\n        '''returns the vector moved one step in the direction of the\n        other, potentially diagonally.'''\n\n        return self + Vector(\n            (\n                (self[0] < other[0]) - (self[0] > other[0]),\n                (self[1] < other[1]) - (self[1] > other[1]),\n            )\n        )", "code_tokens": ["def", "step_towards", "(", "self", ",", "other", ")", ":", "return", "self", "+", "Vector", "(", "(", "(", "self", "[", "0", "]", "<", "other", "[", "0", "]", ")", "-", "(", "self", "[", "0", "]", ">", "other", "[", "0", "]", ")", ",", "(", "self", "[", "1", "]", "<", "other", "[", "1", "]", ")", "-", "(", "self", "[", "1", "]", ">", "other", "[", "1", "]", ")", ",", ")", ")"], "docstring": "returns the vector moved one step in the direction of the\n        other, potentially diagonally.", "docstring_tokens": ["returns", "the", "vector", "moved", "one", "step", "in", "the", "direction", "of", "the", "other", "potentially", "diagonally", "."], "sha": "a3a6ed0a31f6674451e542e7380a8aa16e6f8edf", "url": "https://github.com/ingolemo/python-lenses/blob/a3a6ed0a31f6674451e542e7380a8aa16e6f8edf/examples/robots.py#L58-L67", "partition": "test"}
{"repo": "albertodonato/prometheus-aioexporter", "path": "prometheus_aioexporter/script.py", "func_name": "PrometheusExporterScript._configure_registry", "original_string": "def _configure_registry(self, include_process_stats: bool = False):\n        \"\"\"Configure the MetricRegistry.\"\"\"\n        if include_process_stats:\n            self.registry.register_additional_collector(\n                ProcessCollector(registry=None))", "language": "python", "code": "def _configure_registry(self, include_process_stats: bool = False):\n        \"\"\"Configure the MetricRegistry.\"\"\"\n        if include_process_stats:\n            self.registry.register_additional_collector(\n                ProcessCollector(registry=None))", "code_tokens": ["def", "_configure_registry", "(", "self", ",", "include_process_stats", ":", "bool", "=", "False", ")", ":", "if", "include_process_stats", ":", "self", ".", "registry", ".", "register_additional_collector", "(", "ProcessCollector", "(", "registry", "=", "None", ")", ")"], "docstring": "Configure the MetricRegistry.", "docstring_tokens": ["Configure", "the", "MetricRegistry", "."], "sha": "e1b85544ce72bfaae9182597709a2ecede8c8242", "url": "https://github.com/albertodonato/prometheus-aioexporter/blob/e1b85544ce72bfaae9182597709a2ecede8c8242/prometheus_aioexporter/script.py#L139-L143", "partition": "test"}
{"repo": "Clinical-Genomics/scout", "path": "scout/adapter/mongo/case.py", "func_name": "get_variantid", "original_string": "def get_variantid(variant_obj, family_id):\n    \"\"\"Create a new variant id.\n\n    Args:\n        variant_obj(dict)\n        family_id(str)\n\n    Returns:\n        new_id(str): The new variant id\n    \"\"\"\n    new_id = parse_document_id(\n        chrom=variant_obj['chromosome'],\n        pos=str(variant_obj['position']),\n        ref=variant_obj['reference'],\n        alt=variant_obj['alternative'],\n        variant_type=variant_obj['variant_type'],\n        case_id=family_id,\n    )\n    return new_id", "language": "python", "code": "def get_variantid(variant_obj, family_id):\n    \"\"\"Create a new variant id.\n\n    Args:\n        variant_obj(dict)\n        family_id(str)\n\n    Returns:\n        new_id(str): The new variant id\n    \"\"\"\n    new_id = parse_document_id(\n        chrom=variant_obj['chromosome'],\n        pos=str(variant_obj['position']),\n        ref=variant_obj['reference'],\n        alt=variant_obj['alternative'],\n        variant_type=variant_obj['variant_type'],\n        case_id=family_id,\n    )\n    return new_id", "code_tokens": ["def", "get_variantid", "(", "variant_obj", ",", "family_id", ")", ":", "new_id", "=", "parse_document_id", "(", "chrom", "=", "variant_obj", "[", "'chromosome'", "]", ",", "pos", "=", "str", "(", "variant_obj", "[", "'position'", "]", ")", ",", "ref", "=", "variant_obj", "[", "'reference'", "]", ",", "alt", "=", "variant_obj", "[", "'alternative'", "]", ",", "variant_type", "=", "variant_obj", "[", "'variant_type'", "]", ",", "case_id", "=", "family_id", ",", ")", "return", "new_id"], "docstring": "Create a new variant id.\n\n    Args:\n        variant_obj(dict)\n        family_id(str)\n\n    Returns:\n        new_id(str): The new variant id", "docstring_tokens": ["Create", "a", "new", "variant", "id", "."], "sha": "90a551e2e1653a319e654c2405c2866f93d0ebb9", "url": "https://github.com/Clinical-Genomics/scout/blob/90a551e2e1653a319e654c2405c2866f93d0ebb9/scout/adapter/mongo/case.py#L512-L530", "partition": "test"}
{"repo": "bitlabstudio/django-hero-slider", "path": "hero_slider/templatetags/hero_slider_tags.py", "func_name": "render_hero_slider", "original_string": "def render_hero_slider(context):\n    \"\"\"\n    Renders the hero slider.\n\n    \"\"\"\n    req = context.get('request')\n    qs = SliderItem.objects.published(req).order_by('position')\n    return {\n        'slider_items': qs,\n    }", "language": "python", "code": "def render_hero_slider(context):\n    \"\"\"\n    Renders the hero slider.\n\n    \"\"\"\n    req = context.get('request')\n    qs = SliderItem.objects.published(req).order_by('position')\n    return {\n        'slider_items': qs,\n    }", "code_tokens": ["def", "render_hero_slider", "(", "context", ")", ":", "req", "=", "context", ".", "get", "(", "'request'", ")", "qs", "=", "SliderItem", ".", "objects", ".", "published", "(", "req", ")", ".", "order_by", "(", "'position'", ")", "return", "{", "'slider_items'", ":", "qs", ",", "}"], "docstring": "Renders the hero slider.", "docstring_tokens": ["Renders", "the", "hero", "slider", "."], "sha": "8153b3eece76c47210a266c2edb660725c34a56e", "url": "https://github.com/bitlabstudio/django-hero-slider/blob/8153b3eece76c47210a266c2edb660725c34a56e/hero_slider/templatetags/hero_slider_tags.py#L21-L30", "partition": "test"}
{"repo": "SmokinCaterpillar/pypet", "path": "pypet/trajectory.py", "func_name": "Trajectory.f_remove_items", "original_string": "def f_remove_items(self, iterator, recursive=False):\n        \"\"\"Removes parameters, results or groups from the trajectory.\n\n        This function ONLY removes items from your current trajectory and does not delete\n        data stored to disk. If you want to delete data from disk, take a look at\n        :func:`~pypet.trajectory.Trajectory.f_delete_items`.\n\n        This will also remove all links if items are linked.\n\n        :param iterator:\n\n            A sequence of items you want to remove. Either the instances themselves\n            or strings with the names of the items.\n\n        :param recursive:\n\n            In case you want to remove group nodes, if the children should be removed, too.\n\n        \"\"\"\n\n        # Will format the request in a form that is understood by the storage service\n        # aka (msg, item, args, kwargs)\n        fetched_items = self._nn_interface._fetch_items(REMOVE, iterator, (), {})\n\n        if fetched_items:\n            for _, item, dummy1, dummy2 in fetched_items:\n                self._nn_interface._remove_node_or_leaf(item, recursive=recursive)\n\n        else:\n            self._logger.warning('Your removal was not successful, could not find a single '\n                                 'item to remove.')", "language": "python", "code": "def f_remove_items(self, iterator, recursive=False):\n        \"\"\"Removes parameters, results or groups from the trajectory.\n\n        This function ONLY removes items from your current trajectory and does not delete\n        data stored to disk. If you want to delete data from disk, take a look at\n        :func:`~pypet.trajectory.Trajectory.f_delete_items`.\n\n        This will also remove all links if items are linked.\n\n        :param iterator:\n\n            A sequence of items you want to remove. Either the instances themselves\n            or strings with the names of the items.\n\n        :param recursive:\n\n            In case you want to remove group nodes, if the children should be removed, too.\n\n        \"\"\"\n\n        # Will format the request in a form that is understood by the storage service\n        # aka (msg, item, args, kwargs)\n        fetched_items = self._nn_interface._fetch_items(REMOVE, iterator, (), {})\n\n        if fetched_items:\n            for _, item, dummy1, dummy2 in fetched_items:\n                self._nn_interface._remove_node_or_leaf(item, recursive=recursive)\n\n        else:\n            self._logger.warning('Your removal was not successful, could not find a single '\n                                 'item to remove.')", "code_tokens": ["def", "f_remove_items", "(", "self", ",", "iterator", ",", "recursive", "=", "False", ")", ":", "# Will format the request in a form that is understood by the storage service", "# aka (msg, item, args, kwargs)", "fetched_items", "=", "self", ".", "_nn_interface", ".", "_fetch_items", "(", "REMOVE", ",", "iterator", ",", "(", ")", ",", "{", "}", ")", "if", "fetched_items", ":", "for", "_", ",", "item", ",", "dummy1", ",", "dummy2", "in", "fetched_items", ":", "self", ".", "_nn_interface", ".", "_remove_node_or_leaf", "(", "item", ",", "recursive", "=", "recursive", ")", "else", ":", "self", ".", "_logger", ".", "warning", "(", "'Your removal was not successful, could not find a single '", "'item to remove.'", ")"], "docstring": "Removes parameters, results or groups from the trajectory.\n\n        This function ONLY removes items from your current trajectory and does not delete\n        data stored to disk. If you want to delete data from disk, take a look at\n        :func:`~pypet.trajectory.Trajectory.f_delete_items`.\n\n        This will also remove all links if items are linked.\n\n        :param iterator:\n\n            A sequence of items you want to remove. Either the instances themselves\n            or strings with the names of the items.\n\n        :param recursive:\n\n            In case you want to remove group nodes, if the children should be removed, too.", "docstring_tokens": ["Removes", "parameters", "results", "or", "groups", "from", "the", "trajectory", "."], "sha": "97ad3e80d46dbdea02deeb98ea41f05a19565826", "url": "https://github.com/SmokinCaterpillar/pypet/blob/97ad3e80d46dbdea02deeb98ea41f05a19565826/pypet/trajectory.py#L3796-L3826", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/zmq/kernelmanager.py", "func_name": "KernelManager.stop_channels", "original_string": "def stop_channels(self):\n        \"\"\"Stops all the running channels for this kernel.\n        \"\"\"\n        if self.shell_channel.is_alive():\n            self.shell_channel.stop()\n        if self.sub_channel.is_alive():\n            self.sub_channel.stop()\n        if self.stdin_channel.is_alive():\n            self.stdin_channel.stop()\n        if self.hb_channel.is_alive():\n            self.hb_channel.stop()", "language": "python", "code": "def stop_channels(self):\n        \"\"\"Stops all the running channels for this kernel.\n        \"\"\"\n        if self.shell_channel.is_alive():\n            self.shell_channel.stop()\n        if self.sub_channel.is_alive():\n            self.sub_channel.stop()\n        if self.stdin_channel.is_alive():\n            self.stdin_channel.stop()\n        if self.hb_channel.is_alive():\n            self.hb_channel.stop()", "code_tokens": ["def", "stop_channels", "(", "self", ")", ":", "if", "self", ".", "shell_channel", ".", "is_alive", "(", ")", ":", "self", ".", "shell_channel", ".", "stop", "(", ")", "if", "self", ".", "sub_channel", ".", "is_alive", "(", ")", ":", "self", ".", "sub_channel", ".", "stop", "(", ")", "if", "self", ".", "stdin_channel", ".", "is_alive", "(", ")", ":", "self", ".", "stdin_channel", ".", "stop", "(", ")", "if", "self", ".", "hb_channel", ".", "is_alive", "(", ")", ":", "self", ".", "hb_channel", ".", "stop", "(", ")"], "docstring": "Stops all the running channels for this kernel.", "docstring_tokens": ["Stops", "all", "the", "running", "channels", "for", "this", "kernel", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/zmq/kernelmanager.py#L713-L723", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/frontend/qt/console/mainwindow.py", "func_name": "MainWindow._make_dynamic_magic", "original_string": "def _make_dynamic_magic(self,magic):\n        \"\"\"Return a function `fun` that will execute `magic` on active frontend.\n\n        Parameters\n        ----------\n        magic : string\n            string that will be executed as is when the returned function is called\n\n        Returns\n        -------\n        fun : function\n            function with no parameters, when called will execute `magic` on the\n            current active frontend at call time\n\n        See Also\n        --------\n        populate_all_magic_menu : generate the \"All Magics...\" menu\n\n        Notes\n        -----\n        `fun` executes `magic` in active frontend at the moment it is triggered,\n        not the active frontend at the moment it was created.\n\n        This function is mostly used to create the \"All Magics...\" Menu at run time.\n        \"\"\"\n        # need two level nested function to be sure to pass magic\n        # to active frontend **at run time**.\n        def inner_dynamic_magic():\n            self.active_frontend.execute(magic)\n        inner_dynamic_magic.__name__ = \"dynamics_magic_s\"\n        return inner_dynamic_magic", "language": "python", "code": "def _make_dynamic_magic(self,magic):\n        \"\"\"Return a function `fun` that will execute `magic` on active frontend.\n\n        Parameters\n        ----------\n        magic : string\n            string that will be executed as is when the returned function is called\n\n        Returns\n        -------\n        fun : function\n            function with no parameters, when called will execute `magic` on the\n            current active frontend at call time\n\n        See Also\n        --------\n        populate_all_magic_menu : generate the \"All Magics...\" menu\n\n        Notes\n        -----\n        `fun` executes `magic` in active frontend at the moment it is triggered,\n        not the active frontend at the moment it was created.\n\n        This function is mostly used to create the \"All Magics...\" Menu at run time.\n        \"\"\"\n        # need two level nested function to be sure to pass magic\n        # to active frontend **at run time**.\n        def inner_dynamic_magic():\n            self.active_frontend.execute(magic)\n        inner_dynamic_magic.__name__ = \"dynamics_magic_s\"\n        return inner_dynamic_magic", "code_tokens": ["def", "_make_dynamic_magic", "(", "self", ",", "magic", ")", ":", "# need two level nested function to be sure to pass magic", "# to active frontend **at run time**.", "def", "inner_dynamic_magic", "(", ")", ":", "self", ".", "active_frontend", ".", "execute", "(", "magic", ")", "inner_dynamic_magic", ".", "__name__", "=", "\"dynamics_magic_s\"", "return", "inner_dynamic_magic"], "docstring": "Return a function `fun` that will execute `magic` on active frontend.\n\n        Parameters\n        ----------\n        magic : string\n            string that will be executed as is when the returned function is called\n\n        Returns\n        -------\n        fun : function\n            function with no parameters, when called will execute `magic` on the\n            current active frontend at call time\n\n        See Also\n        --------\n        populate_all_magic_menu : generate the \"All Magics...\" menu\n\n        Notes\n        -----\n        `fun` executes `magic` in active frontend at the moment it is triggered,\n        not the active frontend at the moment it was created.\n\n        This function is mostly used to create the \"All Magics...\" Menu at run time.", "docstring_tokens": ["Return", "a", "function", "fun", "that", "will", "execute", "magic", "on", "active", "frontend", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/frontend/qt/console/mainwindow.py#L566-L596", "partition": "test"}
{"repo": "treycucco/pyebnf", "path": "pyebnf/primitive.py", "func_name": "ParseNode.position", "original_string": "def position(self):\n    \"\"\"Gets the position of the text the ParseNode processed. If the ParseNode does not have its\n    own position, it looks to its first child for its position.\n\n    'Value Nodes' (terminals) must have their own position, otherwise this method will throw an\n    exception when it tries to get the position property of the string child.\n    \"\"\"\n    pos = self._position\n    if pos is None and self.children:\n      ch1 = self.children[0]\n      if isinstance(ch1, ParseNode):\n        pos = ch1.position\n    return pos", "language": "python", "code": "def position(self):\n    \"\"\"Gets the position of the text the ParseNode processed. If the ParseNode does not have its\n    own position, it looks to its first child for its position.\n\n    'Value Nodes' (terminals) must have their own position, otherwise this method will throw an\n    exception when it tries to get the position property of the string child.\n    \"\"\"\n    pos = self._position\n    if pos is None and self.children:\n      ch1 = self.children[0]\n      if isinstance(ch1, ParseNode):\n        pos = ch1.position\n    return pos", "code_tokens": ["def", "position", "(", "self", ")", ":", "pos", "=", "self", ".", "_position", "if", "pos", "is", "None", "and", "self", ".", "children", ":", "ch1", "=", "self", ".", "children", "[", "0", "]", "if", "isinstance", "(", "ch1", ",", "ParseNode", ")", ":", "pos", "=", "ch1", ".", "position", "return", "pos"], "docstring": "Gets the position of the text the ParseNode processed. If the ParseNode does not have its\n    own position, it looks to its first child for its position.\n\n    'Value Nodes' (terminals) must have their own position, otherwise this method will throw an\n    exception when it tries to get the position property of the string child.", "docstring_tokens": ["Gets", "the", "position", "of", "the", "text", "the", "ParseNode", "processed", ".", "If", "the", "ParseNode", "does", "not", "have", "its", "own", "position", "it", "looks", "to", "its", "first", "child", "for", "its", "position", "."], "sha": "3634ddabbe5d73508bcc20f4a591f86a46634e1d", "url": "https://github.com/treycucco/pyebnf/blob/3634ddabbe5d73508bcc20f4a591f86a46634e1d/pyebnf/primitive.py#L49-L61", "partition": "test"}
{"repo": "5monkeys/django-bananas", "path": "bananas/admin/api/views.py", "func_name": "ChangePasswordAPI.create", "original_string": "def create(self, request):\n        \"\"\"\n        Change password for logged in django staff user\n        \"\"\"\n        # TODO: Decorate api with sensitive post parameters as Django admin do?\n\n        password_form = PasswordChangeForm(request.user, data=request.data)\n\n        if not password_form.is_valid():\n            raise serializers.ValidationError(password_form.errors)\n\n        password_form.save()\n        update_session_auth_hash(request, password_form.user)\n\n        return Response(status=status.HTTP_204_NO_CONTENT)", "language": "python", "code": "def create(self, request):\n        \"\"\"\n        Change password for logged in django staff user\n        \"\"\"\n        # TODO: Decorate api with sensitive post parameters as Django admin do?\n\n        password_form = PasswordChangeForm(request.user, data=request.data)\n\n        if not password_form.is_valid():\n            raise serializers.ValidationError(password_form.errors)\n\n        password_form.save()\n        update_session_auth_hash(request, password_form.user)\n\n        return Response(status=status.HTTP_204_NO_CONTENT)", "code_tokens": ["def", "create", "(", "self", ",", "request", ")", ":", "# TODO: Decorate api with sensitive post parameters as Django admin do?", "password_form", "=", "PasswordChangeForm", "(", "request", ".", "user", ",", "data", "=", "request", ".", "data", ")", "if", "not", "password_form", ".", "is_valid", "(", ")", ":", "raise", "serializers", ".", "ValidationError", "(", "password_form", ".", "errors", ")", "password_form", ".", "save", "(", ")", "update_session_auth_hash", "(", "request", ",", "password_form", ".", "user", ")", "return", "Response", "(", "status", "=", "status", ".", "HTTP_204_NO_CONTENT", ")"], "docstring": "Change password for logged in django staff user", "docstring_tokens": ["Change", "password", "for", "logged", "in", "django", "staff", "user"], "sha": "cfd318c737f6c4580036c13d2acf32bca96654bf", "url": "https://github.com/5monkeys/django-bananas/blob/cfd318c737f6c4580036c13d2acf32bca96654bf/bananas/admin/api/views.py#L103-L117", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/hooks/S3_hook.py", "func_name": "S3Hook.get_key", "original_string": "def get_key(self, key, bucket_name=None):\n        \"\"\"\n        Returns a boto3.s3.Object\n\n        :param key: the path to the key\n        :type key: str\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        \"\"\"\n        if not bucket_name:\n            (bucket_name, key) = self.parse_s3_url(key)\n\n        obj = self.get_resource_type('s3').Object(bucket_name, key)\n        obj.load()\n        return obj", "language": "python", "code": "def get_key(self, key, bucket_name=None):\n        \"\"\"\n        Returns a boto3.s3.Object\n\n        :param key: the path to the key\n        :type key: str\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str\n        \"\"\"\n        if not bucket_name:\n            (bucket_name, key) = self.parse_s3_url(key)\n\n        obj = self.get_resource_type('s3').Object(bucket_name, key)\n        obj.load()\n        return obj", "code_tokens": ["def", "get_key", "(", "self", ",", "key", ",", "bucket_name", "=", "None", ")", ":", "if", "not", "bucket_name", ":", "(", "bucket_name", ",", "key", ")", "=", "self", ".", "parse_s3_url", "(", "key", ")", "obj", "=", "self", ".", "get_resource_type", "(", "'s3'", ")", ".", "Object", "(", "bucket_name", ",", "key", ")", "obj", ".", "load", "(", ")", "return", "obj"], "docstring": "Returns a boto3.s3.Object\n\n        :param key: the path to the key\n        :type key: str\n        :param bucket_name: the name of the bucket\n        :type bucket_name: str", "docstring_tokens": ["Returns", "a", "boto3", ".", "s3", ".", "Object"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L204-L218", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/distributions/batch_reshape.py", "func_name": "BatchReshape._call_reshape_input_output", "original_string": "def _call_reshape_input_output(self, fn, x, extra_kwargs=None):\n    \"\"\"Calls `fn`, appropriately reshaping its input `x` and output.\"\"\"\n    # Note: we take `extra_kwargs` as a dict rather than `**extra_kwargs`\n    # because it is possible the user provided extra kwargs would itself\n    # have `fn` and/or `x` as a key.\n    with tf.control_dependencies(self._runtime_assertions +\n                                 self._validate_sample_arg(x)):\n      sample_shape, static_sample_shape = self._sample_shape(x)\n      old_shape = tf.concat(\n          [\n              sample_shape,\n              self.distribution.batch_shape_tensor(),\n              self.event_shape_tensor(),\n          ],\n          axis=0)\n      x_reshape = tf.reshape(x, old_shape)\n      result = fn(x_reshape, **extra_kwargs) if extra_kwargs else fn(x_reshape)\n      new_shape = tf.concat(\n          [\n              sample_shape,\n              self._batch_shape_unexpanded,\n          ], axis=0)\n      result = tf.reshape(result, new_shape)\n      if (tensorshape_util.rank(static_sample_shape) is not None and\n          tensorshape_util.rank(self.batch_shape) is not None):\n        new_shape = tensorshape_util.concatenate(static_sample_shape,\n                                                 self.batch_shape)\n        tensorshape_util.set_shape(result, new_shape)\n      return result", "language": "python", "code": "def _call_reshape_input_output(self, fn, x, extra_kwargs=None):\n    \"\"\"Calls `fn`, appropriately reshaping its input `x` and output.\"\"\"\n    # Note: we take `extra_kwargs` as a dict rather than `**extra_kwargs`\n    # because it is possible the user provided extra kwargs would itself\n    # have `fn` and/or `x` as a key.\n    with tf.control_dependencies(self._runtime_assertions +\n                                 self._validate_sample_arg(x)):\n      sample_shape, static_sample_shape = self._sample_shape(x)\n      old_shape = tf.concat(\n          [\n              sample_shape,\n              self.distribution.batch_shape_tensor(),\n              self.event_shape_tensor(),\n          ],\n          axis=0)\n      x_reshape = tf.reshape(x, old_shape)\n      result = fn(x_reshape, **extra_kwargs) if extra_kwargs else fn(x_reshape)\n      new_shape = tf.concat(\n          [\n              sample_shape,\n              self._batch_shape_unexpanded,\n          ], axis=0)\n      result = tf.reshape(result, new_shape)\n      if (tensorshape_util.rank(static_sample_shape) is not None and\n          tensorshape_util.rank(self.batch_shape) is not None):\n        new_shape = tensorshape_util.concatenate(static_sample_shape,\n                                                 self.batch_shape)\n        tensorshape_util.set_shape(result, new_shape)\n      return result", "code_tokens": ["def", "_call_reshape_input_output", "(", "self", ",", "fn", ",", "x", ",", "extra_kwargs", "=", "None", ")", ":", "# Note: we take `extra_kwargs` as a dict rather than `**extra_kwargs`", "# because it is possible the user provided extra kwargs would itself", "# have `fn` and/or `x` as a key.", "with", "tf", ".", "control_dependencies", "(", "self", ".", "_runtime_assertions", "+", "self", ".", "_validate_sample_arg", "(", "x", ")", ")", ":", "sample_shape", ",", "static_sample_shape", "=", "self", ".", "_sample_shape", "(", "x", ")", "old_shape", "=", "tf", ".", "concat", "(", "[", "sample_shape", ",", "self", ".", "distribution", ".", "batch_shape_tensor", "(", ")", ",", "self", ".", "event_shape_tensor", "(", ")", ",", "]", ",", "axis", "=", "0", ")", "x_reshape", "=", "tf", ".", "reshape", "(", "x", ",", "old_shape", ")", "result", "=", "fn", "(", "x_reshape", ",", "*", "*", "extra_kwargs", ")", "if", "extra_kwargs", "else", "fn", "(", "x_reshape", ")", "new_shape", "=", "tf", ".", "concat", "(", "[", "sample_shape", ",", "self", ".", "_batch_shape_unexpanded", ",", "]", ",", "axis", "=", "0", ")", "result", "=", "tf", ".", "reshape", "(", "result", ",", "new_shape", ")", "if", "(", "tensorshape_util", ".", "rank", "(", "static_sample_shape", ")", "is", "not", "None", "and", "tensorshape_util", ".", "rank", "(", "self", ".", "batch_shape", ")", "is", "not", "None", ")", ":", "new_shape", "=", "tensorshape_util", ".", "concatenate", "(", "static_sample_shape", ",", "self", ".", "batch_shape", ")", "tensorshape_util", ".", "set_shape", "(", "result", ",", "new_shape", ")", "return", "result"], "docstring": "Calls `fn`, appropriately reshaping its input `x` and output.", "docstring_tokens": ["Calls", "fn", "appropriately", "reshaping", "its", "input", "x", "and", "output", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/batch_reshape.py#L234-L262", "partition": "test"}
{"repo": "agabrown/PyGaia", "path": "examples/relativeParallaxErrorsVsDistance.py", "func_name": "makePlot", "original_string": "def makePlot(pdf=False, png=False):\n  \"\"\"\n  Plot relative parallax errors as a function of distance for stars of a given spectral type.\n\n  Parameters\n  ----------\n\n  args - command line arguments\n  \"\"\"\n  logdistancekpc = np.linspace(-1,np.log10(20.0),100)\n  sptVabsAndVmini=OrderedDict([('K0V',(5.58,0.87)), ('G5V',(4.78,0.74)), ('G0V',(4.24,0.67)),\n    ('F5V',(3.50,0.50)), ('F0V',(2.98,0.38)), ('RC',(0.8,1.0))])\n  lines={}\n\n  fig=plt.figure(figsize=(10,6.5))\n  currentAxis=plt.gca()\n\n  for spt in sptVabsAndVmini.keys():\n    vmag=sptVabsAndVmini[spt][0]+5.0*logdistancekpc+10.0\n    indices=(vmag>14) & (vmag<16)\n    gmag=vmag+gminvFromVmini(sptVabsAndVmini[spt][1])\n    parerrors=parallaxErrorSkyAvg(gmag,sptVabsAndVmini[spt][1])\n    relparerrors=parerrors*10**logdistancekpc/1000.0\n    plt.loglog(10**logdistancekpc, relparerrors,'--k',lw=1)\n    plt.loglog(10**logdistancekpc[indices], relparerrors[indices],'-',label=spt)\n  plt.xlim(0.1,20.0)\n  plt.ylim(0.001,0.5)\n  plt.text(0.9, 0.05,'Colours indicate $14<V<16$',\n     horizontalalignment='right',\n     verticalalignment='bottom',\n     transform = currentAxis.transAxes)\n  plt.legend(loc=2)\n  plt.xlabel('distance [kpc]')\n  plt.ylabel('$\\\\sigma_\\\\varpi/\\\\varpi$')\n  plt.grid(which='both')\n  \n  if (args['pdfOutput']):\n    plt.savefig('RelativeParallaxErrorsVsDist.pdf')\n  elif (args['pngOutput']):\n    plt.savefig('RelativeParallaxErrorsVsDist.png')\n  else:\n    plt.show()", "language": "python", "code": "def makePlot(pdf=False, png=False):\n  \"\"\"\n  Plot relative parallax errors as a function of distance for stars of a given spectral type.\n\n  Parameters\n  ----------\n\n  args - command line arguments\n  \"\"\"\n  logdistancekpc = np.linspace(-1,np.log10(20.0),100)\n  sptVabsAndVmini=OrderedDict([('K0V',(5.58,0.87)), ('G5V',(4.78,0.74)), ('G0V',(4.24,0.67)),\n    ('F5V',(3.50,0.50)), ('F0V',(2.98,0.38)), ('RC',(0.8,1.0))])\n  lines={}\n\n  fig=plt.figure(figsize=(10,6.5))\n  currentAxis=plt.gca()\n\n  for spt in sptVabsAndVmini.keys():\n    vmag=sptVabsAndVmini[spt][0]+5.0*logdistancekpc+10.0\n    indices=(vmag>14) & (vmag<16)\n    gmag=vmag+gminvFromVmini(sptVabsAndVmini[spt][1])\n    parerrors=parallaxErrorSkyAvg(gmag,sptVabsAndVmini[spt][1])\n    relparerrors=parerrors*10**logdistancekpc/1000.0\n    plt.loglog(10**logdistancekpc, relparerrors,'--k',lw=1)\n    plt.loglog(10**logdistancekpc[indices], relparerrors[indices],'-',label=spt)\n  plt.xlim(0.1,20.0)\n  plt.ylim(0.001,0.5)\n  plt.text(0.9, 0.05,'Colours indicate $14<V<16$',\n     horizontalalignment='right',\n     verticalalignment='bottom',\n     transform = currentAxis.transAxes)\n  plt.legend(loc=2)\n  plt.xlabel('distance [kpc]')\n  plt.ylabel('$\\\\sigma_\\\\varpi/\\\\varpi$')\n  plt.grid(which='both')\n  \n  if (args['pdfOutput']):\n    plt.savefig('RelativeParallaxErrorsVsDist.pdf')\n  elif (args['pngOutput']):\n    plt.savefig('RelativeParallaxErrorsVsDist.png')\n  else:\n    plt.show()", "code_tokens": ["def", "makePlot", "(", "pdf", "=", "False", ",", "png", "=", "False", ")", ":", "logdistancekpc", "=", "np", ".", "linspace", "(", "-", "1", ",", "np", ".", "log10", "(", "20.0", ")", ",", "100", ")", "sptVabsAndVmini", "=", "OrderedDict", "(", "[", "(", "'K0V'", ",", "(", "5.58", ",", "0.87", ")", ")", ",", "(", "'G5V'", ",", "(", "4.78", ",", "0.74", ")", ")", ",", "(", "'G0V'", ",", "(", "4.24", ",", "0.67", ")", ")", ",", "(", "'F5V'", ",", "(", "3.50", ",", "0.50", ")", ")", ",", "(", "'F0V'", ",", "(", "2.98", ",", "0.38", ")", ")", ",", "(", "'RC'", ",", "(", "0.8", ",", "1.0", ")", ")", "]", ")", "lines", "=", "{", "}", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "(", "10", ",", "6.5", ")", ")", "currentAxis", "=", "plt", ".", "gca", "(", ")", "for", "spt", "in", "sptVabsAndVmini", ".", "keys", "(", ")", ":", "vmag", "=", "sptVabsAndVmini", "[", "spt", "]", "[", "0", "]", "+", "5.0", "*", "logdistancekpc", "+", "10.0", "indices", "=", "(", "vmag", ">", "14", ")", "&", "(", "vmag", "<", "16", ")", "gmag", "=", "vmag", "+", "gminvFromVmini", "(", "sptVabsAndVmini", "[", "spt", "]", "[", "1", "]", ")", "parerrors", "=", "parallaxErrorSkyAvg", "(", "gmag", ",", "sptVabsAndVmini", "[", "spt", "]", "[", "1", "]", ")", "relparerrors", "=", "parerrors", "*", "10", "**", "logdistancekpc", "/", "1000.0", "plt", ".", "loglog", "(", "10", "**", "logdistancekpc", ",", "relparerrors", ",", "'--k'", ",", "lw", "=", "1", ")", "plt", ".", "loglog", "(", "10", "**", "logdistancekpc", "[", "indices", "]", ",", "relparerrors", "[", "indices", "]", ",", "'-'", ",", "label", "=", "spt", ")", "plt", ".", "xlim", "(", "0.1", ",", "20.0", ")", "plt", ".", "ylim", "(", "0.001", ",", "0.5", ")", "plt", ".", "text", "(", "0.9", ",", "0.05", ",", "'Colours indicate $14<V<16$'", ",", "horizontalalignment", "=", "'right'", ",", "verticalalignment", "=", "'bottom'", ",", "transform", "=", "currentAxis", ".", "transAxes", ")", "plt", ".", "legend", "(", "loc", "=", "2", ")", "plt", ".", "xlabel", "(", "'distance [kpc]'", ")", "plt", ".", "ylabel", "(", "'$\\\\sigma_\\\\varpi/\\\\varpi$'", ")", "plt", ".", "grid", "(", "which", "=", "'both'", ")", "if", "(", "args", "[", "'pdfOutput'", "]", ")", ":", "plt", ".", "savefig", "(", "'RelativeParallaxErrorsVsDist.pdf'", ")", "elif", "(", "args", "[", "'pngOutput'", "]", ")", ":", "plt", ".", "savefig", "(", "'RelativeParallaxErrorsVsDist.png'", ")", "else", ":", "plt", ".", "show", "(", ")"], "docstring": "Plot relative parallax errors as a function of distance for stars of a given spectral type.\n\n  Parameters\n  ----------\n\n  args - command line arguments", "docstring_tokens": ["Plot", "relative", "parallax", "errors", "as", "a", "function", "of", "distance", "for", "stars", "of", "a", "given", "spectral", "type", "."], "sha": "ae972b0622a15f713ffae471f925eac25ccdae47", "url": "https://github.com/agabrown/PyGaia/blob/ae972b0622a15f713ffae471f925eac25ccdae47/examples/relativeParallaxErrorsVsDistance.py#L29-L70", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/jobs.py", "func_name": "LocalTaskJob.heartbeat_callback", "original_string": "def heartbeat_callback(self, session=None):\n        \"\"\"Self destruct task if state has been moved away from running externally\"\"\"\n\n        if self.terminating:\n            # ensure termination if processes are created later\n            self.task_runner.terminate()\n            return\n\n        self.task_instance.refresh_from_db()\n        ti = self.task_instance\n\n        fqdn = get_hostname()\n        same_hostname = fqdn == ti.hostname\n        same_process = ti.pid == os.getpid()\n\n        if ti.state == State.RUNNING:\n            if not same_hostname:\n                self.log.warning(\"The recorded hostname %s \"\n                                 \"does not match this instance's hostname \"\n                                 \"%s\", ti.hostname, fqdn)\n                raise AirflowException(\"Hostname of job runner does not match\")\n            elif not same_process:\n                current_pid = os.getpid()\n                self.log.warning(\"Recorded pid %s does not match \"\n                                 \"the current pid %s\", ti.pid, current_pid)\n                raise AirflowException(\"PID of job runner does not match\")\n        elif (\n                self.task_runner.return_code() is None and\n                hasattr(self.task_runner, 'process')\n        ):\n            self.log.warning(\n                \"State of this instance has been externally set to %s. \"\n                \"Taking the poison pill.\",\n                ti.state\n            )\n            self.task_runner.terminate()\n            self.terminating = True", "language": "python", "code": "def heartbeat_callback(self, session=None):\n        \"\"\"Self destruct task if state has been moved away from running externally\"\"\"\n\n        if self.terminating:\n            # ensure termination if processes are created later\n            self.task_runner.terminate()\n            return\n\n        self.task_instance.refresh_from_db()\n        ti = self.task_instance\n\n        fqdn = get_hostname()\n        same_hostname = fqdn == ti.hostname\n        same_process = ti.pid == os.getpid()\n\n        if ti.state == State.RUNNING:\n            if not same_hostname:\n                self.log.warning(\"The recorded hostname %s \"\n                                 \"does not match this instance's hostname \"\n                                 \"%s\", ti.hostname, fqdn)\n                raise AirflowException(\"Hostname of job runner does not match\")\n            elif not same_process:\n                current_pid = os.getpid()\n                self.log.warning(\"Recorded pid %s does not match \"\n                                 \"the current pid %s\", ti.pid, current_pid)\n                raise AirflowException(\"PID of job runner does not match\")\n        elif (\n                self.task_runner.return_code() is None and\n                hasattr(self.task_runner, 'process')\n        ):\n            self.log.warning(\n                \"State of this instance has been externally set to %s. \"\n                \"Taking the poison pill.\",\n                ti.state\n            )\n            self.task_runner.terminate()\n            self.terminating = True", "code_tokens": ["def", "heartbeat_callback", "(", "self", ",", "session", "=", "None", ")", ":", "if", "self", ".", "terminating", ":", "# ensure termination if processes are created later", "self", ".", "task_runner", ".", "terminate", "(", ")", "return", "self", ".", "task_instance", ".", "refresh_from_db", "(", ")", "ti", "=", "self", ".", "task_instance", "fqdn", "=", "get_hostname", "(", ")", "same_hostname", "=", "fqdn", "==", "ti", ".", "hostname", "same_process", "=", "ti", ".", "pid", "==", "os", ".", "getpid", "(", ")", "if", "ti", ".", "state", "==", "State", ".", "RUNNING", ":", "if", "not", "same_hostname", ":", "self", ".", "log", ".", "warning", "(", "\"The recorded hostname %s \"", "\"does not match this instance's hostname \"", "\"%s\"", ",", "ti", ".", "hostname", ",", "fqdn", ")", "raise", "AirflowException", "(", "\"Hostname of job runner does not match\"", ")", "elif", "not", "same_process", ":", "current_pid", "=", "os", ".", "getpid", "(", ")", "self", ".", "log", ".", "warning", "(", "\"Recorded pid %s does not match \"", "\"the current pid %s\"", ",", "ti", ".", "pid", ",", "current_pid", ")", "raise", "AirflowException", "(", "\"PID of job runner does not match\"", ")", "elif", "(", "self", ".", "task_runner", ".", "return_code", "(", ")", "is", "None", "and", "hasattr", "(", "self", ".", "task_runner", ",", "'process'", ")", ")", ":", "self", ".", "log", ".", "warning", "(", "\"State of this instance has been externally set to %s. \"", "\"Taking the poison pill.\"", ",", "ti", ".", "state", ")", "self", ".", "task_runner", ".", "terminate", "(", ")", "self", ".", "terminating", "=", "True"], "docstring": "Self destruct task if state has been moved away from running externally", "docstring_tokens": ["Self", "destruct", "task", "if", "state", "has", "been", "moved", "away", "from", "running", "externally"], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L2637-L2673", "partition": "test"}
{"repo": "ndokter/dsmr_parser", "path": "dsmr_parser/clients/protocol.py", "func_name": "create_tcp_dsmr_reader", "original_string": "def create_tcp_dsmr_reader(host, port, dsmr_version,\n                           telegram_callback, loop=None):\n    \"\"\"Creates a DSMR asyncio protocol coroutine using TCP connection.\"\"\"\n    protocol, _ = create_dsmr_protocol(\n        dsmr_version, telegram_callback, loop=None)\n    conn = loop.create_connection(protocol, host, port)\n    return conn", "language": "python", "code": "def create_tcp_dsmr_reader(host, port, dsmr_version,\n                           telegram_callback, loop=None):\n    \"\"\"Creates a DSMR asyncio protocol coroutine using TCP connection.\"\"\"\n    protocol, _ = create_dsmr_protocol(\n        dsmr_version, telegram_callback, loop=None)\n    conn = loop.create_connection(protocol, host, port)\n    return conn", "code_tokens": ["def", "create_tcp_dsmr_reader", "(", "host", ",", "port", ",", "dsmr_version", ",", "telegram_callback", ",", "loop", "=", "None", ")", ":", "protocol", ",", "_", "=", "create_dsmr_protocol", "(", "dsmr_version", ",", "telegram_callback", ",", "loop", "=", "None", ")", "conn", "=", "loop", ".", "create_connection", "(", "protocol", ",", "host", ",", "port", ")", "return", "conn"], "docstring": "Creates a DSMR asyncio protocol coroutine using TCP connection.", "docstring_tokens": ["Creates", "a", "DSMR", "asyncio", "protocol", "coroutine", "using", "TCP", "connection", "."], "sha": "c04b0a5add58ce70153eede1a87ca171876b61c7", "url": "https://github.com/ndokter/dsmr_parser/blob/c04b0a5add58ce70153eede1a87ca171876b61c7/dsmr_parser/clients/protocol.py#L49-L55", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/models/taskinstance.py", "func_name": "TaskInstance.are_dependents_done", "original_string": "def are_dependents_done(self, session=None):\n        \"\"\"\n        Checks whether the dependents of this task instance have all succeeded.\n        This is meant to be used by wait_for_downstream.\n\n        This is useful when you do not want to start processing the next\n        schedule of a task until the dependents are done. For instance,\n        if the task DROPs and recreates a table.\n        \"\"\"\n        task = self.task\n\n        if not task.downstream_task_ids:\n            return True\n\n        ti = session.query(func.count(TaskInstance.task_id)).filter(\n            TaskInstance.dag_id == self.dag_id,\n            TaskInstance.task_id.in_(task.downstream_task_ids),\n            TaskInstance.execution_date == self.execution_date,\n            TaskInstance.state == State.SUCCESS,\n        )\n        count = ti[0][0]\n        return count == len(task.downstream_task_ids)", "language": "python", "code": "def are_dependents_done(self, session=None):\n        \"\"\"\n        Checks whether the dependents of this task instance have all succeeded.\n        This is meant to be used by wait_for_downstream.\n\n        This is useful when you do not want to start processing the next\n        schedule of a task until the dependents are done. For instance,\n        if the task DROPs and recreates a table.\n        \"\"\"\n        task = self.task\n\n        if not task.downstream_task_ids:\n            return True\n\n        ti = session.query(func.count(TaskInstance.task_id)).filter(\n            TaskInstance.dag_id == self.dag_id,\n            TaskInstance.task_id.in_(task.downstream_task_ids),\n            TaskInstance.execution_date == self.execution_date,\n            TaskInstance.state == State.SUCCESS,\n        )\n        count = ti[0][0]\n        return count == len(task.downstream_task_ids)", "code_tokens": ["def", "are_dependents_done", "(", "self", ",", "session", "=", "None", ")", ":", "task", "=", "self", ".", "task", "if", "not", "task", ".", "downstream_task_ids", ":", "return", "True", "ti", "=", "session", ".", "query", "(", "func", ".", "count", "(", "TaskInstance", ".", "task_id", ")", ")", ".", "filter", "(", "TaskInstance", ".", "dag_id", "==", "self", ".", "dag_id", ",", "TaskInstance", ".", "task_id", ".", "in_", "(", "task", ".", "downstream_task_ids", ")", ",", "TaskInstance", ".", "execution_date", "==", "self", ".", "execution_date", ",", "TaskInstance", ".", "state", "==", "State", ".", "SUCCESS", ",", ")", "count", "=", "ti", "[", "0", "]", "[", "0", "]", "return", "count", "==", "len", "(", "task", ".", "downstream_task_ids", ")"], "docstring": "Checks whether the dependents of this task instance have all succeeded.\n        This is meant to be used by wait_for_downstream.\n\n        This is useful when you do not want to start processing the next\n        schedule of a task until the dependents are done. For instance,\n        if the task DROPs and recreates a table.", "docstring_tokens": ["Checks", "whether", "the", "dependents", "of", "this", "task", "instance", "have", "all", "succeeded", ".", "This", "is", "meant", "to", "be", "used", "by", "wait_for_downstream", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L494-L515", "partition": "test"}
