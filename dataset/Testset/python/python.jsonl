{"repo": "SmBe19/praw-OAuth2Util", "path": "OAuth2Util/OAuth2Util.py", "func_name": "OAuth2Util.refresh", "original_string": "def refresh(self, force=False, _retry=0):\n\t\t\"\"\"\n\t\tCheck if the token is still valid and requests a new if it is not\n\t\tvalid anymore\n\n\t\tCall this method before a call to praw\n\t\tif there might have passed more than one hour\n\n\t\tforce: if true, a new token will be retrieved no matter what\n\t\t\"\"\"\n\t\tif _retry >= 5:\n\t\t\traise ConnectionAbortedError('Reddit is not accessible right now, cannot refresh OAuth2 tokens.')\n\t\tself._check_token_present()\n\n\t\t# We check whether another instance already refreshed the token\n\t\tif time.time() > self._get_value(CONFIGKEY_VALID_UNTIL, float, exception_default=0) - REFRESH_MARGIN:\n\t\t\tself.config.read(self.configfile)\n\n\t\t\tif time.time() < self._get_value(CONFIGKEY_VALID_UNTIL, float, exception_default=0) - REFRESH_MARGIN:\n\t\t\t\tself._log(\"Found new token\")\n\t\t\t\tself.set_access_credentials()\n\n\t\tif force or time.time() > self._get_value(CONFIGKEY_VALID_UNTIL, float, exception_default=0) - REFRESH_MARGIN:\n\t\t\tself._log(\"Refresh Token\")\n\t\t\ttry:\n\t\t\t\tnew_token = self.r.refresh_access_information(self._get_value(CONFIGKEY_REFRESH_TOKEN))\n\t\t\t\tself._change_value(CONFIGKEY_TOKEN, new_token[\"access_token\"])\n\t\t\t\tself._change_value(CONFIGKEY_VALID_UNTIL, time.time() + TOKEN_VALID_DURATION)\n\t\t\t\tself.set_access_credentials()\n\t\t\texcept (praw.errors.OAuthInvalidToken, praw.errors.HTTPException) as e:\n\t\t\t\t# todo check e status code\n\t\t\t\t# self._log('Retrying in 5s.')\n\t\t\t\t# time.sleep(5)\n\t\t\t\t# self.refresh(_retry=_retry + 1)\n\n\t\t\t\tself._log(\"Request new Token (REF)\")\n\t\t\t\tself._get_new_access_information()", "language": "python", "code": "def refresh(self, force=False, _retry=0):\n\t\t\"\"\"\n\t\tCheck if the token is still valid and requests a new if it is not\n\t\tvalid anymore\n\n\t\tCall this method before a call to praw\n\t\tif there might have passed more than one hour\n\n\t\tforce: if true, a new token will be retrieved no matter what\n\t\t\"\"\"\n\t\tif _retry >= 5:\n\t\t\traise ConnectionAbortedError('Reddit is not accessible right now, cannot refresh OAuth2 tokens.')\n\t\tself._check_token_present()\n\n\t\t# We check whether another instance already refreshed the token\n\t\tif time.time() > self._get_value(CONFIGKEY_VALID_UNTIL, float, exception_default=0) - REFRESH_MARGIN:\n\t\t\tself.config.read(self.configfile)\n\n\t\t\tif time.time() < self._get_value(CONFIGKEY_VALID_UNTIL, float, exception_default=0) - REFRESH_MARGIN:\n\t\t\t\tself._log(\"Found new token\")\n\t\t\t\tself.set_access_credentials()\n\n\t\tif force or time.time() > self._get_value(CONFIGKEY_VALID_UNTIL, float, exception_default=0) - REFRESH_MARGIN:\n\t\t\tself._log(\"Refresh Token\")\n\t\t\ttry:\n\t\t\t\tnew_token = self.r.refresh_access_information(self._get_value(CONFIGKEY_REFRESH_TOKEN))\n\t\t\t\tself._change_value(CONFIGKEY_TOKEN, new_token[\"access_token\"])\n\t\t\t\tself._change_value(CONFIGKEY_VALID_UNTIL, time.time() + TOKEN_VALID_DURATION)\n\t\t\t\tself.set_access_credentials()\n\t\t\texcept (praw.errors.OAuthInvalidToken, praw.errors.HTTPException) as e:\n\t\t\t\t# todo check e status code\n\t\t\t\t# self._log('Retrying in 5s.')\n\t\t\t\t# time.sleep(5)\n\t\t\t\t# self.refresh(_retry=_retry + 1)\n\n\t\t\t\tself._log(\"Request new Token (REF)\")\n\t\t\t\tself._get_new_access_information()", "code_tokens": ["def", "refresh", "(", "self", ",", "force", "=", "False", ",", "_retry", "=", "0", ")", ":", "if", "_retry", ">=", "5", ":", "raise", "ConnectionAbortedError", "(", "'Reddit is not accessible right now, cannot refresh OAuth2 tokens.'", ")", "self", ".", "_check_token_present", "(", ")", "# We check whether another instance already refreshed the token", "if", "time", ".", "time", "(", ")", ">", "self", ".", "_get_value", "(", "CONFIGKEY_VALID_UNTIL", ",", "float", ",", "exception_default", "=", "0", ")", "-", "REFRESH_MARGIN", ":", "self", ".", "config", ".", "read", "(", "self", ".", "configfile", ")", "if", "time", ".", "time", "(", ")", "<", "self", ".", "_get_value", "(", "CONFIGKEY_VALID_UNTIL", ",", "float", ",", "exception_default", "=", "0", ")", "-", "REFRESH_MARGIN", ":", "self", ".", "_log", "(", "\"Found new token\"", ")", "self", ".", "set_access_credentials", "(", ")", "if", "force", "or", "time", ".", "time", "(", ")", ">", "self", ".", "_get_value", "(", "CONFIGKEY_VALID_UNTIL", ",", "float", ",", "exception_default", "=", "0", ")", "-", "REFRESH_MARGIN", ":", "self", ".", "_log", "(", "\"Refresh Token\"", ")", "try", ":", "new_token", "=", "self", ".", "r", ".", "refresh_access_information", "(", "self", ".", "_get_value", "(", "CONFIGKEY_REFRESH_TOKEN", ")", ")", "self", ".", "_change_value", "(", "CONFIGKEY_TOKEN", ",", "new_token", "[", "\"access_token\"", "]", ")", "self", ".", "_change_value", "(", "CONFIGKEY_VALID_UNTIL", ",", "time", ".", "time", "(", ")", "+", "TOKEN_VALID_DURATION", ")", "self", ".", "set_access_credentials", "(", ")", "except", "(", "praw", ".", "errors", ".", "OAuthInvalidToken", ",", "praw", ".", "errors", ".", "HTTPException", ")", "as", "e", ":", "# todo check e status code", "# self._log('Retrying in 5s.')", "# time.sleep(5)", "# self.refresh(_retry=_retry + 1)", "self", ".", "_log", "(", "\"Request new Token (REF)\"", ")", "self", ".", "_get_new_access_information", "(", ")"], "docstring": "Check if the token is still valid and requests a new if it is not\n\t\tvalid anymore\n\n\t\tCall this method before a call to praw\n\t\tif there might have passed more than one hour\n\n\t\tforce: if true, a new token will be retrieved no matter what", "docstring_tokens": ["Check", "if", "the", "token", "is", "still", "valid", "and", "requests", "a", "new", "if", "it", "is", "not", "valid", "anymore"], "sha": "ca0a2d4d7eefcc681aac92c9cd4b83cd9ea6c5fe", "url": "https://github.com/SmBe19/praw-OAuth2Util/blob/ca0a2d4d7eefcc681aac92c9cd4b83cd9ea6c5fe/OAuth2Util/OAuth2Util.py#L320-L356", "partition": "test"}
{"repo": "greenbender/pynntp", "path": "nntp/nntp.py", "func_name": "NNTPClient.list_extensions_gen", "original_string": "def list_extensions_gen(self):\n        \"\"\"Generator for the LIST EXTENSIONS command.\n        \"\"\"\n        code, message = self.command(\"LIST EXTENSIONS\")\n        if code != 202:\n            raise NNTPReplyError(code, message)\n\n        for line in self.info_gen(code, message):\n            yield line.strip()", "language": "python", "code": "def list_extensions_gen(self):\n        \"\"\"Generator for the LIST EXTENSIONS command.\n        \"\"\"\n        code, message = self.command(\"LIST EXTENSIONS\")\n        if code != 202:\n            raise NNTPReplyError(code, message)\n\n        for line in self.info_gen(code, message):\n            yield line.strip()", "code_tokens": ["def", "list_extensions_gen", "(", "self", ")", ":", "code", ",", "message", "=", "self", ".", "command", "(", "\"LIST EXTENSIONS\"", ")", "if", "code", "!=", "202", ":", "raise", "NNTPReplyError", "(", "code", ",", "message", ")", "for", "line", "in", "self", ".", "info_gen", "(", "code", ",", "message", ")", ":", "yield", "line", ".", "strip", "(", ")"], "docstring": "Generator for the LIST EXTENSIONS command.", "docstring_tokens": ["Generator", "for", "the", "LIST", "EXTENSIONS", "command", "."], "sha": "991a76331cdf5d8f9dbf5b18f6e29adc80749a2f", "url": "https://github.com/greenbender/pynntp/blob/991a76331cdf5d8f9dbf5b18f6e29adc80749a2f/nntp/nntp.py#L888-L896", "partition": "test"}
{"repo": "katerina7479/pypdflite", "path": "pypdflite/pdfdocument.py", "func_name": "PDFDocument.add_text", "original_string": "def add_text(self, text, cursor=None, justification=None):\r\n        \"\"\" Input text, short or long. Writes in order, within the defined page boundaries. Sequential add_text commands will print without\r\n            additional whitespace. \"\"\"\r\n        if cursor is None:\r\n            cursor = self.page.cursor\r\n\r\n        text = re.sub(\"\\s\\s+\" , \" \", text)\r\n\r\n        if justification is None:\r\n            justification = self.justification\r\n\r\n        if '\\n' in text:\r\n            text_list = text.split('\\n')\r\n            for text in text_list:\r\n                PDFText(self.session, self.page, text, self.font, self.text_color, cursor, justification, self.double_spacing)\r\n                self.add_newline()\r\n        else:\r\n            PDFText(self.session, self.page, text, self.font, self.text_color, cursor, justification, self.double_spacing)", "language": "python", "code": "def add_text(self, text, cursor=None, justification=None):\r\n        \"\"\" Input text, short or long. Writes in order, within the defined page boundaries. Sequential add_text commands will print without\r\n            additional whitespace. \"\"\"\r\n        if cursor is None:\r\n            cursor = self.page.cursor\r\n\r\n        text = re.sub(\"\\s\\s+\" , \" \", text)\r\n\r\n        if justification is None:\r\n            justification = self.justification\r\n\r\n        if '\\n' in text:\r\n            text_list = text.split('\\n')\r\n            for text in text_list:\r\n                PDFText(self.session, self.page, text, self.font, self.text_color, cursor, justification, self.double_spacing)\r\n                self.add_newline()\r\n        else:\r\n            PDFText(self.session, self.page, text, self.font, self.text_color, cursor, justification, self.double_spacing)", "code_tokens": ["def", "add_text", "(", "self", ",", "text", ",", "cursor", "=", "None", ",", "justification", "=", "None", ")", ":", "if", "cursor", "is", "None", ":", "cursor", "=", "self", ".", "page", ".", "cursor", "text", "=", "re", ".", "sub", "(", "\"\\s\\s+\"", ",", "\" \"", ",", "text", ")", "if", "justification", "is", "None", ":", "justification", "=", "self", ".", "justification", "if", "'\\n'", "in", "text", ":", "text_list", "=", "text", ".", "split", "(", "'\\n'", ")", "for", "text", "in", "text_list", ":", "PDFText", "(", "self", ".", "session", ",", "self", ".", "page", ",", "text", ",", "self", ".", "font", ",", "self", ".", "text_color", ",", "cursor", ",", "justification", ",", "self", ".", "double_spacing", ")", "self", ".", "add_newline", "(", ")", "else", ":", "PDFText", "(", "self", ".", "session", ",", "self", ".", "page", ",", "text", ",", "self", ".", "font", ",", "self", ".", "text_color", ",", "cursor", ",", "justification", ",", "self", ".", "double_spacing", ")"], "docstring": "Input text, short or long. Writes in order, within the defined page boundaries. Sequential add_text commands will print without\r\n            additional whitespace.", "docstring_tokens": ["Input", "text", "short", "or", "long", ".", "Writes", "in", "order", "within", "the", "defined", "page", "boundaries", ".", "Sequential", "add_text", "commands", "will", "print", "without", "additional", "whitespace", "."], "sha": "ac2501f30d6619eae9dea5644717575ca9263d0a", "url": "https://github.com/katerina7479/pypdflite/blob/ac2501f30d6619eae9dea5644717575ca9263d0a/pypdflite/pdfdocument.py#L238-L255", "partition": "test"}
{"repo": "yougov/openpack", "path": "openpack/editor.py", "func_name": "EditableFile.get_editor", "original_string": "def get_editor(filepath):\n\t\t\"\"\"\n\t\tGive preference to an XML_EDITOR or EDITOR defined in the\n\t\tenvironment. Otherwise use notepad on Windows and edit on other\n\t\tplatforms.\n\t\t\"\"\"\n\t\tdefault_editor = ['edit', 'notepad'][sys.platform.startswith('win32')]\n\t\treturn os.environ.get(\n\t\t\t'XML_EDITOR',\n\t\t\tos.environ.get('EDITOR', default_editor),\n\t\t)", "language": "python", "code": "def get_editor(filepath):\n\t\t\"\"\"\n\t\tGive preference to an XML_EDITOR or EDITOR defined in the\n\t\tenvironment. Otherwise use notepad on Windows and edit on other\n\t\tplatforms.\n\t\t\"\"\"\n\t\tdefault_editor = ['edit', 'notepad'][sys.platform.startswith('win32')]\n\t\treturn os.environ.get(\n\t\t\t'XML_EDITOR',\n\t\t\tos.environ.get('EDITOR', default_editor),\n\t\t)", "code_tokens": ["def", "get_editor", "(", "filepath", ")", ":", "default_editor", "=", "[", "'edit'", ",", "'notepad'", "]", "[", "sys", ".", "platform", ".", "startswith", "(", "'win32'", ")", "]", "return", "os", ".", "environ", ".", "get", "(", "'XML_EDITOR'", ",", "os", ".", "environ", ".", "get", "(", "'EDITOR'", ",", "default_editor", ")", ",", ")"], "docstring": "Give preference to an XML_EDITOR or EDITOR defined in the\n\t\tenvironment. Otherwise use notepad on Windows and edit on other\n\t\tplatforms.", "docstring_tokens": ["Give", "preference", "to", "an", "XML_EDITOR", "or", "EDITOR", "defined", "in", "the", "environment", ".", "Otherwise", "use", "notepad", "on", "Windows", "and", "edit", "on", "other", "platforms", "."], "sha": "1412ec34c1bab6ba6c8ae5490c2205d696f13717", "url": "https://github.com/yougov/openpack/blob/1412ec34c1bab6ba6c8ae5490c2205d696f13717/openpack/editor.py#L90-L100", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/layers/initializers.py", "func_name": "BlockwiseInitializer.from_config", "original_string": "def from_config(cls, config):\n    \"\"\"Instantiates an initializer from a configuration dictionary.\"\"\"\n    return cls(**{\n        'initializers': [tf.compat.v2.initializers.deserialize(init)\n                         for init in config.get('initializers', [])],\n        'sizes': config.get('sizes', []),\n        'validate_args': config.get('validate_args', False),\n    })", "language": "python", "code": "def from_config(cls, config):\n    \"\"\"Instantiates an initializer from a configuration dictionary.\"\"\"\n    return cls(**{\n        'initializers': [tf.compat.v2.initializers.deserialize(init)\n                         for init in config.get('initializers', [])],\n        'sizes': config.get('sizes', []),\n        'validate_args': config.get('validate_args', False),\n    })", "code_tokens": ["def", "from_config", "(", "cls", ",", "config", ")", ":", "return", "cls", "(", "*", "*", "{", "'initializers'", ":", "[", "tf", ".", "compat", ".", "v2", ".", "initializers", ".", "deserialize", "(", "init", ")", "for", "init", "in", "config", ".", "get", "(", "'initializers'", ",", "[", "]", ")", "]", ",", "'sizes'", ":", "config", ".", "get", "(", "'sizes'", ",", "[", "]", ")", ",", "'validate_args'", ":", "config", ".", "get", "(", "'validate_args'", ",", "False", ")", ",", "}", ")"], "docstring": "Instantiates an initializer from a configuration dictionary.", "docstring_tokens": ["Instantiates", "an", "initializer", "from", "a", "configuration", "dictionary", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/layers/initializers.py#L119-L126", "partition": "test"}
{"repo": "greenbender/pynntp", "path": "nntp/utils.py", "func_name": "unparse_range", "original_string": "def unparse_range(obj):\n    \"\"\"Unparse a range argument.\n\n    Args:\n        obj: An article range. There are a number of valid formats; an integer\n            specifying a single article or a tuple specifying an article range.\n            If the range doesn't give a start article then all articles up to\n            the specified last article are included. If the range doesn't\n            specify a last article then all articles from the first specified\n            article up to the current last article for the group are included.\n\n    Returns:\n        The range as a string that can be used by an NNTP command.\n\n    Note: Sample valid formats.\n        4678\n        (,5234)\n        (4245,)\n        (4245, 5234)\n    \"\"\"\n    if isinstance(obj, (int, long)):\n        return str(obj)\n\n    if isinstance(obj, tuple):\n        arg = str(obj[0]) + \"-\"\n        if len(obj) > 1:\n            arg += str(obj[1])\n        return arg\n\n    raise ValueError(\"Must be an integer or tuple\")", "language": "python", "code": "def unparse_range(obj):\n    \"\"\"Unparse a range argument.\n\n    Args:\n        obj: An article range. There are a number of valid formats; an integer\n            specifying a single article or a tuple specifying an article range.\n            If the range doesn't give a start article then all articles up to\n            the specified last article are included. If the range doesn't\n            specify a last article then all articles from the first specified\n            article up to the current last article for the group are included.\n\n    Returns:\n        The range as a string that can be used by an NNTP command.\n\n    Note: Sample valid formats.\n        4678\n        (,5234)\n        (4245,)\n        (4245, 5234)\n    \"\"\"\n    if isinstance(obj, (int, long)):\n        return str(obj)\n\n    if isinstance(obj, tuple):\n        arg = str(obj[0]) + \"-\"\n        if len(obj) > 1:\n            arg += str(obj[1])\n        return arg\n\n    raise ValueError(\"Must be an integer or tuple\")", "code_tokens": ["def", "unparse_range", "(", "obj", ")", ":", "if", "isinstance", "(", "obj", ",", "(", "int", ",", "long", ")", ")", ":", "return", "str", "(", "obj", ")", "if", "isinstance", "(", "obj", ",", "tuple", ")", ":", "arg", "=", "str", "(", "obj", "[", "0", "]", ")", "+", "\"-\"", "if", "len", "(", "obj", ")", ">", "1", ":", "arg", "+=", "str", "(", "obj", "[", "1", "]", ")", "return", "arg", "raise", "ValueError", "(", "\"Must be an integer or tuple\"", ")"], "docstring": "Unparse a range argument.\n\n    Args:\n        obj: An article range. There are a number of valid formats; an integer\n            specifying a single article or a tuple specifying an article range.\n            If the range doesn't give a start article then all articles up to\n            the specified last article are included. If the range doesn't\n            specify a last article then all articles from the first specified\n            article up to the current last article for the group are included.\n\n    Returns:\n        The range as a string that can be used by an NNTP command.\n\n    Note: Sample valid formats.\n        4678\n        (,5234)\n        (4245,)\n        (4245, 5234)", "docstring_tokens": ["Unparse", "a", "range", "argument", "."], "sha": "991a76331cdf5d8f9dbf5b18f6e29adc80749a2f", "url": "https://github.com/greenbender/pynntp/blob/991a76331cdf5d8f9dbf5b18f6e29adc80749a2f/nntp/utils.py#L49-L78", "partition": "test"}
{"repo": "librosa/librosa", "path": "examples/adjust_tuning.py", "func_name": "adjust_tuning", "original_string": "def adjust_tuning(input_file, output_file):\n    '''Load audio, estimate tuning, apply pitch correction, and save.'''\n    print('Loading ', input_file)\n    y, sr = librosa.load(input_file)\n\n    print('Separating harmonic component ... ')\n    y_harm = librosa.effects.harmonic(y)\n\n    print('Estimating tuning ... ')\n    # Just track the pitches associated with high magnitude\n    tuning = librosa.estimate_tuning(y=y_harm, sr=sr)\n\n    print('{:+0.2f} cents'.format(100 * tuning))\n    print('Applying pitch-correction of {:+0.2f} cents'.format(-100 * tuning))\n    y_tuned = librosa.effects.pitch_shift(y, sr, -tuning)\n\n    print('Saving tuned audio to: ', output_file)\n    librosa.output.write_wav(output_file, y_tuned, sr)", "language": "python", "code": "def adjust_tuning(input_file, output_file):\n    '''Load audio, estimate tuning, apply pitch correction, and save.'''\n    print('Loading ', input_file)\n    y, sr = librosa.load(input_file)\n\n    print('Separating harmonic component ... ')\n    y_harm = librosa.effects.harmonic(y)\n\n    print('Estimating tuning ... ')\n    # Just track the pitches associated with high magnitude\n    tuning = librosa.estimate_tuning(y=y_harm, sr=sr)\n\n    print('{:+0.2f} cents'.format(100 * tuning))\n    print('Applying pitch-correction of {:+0.2f} cents'.format(-100 * tuning))\n    y_tuned = librosa.effects.pitch_shift(y, sr, -tuning)\n\n    print('Saving tuned audio to: ', output_file)\n    librosa.output.write_wav(output_file, y_tuned, sr)", "code_tokens": ["def", "adjust_tuning", "(", "input_file", ",", "output_file", ")", ":", "print", "(", "'Loading '", ",", "input_file", ")", "y", ",", "sr", "=", "librosa", ".", "load", "(", "input_file", ")", "print", "(", "'Separating harmonic component ... '", ")", "y_harm", "=", "librosa", ".", "effects", ".", "harmonic", "(", "y", ")", "print", "(", "'Estimating tuning ... '", ")", "# Just track the pitches associated with high magnitude", "tuning", "=", "librosa", ".", "estimate_tuning", "(", "y", "=", "y_harm", ",", "sr", "=", "sr", ")", "print", "(", "'{:+0.2f} cents'", ".", "format", "(", "100", "*", "tuning", ")", ")", "print", "(", "'Applying pitch-correction of {:+0.2f} cents'", ".", "format", "(", "-", "100", "*", "tuning", ")", ")", "y_tuned", "=", "librosa", ".", "effects", ".", "pitch_shift", "(", "y", ",", "sr", ",", "-", "tuning", ")", "print", "(", "'Saving tuned audio to: '", ",", "output_file", ")", "librosa", ".", "output", ".", "write_wav", "(", "output_file", ",", "y_tuned", ",", "sr", ")"], "docstring": "Load audio, estimate tuning, apply pitch correction, and save.", "docstring_tokens": ["Load", "audio", "estimate", "tuning", "apply", "pitch", "correction", "and", "save", "."], "sha": "180e8e6eb8f958fa6b20b8cba389f7945d508247", "url": "https://github.com/librosa/librosa/blob/180e8e6eb8f958fa6b20b8cba389f7945d508247/examples/adjust_tuning.py#L15-L32", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/gcp_container_hook.py", "func_name": "GKEClusterHook.create_cluster", "original_string": "def create_cluster(self, cluster, project_id=None, retry=DEFAULT, timeout=DEFAULT):\n        \"\"\"\n        Creates a cluster, consisting of the specified number and type of Google Compute\n        Engine instances.\n\n        :param cluster: A Cluster protobuf or dict. If dict is provided, it must\n            be of the same form as the protobuf message\n            :class:`google.cloud.container_v1.types.Cluster`\n        :type cluster: dict or google.cloud.container_v1.types.Cluster\n        :param project_id: Google Cloud Platform project ID\n        :type project_id: str\n        :param retry: A retry object (``google.api_core.retry.Retry``) used to\n            retry requests.\n            If None is specified, requests will not be retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: The amount of time, in seconds, to wait for the request to\n            complete. Note that if retry is specified, the timeout applies to each\n            individual attempt.\n        :type timeout: float\n        :return: The full url to the new, or existing, cluster\n        :raises:\n            ParseError: On JSON parsing problems when trying to convert dict\n            AirflowException: cluster is not dict type nor Cluster proto type\n        \"\"\"\n\n        if isinstance(cluster, dict):\n            cluster_proto = Cluster()\n            cluster = self._dict_to_proto(py_dict=cluster, proto=cluster_proto)\n        elif not isinstance(cluster, Cluster):\n            raise AirflowException(\n                \"cluster is not instance of Cluster proto or python dict\")\n\n        self._append_label(cluster, 'airflow-version', 'v' + version.version)\n\n        self.log.info(\n            \"Creating (project_id=%s, zone=%s, cluster_name=%s)\",\n            self.project_id, self.location, cluster.name\n        )\n        try:\n            op = self.get_client().create_cluster(project_id=project_id or self.project_id,\n                                                  zone=self.location,\n                                                  cluster=cluster,\n                                                  retry=retry,\n                                                  timeout=timeout)\n            op = self.wait_for_operation(op)\n\n            return op.target_link\n        except AlreadyExists as error:\n            self.log.info('Assuming Success: %s', error.message)\n            return self.get_cluster(name=cluster.name).self_link", "language": "python", "code": "def create_cluster(self, cluster, project_id=None, retry=DEFAULT, timeout=DEFAULT):\n        \"\"\"\n        Creates a cluster, consisting of the specified number and type of Google Compute\n        Engine instances.\n\n        :param cluster: A Cluster protobuf or dict. If dict is provided, it must\n            be of the same form as the protobuf message\n            :class:`google.cloud.container_v1.types.Cluster`\n        :type cluster: dict or google.cloud.container_v1.types.Cluster\n        :param project_id: Google Cloud Platform project ID\n        :type project_id: str\n        :param retry: A retry object (``google.api_core.retry.Retry``) used to\n            retry requests.\n            If None is specified, requests will not be retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: The amount of time, in seconds, to wait for the request to\n            complete. Note that if retry is specified, the timeout applies to each\n            individual attempt.\n        :type timeout: float\n        :return: The full url to the new, or existing, cluster\n        :raises:\n            ParseError: On JSON parsing problems when trying to convert dict\n            AirflowException: cluster is not dict type nor Cluster proto type\n        \"\"\"\n\n        if isinstance(cluster, dict):\n            cluster_proto = Cluster()\n            cluster = self._dict_to_proto(py_dict=cluster, proto=cluster_proto)\n        elif not isinstance(cluster, Cluster):\n            raise AirflowException(\n                \"cluster is not instance of Cluster proto or python dict\")\n\n        self._append_label(cluster, 'airflow-version', 'v' + version.version)\n\n        self.log.info(\n            \"Creating (project_id=%s, zone=%s, cluster_name=%s)\",\n            self.project_id, self.location, cluster.name\n        )\n        try:\n            op = self.get_client().create_cluster(project_id=project_id or self.project_id,\n                                                  zone=self.location,\n                                                  cluster=cluster,\n                                                  retry=retry,\n                                                  timeout=timeout)\n            op = self.wait_for_operation(op)\n\n            return op.target_link\n        except AlreadyExists as error:\n            self.log.info('Assuming Success: %s', error.message)\n            return self.get_cluster(name=cluster.name).self_link", "code_tokens": ["def", "create_cluster", "(", "self", ",", "cluster", ",", "project_id", "=", "None", ",", "retry", "=", "DEFAULT", ",", "timeout", "=", "DEFAULT", ")", ":", "if", "isinstance", "(", "cluster", ",", "dict", ")", ":", "cluster_proto", "=", "Cluster", "(", ")", "cluster", "=", "self", ".", "_dict_to_proto", "(", "py_dict", "=", "cluster", ",", "proto", "=", "cluster_proto", ")", "elif", "not", "isinstance", "(", "cluster", ",", "Cluster", ")", ":", "raise", "AirflowException", "(", "\"cluster is not instance of Cluster proto or python dict\"", ")", "self", ".", "_append_label", "(", "cluster", ",", "'airflow-version'", ",", "'v'", "+", "version", ".", "version", ")", "self", ".", "log", ".", "info", "(", "\"Creating (project_id=%s, zone=%s, cluster_name=%s)\"", ",", "self", ".", "project_id", ",", "self", ".", "location", ",", "cluster", ".", "name", ")", "try", ":", "op", "=", "self", ".", "get_client", "(", ")", ".", "create_cluster", "(", "project_id", "=", "project_id", "or", "self", ".", "project_id", ",", "zone", "=", "self", ".", "location", ",", "cluster", "=", "cluster", ",", "retry", "=", "retry", ",", "timeout", "=", "timeout", ")", "op", "=", "self", ".", "wait_for_operation", "(", "op", ")", "return", "op", ".", "target_link", "except", "AlreadyExists", "as", "error", ":", "self", ".", "log", ".", "info", "(", "'Assuming Success: %s'", ",", "error", ".", "message", ")", "return", "self", ".", "get_cluster", "(", "name", "=", "cluster", ".", "name", ")", ".", "self_link"], "docstring": "Creates a cluster, consisting of the specified number and type of Google Compute\n        Engine instances.\n\n        :param cluster: A Cluster protobuf or dict. If dict is provided, it must\n            be of the same form as the protobuf message\n            :class:`google.cloud.container_v1.types.Cluster`\n        :type cluster: dict or google.cloud.container_v1.types.Cluster\n        :param project_id: Google Cloud Platform project ID\n        :type project_id: str\n        :param retry: A retry object (``google.api_core.retry.Retry``) used to\n            retry requests.\n            If None is specified, requests will not be retried.\n        :type retry: google.api_core.retry.Retry\n        :param timeout: The amount of time, in seconds, to wait for the request to\n            complete. Note that if retry is specified, the timeout applies to each\n            individual attempt.\n        :type timeout: float\n        :return: The full url to the new, or existing, cluster\n        :raises:\n            ParseError: On JSON parsing problems when trying to convert dict\n            AirflowException: cluster is not dict type nor Cluster proto type", "docstring_tokens": ["Creates", "a", "cluster", "consisting", "of", "the", "specified", "number", "and", "type", "of", "Google", "Compute", "Engine", "instances", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_container_hook.py#L170-L219", "partition": "test"}
{"repo": "codeinthehole/django-async-messages", "path": "async_messages/__init__.py", "func_name": "message_user", "original_string": "def message_user(user, message, level=constants.INFO):\n    \"\"\"\n    Send a message to a particular user.\n\n    :param user: User instance\n    :param message: Message to show\n    :param level: Message level\n    \"\"\"\n    # We store a list of messages in the cache so we can have multiple messages\n    # queued up for a user.\n    user_key = _user_key(user)\n    messages = cache.get(user_key) or []\n    messages.append((message, level))\n    cache.set(user_key, messages)", "language": "python", "code": "def message_user(user, message, level=constants.INFO):\n    \"\"\"\n    Send a message to a particular user.\n\n    :param user: User instance\n    :param message: Message to show\n    :param level: Message level\n    \"\"\"\n    # We store a list of messages in the cache so we can have multiple messages\n    # queued up for a user.\n    user_key = _user_key(user)\n    messages = cache.get(user_key) or []\n    messages.append((message, level))\n    cache.set(user_key, messages)", "code_tokens": ["def", "message_user", "(", "user", ",", "message", ",", "level", "=", "constants", ".", "INFO", ")", ":", "# We store a list of messages in the cache so we can have multiple messages", "# queued up for a user.", "user_key", "=", "_user_key", "(", "user", ")", "messages", "=", "cache", ".", "get", "(", "user_key", ")", "or", "[", "]", "messages", ".", "append", "(", "(", "message", ",", "level", ")", ")", "cache", ".", "set", "(", "user_key", ",", "messages", ")"], "docstring": "Send a message to a particular user.\n\n    :param user: User instance\n    :param message: Message to show\n    :param level: Message level", "docstring_tokens": ["Send", "a", "message", "to", "a", "particular", "user", "."], "sha": "292cb2fc517521dabc67b90e7ca5b1617f59e214", "url": "https://github.com/codeinthehole/django-async-messages/blob/292cb2fc517521dabc67b90e7ca5b1617f59e214/async_messages/__init__.py#L5-L18", "partition": "test"}
{"repo": "OpenKMIP/PyKMIP", "path": "kmip/core/messages/payloads/check.py", "func_name": "CheckResponsePayload.write", "original_string": "def write(self, output_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Write the data encoding the Check response payload to a stream.\n\n        Args:\n            output_stream (stream): A data stream in which to encode object\n                data, supporting a write method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be encoded. Optional,\n                defaults to KMIP 1.0.\n\n        Raises:\n            ValueError: Raised if the data attribute is not defined.\n        \"\"\"\n        local_stream = utils.BytearrayStream()\n\n        if self._unique_identifier:\n            self._unique_identifier.write(\n                local_stream,\n                kmip_version=kmip_version\n            )\n        if self._usage_limits_count:\n            self._usage_limits_count.write(\n                local_stream,\n                kmip_version=kmip_version\n            )\n        if self._cryptographic_usage_mask:\n            self._cryptographic_usage_mask.write(\n                local_stream,\n                kmip_version=kmip_version\n            )\n        if self._lease_time:\n            self._lease_time.write(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        self.length = local_stream.length()\n        super(CheckResponsePayload, self).write(\n            output_stream,\n            kmip_version=kmip_version\n        )\n        output_stream.write(local_stream.buffer)", "language": "python", "code": "def write(self, output_stream, kmip_version=enums.KMIPVersion.KMIP_1_0):\n        \"\"\"\n        Write the data encoding the Check response payload to a stream.\n\n        Args:\n            output_stream (stream): A data stream in which to encode object\n                data, supporting a write method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be encoded. Optional,\n                defaults to KMIP 1.0.\n\n        Raises:\n            ValueError: Raised if the data attribute is not defined.\n        \"\"\"\n        local_stream = utils.BytearrayStream()\n\n        if self._unique_identifier:\n            self._unique_identifier.write(\n                local_stream,\n                kmip_version=kmip_version\n            )\n        if self._usage_limits_count:\n            self._usage_limits_count.write(\n                local_stream,\n                kmip_version=kmip_version\n            )\n        if self._cryptographic_usage_mask:\n            self._cryptographic_usage_mask.write(\n                local_stream,\n                kmip_version=kmip_version\n            )\n        if self._lease_time:\n            self._lease_time.write(\n                local_stream,\n                kmip_version=kmip_version\n            )\n\n        self.length = local_stream.length()\n        super(CheckResponsePayload, self).write(\n            output_stream,\n            kmip_version=kmip_version\n        )\n        output_stream.write(local_stream.buffer)", "code_tokens": ["def", "write", "(", "self", ",", "output_stream", ",", "kmip_version", "=", "enums", ".", "KMIPVersion", ".", "KMIP_1_0", ")", ":", "local_stream", "=", "utils", ".", "BytearrayStream", "(", ")", "if", "self", ".", "_unique_identifier", ":", "self", ".", "_unique_identifier", ".", "write", "(", "local_stream", ",", "kmip_version", "=", "kmip_version", ")", "if", "self", ".", "_usage_limits_count", ":", "self", ".", "_usage_limits_count", ".", "write", "(", "local_stream", ",", "kmip_version", "=", "kmip_version", ")", "if", "self", ".", "_cryptographic_usage_mask", ":", "self", ".", "_cryptographic_usage_mask", ".", "write", "(", "local_stream", ",", "kmip_version", "=", "kmip_version", ")", "if", "self", ".", "_lease_time", ":", "self", ".", "_lease_time", ".", "write", "(", "local_stream", ",", "kmip_version", "=", "kmip_version", ")", "self", ".", "length", "=", "local_stream", ".", "length", "(", ")", "super", "(", "CheckResponsePayload", ",", "self", ")", ".", "write", "(", "output_stream", ",", "kmip_version", "=", "kmip_version", ")", "output_stream", ".", "write", "(", "local_stream", ".", "buffer", ")"], "docstring": "Write the data encoding the Check response payload to a stream.\n\n        Args:\n            output_stream (stream): A data stream in which to encode object\n                data, supporting a write method; usually a BytearrayStream\n                object.\n            kmip_version (KMIPVersion): An enumeration defining the KMIP\n                version with which the object will be encoded. Optional,\n                defaults to KMIP 1.0.\n\n        Raises:\n            ValueError: Raised if the data attribute is not defined.", "docstring_tokens": ["Write", "the", "data", "encoding", "the", "Check", "response", "payload", "to", "a", "stream", "."], "sha": "b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e", "url": "https://github.com/OpenKMIP/PyKMIP/blob/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e/kmip/core/messages/payloads/check.py#L469-L512", "partition": "test"}
{"repo": "iotile/typedargs", "path": "typedargs/annotate.py", "func_name": "annotated", "original_string": "def annotated(func, name=None):\n    \"\"\"Mark a function as callable from the command line.\n\n    This function is meant to be called as decorator.  This function\n    also initializes metadata about the function's arguments that is\n    built up by the param decorator.\n\n    Args:\n        func (callable): The function that we wish to mark as callable\n            from the command line.\n        name (str): Optional string that will override the function's\n            built-in name.\n    \"\"\"\n\n    if hasattr(func, 'metadata'):\n        if name is not None:\n            func.metadata = AnnotatedMetadata(func, name)\n        return func\n\n    func.metadata = AnnotatedMetadata(func, name)\n\n    func.finalizer = False\n    func.takes_cmdline = False\n    func.decorated = False\n    func.context = False\n\n    return func", "language": "python", "code": "def annotated(func, name=None):\n    \"\"\"Mark a function as callable from the command line.\n\n    This function is meant to be called as decorator.  This function\n    also initializes metadata about the function's arguments that is\n    built up by the param decorator.\n\n    Args:\n        func (callable): The function that we wish to mark as callable\n            from the command line.\n        name (str): Optional string that will override the function's\n            built-in name.\n    \"\"\"\n\n    if hasattr(func, 'metadata'):\n        if name is not None:\n            func.metadata = AnnotatedMetadata(func, name)\n        return func\n\n    func.metadata = AnnotatedMetadata(func, name)\n\n    func.finalizer = False\n    func.takes_cmdline = False\n    func.decorated = False\n    func.context = False\n\n    return func", "code_tokens": ["def", "annotated", "(", "func", ",", "name", "=", "None", ")", ":", "if", "hasattr", "(", "func", ",", "'metadata'", ")", ":", "if", "name", "is", "not", "None", ":", "func", ".", "metadata", "=", "AnnotatedMetadata", "(", "func", ",", "name", ")", "return", "func", "func", ".", "metadata", "=", "AnnotatedMetadata", "(", "func", ",", "name", ")", "func", ".", "finalizer", "=", "False", "func", ".", "takes_cmdline", "=", "False", "func", ".", "decorated", "=", "False", "func", ".", "context", "=", "False", "return", "func"], "docstring": "Mark a function as callable from the command line.\n\n    This function is meant to be called as decorator.  This function\n    also initializes metadata about the function's arguments that is\n    built up by the param decorator.\n\n    Args:\n        func (callable): The function that we wish to mark as callable\n            from the command line.\n        name (str): Optional string that will override the function's\n            built-in name.", "docstring_tokens": ["Mark", "a", "function", "as", "callable", "from", "the", "command", "line", "."], "sha": "0a5091a664b9b4d836e091e9ba583e944f438fd8", "url": "https://github.com/iotile/typedargs/blob/0a5091a664b9b4d836e091e9ba583e944f438fd8/typedargs/annotate.py#L250-L276", "partition": "test"}
{"repo": "instagrambot/instabot", "path": "instabot/bot/bot_filter.py", "func_name": "check_not_bot", "original_string": "def check_not_bot(self, user_id):\n    \"\"\" Filter bot from real users. \"\"\"\n    self.small_delay()\n    user_id = self.convert_to_user_id(user_id)\n    if not user_id:\n        return False\n    if user_id in self.whitelist:\n        return True\n    if user_id in self.blacklist:\n        return False\n\n    user_info = self.get_user_info(user_id)\n    if not user_info:\n        return True  # closed acc\n\n    skipped = self.skipped_file\n    if \"following_count\" in user_info and user_info[\"following_count\"] > self.max_following_to_block:\n        msg = 'following_count > bot.max_following_to_block, skipping!'\n        self.console_print(msg, 'red')\n        skipped.append(user_id)\n        return False  # massfollower\n\n    if search_stop_words_in_user(self, user_info):\n        msg = '`bot.search_stop_words_in_user` found in user, skipping!'\n        skipped.append(user_id)\n        return False\n\n    return True", "language": "python", "code": "def check_not_bot(self, user_id):\n    \"\"\" Filter bot from real users. \"\"\"\n    self.small_delay()\n    user_id = self.convert_to_user_id(user_id)\n    if not user_id:\n        return False\n    if user_id in self.whitelist:\n        return True\n    if user_id in self.blacklist:\n        return False\n\n    user_info = self.get_user_info(user_id)\n    if not user_info:\n        return True  # closed acc\n\n    skipped = self.skipped_file\n    if \"following_count\" in user_info and user_info[\"following_count\"] > self.max_following_to_block:\n        msg = 'following_count > bot.max_following_to_block, skipping!'\n        self.console_print(msg, 'red')\n        skipped.append(user_id)\n        return False  # massfollower\n\n    if search_stop_words_in_user(self, user_info):\n        msg = '`bot.search_stop_words_in_user` found in user, skipping!'\n        skipped.append(user_id)\n        return False\n\n    return True", "code_tokens": ["def", "check_not_bot", "(", "self", ",", "user_id", ")", ":", "self", ".", "small_delay", "(", ")", "user_id", "=", "self", ".", "convert_to_user_id", "(", "user_id", ")", "if", "not", "user_id", ":", "return", "False", "if", "user_id", "in", "self", ".", "whitelist", ":", "return", "True", "if", "user_id", "in", "self", ".", "blacklist", ":", "return", "False", "user_info", "=", "self", ".", "get_user_info", "(", "user_id", ")", "if", "not", "user_info", ":", "return", "True", "# closed acc", "skipped", "=", "self", ".", "skipped_file", "if", "\"following_count\"", "in", "user_info", "and", "user_info", "[", "\"following_count\"", "]", ">", "self", ".", "max_following_to_block", ":", "msg", "=", "'following_count > bot.max_following_to_block, skipping!'", "self", ".", "console_print", "(", "msg", ",", "'red'", ")", "skipped", ".", "append", "(", "user_id", ")", "return", "False", "# massfollower", "if", "search_stop_words_in_user", "(", "self", ",", "user_info", ")", ":", "msg", "=", "'`bot.search_stop_words_in_user` found in user, skipping!'", "skipped", ".", "append", "(", "user_id", ")", "return", "False", "return", "True"], "docstring": "Filter bot from real users.", "docstring_tokens": ["Filter", "bot", "from", "real", "users", "."], "sha": "d734f892ac4cc35d22746a4f2680425ffaff0927", "url": "https://github.com/instagrambot/instabot/blob/d734f892ac4cc35d22746a4f2680425ffaff0927/instabot/bot/bot_filter.py#L230-L257", "partition": "test"}
{"repo": "AkihikoITOH/capybara", "path": "capybara/virtualenv/lib/python2.7/site-packages/pip/_vendor/cachecontrol/adapter.py", "func_name": "CacheControlAdapter.build_response", "original_string": "def build_response(self, request, response, from_cache=False):\n        \"\"\"\n        Build a response by making a request or using the cache.\n\n        This will end up calling send and returning a potentially\n        cached response\n        \"\"\"\n        if not from_cache and request.method == 'GET':\n\n            # apply any expiration heuristics\n            if response.status == 304:\n                # We must have sent an ETag request. This could mean\n                # that we've been expired already or that we simply\n                # have an etag. In either case, we want to try and\n                # update the cache if that is the case.\n                cached_response = self.controller.update_cached_response(\n                    request, response\n                )\n\n                if cached_response is not response:\n                    from_cache = True\n\n                # We are done with the server response, read a\n                # possible response body (compliant servers will\n                # not return one, but we cannot be 100% sure) and\n                # release the connection back to the pool.\n                response.read(decode_content=False)\n                response.release_conn()\n\n                response = cached_response\n\n            # We always cache the 301 responses\n            elif response.status == 301:\n                self.controller.cache_response(request, response)\n            else:\n                # Check for any heuristics that might update headers\n                # before trying to cache.\n                if self.heuristic:\n                    response = self.heuristic.apply(response)\n\n                # Wrap the response file with a wrapper that will cache the\n                #   response when the stream has been consumed.\n                response._fp = CallbackFileWrapper(\n                    response._fp,\n                    functools.partial(\n                        self.controller.cache_response,\n                        request,\n                        response,\n                    )\n                )\n\n        resp = super(CacheControlAdapter, self).build_response(\n            request, response\n        )\n\n        # See if we should invalidate the cache.\n        if request.method in self.invalidating_methods and resp.ok:\n            cache_url = self.controller.cache_url(request.url)\n            self.cache.delete(cache_url)\n\n        # Give the request a from_cache attr to let people use it\n        resp.from_cache = from_cache\n\n        return resp", "language": "python", "code": "def build_response(self, request, response, from_cache=False):\n        \"\"\"\n        Build a response by making a request or using the cache.\n\n        This will end up calling send and returning a potentially\n        cached response\n        \"\"\"\n        if not from_cache and request.method == 'GET':\n\n            # apply any expiration heuristics\n            if response.status == 304:\n                # We must have sent an ETag request. This could mean\n                # that we've been expired already or that we simply\n                # have an etag. In either case, we want to try and\n                # update the cache if that is the case.\n                cached_response = self.controller.update_cached_response(\n                    request, response\n                )\n\n                if cached_response is not response:\n                    from_cache = True\n\n                # We are done with the server response, read a\n                # possible response body (compliant servers will\n                # not return one, but we cannot be 100% sure) and\n                # release the connection back to the pool.\n                response.read(decode_content=False)\n                response.release_conn()\n\n                response = cached_response\n\n            # We always cache the 301 responses\n            elif response.status == 301:\n                self.controller.cache_response(request, response)\n            else:\n                # Check for any heuristics that might update headers\n                # before trying to cache.\n                if self.heuristic:\n                    response = self.heuristic.apply(response)\n\n                # Wrap the response file with a wrapper that will cache the\n                #   response when the stream has been consumed.\n                response._fp = CallbackFileWrapper(\n                    response._fp,\n                    functools.partial(\n                        self.controller.cache_response,\n                        request,\n                        response,\n                    )\n                )\n\n        resp = super(CacheControlAdapter, self).build_response(\n            request, response\n        )\n\n        # See if we should invalidate the cache.\n        if request.method in self.invalidating_methods and resp.ok:\n            cache_url = self.controller.cache_url(request.url)\n            self.cache.delete(cache_url)\n\n        # Give the request a from_cache attr to let people use it\n        resp.from_cache = from_cache\n\n        return resp", "code_tokens": ["def", "build_response", "(", "self", ",", "request", ",", "response", ",", "from_cache", "=", "False", ")", ":", "if", "not", "from_cache", "and", "request", ".", "method", "==", "'GET'", ":", "# apply any expiration heuristics", "if", "response", ".", "status", "==", "304", ":", "# We must have sent an ETag request. This could mean", "# that we've been expired already or that we simply", "# have an etag. In either case, we want to try and", "# update the cache if that is the case.", "cached_response", "=", "self", ".", "controller", ".", "update_cached_response", "(", "request", ",", "response", ")", "if", "cached_response", "is", "not", "response", ":", "from_cache", "=", "True", "# We are done with the server response, read a", "# possible response body (compliant servers will", "# not return one, but we cannot be 100% sure) and", "# release the connection back to the pool.", "response", ".", "read", "(", "decode_content", "=", "False", ")", "response", ".", "release_conn", "(", ")", "response", "=", "cached_response", "# We always cache the 301 responses", "elif", "response", ".", "status", "==", "301", ":", "self", ".", "controller", ".", "cache_response", "(", "request", ",", "response", ")", "else", ":", "# Check for any heuristics that might update headers", "# before trying to cache.", "if", "self", ".", "heuristic", ":", "response", "=", "self", ".", "heuristic", ".", "apply", "(", "response", ")", "# Wrap the response file with a wrapper that will cache the", "#   response when the stream has been consumed.", "response", ".", "_fp", "=", "CallbackFileWrapper", "(", "response", ".", "_fp", ",", "functools", ".", "partial", "(", "self", ".", "controller", ".", "cache_response", ",", "request", ",", "response", ",", ")", ")", "resp", "=", "super", "(", "CacheControlAdapter", ",", "self", ")", ".", "build_response", "(", "request", ",", "response", ")", "# See if we should invalidate the cache.", "if", "request", ".", "method", "in", "self", ".", "invalidating_methods", "and", "resp", ".", "ok", ":", "cache_url", "=", "self", ".", "controller", ".", "cache_url", "(", "request", ".", "url", ")", "self", ".", "cache", ".", "delete", "(", "cache_url", ")", "# Give the request a from_cache attr to let people use it", "resp", ".", "from_cache", "=", "from_cache", "return", "resp"], "docstring": "Build a response by making a request or using the cache.\n\n        This will end up calling send and returning a potentially\n        cached response", "docstring_tokens": ["Build", "a", "response", "by", "making", "a", "request", "or", "using", "the", "cache", "."], "sha": "e86c2173ea386654f4ae061148e8fbe3f25e715c", "url": "https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/pip/_vendor/cachecontrol/adapter.py#L50-L113", "partition": "test"}
{"repo": "rollbar/pyrollbar", "path": "rollbar/__init__.py", "func_name": "_build_server_data", "original_string": "def _build_server_data():\n    \"\"\"\n    Returns a dictionary containing information about the server environment.\n    \"\"\"\n    # server environment\n    server_data = {\n        'host': socket.gethostname(),\n        'pid': os.getpid()\n    }\n\n    # argv does not always exist in embedded python environments\n    argv = getattr(sys, 'argv', None)\n    if argv:\n         server_data['argv'] = argv\n\n    for key in ['branch', 'root']:\n        if SETTINGS.get(key):\n            server_data[key] = SETTINGS[key]\n\n    return server_data", "language": "python", "code": "def _build_server_data():\n    \"\"\"\n    Returns a dictionary containing information about the server environment.\n    \"\"\"\n    # server environment\n    server_data = {\n        'host': socket.gethostname(),\n        'pid': os.getpid()\n    }\n\n    # argv does not always exist in embedded python environments\n    argv = getattr(sys, 'argv', None)\n    if argv:\n         server_data['argv'] = argv\n\n    for key in ['branch', 'root']:\n        if SETTINGS.get(key):\n            server_data[key] = SETTINGS[key]\n\n    return server_data", "code_tokens": ["def", "_build_server_data", "(", ")", ":", "# server environment", "server_data", "=", "{", "'host'", ":", "socket", ".", "gethostname", "(", ")", ",", "'pid'", ":", "os", ".", "getpid", "(", ")", "}", "# argv does not always exist in embedded python environments", "argv", "=", "getattr", "(", "sys", ",", "'argv'", ",", "None", ")", "if", "argv", ":", "server_data", "[", "'argv'", "]", "=", "argv", "for", "key", "in", "[", "'branch'", ",", "'root'", "]", ":", "if", "SETTINGS", ".", "get", "(", "key", ")", ":", "server_data", "[", "key", "]", "=", "SETTINGS", "[", "key", "]", "return", "server_data"], "docstring": "Returns a dictionary containing information about the server environment.", "docstring_tokens": ["Returns", "a", "dictionary", "containing", "information", "about", "the", "server", "environment", "."], "sha": "33ef2e723a33d09dd6302f978f4a3908be95b9d2", "url": "https://github.com/rollbar/pyrollbar/blob/33ef2e723a33d09dd6302f978f4a3908be95b9d2/rollbar/__init__.py#L1299-L1318", "partition": "test"}
{"repo": "librosa/librosa", "path": "librosa/feature/spectral.py", "func_name": "chroma_stft", "original_string": "def chroma_stft(y=None, sr=22050, S=None, norm=np.inf, n_fft=2048,\n                hop_length=512, win_length=None, window='hann', center=True,\n                pad_mode='reflect', tuning=None, **kwargs):\n    \"\"\"Compute a chromagram from a waveform or power spectrogram.\n\n    This implementation is derived from `chromagram_E` [1]_\n\n    .. [1] Ellis, Daniel P.W.  \"Chroma feature analysis and synthesis\"\n           2007/04/21\n           http://labrosa.ee.columbia.edu/matlab/chroma-ansyn/\n\n    Parameters\n    ----------\n    y : np.ndarray [shape=(n,)] or None\n        audio time series\n\n    sr : number > 0 [scalar]\n        sampling rate of `y`\n\n    S : np.ndarray [shape=(d, t)] or None\n        power spectrogram\n\n    norm : float or None\n        Column-wise normalization.\n        See `librosa.util.normalize` for details.\n\n        If `None`, no normalization is performed.\n\n    n_fft : int  > 0 [scalar]\n        FFT window size if provided `y, sr` instead of `S`\n\n    hop_length : int > 0 [scalar]\n        hop length if provided `y, sr` instead of `S`\n\n    win_length : int <= n_fft [scalar]\n        Each frame of audio is windowed by `window()`.\n        The window will be of length `win_length` and then padded\n        with zeros to match `n_fft`.\n\n        If unspecified, defaults to ``win_length = n_fft``.\n\n    window : string, tuple, number, function, or np.ndarray [shape=(n_fft,)]\n        - a window specification (string, tuple, or number);\n          see `scipy.signal.get_window`\n        - a window function, such as `scipy.signal.hanning`\n        - a vector or array of length `n_fft`\n\n        .. see also:: `filters.get_window`\n\n    center : boolean\n        - If `True`, the signal `y` is padded so that frame\n          `t` is centered at `y[t * hop_length]`.\n        - If `False`, then frame `t` begins at `y[t * hop_length]`\n\n    pad_mode : string\n        If `center=True`, the padding mode to use at the edges of the signal.\n        By default, STFT uses reflection padding.\n\n\n    tuning : float in `[-0.5, 0.5)` [scalar] or None.\n        Deviation from A440 tuning in fractional bins (cents).\n        If `None`, it is automatically estimated.\n\n    kwargs : additional keyword arguments\n        Arguments to parameterize chroma filters.\n        See `librosa.filters.chroma` for details.\n\n    Returns\n    -------\n    chromagram : np.ndarray [shape=(n_chroma, t)]\n        Normalized energy for each chroma bin at each frame.\n\n    See Also\n    --------\n    librosa.filters.chroma\n        Chroma filter bank construction\n    librosa.util.normalize\n        Vector normalization\n\n    Examples\n    --------\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> librosa.feature.chroma_stft(y=y, sr=sr)\n    array([[ 0.974,  0.881, ...,  0.925,  1.   ],\n           [ 1.   ,  0.841, ...,  0.882,  0.878],\n           ...,\n           [ 0.658,  0.985, ...,  0.878,  0.764],\n           [ 0.969,  0.92 , ...,  0.974,  0.915]])\n\n    Use an energy (magnitude) spectrum instead of power spectrogram\n\n    >>> S = np.abs(librosa.stft(y))\n    >>> chroma = librosa.feature.chroma_stft(S=S, sr=sr)\n    >>> chroma\n    array([[ 0.884,  0.91 , ...,  0.861,  0.858],\n           [ 0.963,  0.785, ...,  0.968,  0.896],\n           ...,\n           [ 0.871,  1.   , ...,  0.928,  0.829],\n           [ 1.   ,  0.982, ...,  0.93 ,  0.878]])\n\n    Use a pre-computed power spectrogram with a larger frame\n\n    >>> S = np.abs(librosa.stft(y, n_fft=4096))**2\n    >>> chroma = librosa.feature.chroma_stft(S=S, sr=sr)\n    >>> chroma\n    array([[ 0.685,  0.477, ...,  0.961,  0.986],\n           [ 0.674,  0.452, ...,  0.952,  0.926],\n           ...,\n           [ 0.844,  0.575, ...,  0.934,  0.869],\n           [ 0.793,  0.663, ...,  0.964,  0.972]])\n\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure(figsize=(10, 4))\n    >>> librosa.display.specshow(chroma, y_axis='chroma', x_axis='time')\n    >>> plt.colorbar()\n    >>> plt.title('Chromagram')\n    >>> plt.tight_layout()\n\n    \"\"\"\n\n    S, n_fft = _spectrogram(y=y, S=S, n_fft=n_fft, hop_length=hop_length, power=2,\n                            win_length=win_length, window=window, center=center,\n                            pad_mode=pad_mode)\n\n    n_chroma = kwargs.get('n_chroma', 12)\n\n    if tuning is None:\n        tuning = estimate_tuning(S=S, sr=sr, bins_per_octave=n_chroma)\n\n    # Get the filter bank\n    if 'A440' not in kwargs:\n        kwargs['A440'] = 440.0 * 2.0**(float(tuning) / n_chroma)\n\n    chromafb = filters.chroma(sr, n_fft, **kwargs)\n\n    # Compute raw chroma\n    raw_chroma = np.dot(chromafb, S)\n\n    # Compute normalization factor for each frame\n    return util.normalize(raw_chroma, norm=norm, axis=0)", "language": "python", "code": "def chroma_stft(y=None, sr=22050, S=None, norm=np.inf, n_fft=2048,\n                hop_length=512, win_length=None, window='hann', center=True,\n                pad_mode='reflect', tuning=None, **kwargs):\n    \"\"\"Compute a chromagram from a waveform or power spectrogram.\n\n    This implementation is derived from `chromagram_E` [1]_\n\n    .. [1] Ellis, Daniel P.W.  \"Chroma feature analysis and synthesis\"\n           2007/04/21\n           http://labrosa.ee.columbia.edu/matlab/chroma-ansyn/\n\n    Parameters\n    ----------\n    y : np.ndarray [shape=(n,)] or None\n        audio time series\n\n    sr : number > 0 [scalar]\n        sampling rate of `y`\n\n    S : np.ndarray [shape=(d, t)] or None\n        power spectrogram\n\n    norm : float or None\n        Column-wise normalization.\n        See `librosa.util.normalize` for details.\n\n        If `None`, no normalization is performed.\n\n    n_fft : int  > 0 [scalar]\n        FFT window size if provided `y, sr` instead of `S`\n\n    hop_length : int > 0 [scalar]\n        hop length if provided `y, sr` instead of `S`\n\n    win_length : int <= n_fft [scalar]\n        Each frame of audio is windowed by `window()`.\n        The window will be of length `win_length` and then padded\n        with zeros to match `n_fft`.\n\n        If unspecified, defaults to ``win_length = n_fft``.\n\n    window : string, tuple, number, function, or np.ndarray [shape=(n_fft,)]\n        - a window specification (string, tuple, or number);\n          see `scipy.signal.get_window`\n        - a window function, such as `scipy.signal.hanning`\n        - a vector or array of length `n_fft`\n\n        .. see also:: `filters.get_window`\n\n    center : boolean\n        - If `True`, the signal `y` is padded so that frame\n          `t` is centered at `y[t * hop_length]`.\n        - If `False`, then frame `t` begins at `y[t * hop_length]`\n\n    pad_mode : string\n        If `center=True`, the padding mode to use at the edges of the signal.\n        By default, STFT uses reflection padding.\n\n\n    tuning : float in `[-0.5, 0.5)` [scalar] or None.\n        Deviation from A440 tuning in fractional bins (cents).\n        If `None`, it is automatically estimated.\n\n    kwargs : additional keyword arguments\n        Arguments to parameterize chroma filters.\n        See `librosa.filters.chroma` for details.\n\n    Returns\n    -------\n    chromagram : np.ndarray [shape=(n_chroma, t)]\n        Normalized energy for each chroma bin at each frame.\n\n    See Also\n    --------\n    librosa.filters.chroma\n        Chroma filter bank construction\n    librosa.util.normalize\n        Vector normalization\n\n    Examples\n    --------\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> librosa.feature.chroma_stft(y=y, sr=sr)\n    array([[ 0.974,  0.881, ...,  0.925,  1.   ],\n           [ 1.   ,  0.841, ...,  0.882,  0.878],\n           ...,\n           [ 0.658,  0.985, ...,  0.878,  0.764],\n           [ 0.969,  0.92 , ...,  0.974,  0.915]])\n\n    Use an energy (magnitude) spectrum instead of power spectrogram\n\n    >>> S = np.abs(librosa.stft(y))\n    >>> chroma = librosa.feature.chroma_stft(S=S, sr=sr)\n    >>> chroma\n    array([[ 0.884,  0.91 , ...,  0.861,  0.858],\n           [ 0.963,  0.785, ...,  0.968,  0.896],\n           ...,\n           [ 0.871,  1.   , ...,  0.928,  0.829],\n           [ 1.   ,  0.982, ...,  0.93 ,  0.878]])\n\n    Use a pre-computed power spectrogram with a larger frame\n\n    >>> S = np.abs(librosa.stft(y, n_fft=4096))**2\n    >>> chroma = librosa.feature.chroma_stft(S=S, sr=sr)\n    >>> chroma\n    array([[ 0.685,  0.477, ...,  0.961,  0.986],\n           [ 0.674,  0.452, ...,  0.952,  0.926],\n           ...,\n           [ 0.844,  0.575, ...,  0.934,  0.869],\n           [ 0.793,  0.663, ...,  0.964,  0.972]])\n\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure(figsize=(10, 4))\n    >>> librosa.display.specshow(chroma, y_axis='chroma', x_axis='time')\n    >>> plt.colorbar()\n    >>> plt.title('Chromagram')\n    >>> plt.tight_layout()\n\n    \"\"\"\n\n    S, n_fft = _spectrogram(y=y, S=S, n_fft=n_fft, hop_length=hop_length, power=2,\n                            win_length=win_length, window=window, center=center,\n                            pad_mode=pad_mode)\n\n    n_chroma = kwargs.get('n_chroma', 12)\n\n    if tuning is None:\n        tuning = estimate_tuning(S=S, sr=sr, bins_per_octave=n_chroma)\n\n    # Get the filter bank\n    if 'A440' not in kwargs:\n        kwargs['A440'] = 440.0 * 2.0**(float(tuning) / n_chroma)\n\n    chromafb = filters.chroma(sr, n_fft, **kwargs)\n\n    # Compute raw chroma\n    raw_chroma = np.dot(chromafb, S)\n\n    # Compute normalization factor for each frame\n    return util.normalize(raw_chroma, norm=norm, axis=0)", "code_tokens": ["def", "chroma_stft", "(", "y", "=", "None", ",", "sr", "=", "22050", ",", "S", "=", "None", ",", "norm", "=", "np", ".", "inf", ",", "n_fft", "=", "2048", ",", "hop_length", "=", "512", ",", "win_length", "=", "None", ",", "window", "=", "'hann'", ",", "center", "=", "True", ",", "pad_mode", "=", "'reflect'", ",", "tuning", "=", "None", ",", "*", "*", "kwargs", ")", ":", "S", ",", "n_fft", "=", "_spectrogram", "(", "y", "=", "y", ",", "S", "=", "S", ",", "n_fft", "=", "n_fft", ",", "hop_length", "=", "hop_length", ",", "power", "=", "2", ",", "win_length", "=", "win_length", ",", "window", "=", "window", ",", "center", "=", "center", ",", "pad_mode", "=", "pad_mode", ")", "n_chroma", "=", "kwargs", ".", "get", "(", "'n_chroma'", ",", "12", ")", "if", "tuning", "is", "None", ":", "tuning", "=", "estimate_tuning", "(", "S", "=", "S", ",", "sr", "=", "sr", ",", "bins_per_octave", "=", "n_chroma", ")", "# Get the filter bank", "if", "'A440'", "not", "in", "kwargs", ":", "kwargs", "[", "'A440'", "]", "=", "440.0", "*", "2.0", "**", "(", "float", "(", "tuning", ")", "/", "n_chroma", ")", "chromafb", "=", "filters", ".", "chroma", "(", "sr", ",", "n_fft", ",", "*", "*", "kwargs", ")", "# Compute raw chroma", "raw_chroma", "=", "np", ".", "dot", "(", "chromafb", ",", "S", ")", "# Compute normalization factor for each frame", "return", "util", ".", "normalize", "(", "raw_chroma", ",", "norm", "=", "norm", ",", "axis", "=", "0", ")"], "docstring": "Compute a chromagram from a waveform or power spectrogram.\n\n    This implementation is derived from `chromagram_E` [1]_\n\n    .. [1] Ellis, Daniel P.W.  \"Chroma feature analysis and synthesis\"\n           2007/04/21\n           http://labrosa.ee.columbia.edu/matlab/chroma-ansyn/\n\n    Parameters\n    ----------\n    y : np.ndarray [shape=(n,)] or None\n        audio time series\n\n    sr : number > 0 [scalar]\n        sampling rate of `y`\n\n    S : np.ndarray [shape=(d, t)] or None\n        power spectrogram\n\n    norm : float or None\n        Column-wise normalization.\n        See `librosa.util.normalize` for details.\n\n        If `None`, no normalization is performed.\n\n    n_fft : int  > 0 [scalar]\n        FFT window size if provided `y, sr` instead of `S`\n\n    hop_length : int > 0 [scalar]\n        hop length if provided `y, sr` instead of `S`\n\n    win_length : int <= n_fft [scalar]\n        Each frame of audio is windowed by `window()`.\n        The window will be of length `win_length` and then padded\n        with zeros to match `n_fft`.\n\n        If unspecified, defaults to ``win_length = n_fft``.\n\n    window : string, tuple, number, function, or np.ndarray [shape=(n_fft,)]\n        - a window specification (string, tuple, or number);\n          see `scipy.signal.get_window`\n        - a window function, such as `scipy.signal.hanning`\n        - a vector or array of length `n_fft`\n\n        .. see also:: `filters.get_window`\n\n    center : boolean\n        - If `True`, the signal `y` is padded so that frame\n          `t` is centered at `y[t * hop_length]`.\n        - If `False`, then frame `t` begins at `y[t * hop_length]`\n\n    pad_mode : string\n        If `center=True`, the padding mode to use at the edges of the signal.\n        By default, STFT uses reflection padding.\n\n\n    tuning : float in `[-0.5, 0.5)` [scalar] or None.\n        Deviation from A440 tuning in fractional bins (cents).\n        If `None`, it is automatically estimated.\n\n    kwargs : additional keyword arguments\n        Arguments to parameterize chroma filters.\n        See `librosa.filters.chroma` for details.\n\n    Returns\n    -------\n    chromagram : np.ndarray [shape=(n_chroma, t)]\n        Normalized energy for each chroma bin at each frame.\n\n    See Also\n    --------\n    librosa.filters.chroma\n        Chroma filter bank construction\n    librosa.util.normalize\n        Vector normalization\n\n    Examples\n    --------\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> librosa.feature.chroma_stft(y=y, sr=sr)\n    array([[ 0.974,  0.881, ...,  0.925,  1.   ],\n           [ 1.   ,  0.841, ...,  0.882,  0.878],\n           ...,\n           [ 0.658,  0.985, ...,  0.878,  0.764],\n           [ 0.969,  0.92 , ...,  0.974,  0.915]])\n\n    Use an energy (magnitude) spectrum instead of power spectrogram\n\n    >>> S = np.abs(librosa.stft(y))\n    >>> chroma = librosa.feature.chroma_stft(S=S, sr=sr)\n    >>> chroma\n    array([[ 0.884,  0.91 , ...,  0.861,  0.858],\n           [ 0.963,  0.785, ...,  0.968,  0.896],\n           ...,\n           [ 0.871,  1.   , ...,  0.928,  0.829],\n           [ 1.   ,  0.982, ...,  0.93 ,  0.878]])\n\n    Use a pre-computed power spectrogram with a larger frame\n\n    >>> S = np.abs(librosa.stft(y, n_fft=4096))**2\n    >>> chroma = librosa.feature.chroma_stft(S=S, sr=sr)\n    >>> chroma\n    array([[ 0.685,  0.477, ...,  0.961,  0.986],\n           [ 0.674,  0.452, ...,  0.952,  0.926],\n           ...,\n           [ 0.844,  0.575, ...,  0.934,  0.869],\n           [ 0.793,  0.663, ...,  0.964,  0.972]])\n\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure(figsize=(10, 4))\n    >>> librosa.display.specshow(chroma, y_axis='chroma', x_axis='time')\n    >>> plt.colorbar()\n    >>> plt.title('Chromagram')\n    >>> plt.tight_layout()", "docstring_tokens": ["Compute", "a", "chromagram", "from", "a", "waveform", "or", "power", "spectrogram", "."], "sha": "180e8e6eb8f958fa6b20b8cba389f7945d508247", "url": "https://github.com/librosa/librosa/blob/180e8e6eb8f958fa6b20b8cba389f7945d508247/librosa/feature/spectral.py#L1023-L1162", "partition": "test"}
{"repo": "apache/airflow", "path": "airflow/contrib/hooks/azure_cosmos_hook.py", "func_name": "AzureCosmosDBHook.get_documents", "original_string": "def get_documents(self, sql_string, database_name=None, collection_name=None, partition_key=None):\n        \"\"\"\n        Get a list of documents from an existing collection in the CosmosDB database via SQL query.\n        \"\"\"\n        if sql_string is None:\n            raise AirflowBadRequest(\"SQL query string cannot be None\")\n\n        # Query them in SQL\n        query = {'query': sql_string}\n\n        try:\n            result_iterable = self.get_conn().QueryItems(\n                get_collection_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name)),\n                query,\n                partition_key)\n\n            return list(result_iterable)\n        except HTTPFailure:\n            return None", "language": "python", "code": "def get_documents(self, sql_string, database_name=None, collection_name=None, partition_key=None):\n        \"\"\"\n        Get a list of documents from an existing collection in the CosmosDB database via SQL query.\n        \"\"\"\n        if sql_string is None:\n            raise AirflowBadRequest(\"SQL query string cannot be None\")\n\n        # Query them in SQL\n        query = {'query': sql_string}\n\n        try:\n            result_iterable = self.get_conn().QueryItems(\n                get_collection_link(\n                    self.__get_database_name(database_name),\n                    self.__get_collection_name(collection_name)),\n                query,\n                partition_key)\n\n            return list(result_iterable)\n        except HTTPFailure:\n            return None", "code_tokens": ["def", "get_documents", "(", "self", ",", "sql_string", ",", "database_name", "=", "None", ",", "collection_name", "=", "None", ",", "partition_key", "=", "None", ")", ":", "if", "sql_string", "is", "None", ":", "raise", "AirflowBadRequest", "(", "\"SQL query string cannot be None\"", ")", "# Query them in SQL", "query", "=", "{", "'query'", ":", "sql_string", "}", "try", ":", "result_iterable", "=", "self", ".", "get_conn", "(", ")", ".", "QueryItems", "(", "get_collection_link", "(", "self", ".", "__get_database_name", "(", "database_name", ")", ",", "self", ".", "__get_collection_name", "(", "collection_name", ")", ")", ",", "query", ",", "partition_key", ")", "return", "list", "(", "result_iterable", ")", "except", "HTTPFailure", ":", "return", "None"], "docstring": "Get a list of documents from an existing collection in the CosmosDB database via SQL query.", "docstring_tokens": ["Get", "a", "list", "of", "documents", "from", "an", "existing", "collection", "in", "the", "CosmosDB", "database", "via", "SQL", "query", "."], "sha": "b69c686ad8a0c89b9136bb4b31767257eb7b2597", "url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L255-L275", "partition": "test"}
{"repo": "3DLIRIOUS/MeshLabXML", "path": "meshlabxml/mlx.py", "func_name": "begin", "original_string": "def begin(script='TEMP3D_default.mlx', file_in=None, mlp_in=None):\n    \"\"\"Create new mlx script and write opening tags.\n\n    Performs special processing on stl files.\n\n    If no input files are provided this will create a dummy\n    file and delete it as the first filter. This works around\n    the meshlab limitation that it must be provided an input\n    file, even if you will be creating a mesh as the first\n    filter.\n\n    \"\"\"\n    script_file = open(script, 'w')\n    script_file.write(''.join(['<!DOCTYPE FilterScript>\\n',\n                               '<FilterScript>\\n']))\n    script_file.close()\n\n    current_layer = -1\n    last_layer = -1\n    stl = False\n\n    # Process project files first\n    if mlp_in is not None:\n        # make a list if it isn't already\n        if not isinstance(mlp_in, list):\n            mlp_in = [mlp_in]\n        for val in mlp_in:\n            tree = ET.parse(val)\n            #root = tree.getroot()\n            for elem in tree.iter(tag='MLMesh'):\n                filename = (elem.attrib['filename'])\n                current_layer += 1\n                last_layer += 1\n                # If the mesh file extension is stl, change to that layer and\n                # run clean.merge_vert\n                if os.path.splitext(filename)[1][1:].strip().lower() == 'stl':\n                    layers.change(script, current_layer)\n                    clean.merge_vert(script)\n                    stl = True\n\n    # Process separate input files next\n    if file_in is not None:\n        # make a list if it isn't already\n        if not isinstance(file_in, list):\n            file_in = [file_in]\n        for val in file_in:\n            current_layer += 1\n            last_layer += 1\n            # If the mesh file extension is stl, change to that layer and\n            # run clean.merge_vert\n            if os.path.splitext(val)[1][1:].strip().lower() == 'stl':\n                layers.change(script, current_layer)\n                clean.merge_vert(script)\n                stl = True\n\n    # If some input files were stl, we need to change back to the last layer\n    if stl:\n        layers.change(script, last_layer)  # Change back to the last layer\n    elif last_layer == -1:\n        # If no input files are provided, create a dummy file\n        # with a single vertex and delete it first in the script.\n        # This works around the fact that meshlabserver will\n        # not run without an input file.\n        file_in = ['TEMP3D.xyz']\n        file_in_descriptor = open(file_in[0], 'w')\n        file_in_descriptor.write('0 0 0')\n        file_in_descriptor.close()\n        layers.delete(script)\n    return current_layer, last_layer", "language": "python", "code": "def begin(script='TEMP3D_default.mlx', file_in=None, mlp_in=None):\n    \"\"\"Create new mlx script and write opening tags.\n\n    Performs special processing on stl files.\n\n    If no input files are provided this will create a dummy\n    file and delete it as the first filter. This works around\n    the meshlab limitation that it must be provided an input\n    file, even if you will be creating a mesh as the first\n    filter.\n\n    \"\"\"\n    script_file = open(script, 'w')\n    script_file.write(''.join(['<!DOCTYPE FilterScript>\\n',\n                               '<FilterScript>\\n']))\n    script_file.close()\n\n    current_layer = -1\n    last_layer = -1\n    stl = False\n\n    # Process project files first\n    if mlp_in is not None:\n        # make a list if it isn't already\n        if not isinstance(mlp_in, list):\n            mlp_in = [mlp_in]\n        for val in mlp_in:\n            tree = ET.parse(val)\n            #root = tree.getroot()\n            for elem in tree.iter(tag='MLMesh'):\n                filename = (elem.attrib['filename'])\n                current_layer += 1\n                last_layer += 1\n                # If the mesh file extension is stl, change to that layer and\n                # run clean.merge_vert\n                if os.path.splitext(filename)[1][1:].strip().lower() == 'stl':\n                    layers.change(script, current_layer)\n                    clean.merge_vert(script)\n                    stl = True\n\n    # Process separate input files next\n    if file_in is not None:\n        # make a list if it isn't already\n        if not isinstance(file_in, list):\n            file_in = [file_in]\n        for val in file_in:\n            current_layer += 1\n            last_layer += 1\n            # If the mesh file extension is stl, change to that layer and\n            # run clean.merge_vert\n            if os.path.splitext(val)[1][1:].strip().lower() == 'stl':\n                layers.change(script, current_layer)\n                clean.merge_vert(script)\n                stl = True\n\n    # If some input files were stl, we need to change back to the last layer\n    if stl:\n        layers.change(script, last_layer)  # Change back to the last layer\n    elif last_layer == -1:\n        # If no input files are provided, create a dummy file\n        # with a single vertex and delete it first in the script.\n        # This works around the fact that meshlabserver will\n        # not run without an input file.\n        file_in = ['TEMP3D.xyz']\n        file_in_descriptor = open(file_in[0], 'w')\n        file_in_descriptor.write('0 0 0')\n        file_in_descriptor.close()\n        layers.delete(script)\n    return current_layer, last_layer", "code_tokens": ["def", "begin", "(", "script", "=", "'TEMP3D_default.mlx'", ",", "file_in", "=", "None", ",", "mlp_in", "=", "None", ")", ":", "script_file", "=", "open", "(", "script", ",", "'w'", ")", "script_file", ".", "write", "(", "''", ".", "join", "(", "[", "'<!DOCTYPE FilterScript>\\n'", ",", "'<FilterScript>\\n'", "]", ")", ")", "script_file", ".", "close", "(", ")", "current_layer", "=", "-", "1", "last_layer", "=", "-", "1", "stl", "=", "False", "# Process project files first", "if", "mlp_in", "is", "not", "None", ":", "# make a list if it isn't already", "if", "not", "isinstance", "(", "mlp_in", ",", "list", ")", ":", "mlp_in", "=", "[", "mlp_in", "]", "for", "val", "in", "mlp_in", ":", "tree", "=", "ET", ".", "parse", "(", "val", ")", "#root = tree.getroot()", "for", "elem", "in", "tree", ".", "iter", "(", "tag", "=", "'MLMesh'", ")", ":", "filename", "=", "(", "elem", ".", "attrib", "[", "'filename'", "]", ")", "current_layer", "+=", "1", "last_layer", "+=", "1", "# If the mesh file extension is stl, change to that layer and", "# run clean.merge_vert", "if", "os", ".", "path", ".", "splitext", "(", "filename", ")", "[", "1", "]", "[", "1", ":", "]", ".", "strip", "(", ")", ".", "lower", "(", ")", "==", "'stl'", ":", "layers", ".", "change", "(", "script", ",", "current_layer", ")", "clean", ".", "merge_vert", "(", "script", ")", "stl", "=", "True", "# Process separate input files next", "if", "file_in", "is", "not", "None", ":", "# make a list if it isn't already", "if", "not", "isinstance", "(", "file_in", ",", "list", ")", ":", "file_in", "=", "[", "file_in", "]", "for", "val", "in", "file_in", ":", "current_layer", "+=", "1", "last_layer", "+=", "1", "# If the mesh file extension is stl, change to that layer and", "# run clean.merge_vert", "if", "os", ".", "path", ".", "splitext", "(", "val", ")", "[", "1", "]", "[", "1", ":", "]", ".", "strip", "(", ")", ".", "lower", "(", ")", "==", "'stl'", ":", "layers", ".", "change", "(", "script", ",", "current_layer", ")", "clean", ".", "merge_vert", "(", "script", ")", "stl", "=", "True", "# If some input files were stl, we need to change back to the last layer", "if", "stl", ":", "layers", ".", "change", "(", "script", ",", "last_layer", ")", "# Change back to the last layer", "elif", "last_layer", "==", "-", "1", ":", "# If no input files are provided, create a dummy file", "# with a single vertex and delete it first in the script.", "# This works around the fact that meshlabserver will", "# not run without an input file.", "file_in", "=", "[", "'TEMP3D.xyz'", "]", "file_in_descriptor", "=", "open", "(", "file_in", "[", "0", "]", ",", "'w'", ")", "file_in_descriptor", ".", "write", "(", "'0 0 0'", ")", "file_in_descriptor", ".", "close", "(", ")", "layers", ".", "delete", "(", "script", ")", "return", "current_layer", ",", "last_layer"], "docstring": "Create new mlx script and write opening tags.\n\n    Performs special processing on stl files.\n\n    If no input files are provided this will create a dummy\n    file and delete it as the first filter. This works around\n    the meshlab limitation that it must be provided an input\n    file, even if you will be creating a mesh as the first\n    filter.", "docstring_tokens": ["Create", "new", "mlx", "script", "and", "write", "opening", "tags", "."], "sha": "177cce21e92baca500f56a932d66bd9a33257af8", "url": "https://github.com/3DLIRIOUS/MeshLabXML/blob/177cce21e92baca500f56a932d66bd9a33257af8/meshlabxml/mlx.py#L623-L691", "partition": "test"}
{"repo": "chaoss/grimoirelab-perceval-mozilla", "path": "perceval/backends/mozilla/crates.py", "func_name": "CratesClient.crates", "original_string": "def crates(self, from_page=1):\n        \"\"\"Get crates in alphabetical order\"\"\"\n\n        path = urijoin(CRATES_API_URL, CATEGORY_CRATES)\n        raw_crates = self.__fetch_items(path, from_page)\n\n        return raw_crates", "language": "python", "code": "def crates(self, from_page=1):\n        \"\"\"Get crates in alphabetical order\"\"\"\n\n        path = urijoin(CRATES_API_URL, CATEGORY_CRATES)\n        raw_crates = self.__fetch_items(path, from_page)\n\n        return raw_crates", "code_tokens": ["def", "crates", "(", "self", ",", "from_page", "=", "1", ")", ":", "path", "=", "urijoin", "(", "CRATES_API_URL", ",", "CATEGORY_CRATES", ")", "raw_crates", "=", "self", ".", "__fetch_items", "(", "path", ",", "from_page", ")", "return", "raw_crates"], "docstring": "Get crates in alphabetical order", "docstring_tokens": ["Get", "crates", "in", "alphabetical", "order"], "sha": "4514f8d3d609d3cb79d83c72d51fcc4b4a7daeb4", "url": "https://github.com/chaoss/grimoirelab-perceval-mozilla/blob/4514f8d3d609d3cb79d83c72d51fcc4b4a7daeb4/perceval/backends/mozilla/crates.py#L275-L281", "partition": "test"}
{"repo": "PyCQA/pylint", "path": "pylint/checkers/format.py", "func_name": "FormatChecker.check_lines", "original_string": "def check_lines(self, lines, i):\n        \"\"\"check lines have less than a maximum number of characters\n        \"\"\"\n        max_chars = self.config.max_line_length\n        ignore_long_line = self.config.ignore_long_lines\n\n        def check_line(line, i):\n            if not line.endswith(\"\\n\"):\n                self.add_message(\"missing-final-newline\", line=i)\n            else:\n                # exclude \\f (formfeed) from the rstrip\n                stripped_line = line.rstrip(\"\\t\\n\\r\\v \")\n                if not stripped_line and _EMPTY_LINE in self.config.no_space_check:\n                    # allow empty lines\n                    pass\n                elif line[len(stripped_line) :] not in (\"\\n\", \"\\r\\n\"):\n                    self.add_message(\n                        \"trailing-whitespace\", line=i, col_offset=len(stripped_line)\n                    )\n                # Don't count excess whitespace in the line length.\n                line = stripped_line\n            mobj = OPTION_RGX.search(line)\n            if mobj and \"=\" in line:\n                front_of_equal, _, back_of_equal = mobj.group(1).partition(\"=\")\n                if front_of_equal.strip() == \"disable\":\n                    if \"line-too-long\" in {\n                        _msg_id.strip() for _msg_id in back_of_equal.split(\",\")\n                    }:\n                        return None\n                    line = line.rsplit(\"#\", 1)[0].rstrip()\n\n            if len(line) > max_chars and not ignore_long_line.search(line):\n                self.add_message(\"line-too-long\", line=i, args=(len(line), max_chars))\n            return i + 1\n\n        unsplit_ends = {\n            \"\\v\",\n            \"\\x0b\",\n            \"\\f\",\n            \"\\x0c\",\n            \"\\x1c\",\n            \"\\x1d\",\n            \"\\x1e\",\n            \"\\x85\",\n            \"\\u2028\",\n            \"\\u2029\",\n        }\n        unsplit = []\n        for line in lines.splitlines(True):\n            if line[-1] in unsplit_ends:\n                unsplit.append(line)\n                continue\n\n            if unsplit:\n                unsplit.append(line)\n                line = \"\".join(unsplit)\n                unsplit = []\n\n            i = check_line(line, i)\n            if i is None:\n                break\n\n        if unsplit:\n            check_line(\"\".join(unsplit), i)", "language": "python", "code": "def check_lines(self, lines, i):\n        \"\"\"check lines have less than a maximum number of characters\n        \"\"\"\n        max_chars = self.config.max_line_length\n        ignore_long_line = self.config.ignore_long_lines\n\n        def check_line(line, i):\n            if not line.endswith(\"\\n\"):\n                self.add_message(\"missing-final-newline\", line=i)\n            else:\n                # exclude \\f (formfeed) from the rstrip\n                stripped_line = line.rstrip(\"\\t\\n\\r\\v \")\n                if not stripped_line and _EMPTY_LINE in self.config.no_space_check:\n                    # allow empty lines\n                    pass\n                elif line[len(stripped_line) :] not in (\"\\n\", \"\\r\\n\"):\n                    self.add_message(\n                        \"trailing-whitespace\", line=i, col_offset=len(stripped_line)\n                    )\n                # Don't count excess whitespace in the line length.\n                line = stripped_line\n            mobj = OPTION_RGX.search(line)\n            if mobj and \"=\" in line:\n                front_of_equal, _, back_of_equal = mobj.group(1).partition(\"=\")\n                if front_of_equal.strip() == \"disable\":\n                    if \"line-too-long\" in {\n                        _msg_id.strip() for _msg_id in back_of_equal.split(\",\")\n                    }:\n                        return None\n                    line = line.rsplit(\"#\", 1)[0].rstrip()\n\n            if len(line) > max_chars and not ignore_long_line.search(line):\n                self.add_message(\"line-too-long\", line=i, args=(len(line), max_chars))\n            return i + 1\n\n        unsplit_ends = {\n            \"\\v\",\n            \"\\x0b\",\n            \"\\f\",\n            \"\\x0c\",\n            \"\\x1c\",\n            \"\\x1d\",\n            \"\\x1e\",\n            \"\\x85\",\n            \"\\u2028\",\n            \"\\u2029\",\n        }\n        unsplit = []\n        for line in lines.splitlines(True):\n            if line[-1] in unsplit_ends:\n                unsplit.append(line)\n                continue\n\n            if unsplit:\n                unsplit.append(line)\n                line = \"\".join(unsplit)\n                unsplit = []\n\n            i = check_line(line, i)\n            if i is None:\n                break\n\n        if unsplit:\n            check_line(\"\".join(unsplit), i)", "code_tokens": ["def", "check_lines", "(", "self", ",", "lines", ",", "i", ")", ":", "max_chars", "=", "self", ".", "config", ".", "max_line_length", "ignore_long_line", "=", "self", ".", "config", ".", "ignore_long_lines", "def", "check_line", "(", "line", ",", "i", ")", ":", "if", "not", "line", ".", "endswith", "(", "\"\\n\"", ")", ":", "self", ".", "add_message", "(", "\"missing-final-newline\"", ",", "line", "=", "i", ")", "else", ":", "# exclude \\f (formfeed) from the rstrip", "stripped_line", "=", "line", ".", "rstrip", "(", "\"\\t\\n\\r\\v \"", ")", "if", "not", "stripped_line", "and", "_EMPTY_LINE", "in", "self", ".", "config", ".", "no_space_check", ":", "# allow empty lines", "pass", "elif", "line", "[", "len", "(", "stripped_line", ")", ":", "]", "not", "in", "(", "\"\\n\"", ",", "\"\\r\\n\"", ")", ":", "self", ".", "add_message", "(", "\"trailing-whitespace\"", ",", "line", "=", "i", ",", "col_offset", "=", "len", "(", "stripped_line", ")", ")", "# Don't count excess whitespace in the line length.", "line", "=", "stripped_line", "mobj", "=", "OPTION_RGX", ".", "search", "(", "line", ")", "if", "mobj", "and", "\"=\"", "in", "line", ":", "front_of_equal", ",", "_", ",", "back_of_equal", "=", "mobj", ".", "group", "(", "1", ")", ".", "partition", "(", "\"=\"", ")", "if", "front_of_equal", ".", "strip", "(", ")", "==", "\"disable\"", ":", "if", "\"line-too-long\"", "in", "{", "_msg_id", ".", "strip", "(", ")", "for", "_msg_id", "in", "back_of_equal", ".", "split", "(", "\",\"", ")", "}", ":", "return", "None", "line", "=", "line", ".", "rsplit", "(", "\"#\"", ",", "1", ")", "[", "0", "]", ".", "rstrip", "(", ")", "if", "len", "(", "line", ")", ">", "max_chars", "and", "not", "ignore_long_line", ".", "search", "(", "line", ")", ":", "self", ".", "add_message", "(", "\"line-too-long\"", ",", "line", "=", "i", ",", "args", "=", "(", "len", "(", "line", ")", ",", "max_chars", ")", ")", "return", "i", "+", "1", "unsplit_ends", "=", "{", "\"\\v\"", ",", "\"\\x0b\"", ",", "\"\\f\"", ",", "\"\\x0c\"", ",", "\"\\x1c\"", ",", "\"\\x1d\"", ",", "\"\\x1e\"", ",", "\"\\x85\"", ",", "\"\\u2028\"", ",", "\"\\u2029\"", ",", "}", "unsplit", "=", "[", "]", "for", "line", "in", "lines", ".", "splitlines", "(", "True", ")", ":", "if", "line", "[", "-", "1", "]", "in", "unsplit_ends", ":", "unsplit", ".", "append", "(", "line", ")", "continue", "if", "unsplit", ":", "unsplit", ".", "append", "(", "line", ")", "line", "=", "\"\"", ".", "join", "(", "unsplit", ")", "unsplit", "=", "[", "]", "i", "=", "check_line", "(", "line", ",", "i", ")", "if", "i", "is", "None", ":", "break", "if", "unsplit", ":", "check_line", "(", "\"\"", ".", "join", "(", "unsplit", ")", ",", "i", ")"], "docstring": "check lines have less than a maximum number of characters", "docstring_tokens": ["check", "lines", "have", "less", "than", "a", "maximum", "number", "of", "characters"], "sha": "2bf5c61a3ff6ae90613b81679de42c0f19aea600", "url": "https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/pylint/checkers/format.py#L1234-L1297", "partition": "test"}
{"repo": "hamelsmu/ktext", "path": "ktext/preprocess.py", "func_name": "processor.parallel_process_text", "original_string": "def parallel_process_text(self, data: List[str]) -> List[List[str]]:\n        \"\"\"Apply cleaner -> tokenizer.\"\"\"\n        process_text = process_text_constructor(cleaner=self.cleaner,\n                                                tokenizer=self.tokenizer,\n                                                append_indicators=self.append_indicators,\n                                                start_tok=self.start_tok,\n                                                end_tok=self.end_tok)\n        n_cores = self.num_cores\n        return flattenlist(apply_parallel(process_text, data, n_cores))", "language": "python", "code": "def parallel_process_text(self, data: List[str]) -> List[List[str]]:\n        \"\"\"Apply cleaner -> tokenizer.\"\"\"\n        process_text = process_text_constructor(cleaner=self.cleaner,\n                                                tokenizer=self.tokenizer,\n                                                append_indicators=self.append_indicators,\n                                                start_tok=self.start_tok,\n                                                end_tok=self.end_tok)\n        n_cores = self.num_cores\n        return flattenlist(apply_parallel(process_text, data, n_cores))", "code_tokens": ["def", "parallel_process_text", "(", "self", ",", "data", ":", "List", "[", "str", "]", ")", "->", "List", "[", "List", "[", "str", "]", "]", ":", "process_text", "=", "process_text_constructor", "(", "cleaner", "=", "self", ".", "cleaner", ",", "tokenizer", "=", "self", ".", "tokenizer", ",", "append_indicators", "=", "self", ".", "append_indicators", ",", "start_tok", "=", "self", ".", "start_tok", ",", "end_tok", "=", "self", ".", "end_tok", ")", "n_cores", "=", "self", ".", "num_cores", "return", "flattenlist", "(", "apply_parallel", "(", "process_text", ",", "data", ",", "n_cores", ")", ")"], "docstring": "Apply cleaner -> tokenizer.", "docstring_tokens": ["Apply", "cleaner", "-", ">", "tokenizer", "."], "sha": "221f09f5b1762705075fd1bd914881c0724d5e02", "url": "https://github.com/hamelsmu/ktext/blob/221f09f5b1762705075fd1bd914881c0724d5e02/ktext/preprocess.py#L227-L235", "partition": "test"}
{"repo": "tomMoral/loky", "path": "loky/backend/fork_exec.py", "func_name": "close_fds", "original_string": "def close_fds(keep_fds):  # pragma: no cover\n    \"\"\"Close all the file descriptors except those in keep_fds.\"\"\"\n\n    # Make sure to keep stdout and stderr open for logging purpose\n    keep_fds = set(keep_fds).union([1, 2])\n\n    # We try to retrieve all the open fds\n    try:\n        open_fds = set(int(fd) for fd in os.listdir('/proc/self/fd'))\n    except FileNotFoundError:\n        import resource\n        max_nfds = resource.getrlimit(resource.RLIMIT_NOFILE)[0]\n        open_fds = set(fd for fd in range(3, max_nfds))\n        open_fds.add(0)\n\n    for i in open_fds - keep_fds:\n        try:\n            os.close(i)\n        except OSError:\n            pass", "language": "python", "code": "def close_fds(keep_fds):  # pragma: no cover\n    \"\"\"Close all the file descriptors except those in keep_fds.\"\"\"\n\n    # Make sure to keep stdout and stderr open for logging purpose\n    keep_fds = set(keep_fds).union([1, 2])\n\n    # We try to retrieve all the open fds\n    try:\n        open_fds = set(int(fd) for fd in os.listdir('/proc/self/fd'))\n    except FileNotFoundError:\n        import resource\n        max_nfds = resource.getrlimit(resource.RLIMIT_NOFILE)[0]\n        open_fds = set(fd for fd in range(3, max_nfds))\n        open_fds.add(0)\n\n    for i in open_fds - keep_fds:\n        try:\n            os.close(i)\n        except OSError:\n            pass", "code_tokens": ["def", "close_fds", "(", "keep_fds", ")", ":", "# pragma: no cover", "# Make sure to keep stdout and stderr open for logging purpose", "keep_fds", "=", "set", "(", "keep_fds", ")", ".", "union", "(", "[", "1", ",", "2", "]", ")", "# We try to retrieve all the open fds", "try", ":", "open_fds", "=", "set", "(", "int", "(", "fd", ")", "for", "fd", "in", "os", ".", "listdir", "(", "'/proc/self/fd'", ")", ")", "except", "FileNotFoundError", ":", "import", "resource", "max_nfds", "=", "resource", ".", "getrlimit", "(", "resource", ".", "RLIMIT_NOFILE", ")", "[", "0", "]", "open_fds", "=", "set", "(", "fd", "for", "fd", "in", "range", "(", "3", ",", "max_nfds", ")", ")", "open_fds", ".", "add", "(", "0", ")", "for", "i", "in", "open_fds", "-", "keep_fds", ":", "try", ":", "os", ".", "close", "(", "i", ")", "except", "OSError", ":", "pass"], "docstring": "Close all the file descriptors except those in keep_fds.", "docstring_tokens": ["Close", "all", "the", "file", "descriptors", "except", "those", "in", "keep_fds", "."], "sha": "dc2d941d8285a96f3a5b666a4bd04875b0b25984", "url": "https://github.com/tomMoral/loky/blob/dc2d941d8285a96f3a5b666a4bd04875b0b25984/loky/backend/fork_exec.py#L14-L33", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/frontend/qt/console/pygments_highlighter.py", "func_name": "PygmentsHighlighter.set_style", "original_string": "def set_style(self, style):\n        \"\"\" Sets the style to the specified Pygments style.\n        \"\"\"\n        if isinstance(style, basestring):\n            style = get_style_by_name(style)\n        self._style = style\n        self._clear_caches()", "language": "python", "code": "def set_style(self, style):\n        \"\"\" Sets the style to the specified Pygments style.\n        \"\"\"\n        if isinstance(style, basestring):\n            style = get_style_by_name(style)\n        self._style = style\n        self._clear_caches()", "code_tokens": ["def", "set_style", "(", "self", ",", "style", ")", ":", "if", "isinstance", "(", "style", ",", "basestring", ")", ":", "style", "=", "get_style_by_name", "(", "style", ")", "self", ".", "_style", "=", "style", "self", ".", "_clear_caches", "(", ")"], "docstring": "Sets the style to the specified Pygments style.", "docstring_tokens": ["Sets", "the", "style", "to", "the", "specified", "Pygments", "style", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/frontend/qt/console/pygments_highlighter.py#L129-L135", "partition": "test"}
{"repo": "kgiusti/pyngus", "path": "pyngus/connection.py", "func_name": "Connection.create_receiver", "original_string": "def create_receiver(self, target_address, source_address=None,\n                        event_handler=None, name=None, properties=None):\n        \"\"\"Factory method for creating Receive links.\"\"\"\n        ident = name or str(target_address)\n        if ident in self._receiver_links:\n            raise KeyError(\"Receiver %s already exists!\" % ident)\n\n        session = _SessionProxy(\"session-%s\" % ident, self)\n        session.open()\n        rl = session.new_receiver(ident)\n        rl.configure(target_address, source_address, event_handler, properties)\n        self._receiver_links[ident] = rl\n        return rl", "language": "python", "code": "def create_receiver(self, target_address, source_address=None,\n                        event_handler=None, name=None, properties=None):\n        \"\"\"Factory method for creating Receive links.\"\"\"\n        ident = name or str(target_address)\n        if ident in self._receiver_links:\n            raise KeyError(\"Receiver %s already exists!\" % ident)\n\n        session = _SessionProxy(\"session-%s\" % ident, self)\n        session.open()\n        rl = session.new_receiver(ident)\n        rl.configure(target_address, source_address, event_handler, properties)\n        self._receiver_links[ident] = rl\n        return rl", "code_tokens": ["def", "create_receiver", "(", "self", ",", "target_address", ",", "source_address", "=", "None", ",", "event_handler", "=", "None", ",", "name", "=", "None", ",", "properties", "=", "None", ")", ":", "ident", "=", "name", "or", "str", "(", "target_address", ")", "if", "ident", "in", "self", ".", "_receiver_links", ":", "raise", "KeyError", "(", "\"Receiver %s already exists!\"", "%", "ident", ")", "session", "=", "_SessionProxy", "(", "\"session-%s\"", "%", "ident", ",", "self", ")", "session", ".", "open", "(", ")", "rl", "=", "session", ".", "new_receiver", "(", "ident", ")", "rl", ".", "configure", "(", "target_address", ",", "source_address", ",", "event_handler", ",", "properties", ")", "self", ".", "_receiver_links", "[", "ident", "]", "=", "rl", "return", "rl"], "docstring": "Factory method for creating Receive links.", "docstring_tokens": ["Factory", "method", "for", "creating", "Receive", "links", "."], "sha": "5392392046989f1bb84ba938c30e4d48311075f1", "url": "https://github.com/kgiusti/pyngus/blob/5392392046989f1bb84ba938c30e4d48311075f1/pyngus/connection.py#L681-L693", "partition": "test"}
{"repo": "pmacosta/peng", "path": "peng/wave_functions.py", "func_name": "atanh", "original_string": "def atanh(wave):\n    r\"\"\"\n    Return the hyperbolic arc tangent of a waveform's dependent variable vector.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.atanh\n\n    :raises:\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * ValueError (Math domain error)\n\n    .. [[[end]]]\n    \"\"\"\n    pexdoc.exh.addex(\n        ValueError,\n        \"Math domain error\",\n        bool((min(wave._dep_vector) < -1) or (max(wave._dep_vector) > 1)),\n    )\n    return _operation(wave, \"atanh\", \"\", np.arctanh)", "language": "python", "code": "def atanh(wave):\n    r\"\"\"\n    Return the hyperbolic arc tangent of a waveform's dependent variable vector.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.atanh\n\n    :raises:\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * ValueError (Math domain error)\n\n    .. [[[end]]]\n    \"\"\"\n    pexdoc.exh.addex(\n        ValueError,\n        \"Math domain error\",\n        bool((min(wave._dep_vector) < -1) or (max(wave._dep_vector) > 1)),\n    )\n    return _operation(wave, \"atanh\", \"\", np.arctanh)", "code_tokens": ["def", "atanh", "(", "wave", ")", ":", "pexdoc", ".", "exh", ".", "addex", "(", "ValueError", ",", "\"Math domain error\"", ",", "bool", "(", "(", "min", "(", "wave", ".", "_dep_vector", ")", "<", "-", "1", ")", "or", "(", "max", "(", "wave", ".", "_dep_vector", ")", ">", "1", ")", ")", ",", ")", "return", "_operation", "(", "wave", ",", "\"atanh\"", ",", "\"\"", ",", "np", ".", "arctanh", ")"], "docstring": "r\"\"\"\n    Return the hyperbolic arc tangent of a waveform's dependent variable vector.\n\n    :param wave: Waveform\n    :type  wave: :py:class:`peng.eng.Waveform`\n\n    :rtype: :py:class:`peng.eng.Waveform`\n\n    .. [[[cog cog.out(exobj_eng.get_sphinx_autodoc()) ]]]\n    .. Auto-generated exceptions documentation for\n    .. peng.wave_functions.atanh\n\n    :raises:\n     * RuntimeError (Argument \\`wave\\` is not valid)\n\n     * ValueError (Math domain error)\n\n    .. [[[end]]]", "docstring_tokens": ["r", "Return", "the", "hyperbolic", "arc", "tangent", "of", "a", "waveform", "s", "dependent", "variable", "vector", "."], "sha": "976935377adaa3de26fc5677aceb2cdfbd6f93a7", "url": "https://github.com/pmacosta/peng/blob/976935377adaa3de26fc5677aceb2cdfbd6f93a7/peng/wave_functions.py#L259-L284", "partition": "test"}
{"repo": "ekzhu/datasketch", "path": "datasketch/lshensemble_partition.py", "func_name": "_compute_nfps_uniform", "original_string": "def _compute_nfps_uniform(cum_counts, sizes):\n    \"\"\"Computes the matrix of expected false positives for all possible\n    sub-intervals of the complete domain of set sizes, assuming uniform\n    distribution of set_sizes within each sub-intervals.\n\n    Args:\n        cum_counts: the complete cummulative distribution of set sizes.\n        sizes: the complete domain of set sizes.\n\n    Return (np.array): the 2-D array of expected number of false positives\n        for every pair of [l, u] interval, where l is axis-0 and u is\n        axis-1.\n    \"\"\"\n    nfps = np.zeros((len(sizes), len(sizes)))\n    # All u an l are inclusive bounds for intervals.\n    # Compute p = 1, the NFPs\n    for l in range(len(sizes)):\n        for u in range(l, len(sizes)):\n            nfps[l, u] = _compute_nfp_uniform(l, u, cum_counts, sizes)\n    return nfps", "language": "python", "code": "def _compute_nfps_uniform(cum_counts, sizes):\n    \"\"\"Computes the matrix of expected false positives for all possible\n    sub-intervals of the complete domain of set sizes, assuming uniform\n    distribution of set_sizes within each sub-intervals.\n\n    Args:\n        cum_counts: the complete cummulative distribution of set sizes.\n        sizes: the complete domain of set sizes.\n\n    Return (np.array): the 2-D array of expected number of false positives\n        for every pair of [l, u] interval, where l is axis-0 and u is\n        axis-1.\n    \"\"\"\n    nfps = np.zeros((len(sizes), len(sizes)))\n    # All u an l are inclusive bounds for intervals.\n    # Compute p = 1, the NFPs\n    for l in range(len(sizes)):\n        for u in range(l, len(sizes)):\n            nfps[l, u] = _compute_nfp_uniform(l, u, cum_counts, sizes)\n    return nfps", "code_tokens": ["def", "_compute_nfps_uniform", "(", "cum_counts", ",", "sizes", ")", ":", "nfps", "=", "np", ".", "zeros", "(", "(", "len", "(", "sizes", ")", ",", "len", "(", "sizes", ")", ")", ")", "# All u an l are inclusive bounds for intervals.", "# Compute p = 1, the NFPs", "for", "l", "in", "range", "(", "len", "(", "sizes", ")", ")", ":", "for", "u", "in", "range", "(", "l", ",", "len", "(", "sizes", ")", ")", ":", "nfps", "[", "l", ",", "u", "]", "=", "_compute_nfp_uniform", "(", "l", ",", "u", ",", "cum_counts", ",", "sizes", ")", "return", "nfps"], "docstring": "Computes the matrix of expected false positives for all possible\n    sub-intervals of the complete domain of set sizes, assuming uniform\n    distribution of set_sizes within each sub-intervals.\n\n    Args:\n        cum_counts: the complete cummulative distribution of set sizes.\n        sizes: the complete domain of set sizes.\n\n    Return (np.array): the 2-D array of expected number of false positives\n        for every pair of [l, u] interval, where l is axis-0 and u is\n        axis-1.", "docstring_tokens": ["Computes", "the", "matrix", "of", "expected", "false", "positives", "for", "all", "possible", "sub", "-", "intervals", "of", "the", "complete", "domain", "of", "set", "sizes", "assuming", "uniform", "distribution", "of", "set_sizes", "within", "each", "sub", "-", "intervals", "."], "sha": "b3e4129987890a2beb04f2c0b6dc618ae35f2e14", "url": "https://github.com/ekzhu/datasketch/blob/b3e4129987890a2beb04f2c0b6dc618ae35f2e14/datasketch/lshensemble_partition.py#L35-L54", "partition": "test"}
{"repo": "RRZE-HPC/kerncraft", "path": "kerncraft/kerncraft.py", "func_name": "space", "original_string": "def space(start, stop, num, endpoint=True, log=False, base=10):\n    \"\"\"\n    Return list of evenly spaced integers over an interval.\n\n    Numbers can either be evenly distributed in a linear space (if *log* is False) or in a log\n    space (if *log* is True). If *log* is True, base is used to define the log space basis.\n\n    If *endpoint* is True, *stop* will be the last retruned value, as long as *num* >= 2.\n    \"\"\"\n    assert type(start) is int and type(stop) is int and type(num) is int, \\\n        \"start, stop and num need to be intergers\"\n    assert num >= 2, \"num has to be atleast 2\"\n\n    if log:\n        start = math.log(start, base)\n        stop = math.log(stop, base)\n\n    if endpoint:\n        step_length = float((stop - start)) / float(num - 1)\n    else:\n        step_length = float((stop - start)) / float(num)\n\n    i = 0\n    while i < num:\n        if log:\n            yield int(round(base ** (start + i * step_length)))\n        else:\n            yield int(round(start + i * step_length))\n        i += 1", "language": "python", "code": "def space(start, stop, num, endpoint=True, log=False, base=10):\n    \"\"\"\n    Return list of evenly spaced integers over an interval.\n\n    Numbers can either be evenly distributed in a linear space (if *log* is False) or in a log\n    space (if *log* is True). If *log* is True, base is used to define the log space basis.\n\n    If *endpoint* is True, *stop* will be the last retruned value, as long as *num* >= 2.\n    \"\"\"\n    assert type(start) is int and type(stop) is int and type(num) is int, \\\n        \"start, stop and num need to be intergers\"\n    assert num >= 2, \"num has to be atleast 2\"\n\n    if log:\n        start = math.log(start, base)\n        stop = math.log(stop, base)\n\n    if endpoint:\n        step_length = float((stop - start)) / float(num - 1)\n    else:\n        step_length = float((stop - start)) / float(num)\n\n    i = 0\n    while i < num:\n        if log:\n            yield int(round(base ** (start + i * step_length)))\n        else:\n            yield int(round(start + i * step_length))\n        i += 1", "code_tokens": ["def", "space", "(", "start", ",", "stop", ",", "num", ",", "endpoint", "=", "True", ",", "log", "=", "False", ",", "base", "=", "10", ")", ":", "assert", "type", "(", "start", ")", "is", "int", "and", "type", "(", "stop", ")", "is", "int", "and", "type", "(", "num", ")", "is", "int", ",", "\"start, stop and num need to be intergers\"", "assert", "num", ">=", "2", ",", "\"num has to be atleast 2\"", "if", "log", ":", "start", "=", "math", ".", "log", "(", "start", ",", "base", ")", "stop", "=", "math", ".", "log", "(", "stop", ",", "base", ")", "if", "endpoint", ":", "step_length", "=", "float", "(", "(", "stop", "-", "start", ")", ")", "/", "float", "(", "num", "-", "1", ")", "else", ":", "step_length", "=", "float", "(", "(", "stop", "-", "start", ")", ")", "/", "float", "(", "num", ")", "i", "=", "0", "while", "i", "<", "num", ":", "if", "log", ":", "yield", "int", "(", "round", "(", "base", "**", "(", "start", "+", "i", "*", "step_length", ")", ")", ")", "else", ":", "yield", "int", "(", "round", "(", "start", "+", "i", "*", "step_length", ")", ")", "i", "+=", "1"], "docstring": "Return list of evenly spaced integers over an interval.\n\n    Numbers can either be evenly distributed in a linear space (if *log* is False) or in a log\n    space (if *log* is True). If *log* is True, base is used to define the log space basis.\n\n    If *endpoint* is True, *stop* will be the last retruned value, as long as *num* >= 2.", "docstring_tokens": ["Return", "list", "of", "evenly", "spaced", "integers", "over", "an", "interval", "."], "sha": "c60baf8043e4da8d8d66da7575021c2f4c6c78af", "url": "https://github.com/RRZE-HPC/kerncraft/blob/c60baf8043e4da8d8d66da7575021c2f4c6c78af/kerncraft/kerncraft.py#L24-L52", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/external/pexpect/_pexpect.py", "func_name": "spawnb.next", "original_string": "def next (self):    # File-like object.\n\n        \"\"\"This is to support iterators over a file-like object.\n        \"\"\"\n\n        result = self.readline()\n        if result == self._empty_buffer:\n            raise StopIteration\n        return result", "language": "python", "code": "def next (self):    # File-like object.\n\n        \"\"\"This is to support iterators over a file-like object.\n        \"\"\"\n\n        result = self.readline()\n        if result == self._empty_buffer:\n            raise StopIteration\n        return result", "code_tokens": ["def", "next", "(", "self", ")", ":", "# File-like object.", "result", "=", "self", ".", "readline", "(", ")", "if", "result", "==", "self", ".", "_empty_buffer", ":", "raise", "StopIteration", "return", "result"], "docstring": "This is to support iterators over a file-like object.", "docstring_tokens": ["This", "is", "to", "support", "iterators", "over", "a", "file", "-", "like", "object", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/external/pexpect/_pexpect.py#L933-L941", "partition": "test"}
{"repo": "tensorflow/probability", "path": "tensorflow_probability/python/internal/special_math.py", "func_name": "erfinv", "original_string": "def erfinv(x, name=\"erfinv\"):\n  \"\"\"The inverse function for erf, the error function.\n\n  Args:\n    x: `Tensor` of type `float32`, `float64`.\n    name: Python string. A name for the operation (default=\"erfinv\").\n\n  Returns:\n    x: `Tensor` with `dtype=x.dtype`.\n\n  Raises:\n    TypeError: if `x` is not floating-type.\n  \"\"\"\n\n  with tf.name_scope(name):\n    x = tf.convert_to_tensor(value=x, name=\"x\")\n    if dtype_util.as_numpy_dtype(x.dtype) not in [np.float32, np.float64]:\n      raise TypeError(\"x.dtype={} is not handled, see docstring for supported \"\n                      \"types.\".format(dtype_util.name(x.dtype)))\n    return ndtri((x + 1.) / 2.) / np.sqrt(2.)", "language": "python", "code": "def erfinv(x, name=\"erfinv\"):\n  \"\"\"The inverse function for erf, the error function.\n\n  Args:\n    x: `Tensor` of type `float32`, `float64`.\n    name: Python string. A name for the operation (default=\"erfinv\").\n\n  Returns:\n    x: `Tensor` with `dtype=x.dtype`.\n\n  Raises:\n    TypeError: if `x` is not floating-type.\n  \"\"\"\n\n  with tf.name_scope(name):\n    x = tf.convert_to_tensor(value=x, name=\"x\")\n    if dtype_util.as_numpy_dtype(x.dtype) not in [np.float32, np.float64]:\n      raise TypeError(\"x.dtype={} is not handled, see docstring for supported \"\n                      \"types.\".format(dtype_util.name(x.dtype)))\n    return ndtri((x + 1.) / 2.) / np.sqrt(2.)", "code_tokens": ["def", "erfinv", "(", "x", ",", "name", "=", "\"erfinv\"", ")", ":", "with", "tf", ".", "name_scope", "(", "name", ")", ":", "x", "=", "tf", ".", "convert_to_tensor", "(", "value", "=", "x", ",", "name", "=", "\"x\"", ")", "if", "dtype_util", ".", "as_numpy_dtype", "(", "x", ".", "dtype", ")", "not", "in", "[", "np", ".", "float32", ",", "np", ".", "float64", "]", ":", "raise", "TypeError", "(", "\"x.dtype={} is not handled, see docstring for supported \"", "\"types.\"", ".", "format", "(", "dtype_util", ".", "name", "(", "x", ".", "dtype", ")", ")", ")", "return", "ndtri", "(", "(", "x", "+", "1.", ")", "/", "2.", ")", "/", "np", ".", "sqrt", "(", "2.", ")"], "docstring": "The inverse function for erf, the error function.\n\n  Args:\n    x: `Tensor` of type `float32`, `float64`.\n    name: Python string. A name for the operation (default=\"erfinv\").\n\n  Returns:\n    x: `Tensor` with `dtype=x.dtype`.\n\n  Raises:\n    TypeError: if `x` is not floating-type.", "docstring_tokens": ["The", "inverse", "function", "for", "erf", "the", "error", "function", "."], "sha": "e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5", "url": "https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/internal/special_math.py#L410-L429", "partition": "test"}
{"repo": "jazzband/django-ddp", "path": "dddp/api.py", "func_name": "APIMixin.clear_api_path_map_cache", "original_string": "def clear_api_path_map_cache(self):\n        \"\"\"Clear out cache for api_path_map.\"\"\"\n        self._api_path_cache = None\n        for api_provider in self.api_providers:\n            if six.get_method_self(\n                api_provider.clear_api_path_map_cache,\n            ) is not None:\n                api_provider.clear_api_path_map_cache()", "language": "python", "code": "def clear_api_path_map_cache(self):\n        \"\"\"Clear out cache for api_path_map.\"\"\"\n        self._api_path_cache = None\n        for api_provider in self.api_providers:\n            if six.get_method_self(\n                api_provider.clear_api_path_map_cache,\n            ) is not None:\n                api_provider.clear_api_path_map_cache()", "code_tokens": ["def", "clear_api_path_map_cache", "(", "self", ")", ":", "self", ".", "_api_path_cache", "=", "None", "for", "api_provider", "in", "self", ".", "api_providers", ":", "if", "six", ".", "get_method_self", "(", "api_provider", ".", "clear_api_path_map_cache", ",", ")", "is", "not", "None", ":", "api_provider", ".", "clear_api_path_map_cache", "(", ")"], "docstring": "Clear out cache for api_path_map.", "docstring_tokens": ["Clear", "out", "cache", "for", "api_path_map", "."], "sha": "1e1954b06fe140346acea43582515991685e4e01", "url": "https://github.com/jazzband/django-ddp/blob/1e1954b06fe140346acea43582515991685e4e01/dddp/api.py#L159-L166", "partition": "test"}
{"repo": "Clinical-Genomics/scout", "path": "scout/parse/ensembl.py", "func_name": "parse_ensembl_exons", "original_string": "def parse_ensembl_exons(lines):\n    \"\"\"Parse lines with ensembl formated exons\n\n        This is designed to take a biomart dump with exons from ensembl.\n        Check documentation for spec for download\n\n        Args:\n            lines(iterable(str)): An iterable with ensembl formated exons\n        Yields:\n            ensembl_gene(dict): A dictionary with the relevant information\n    \"\"\"\n    header = []\n    LOG.debug(\"Parsing ensembl exons...\")\n    for index, line in enumerate(lines):\n\n        # File allways start with a header line\n        if index == 0:\n            header = line.rstrip().split('\\t')\n            continue\n\n        exon_info = parse_ensembl_line(line, header)\n        chrom = exon_info['chrom']\n        start = exon_info['exon_start']\n        end = exon_info['exon_end']\n        transcript = exon_info['ensembl_transcript_id']\n        gene = exon_info['ensembl_gene_id']\n\n        rank = exon_info['exon_rank']\n        strand = exon_info['strand']\n\n        # Recalculate start and stop (taking UTR regions into account for end exons)\n        if strand == 1:\n            # highest position: start of exon or end of 5' UTR\n            # If no 5' UTR make sure exon_start is allways choosen\n            start = max(start, exon_info.get('utr_5_end') or -1)\n            # lowest position: end of exon or start of 3' UTR\n            end = min(end, exon_info.get('utr_3_start') or float('inf'))\n        elif strand == -1:\n            # highest position: start of exon or end of 3' UTR\n            start = max(start, exon_info.get('utr_3_end') or -1)\n            # lowest position: end of exon or start of 5' UTR\n            end = min(end, exon_info.get('utr_5_start') or float('inf'))\n\n        exon_id = \"-\".join([chrom, str(start), str(end)])\n\n        if start > end:\n            raise ValueError(\"ERROR: %s\" % exon_id)\n        data = {\n            \"exon_id\": exon_id,\n            \"chrom\": chrom,\n            \"start\": start,\n            \"end\": end,\n            \"transcript\": transcript,\n            \"gene\": gene,\n            \"rank\": rank,\n        }\n\n        yield data", "language": "python", "code": "def parse_ensembl_exons(lines):\n    \"\"\"Parse lines with ensembl formated exons\n\n        This is designed to take a biomart dump with exons from ensembl.\n        Check documentation for spec for download\n\n        Args:\n            lines(iterable(str)): An iterable with ensembl formated exons\n        Yields:\n            ensembl_gene(dict): A dictionary with the relevant information\n    \"\"\"\n    header = []\n    LOG.debug(\"Parsing ensembl exons...\")\n    for index, line in enumerate(lines):\n\n        # File allways start with a header line\n        if index == 0:\n            header = line.rstrip().split('\\t')\n            continue\n\n        exon_info = parse_ensembl_line(line, header)\n        chrom = exon_info['chrom']\n        start = exon_info['exon_start']\n        end = exon_info['exon_end']\n        transcript = exon_info['ensembl_transcript_id']\n        gene = exon_info['ensembl_gene_id']\n\n        rank = exon_info['exon_rank']\n        strand = exon_info['strand']\n\n        # Recalculate start and stop (taking UTR regions into account for end exons)\n        if strand == 1:\n            # highest position: start of exon or end of 5' UTR\n            # If no 5' UTR make sure exon_start is allways choosen\n            start = max(start, exon_info.get('utr_5_end') or -1)\n            # lowest position: end of exon or start of 3' UTR\n            end = min(end, exon_info.get('utr_3_start') or float('inf'))\n        elif strand == -1:\n            # highest position: start of exon or end of 3' UTR\n            start = max(start, exon_info.get('utr_3_end') or -1)\n            # lowest position: end of exon or start of 5' UTR\n            end = min(end, exon_info.get('utr_5_start') or float('inf'))\n\n        exon_id = \"-\".join([chrom, str(start), str(end)])\n\n        if start > end:\n            raise ValueError(\"ERROR: %s\" % exon_id)\n        data = {\n            \"exon_id\": exon_id,\n            \"chrom\": chrom,\n            \"start\": start,\n            \"end\": end,\n            \"transcript\": transcript,\n            \"gene\": gene,\n            \"rank\": rank,\n        }\n\n        yield data", "code_tokens": ["def", "parse_ensembl_exons", "(", "lines", ")", ":", "header", "=", "[", "]", "LOG", ".", "debug", "(", "\"Parsing ensembl exons...\"", ")", "for", "index", ",", "line", "in", "enumerate", "(", "lines", ")", ":", "# File allways start with a header line", "if", "index", "==", "0", ":", "header", "=", "line", ".", "rstrip", "(", ")", ".", "split", "(", "'\\t'", ")", "continue", "exon_info", "=", "parse_ensembl_line", "(", "line", ",", "header", ")", "chrom", "=", "exon_info", "[", "'chrom'", "]", "start", "=", "exon_info", "[", "'exon_start'", "]", "end", "=", "exon_info", "[", "'exon_end'", "]", "transcript", "=", "exon_info", "[", "'ensembl_transcript_id'", "]", "gene", "=", "exon_info", "[", "'ensembl_gene_id'", "]", "rank", "=", "exon_info", "[", "'exon_rank'", "]", "strand", "=", "exon_info", "[", "'strand'", "]", "# Recalculate start and stop (taking UTR regions into account for end exons)", "if", "strand", "==", "1", ":", "# highest position: start of exon or end of 5' UTR", "# If no 5' UTR make sure exon_start is allways choosen", "start", "=", "max", "(", "start", ",", "exon_info", ".", "get", "(", "'utr_5_end'", ")", "or", "-", "1", ")", "# lowest position: end of exon or start of 3' UTR", "end", "=", "min", "(", "end", ",", "exon_info", ".", "get", "(", "'utr_3_start'", ")", "or", "float", "(", "'inf'", ")", ")", "elif", "strand", "==", "-", "1", ":", "# highest position: start of exon or end of 3' UTR", "start", "=", "max", "(", "start", ",", "exon_info", ".", "get", "(", "'utr_3_end'", ")", "or", "-", "1", ")", "# lowest position: end of exon or start of 5' UTR", "end", "=", "min", "(", "end", ",", "exon_info", ".", "get", "(", "'utr_5_start'", ")", "or", "float", "(", "'inf'", ")", ")", "exon_id", "=", "\"-\"", ".", "join", "(", "[", "chrom", ",", "str", "(", "start", ")", ",", "str", "(", "end", ")", "]", ")", "if", "start", ">", "end", ":", "raise", "ValueError", "(", "\"ERROR: %s\"", "%", "exon_id", ")", "data", "=", "{", "\"exon_id\"", ":", "exon_id", ",", "\"chrom\"", ":", "chrom", ",", "\"start\"", ":", "start", ",", "\"end\"", ":", "end", ",", "\"transcript\"", ":", "transcript", ",", "\"gene\"", ":", "gene", ",", "\"rank\"", ":", "rank", ",", "}", "yield", "data"], "docstring": "Parse lines with ensembl formated exons\n\n        This is designed to take a biomart dump with exons from ensembl.\n        Check documentation for spec for download\n\n        Args:\n            lines(iterable(str)): An iterable with ensembl formated exons\n        Yields:\n            ensembl_gene(dict): A dictionary with the relevant information", "docstring_tokens": ["Parse", "lines", "with", "ensembl", "formated", "exons"], "sha": "90a551e2e1653a319e654c2405c2866f93d0ebb9", "url": "https://github.com/Clinical-Genomics/scout/blob/90a551e2e1653a319e654c2405c2866f93d0ebb9/scout/parse/ensembl.py#L280-L337", "partition": "test"}
{"repo": "ecometrica/grandfatherson", "path": "grandfatherson/__init__.py", "func_name": "dates_to_delete", "original_string": "def dates_to_delete(dates,\n                    years=0, months=0, weeks=0, days=0, firstweekday=SATURDAY,\n                    now=None):\n    \"\"\"\n    Return a set of date that should be deleted, out of ``dates``.\n\n    See ``to_keep`` for a description of arguments.\n    \"\"\"\n    dates = set(dates)\n    return dates - dates_to_keep(dates,\n                                 years=years, months=months,\n                                 weeks=weeks, days=days,\n                                 firstweekday=firstweekday, now=now)", "language": "python", "code": "def dates_to_delete(dates,\n                    years=0, months=0, weeks=0, days=0, firstweekday=SATURDAY,\n                    now=None):\n    \"\"\"\n    Return a set of date that should be deleted, out of ``dates``.\n\n    See ``to_keep`` for a description of arguments.\n    \"\"\"\n    dates = set(dates)\n    return dates - dates_to_keep(dates,\n                                 years=years, months=months,\n                                 weeks=weeks, days=days,\n                                 firstweekday=firstweekday, now=now)", "code_tokens": ["def", "dates_to_delete", "(", "dates", ",", "years", "=", "0", ",", "months", "=", "0", ",", "weeks", "=", "0", ",", "days", "=", "0", ",", "firstweekday", "=", "SATURDAY", ",", "now", "=", "None", ")", ":", "dates", "=", "set", "(", "dates", ")", "return", "dates", "-", "dates_to_keep", "(", "dates", ",", "years", "=", "years", ",", "months", "=", "months", ",", "weeks", "=", "weeks", ",", "days", "=", "days", ",", "firstweekday", "=", "firstweekday", ",", "now", "=", "now", ")"], "docstring": "Return a set of date that should be deleted, out of ``dates``.\n\n    See ``to_keep`` for a description of arguments.", "docstring_tokens": ["Return", "a", "set", "of", "date", "that", "should", "be", "deleted", "out", "of", "dates", "."], "sha": "b166e4e44887960c3066ebd28eecadfae19561e1", "url": "https://github.com/ecometrica/grandfatherson/blob/b166e4e44887960c3066ebd28eecadfae19561e1/grandfatherson/__init__.py#L210-L222", "partition": "test"}
{"repo": "solvebio/solvebio-python", "path": "solvebio/utils/tabulate.py", "func_name": "_format", "original_string": "def _format(val, valtype, floatfmt, missingval=\"\"):\n    \"\"\"\n    Format a value accoding to its type.\n\n    Unicode is supported:\n\n    >>> hrow = ['\\u0431\\u0443\\u043a\\u0432\\u0430', \\\n                '\\u0446\\u0438\\u0444\\u0440\\u0430'] ; \\\n        tbl = [['\\u0430\\u0437', 2], ['\\u0431\\u0443\\u043a\\u0438', 4]] ; \\\n        good_result = '\\\\u0431\\\\u0443\\\\u043a\\\\u0432\\\\u0430      \\\n                        \\\\u0446\\\\u0438\\\\u0444\\\\u0440\\\\u0430\\\\n-------\\\n                          -------\\\\n\\\\u0430\\\\u0437             \\\n                          2\\\\n\\\\u0431\\\\u0443\\\\u043a\\\\u0438           4' ; \\\n        tabulate(tbl, headers=hrow) == good_result\n    True\n\n    \"\"\"\n    if val is None:\n        return missingval\n\n    if valtype in [int, _binary_type, _text_type]:\n        return \"{0}\".format(val)\n    elif valtype is float:\n        return format(float(val), floatfmt)\n    else:\n        return \"{0}\".format(val)", "language": "python", "code": "def _format(val, valtype, floatfmt, missingval=\"\"):\n    \"\"\"\n    Format a value accoding to its type.\n\n    Unicode is supported:\n\n    >>> hrow = ['\\u0431\\u0443\\u043a\\u0432\\u0430', \\\n                '\\u0446\\u0438\\u0444\\u0440\\u0430'] ; \\\n        tbl = [['\\u0430\\u0437', 2], ['\\u0431\\u0443\\u043a\\u0438', 4]] ; \\\n        good_result = '\\\\u0431\\\\u0443\\\\u043a\\\\u0432\\\\u0430      \\\n                        \\\\u0446\\\\u0438\\\\u0444\\\\u0440\\\\u0430\\\\n-------\\\n                          -------\\\\n\\\\u0430\\\\u0437             \\\n                          2\\\\n\\\\u0431\\\\u0443\\\\u043a\\\\u0438           4' ; \\\n        tabulate(tbl, headers=hrow) == good_result\n    True\n\n    \"\"\"\n    if val is None:\n        return missingval\n\n    if valtype in [int, _binary_type, _text_type]:\n        return \"{0}\".format(val)\n    elif valtype is float:\n        return format(float(val), floatfmt)\n    else:\n        return \"{0}\".format(val)", "code_tokens": ["def", "_format", "(", "val", ",", "valtype", ",", "floatfmt", ",", "missingval", "=", "\"\"", ")", ":", "if", "val", "is", "None", ":", "return", "missingval", "if", "valtype", "in", "[", "int", ",", "_binary_type", ",", "_text_type", "]", ":", "return", "\"{0}\"", ".", "format", "(", "val", ")", "elif", "valtype", "is", "float", ":", "return", "format", "(", "float", "(", "val", ")", ",", "floatfmt", ")", "else", ":", "return", "\"{0}\"", ".", "format", "(", "val", ")"], "docstring": "Format a value accoding to its type.\n\n    Unicode is supported:\n\n    >>> hrow = ['\\u0431\\u0443\\u043a\\u0432\\u0430', \\\n                '\\u0446\\u0438\\u0444\\u0440\\u0430'] ; \\\n        tbl = [['\\u0430\\u0437', 2], ['\\u0431\\u0443\\u043a\\u0438', 4]] ; \\\n        good_result = '\\\\u0431\\\\u0443\\\\u043a\\\\u0432\\\\u0430      \\\n                        \\\\u0446\\\\u0438\\\\u0444\\\\u0440\\\\u0430\\\\n-------\\\n                          -------\\\\n\\\\u0430\\\\u0437             \\\n                          2\\\\n\\\\u0431\\\\u0443\\\\u043a\\\\u0438           4' ; \\\n        tabulate(tbl, headers=hrow) == good_result\n    True", "docstring_tokens": ["Format", "a", "value", "accoding", "to", "its", "type", "."], "sha": "b29614643043afd19c1d8074e8f25c6700d51a73", "url": "https://github.com/solvebio/solvebio-python/blob/b29614643043afd19c1d8074e8f25c6700d51a73/solvebio/utils/tabulate.py#L364-L389", "partition": "test"}
{"repo": "sublee/zeronimo", "path": "zeronimo/application.py", "func_name": "rpc_spec_table", "original_string": "def rpc_spec_table(app):\n    \"\"\"Collects methods which are speced as RPC.\"\"\"\n    table = {}\n    for attr, value in inspect.getmembers(app):\n        rpc_spec = get_rpc_spec(value, default=None)\n        if rpc_spec is None:\n            continue\n        table[rpc_spec.name] = (value, rpc_spec)\n    return table", "language": "python", "code": "def rpc_spec_table(app):\n    \"\"\"Collects methods which are speced as RPC.\"\"\"\n    table = {}\n    for attr, value in inspect.getmembers(app):\n        rpc_spec = get_rpc_spec(value, default=None)\n        if rpc_spec is None:\n            continue\n        table[rpc_spec.name] = (value, rpc_spec)\n    return table", "code_tokens": ["def", "rpc_spec_table", "(", "app", ")", ":", "table", "=", "{", "}", "for", "attr", ",", "value", "in", "inspect", ".", "getmembers", "(", "app", ")", ":", "rpc_spec", "=", "get_rpc_spec", "(", "value", ",", "default", "=", "None", ")", "if", "rpc_spec", "is", "None", ":", "continue", "table", "[", "rpc_spec", ".", "name", "]", "=", "(", "value", ",", "rpc_spec", ")", "return", "table"], "docstring": "Collects methods which are speced as RPC.", "docstring_tokens": ["Collects", "methods", "which", "are", "speced", "as", "RPC", "."], "sha": "b216638232932718d2cbc5eabd870c8f5b5e83fb", "url": "https://github.com/sublee/zeronimo/blob/b216638232932718d2cbc5eabd870c8f5b5e83fb/zeronimo/application.py#L70-L78", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/nbformat/v3/rwbase.py", "func_name": "NotebookReader.read", "original_string": "def read(self, fp, **kwargs):\n        \"\"\"Read a notebook from a file like object\"\"\"\n        nbs = fp.read()\n        if not py3compat.PY3 and not isinstance(nbs, unicode):\n            nbs = py3compat.str_to_unicode(nbs)\n        return self.reads(nbs, **kwargs)", "language": "python", "code": "def read(self, fp, **kwargs):\n        \"\"\"Read a notebook from a file like object\"\"\"\n        nbs = fp.read()\n        if not py3compat.PY3 and not isinstance(nbs, unicode):\n            nbs = py3compat.str_to_unicode(nbs)\n        return self.reads(nbs, **kwargs)", "code_tokens": ["def", "read", "(", "self", ",", "fp", ",", "*", "*", "kwargs", ")", ":", "nbs", "=", "fp", ".", "read", "(", ")", "if", "not", "py3compat", ".", "PY3", "and", "not", "isinstance", "(", "nbs", ",", "unicode", ")", ":", "nbs", "=", "py3compat", ".", "str_to_unicode", "(", "nbs", ")", "return", "self", ".", "reads", "(", "nbs", ",", "*", "*", "kwargs", ")"], "docstring": "Read a notebook from a file like object", "docstring_tokens": ["Read", "a", "notebook", "from", "a", "file", "like", "object"], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/nbformat/v3/rwbase.py#L166-L171", "partition": "test"}
{"repo": "Jasily/jasily-python", "path": "jasily/lang/proxy.py", "func_name": "mutable", "original_string": "def mutable(obj):\n    '''\n    return a mutable proxy for the `obj`.\n\n    all modify on the proxy will not apply on origin object.\n    '''\n    base_cls = type(obj)\n\n    class Proxy(base_cls):\n        def __getattribute__(self, name):\n            try:\n                return super().__getattribute__(name)\n            except AttributeError:\n                return getattr(obj, name)\n\n    update_wrapper(Proxy, base_cls, updated = ())\n    return Proxy()", "language": "python", "code": "def mutable(obj):\n    '''\n    return a mutable proxy for the `obj`.\n\n    all modify on the proxy will not apply on origin object.\n    '''\n    base_cls = type(obj)\n\n    class Proxy(base_cls):\n        def __getattribute__(self, name):\n            try:\n                return super().__getattribute__(name)\n            except AttributeError:\n                return getattr(obj, name)\n\n    update_wrapper(Proxy, base_cls, updated = ())\n    return Proxy()", "code_tokens": ["def", "mutable", "(", "obj", ")", ":", "base_cls", "=", "type", "(", "obj", ")", "class", "Proxy", "(", "base_cls", ")", ":", "def", "__getattribute__", "(", "self", ",", "name", ")", ":", "try", ":", "return", "super", "(", ")", ".", "__getattribute__", "(", "name", ")", "except", "AttributeError", ":", "return", "getattr", "(", "obj", ",", "name", ")", "update_wrapper", "(", "Proxy", ",", "base_cls", ",", "updated", "=", "(", ")", ")", "return", "Proxy", "(", ")"], "docstring": "return a mutable proxy for the `obj`.\n\n    all modify on the proxy will not apply on origin object.", "docstring_tokens": ["return", "a", "mutable", "proxy", "for", "the", "obj", "."], "sha": "1c821a120ebbbbc3c5761f5f1e8a73588059242a", "url": "https://github.com/Jasily/jasily-python/blob/1c821a120ebbbbc3c5761f5f1e8a73588059242a/jasily/lang/proxy.py#L12-L28", "partition": "test"}
{"repo": "SmokinCaterpillar/pypet", "path": "pypet/trajectory.py", "func_name": "Trajectory.f_set_crun", "original_string": "def f_set_crun(self, name_or_idx):\n        \"\"\"Can make the trajectory behave as during a particular single run.\n\n        It allows easier data analysis.\n\n         Has the following effects:\n\n        *\n            `v_idx` and `v_crun` are set to the appropriate index and run name\n\n        *\n            All explored parameters are set to the corresponding value in the exploration\n            ranges, i.e. when you call :func:`~pypet.parameter.Parameter.f_get` (or fast access)\n            on them you will get in return the value at the corresponding `v_idx` position\n            in the exploration range.\n\n        *\n            If you perform a search in the trajectory tree, the trajectory will\n            only search the run subtree under *results* and *derived_parameters* with the\n            corresponding index.\n            For instance, if you use `f_set_crun('run_00000007')` or `f_set_crun(7)`\n            and search for `traj.results.z` this will search for `z` only in the subtree\n            `traj.results.run_00000007`. Yet, you can still explicitly name other subtrees,\n            i.e. `traj.results.run_00000004.z` will still work.\n\n        \"\"\"\n        if (name_or_idx is None or name_or_idx == self.f_wildcard('$', -1) or\n                    name_or_idx == -1):\n            self.f_restore_default()\n        else:\n            if isinstance(name_or_idx, str):\n                self._idx = self.f_idx_to_run(name_or_idx)\n                self._crun = name_or_idx\n            else:\n                self._crun = self.f_idx_to_run(name_or_idx)\n                self._idx = name_or_idx\n\n            self._set_explored_parameters_to_idx(self.v_idx)", "language": "python", "code": "def f_set_crun(self, name_or_idx):\n        \"\"\"Can make the trajectory behave as during a particular single run.\n\n        It allows easier data analysis.\n\n         Has the following effects:\n\n        *\n            `v_idx` and `v_crun` are set to the appropriate index and run name\n\n        *\n            All explored parameters are set to the corresponding value in the exploration\n            ranges, i.e. when you call :func:`~pypet.parameter.Parameter.f_get` (or fast access)\n            on them you will get in return the value at the corresponding `v_idx` position\n            in the exploration range.\n\n        *\n            If you perform a search in the trajectory tree, the trajectory will\n            only search the run subtree under *results* and *derived_parameters* with the\n            corresponding index.\n            For instance, if you use `f_set_crun('run_00000007')` or `f_set_crun(7)`\n            and search for `traj.results.z` this will search for `z` only in the subtree\n            `traj.results.run_00000007`. Yet, you can still explicitly name other subtrees,\n            i.e. `traj.results.run_00000004.z` will still work.\n\n        \"\"\"\n        if (name_or_idx is None or name_or_idx == self.f_wildcard('$', -1) or\n                    name_or_idx == -1):\n            self.f_restore_default()\n        else:\n            if isinstance(name_or_idx, str):\n                self._idx = self.f_idx_to_run(name_or_idx)\n                self._crun = name_or_idx\n            else:\n                self._crun = self.f_idx_to_run(name_or_idx)\n                self._idx = name_or_idx\n\n            self._set_explored_parameters_to_idx(self.v_idx)", "code_tokens": ["def", "f_set_crun", "(", "self", ",", "name_or_idx", ")", ":", "if", "(", "name_or_idx", "is", "None", "or", "name_or_idx", "==", "self", ".", "f_wildcard", "(", "'$'", ",", "-", "1", ")", "or", "name_or_idx", "==", "-", "1", ")", ":", "self", ".", "f_restore_default", "(", ")", "else", ":", "if", "isinstance", "(", "name_or_idx", ",", "str", ")", ":", "self", ".", "_idx", "=", "self", ".", "f_idx_to_run", "(", "name_or_idx", ")", "self", ".", "_crun", "=", "name_or_idx", "else", ":", "self", ".", "_crun", "=", "self", ".", "f_idx_to_run", "(", "name_or_idx", ")", "self", ".", "_idx", "=", "name_or_idx", "self", ".", "_set_explored_parameters_to_idx", "(", "self", ".", "v_idx", ")"], "docstring": "Can make the trajectory behave as during a particular single run.\n\n        It allows easier data analysis.\n\n         Has the following effects:\n\n        *\n            `v_idx` and `v_crun` are set to the appropriate index and run name\n\n        *\n            All explored parameters are set to the corresponding value in the exploration\n            ranges, i.e. when you call :func:`~pypet.parameter.Parameter.f_get` (or fast access)\n            on them you will get in return the value at the corresponding `v_idx` position\n            in the exploration range.\n\n        *\n            If you perform a search in the trajectory tree, the trajectory will\n            only search the run subtree under *results* and *derived_parameters* with the\n            corresponding index.\n            For instance, if you use `f_set_crun('run_00000007')` or `f_set_crun(7)`\n            and search for `traj.results.z` this will search for `z` only in the subtree\n            `traj.results.run_00000007`. Yet, you can still explicitly name other subtrees,\n            i.e. `traj.results.run_00000004.z` will still work.", "docstring_tokens": ["Can", "make", "the", "trajectory", "behave", "as", "during", "a", "particular", "single", "run", "."], "sha": "97ad3e80d46dbdea02deeb98ea41f05a19565826", "url": "https://github.com/SmokinCaterpillar/pypet/blob/97ad3e80d46dbdea02deeb98ea41f05a19565826/pypet/trajectory.py#L586-L623", "partition": "test"}
{"repo": "neurodata/ndio", "path": "ndio/remote/data.py", "func_name": "data.get_block_size", "original_string": "def get_block_size(self, token, resolution=None):\n        \"\"\"\n        Gets the block-size for a given token at a given resolution.\n\n        Arguments:\n            token (str): The token to inspect\n            resolution (int : None): The resolution at which to inspect data.\n                If none is specified, uses the minimum available.\n\n        Returns:\n            int[3]: The xyz blocksize.\n        \"\"\"\n        cdims = self.get_metadata(token)['dataset']['cube_dimension']\n        if resolution is None:\n            resolution = min(cdims.keys())\n        return cdims[str(resolution)]", "language": "python", "code": "def get_block_size(self, token, resolution=None):\n        \"\"\"\n        Gets the block-size for a given token at a given resolution.\n\n        Arguments:\n            token (str): The token to inspect\n            resolution (int : None): The resolution at which to inspect data.\n                If none is specified, uses the minimum available.\n\n        Returns:\n            int[3]: The xyz blocksize.\n        \"\"\"\n        cdims = self.get_metadata(token)['dataset']['cube_dimension']\n        if resolution is None:\n            resolution = min(cdims.keys())\n        return cdims[str(resolution)]", "code_tokens": ["def", "get_block_size", "(", "self", ",", "token", ",", "resolution", "=", "None", ")", ":", "cdims", "=", "self", ".", "get_metadata", "(", "token", ")", "[", "'dataset'", "]", "[", "'cube_dimension'", "]", "if", "resolution", "is", "None", ":", "resolution", "=", "min", "(", "cdims", ".", "keys", "(", ")", ")", "return", "cdims", "[", "str", "(", "resolution", ")", "]"], "docstring": "Gets the block-size for a given token at a given resolution.\n\n        Arguments:\n            token (str): The token to inspect\n            resolution (int : None): The resolution at which to inspect data.\n                If none is specified, uses the minimum available.\n\n        Returns:\n            int[3]: The xyz blocksize.", "docstring_tokens": ["Gets", "the", "block", "-", "size", "for", "a", "given", "token", "at", "a", "given", "resolution", "."], "sha": "792dd5816bc770b05a3db2f4327da42ff6253531", "url": "https://github.com/neurodata/ndio/blob/792dd5816bc770b05a3db2f4327da42ff6253531/ndio/remote/data.py#L87-L102", "partition": "test"}
{"repo": "astrorafael/twisted-mqtt", "path": "mqtt/pdu.py", "func_name": "decodeLength", "original_string": "def decodeLength(encoded):\n    '''\n    Decodes a variable length value defined in the MQTT protocol.\n    This value typically represents remaining field lengths\n    '''\n    value      = 0\n    multiplier = 1\n    for i in encoded:\n        value += (i & 0x7F) * multiplier\n        multiplier *= 0x80\n        if (i & 0x80) != 0x80:\n            break\n    return value", "language": "python", "code": "def decodeLength(encoded):\n    '''\n    Decodes a variable length value defined in the MQTT protocol.\n    This value typically represents remaining field lengths\n    '''\n    value      = 0\n    multiplier = 1\n    for i in encoded:\n        value += (i & 0x7F) * multiplier\n        multiplier *= 0x80\n        if (i & 0x80) != 0x80:\n            break\n    return value", "code_tokens": ["def", "decodeLength", "(", "encoded", ")", ":", "value", "=", "0", "multiplier", "=", "1", "for", "i", "in", "encoded", ":", "value", "+=", "(", "i", "&", "0x7F", ")", "*", "multiplier", "multiplier", "*=", "0x80", "if", "(", "i", "&", "0x80", ")", "!=", "0x80", ":", "break", "return", "value"], "docstring": "Decodes a variable length value defined in the MQTT protocol.\n    This value typically represents remaining field lengths", "docstring_tokens": ["Decodes", "a", "variable", "length", "value", "defined", "in", "the", "MQTT", "protocol", ".", "This", "value", "typically", "represents", "remaining", "field", "lengths"], "sha": "5b322f7c2b82a502b1e1b70703ae45f1f668d07d", "url": "https://github.com/astrorafael/twisted-mqtt/blob/5b322f7c2b82a502b1e1b70703ae45f1f668d07d/mqtt/pdu.py#L109-L121", "partition": "test"}
{"repo": "cloud9ers/gurumate", "path": "environment/lib/python2.7/site-packages/IPython/core/shellapp.py", "func_name": "InteractiveShellApp._run_module", "original_string": "def _run_module(self):\n        \"\"\"Run module specified at the command-line.\"\"\"\n        if self.module_to_run:\n            # Make sure that the module gets a proper sys.argv as if it were\n            # run using `python -m`.\n            save_argv = sys.argv\n            sys.argv = [sys.executable] + self.extra_args\n            try:\n                self.shell.safe_run_module(self.module_to_run,\n                                           self.shell.user_ns)\n            finally:\n                sys.argv = save_argv", "language": "python", "code": "def _run_module(self):\n        \"\"\"Run module specified at the command-line.\"\"\"\n        if self.module_to_run:\n            # Make sure that the module gets a proper sys.argv as if it were\n            # run using `python -m`.\n            save_argv = sys.argv\n            sys.argv = [sys.executable] + self.extra_args\n            try:\n                self.shell.safe_run_module(self.module_to_run,\n                                           self.shell.user_ns)\n            finally:\n                sys.argv = save_argv", "code_tokens": ["def", "_run_module", "(", "self", ")", ":", "if", "self", ".", "module_to_run", ":", "# Make sure that the module gets a proper sys.argv as if it were", "# run using `python -m`.", "save_argv", "=", "sys", ".", "argv", "sys", ".", "argv", "=", "[", "sys", ".", "executable", "]", "+", "self", ".", "extra_args", "try", ":", "self", ".", "shell", ".", "safe_run_module", "(", "self", ".", "module_to_run", ",", "self", ".", "shell", ".", "user_ns", ")", "finally", ":", "sys", ".", "argv", "=", "save_argv"], "docstring": "Run module specified at the command-line.", "docstring_tokens": ["Run", "module", "specified", "at", "the", "command", "-", "line", "."], "sha": "075dc74d1ee62a8c6b7a8bf2b271364f01629d1e", "url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/core/shellapp.py#L349-L360", "partition": "test"}
{"repo": "librosa/librosa", "path": "librosa/util/utils.py", "func_name": "normalize", "original_string": "def normalize(S, norm=np.inf, axis=0, threshold=None, fill=None):\n    '''Normalize an array along a chosen axis.\n\n    Given a norm (described below) and a target axis, the input\n    array is scaled so that\n\n        `norm(S, axis=axis) == 1`\n\n    For example, `axis=0` normalizes each column of a 2-d array\n    by aggregating over the rows (0-axis).\n    Similarly, `axis=1` normalizes each row of a 2-d array.\n\n    This function also supports thresholding small-norm slices:\n    any slice (i.e., row or column) with norm below a specified\n    `threshold` can be left un-normalized, set to all-zeros, or\n    filled with uniform non-zero values that normalize to 1.\n\n    Note: the semantics of this function differ from\n    `scipy.linalg.norm` in two ways: multi-dimensional arrays\n    are supported, but matrix-norms are not.\n\n\n    Parameters\n    ----------\n    S : np.ndarray\n        The matrix to normalize\n\n    norm : {np.inf, -np.inf, 0, float > 0, None}\n        - `np.inf`  : maximum absolute value\n        - `-np.inf` : mininum absolute value\n        - `0`    : number of non-zeros (the support)\n        - float  : corresponding l_p norm\n            See `scipy.linalg.norm` for details.\n        - None : no normalization is performed\n\n    axis : int [scalar]\n        Axis along which to compute the norm.\n\n    threshold : number > 0 [optional]\n        Only the columns (or rows) with norm at least `threshold` are\n        normalized.\n\n        By default, the threshold is determined from\n        the numerical precision of `S.dtype`.\n\n    fill : None or bool\n        If None, then columns (or rows) with norm below `threshold`\n        are left as is.\n\n        If False, then columns (rows) with norm below `threshold`\n        are set to 0.\n\n        If True, then columns (rows) with norm below `threshold`\n        are filled uniformly such that the corresponding norm is 1.\n\n        .. note:: `fill=True` is incompatible with `norm=0` because\n            no uniform vector exists with l0 \"norm\" equal to 1.\n\n    Returns\n    -------\n    S_norm : np.ndarray [shape=S.shape]\n        Normalized array\n\n    Raises\n    ------\n    ParameterError\n        If `norm` is not among the valid types defined above\n\n        If `S` is not finite\n\n        If `fill=True` and `norm=0`\n\n    See Also\n    --------\n    scipy.linalg.norm\n\n    Notes\n    -----\n    This function caches at level 40.\n\n    Examples\n    --------\n    >>> # Construct an example matrix\n    >>> S = np.vander(np.arange(-2.0, 2.0))\n    >>> S\n    array([[-8.,  4., -2.,  1.],\n           [-1.,  1., -1.,  1.],\n           [ 0.,  0.,  0.,  1.],\n           [ 1.,  1.,  1.,  1.]])\n    >>> # Max (l-infinity)-normalize the columns\n    >>> librosa.util.normalize(S)\n    array([[-1.   ,  1.   , -1.   ,  1.   ],\n           [-0.125,  0.25 , -0.5  ,  1.   ],\n           [ 0.   ,  0.   ,  0.   ,  1.   ],\n           [ 0.125,  0.25 ,  0.5  ,  1.   ]])\n    >>> # Max (l-infinity)-normalize the rows\n    >>> librosa.util.normalize(S, axis=1)\n    array([[-1.   ,  0.5  , -0.25 ,  0.125],\n           [-1.   ,  1.   , -1.   ,  1.   ],\n           [ 0.   ,  0.   ,  0.   ,  1.   ],\n           [ 1.   ,  1.   ,  1.   ,  1.   ]])\n    >>> # l1-normalize the columns\n    >>> librosa.util.normalize(S, norm=1)\n    array([[-0.8  ,  0.667, -0.5  ,  0.25 ],\n           [-0.1  ,  0.167, -0.25 ,  0.25 ],\n           [ 0.   ,  0.   ,  0.   ,  0.25 ],\n           [ 0.1  ,  0.167,  0.25 ,  0.25 ]])\n    >>> # l2-normalize the columns\n    >>> librosa.util.normalize(S, norm=2)\n    array([[-0.985,  0.943, -0.816,  0.5  ],\n           [-0.123,  0.236, -0.408,  0.5  ],\n           [ 0.   ,  0.   ,  0.   ,  0.5  ],\n           [ 0.123,  0.236,  0.408,  0.5  ]])\n\n    >>> # Thresholding and filling\n    >>> S[:, -1] = 1e-308\n    >>> S\n    array([[ -8.000e+000,   4.000e+000,  -2.000e+000,\n              1.000e-308],\n           [ -1.000e+000,   1.000e+000,  -1.000e+000,\n              1.000e-308],\n           [  0.000e+000,   0.000e+000,   0.000e+000,\n              1.000e-308],\n           [  1.000e+000,   1.000e+000,   1.000e+000,\n              1.000e-308]])\n\n    >>> # By default, small-norm columns are left untouched\n    >>> librosa.util.normalize(S)\n    array([[ -1.000e+000,   1.000e+000,  -1.000e+000,\n              1.000e-308],\n           [ -1.250e-001,   2.500e-001,  -5.000e-001,\n              1.000e-308],\n           [  0.000e+000,   0.000e+000,   0.000e+000,\n              1.000e-308],\n           [  1.250e-001,   2.500e-001,   5.000e-001,\n              1.000e-308]])\n    >>> # Small-norm columns can be zeroed out\n    >>> librosa.util.normalize(S, fill=False)\n    array([[-1.   ,  1.   , -1.   ,  0.   ],\n           [-0.125,  0.25 , -0.5  ,  0.   ],\n           [ 0.   ,  0.   ,  0.   ,  0.   ],\n           [ 0.125,  0.25 ,  0.5  ,  0.   ]])\n    >>> # Or set to constant with unit-norm\n    >>> librosa.util.normalize(S, fill=True)\n    array([[-1.   ,  1.   , -1.   ,  1.   ],\n           [-0.125,  0.25 , -0.5  ,  1.   ],\n           [ 0.   ,  0.   ,  0.   ,  1.   ],\n           [ 0.125,  0.25 ,  0.5  ,  1.   ]])\n    >>> # With an l1 norm instead of max-norm\n    >>> librosa.util.normalize(S, norm=1, fill=True)\n    array([[-0.8  ,  0.667, -0.5  ,  0.25 ],\n           [-0.1  ,  0.167, -0.25 ,  0.25 ],\n           [ 0.   ,  0.   ,  0.   ,  0.25 ],\n           [ 0.1  ,  0.167,  0.25 ,  0.25 ]])\n    '''\n\n    # Avoid div-by-zero\n    if threshold is None:\n        threshold = tiny(S)\n\n    elif threshold <= 0:\n        raise ParameterError('threshold={} must be strictly '\n                             'positive'.format(threshold))\n\n    if fill not in [None, False, True]:\n        raise ParameterError('fill={} must be None or boolean'.format(fill))\n\n    if not np.all(np.isfinite(S)):\n        raise ParameterError('Input must be finite')\n\n    # All norms only depend on magnitude, let's do that first\n    mag = np.abs(S).astype(np.float)\n\n    # For max/min norms, filling with 1 works\n    fill_norm = 1\n\n    if norm == np.inf:\n        length = np.max(mag, axis=axis, keepdims=True)\n\n    elif norm == -np.inf:\n        length = np.min(mag, axis=axis, keepdims=True)\n\n    elif norm == 0:\n        if fill is True:\n            raise ParameterError('Cannot normalize with norm=0 and fill=True')\n\n        length = np.sum(mag > 0, axis=axis, keepdims=True, dtype=mag.dtype)\n\n    elif np.issubdtype(type(norm), np.number) and norm > 0:\n        length = np.sum(mag**norm, axis=axis, keepdims=True)**(1./norm)\n\n        if axis is None:\n            fill_norm = mag.size**(-1./norm)\n        else:\n            fill_norm = mag.shape[axis]**(-1./norm)\n\n    elif norm is None:\n        return S\n\n    else:\n        raise ParameterError('Unsupported norm: {}'.format(repr(norm)))\n\n    # indices where norm is below the threshold\n    small_idx = length < threshold\n\n    Snorm = np.empty_like(S)\n    if fill is None:\n        # Leave small indices un-normalized\n        length[small_idx] = 1.0\n        Snorm[:] = S / length\n\n    elif fill:\n        # If we have a non-zero fill value, we locate those entries by\n        # doing a nan-divide.\n        # If S was finite, then length is finite (except for small positions)\n        length[small_idx] = np.nan\n        Snorm[:] = S / length\n        Snorm[np.isnan(Snorm)] = fill_norm\n    else:\n        # Set small values to zero by doing an inf-divide.\n        # This is safe (by IEEE-754) as long as S is finite.\n        length[small_idx] = np.inf\n        Snorm[:] = S / length\n\n    return Snorm", "language": "python", "code": "def normalize(S, norm=np.inf, axis=0, threshold=None, fill=None):\n    '''Normalize an array along a chosen axis.\n\n    Given a norm (described below) and a target axis, the input\n    array is scaled so that\n\n        `norm(S, axis=axis) == 1`\n\n    For example, `axis=0` normalizes each column of a 2-d array\n    by aggregating over the rows (0-axis).\n    Similarly, `axis=1` normalizes each row of a 2-d array.\n\n    This function also supports thresholding small-norm slices:\n    any slice (i.e., row or column) with norm below a specified\n    `threshold` can be left un-normalized, set to all-zeros, or\n    filled with uniform non-zero values that normalize to 1.\n\n    Note: the semantics of this function differ from\n    `scipy.linalg.norm` in two ways: multi-dimensional arrays\n    are supported, but matrix-norms are not.\n\n\n    Parameters\n    ----------\n    S : np.ndarray\n        The matrix to normalize\n\n    norm : {np.inf, -np.inf, 0, float > 0, None}\n        - `np.inf`  : maximum absolute value\n        - `-np.inf` : mininum absolute value\n        - `0`    : number of non-zeros (the support)\n        - float  : corresponding l_p norm\n            See `scipy.linalg.norm` for details.\n        - None : no normalization is performed\n\n    axis : int [scalar]\n        Axis along which to compute the norm.\n\n    threshold : number > 0 [optional]\n        Only the columns (or rows) with norm at least `threshold` are\n        normalized.\n\n        By default, the threshold is determined from\n        the numerical precision of `S.dtype`.\n\n    fill : None or bool\n        If None, then columns (or rows) with norm below `threshold`\n        are left as is.\n\n        If False, then columns (rows) with norm below `threshold`\n        are set to 0.\n\n        If True, then columns (rows) with norm below `threshold`\n        are filled uniformly such that the corresponding norm is 1.\n\n        .. note:: `fill=True` is incompatible with `norm=0` because\n            no uniform vector exists with l0 \"norm\" equal to 1.\n\n    Returns\n    -------\n    S_norm : np.ndarray [shape=S.shape]\n        Normalized array\n\n    Raises\n    ------\n    ParameterError\n        If `norm` is not among the valid types defined above\n\n        If `S` is not finite\n\n        If `fill=True` and `norm=0`\n\n    See Also\n    --------\n    scipy.linalg.norm\n\n    Notes\n    -----\n    This function caches at level 40.\n\n    Examples\n    --------\n    >>> # Construct an example matrix\n    >>> S = np.vander(np.arange(-2.0, 2.0))\n    >>> S\n    array([[-8.,  4., -2.,  1.],\n           [-1.,  1., -1.,  1.],\n           [ 0.,  0.,  0.,  1.],\n           [ 1.,  1.,  1.,  1.]])\n    >>> # Max (l-infinity)-normalize the columns\n    >>> librosa.util.normalize(S)\n    array([[-1.   ,  1.   , -1.   ,  1.   ],\n           [-0.125,  0.25 , -0.5  ,  1.   ],\n           [ 0.   ,  0.   ,  0.   ,  1.   ],\n           [ 0.125,  0.25 ,  0.5  ,  1.   ]])\n    >>> # Max (l-infinity)-normalize the rows\n    >>> librosa.util.normalize(S, axis=1)\n    array([[-1.   ,  0.5  , -0.25 ,  0.125],\n           [-1.   ,  1.   , -1.   ,  1.   ],\n           [ 0.   ,  0.   ,  0.   ,  1.   ],\n           [ 1.   ,  1.   ,  1.   ,  1.   ]])\n    >>> # l1-normalize the columns\n    >>> librosa.util.normalize(S, norm=1)\n    array([[-0.8  ,  0.667, -0.5  ,  0.25 ],\n           [-0.1  ,  0.167, -0.25 ,  0.25 ],\n           [ 0.   ,  0.   ,  0.   ,  0.25 ],\n           [ 0.1  ,  0.167,  0.25 ,  0.25 ]])\n    >>> # l2-normalize the columns\n    >>> librosa.util.normalize(S, norm=2)\n    array([[-0.985,  0.943, -0.816,  0.5  ],\n           [-0.123,  0.236, -0.408,  0.5  ],\n           [ 0.   ,  0.   ,  0.   ,  0.5  ],\n           [ 0.123,  0.236,  0.408,  0.5  ]])\n\n    >>> # Thresholding and filling\n    >>> S[:, -1] = 1e-308\n    >>> S\n    array([[ -8.000e+000,   4.000e+000,  -2.000e+000,\n              1.000e-308],\n           [ -1.000e+000,   1.000e+000,  -1.000e+000,\n              1.000e-308],\n           [  0.000e+000,   0.000e+000,   0.000e+000,\n              1.000e-308],\n           [  1.000e+000,   1.000e+000,   1.000e+000,\n              1.000e-308]])\n\n    >>> # By default, small-norm columns are left untouched\n    >>> librosa.util.normalize(S)\n    array([[ -1.000e+000,   1.000e+000,  -1.000e+000,\n              1.000e-308],\n           [ -1.250e-001,   2.500e-001,  -5.000e-001,\n              1.000e-308],\n           [  0.000e+000,   0.000e+000,   0.000e+000,\n              1.000e-308],\n           [  1.250e-001,   2.500e-001,   5.000e-001,\n              1.000e-308]])\n    >>> # Small-norm columns can be zeroed out\n    >>> librosa.util.normalize(S, fill=False)\n    array([[-1.   ,  1.   , -1.   ,  0.   ],\n           [-0.125,  0.25 , -0.5  ,  0.   ],\n           [ 0.   ,  0.   ,  0.   ,  0.   ],\n           [ 0.125,  0.25 ,  0.5  ,  0.   ]])\n    >>> # Or set to constant with unit-norm\n    >>> librosa.util.normalize(S, fill=True)\n    array([[-1.   ,  1.   , -1.   ,  1.   ],\n           [-0.125,  0.25 , -0.5  ,  1.   ],\n           [ 0.   ,  0.   ,  0.   ,  1.   ],\n           [ 0.125,  0.25 ,  0.5  ,  1.   ]])\n    >>> # With an l1 norm instead of max-norm\n    >>> librosa.util.normalize(S, norm=1, fill=True)\n    array([[-0.8  ,  0.667, -0.5  ,  0.25 ],\n           [-0.1  ,  0.167, -0.25 ,  0.25 ],\n           [ 0.   ,  0.   ,  0.   ,  0.25 ],\n           [ 0.1  ,  0.167,  0.25 ,  0.25 ]])\n    '''\n\n    # Avoid div-by-zero\n    if threshold is None:\n        threshold = tiny(S)\n\n    elif threshold <= 0:\n        raise ParameterError('threshold={} must be strictly '\n                             'positive'.format(threshold))\n\n    if fill not in [None, False, True]:\n        raise ParameterError('fill={} must be None or boolean'.format(fill))\n\n    if not np.all(np.isfinite(S)):\n        raise ParameterError('Input must be finite')\n\n    # All norms only depend on magnitude, let's do that first\n    mag = np.abs(S).astype(np.float)\n\n    # For max/min norms, filling with 1 works\n    fill_norm = 1\n\n    if norm == np.inf:\n        length = np.max(mag, axis=axis, keepdims=True)\n\n    elif norm == -np.inf:\n        length = np.min(mag, axis=axis, keepdims=True)\n\n    elif norm == 0:\n        if fill is True:\n            raise ParameterError('Cannot normalize with norm=0 and fill=True')\n\n        length = np.sum(mag > 0, axis=axis, keepdims=True, dtype=mag.dtype)\n\n    elif np.issubdtype(type(norm), np.number) and norm > 0:\n        length = np.sum(mag**norm, axis=axis, keepdims=True)**(1./norm)\n\n        if axis is None:\n            fill_norm = mag.size**(-1./norm)\n        else:\n            fill_norm = mag.shape[axis]**(-1./norm)\n\n    elif norm is None:\n        return S\n\n    else:\n        raise ParameterError('Unsupported norm: {}'.format(repr(norm)))\n\n    # indices where norm is below the threshold\n    small_idx = length < threshold\n\n    Snorm = np.empty_like(S)\n    if fill is None:\n        # Leave small indices un-normalized\n        length[small_idx] = 1.0\n        Snorm[:] = S / length\n\n    elif fill:\n        # If we have a non-zero fill value, we locate those entries by\n        # doing a nan-divide.\n        # If S was finite, then length is finite (except for small positions)\n        length[small_idx] = np.nan\n        Snorm[:] = S / length\n        Snorm[np.isnan(Snorm)] = fill_norm\n    else:\n        # Set small values to zero by doing an inf-divide.\n        # This is safe (by IEEE-754) as long as S is finite.\n        length[small_idx] = np.inf\n        Snorm[:] = S / length\n\n    return Snorm", "code_tokens": ["def", "normalize", "(", "S", ",", "norm", "=", "np", ".", "inf", ",", "axis", "=", "0", ",", "threshold", "=", "None", ",", "fill", "=", "None", ")", ":", "# Avoid div-by-zero", "if", "threshold", "is", "None", ":", "threshold", "=", "tiny", "(", "S", ")", "elif", "threshold", "<=", "0", ":", "raise", "ParameterError", "(", "'threshold={} must be strictly '", "'positive'", ".", "format", "(", "threshold", ")", ")", "if", "fill", "not", "in", "[", "None", ",", "False", ",", "True", "]", ":", "raise", "ParameterError", "(", "'fill={} must be None or boolean'", ".", "format", "(", "fill", ")", ")", "if", "not", "np", ".", "all", "(", "np", ".", "isfinite", "(", "S", ")", ")", ":", "raise", "ParameterError", "(", "'Input must be finite'", ")", "# All norms only depend on magnitude, let's do that first", "mag", "=", "np", ".", "abs", "(", "S", ")", ".", "astype", "(", "np", ".", "float", ")", "# For max/min norms, filling with 1 works", "fill_norm", "=", "1", "if", "norm", "==", "np", ".", "inf", ":", "length", "=", "np", ".", "max", "(", "mag", ",", "axis", "=", "axis", ",", "keepdims", "=", "True", ")", "elif", "norm", "==", "-", "np", ".", "inf", ":", "length", "=", "np", ".", "min", "(", "mag", ",", "axis", "=", "axis", ",", "keepdims", "=", "True", ")", "elif", "norm", "==", "0", ":", "if", "fill", "is", "True", ":", "raise", "ParameterError", "(", "'Cannot normalize with norm=0 and fill=True'", ")", "length", "=", "np", ".", "sum", "(", "mag", ">", "0", ",", "axis", "=", "axis", ",", "keepdims", "=", "True", ",", "dtype", "=", "mag", ".", "dtype", ")", "elif", "np", ".", "issubdtype", "(", "type", "(", "norm", ")", ",", "np", ".", "number", ")", "and", "norm", ">", "0", ":", "length", "=", "np", ".", "sum", "(", "mag", "**", "norm", ",", "axis", "=", "axis", ",", "keepdims", "=", "True", ")", "**", "(", "1.", "/", "norm", ")", "if", "axis", "is", "None", ":", "fill_norm", "=", "mag", ".", "size", "**", "(", "-", "1.", "/", "norm", ")", "else", ":", "fill_norm", "=", "mag", ".", "shape", "[", "axis", "]", "**", "(", "-", "1.", "/", "norm", ")", "elif", "norm", "is", "None", ":", "return", "S", "else", ":", "raise", "ParameterError", "(", "'Unsupported norm: {}'", ".", "format", "(", "repr", "(", "norm", ")", ")", ")", "# indices where norm is below the threshold", "small_idx", "=", "length", "<", "threshold", "Snorm", "=", "np", ".", "empty_like", "(", "S", ")", "if", "fill", "is", "None", ":", "# Leave small indices un-normalized", "length", "[", "small_idx", "]", "=", "1.0", "Snorm", "[", ":", "]", "=", "S", "/", "length", "elif", "fill", ":", "# If we have a non-zero fill value, we locate those entries by", "# doing a nan-divide.", "# If S was finite, then length is finite (except for small positions)", "length", "[", "small_idx", "]", "=", "np", ".", "nan", "Snorm", "[", ":", "]", "=", "S", "/", "length", "Snorm", "[", "np", ".", "isnan", "(", "Snorm", ")", "]", "=", "fill_norm", "else", ":", "# Set small values to zero by doing an inf-divide.", "# This is safe (by IEEE-754) as long as S is finite.", "length", "[", "small_idx", "]", "=", "np", ".", "inf", "Snorm", "[", ":", "]", "=", "S", "/", "length", "return", "Snorm"], "docstring": "Normalize an array along a chosen axis.\n\n    Given a norm (described below) and a target axis, the input\n    array is scaled so that\n\n        `norm(S, axis=axis) == 1`\n\n    For example, `axis=0` normalizes each column of a 2-d array\n    by aggregating over the rows (0-axis).\n    Similarly, `axis=1` normalizes each row of a 2-d array.\n\n    This function also supports thresholding small-norm slices:\n    any slice (i.e., row or column) with norm below a specified\n    `threshold` can be left un-normalized, set to all-zeros, or\n    filled with uniform non-zero values that normalize to 1.\n\n    Note: the semantics of this function differ from\n    `scipy.linalg.norm` in two ways: multi-dimensional arrays\n    are supported, but matrix-norms are not.\n\n\n    Parameters\n    ----------\n    S : np.ndarray\n        The matrix to normalize\n\n    norm : {np.inf, -np.inf, 0, float > 0, None}\n        - `np.inf`  : maximum absolute value\n        - `-np.inf` : mininum absolute value\n        - `0`    : number of non-zeros (the support)\n        - float  : corresponding l_p norm\n            See `scipy.linalg.norm` for details.\n        - None : no normalization is performed\n\n    axis : int [scalar]\n        Axis along which to compute the norm.\n\n    threshold : number > 0 [optional]\n        Only the columns (or rows) with norm at least `threshold` are\n        normalized.\n\n        By default, the threshold is determined from\n        the numerical precision of `S.dtype`.\n\n    fill : None or bool\n        If None, then columns (or rows) with norm below `threshold`\n        are left as is.\n\n        If False, then columns (rows) with norm below `threshold`\n        are set to 0.\n\n        If True, then columns (rows) with norm below `threshold`\n        are filled uniformly such that the corresponding norm is 1.\n\n        .. note:: `fill=True` is incompatible with `norm=0` because\n            no uniform vector exists with l0 \"norm\" equal to 1.\n\n    Returns\n    -------\n    S_norm : np.ndarray [shape=S.shape]\n        Normalized array\n\n    Raises\n    ------\n    ParameterError\n        If `norm` is not among the valid types defined above\n\n        If `S` is not finite\n\n        If `fill=True` and `norm=0`\n\n    See Also\n    --------\n    scipy.linalg.norm\n\n    Notes\n    -----\n    This function caches at level 40.\n\n    Examples\n    --------\n    >>> # Construct an example matrix\n    >>> S = np.vander(np.arange(-2.0, 2.0))\n    >>> S\n    array([[-8.,  4., -2.,  1.],\n           [-1.,  1., -1.,  1.],\n           [ 0.,  0.,  0.,  1.],\n           [ 1.,  1.,  1.,  1.]])\n    >>> # Max (l-infinity)-normalize the columns\n    >>> librosa.util.normalize(S)\n    array([[-1.   ,  1.   , -1.   ,  1.   ],\n           [-0.125,  0.25 , -0.5  ,  1.   ],\n           [ 0.   ,  0.   ,  0.   ,  1.   ],\n           [ 0.125,  0.25 ,  0.5  ,  1.   ]])\n    >>> # Max (l-infinity)-normalize the rows\n    >>> librosa.util.normalize(S, axis=1)\n    array([[-1.   ,  0.5  , -0.25 ,  0.125],\n           [-1.   ,  1.   , -1.   ,  1.   ],\n           [ 0.   ,  0.   ,  0.   ,  1.   ],\n           [ 1.   ,  1.   ,  1.   ,  1.   ]])\n    >>> # l1-normalize the columns\n    >>> librosa.util.normalize(S, norm=1)\n    array([[-0.8  ,  0.667, -0.5  ,  0.25 ],\n           [-0.1  ,  0.167, -0.25 ,  0.25 ],\n           [ 0.   ,  0.   ,  0.   ,  0.25 ],\n           [ 0.1  ,  0.167,  0.25 ,  0.25 ]])\n    >>> # l2-normalize the columns\n    >>> librosa.util.normalize(S, norm=2)\n    array([[-0.985,  0.943, -0.816,  0.5  ],\n           [-0.123,  0.236, -0.408,  0.5  ],\n           [ 0.   ,  0.   ,  0.   ,  0.5  ],\n           [ 0.123,  0.236,  0.408,  0.5  ]])\n\n    >>> # Thresholding and filling\n    >>> S[:, -1] = 1e-308\n    >>> S\n    array([[ -8.000e+000,   4.000e+000,  -2.000e+000,\n              1.000e-308],\n           [ -1.000e+000,   1.000e+000,  -1.000e+000,\n              1.000e-308],\n           [  0.000e+000,   0.000e+000,   0.000e+000,\n              1.000e-308],\n           [  1.000e+000,   1.000e+000,   1.000e+000,\n              1.000e-308]])\n\n    >>> # By default, small-norm columns are left untouched\n    >>> librosa.util.normalize(S)\n    array([[ -1.000e+000,   1.000e+000,  -1.000e+000,\n              1.000e-308],\n           [ -1.250e-001,   2.500e-001,  -5.000e-001,\n              1.000e-308],\n           [  0.000e+000,   0.000e+000,   0.000e+000,\n              1.000e-308],\n           [  1.250e-001,   2.500e-001,   5.000e-001,\n              1.000e-308]])\n    >>> # Small-norm columns can be zeroed out\n    >>> librosa.util.normalize(S, fill=False)\n    array([[-1.   ,  1.   , -1.   ,  0.   ],\n           [-0.125,  0.25 , -0.5  ,  0.   ],\n           [ 0.   ,  0.   ,  0.   ,  0.   ],\n           [ 0.125,  0.25 ,  0.5  ,  0.   ]])\n    >>> # Or set to constant with unit-norm\n    >>> librosa.util.normalize(S, fill=True)\n    array([[-1.   ,  1.   , -1.   ,  1.   ],\n           [-0.125,  0.25 , -0.5  ,  1.   ],\n           [ 0.   ,  0.   ,  0.   ,  1.   ],\n           [ 0.125,  0.25 ,  0.5  ,  1.   ]])\n    >>> # With an l1 norm instead of max-norm\n    >>> librosa.util.normalize(S, norm=1, fill=True)\n    array([[-0.8  ,  0.667, -0.5  ,  0.25 ],\n           [-0.1  ,  0.167, -0.25 ,  0.25 ],\n           [ 0.   ,  0.   ,  0.   ,  0.25 ],\n           [ 0.1  ,  0.167,  0.25 ,  0.25 ]])", "docstring_tokens": ["Normalize", "an", "array", "along", "a", "chosen", "axis", "."], "sha": "180e8e6eb8f958fa6b20b8cba389f7945d508247", "url": "https://github.com/librosa/librosa/blob/180e8e6eb8f958fa6b20b8cba389f7945d508247/librosa/util/utils.py#L553-L777", "partition": "test"}
{"repo": "tnkteja/myhelp", "path": "virtualEnvironment/lib/python2.7/site-packages/coverage/data.py", "func_name": "CoverageData.summary", "original_string": "def summary(self, fullpath=False):\n        \"\"\"Return a dict summarizing the coverage data.\n\n        Keys are based on the filenames, and values are the number of executed\n        lines.  If `fullpath` is true, then the keys are the full pathnames of\n        the files, otherwise they are the basenames of the files.\n\n        \"\"\"\n        summ = {}\n        if fullpath:\n            filename_fn = lambda f: f\n        else:\n            filename_fn = os.path.basename\n        for filename, lines in iitems(self.lines):\n            summ[filename_fn(filename)] = len(lines)\n        return summ", "language": "python", "code": "def summary(self, fullpath=False):\n        \"\"\"Return a dict summarizing the coverage data.\n\n        Keys are based on the filenames, and values are the number of executed\n        lines.  If `fullpath` is true, then the keys are the full pathnames of\n        the files, otherwise they are the basenames of the files.\n\n        \"\"\"\n        summ = {}\n        if fullpath:\n            filename_fn = lambda f: f\n        else:\n            filename_fn = os.path.basename\n        for filename, lines in iitems(self.lines):\n            summ[filename_fn(filename)] = len(lines)\n        return summ", "code_tokens": ["def", "summary", "(", "self", ",", "fullpath", "=", "False", ")", ":", "summ", "=", "{", "}", "if", "fullpath", ":", "filename_fn", "=", "lambda", "f", ":", "f", "else", ":", "filename_fn", "=", "os", ".", "path", ".", "basename", "for", "filename", ",", "lines", "in", "iitems", "(", "self", ".", "lines", ")", ":", "summ", "[", "filename_fn", "(", "filename", ")", "]", "=", "len", "(", "lines", ")", "return", "summ"], "docstring": "Return a dict summarizing the coverage data.\n\n        Keys are based on the filenames, and values are the number of executed\n        lines.  If `fullpath` is true, then the keys are the full pathnames of\n        the files, otherwise they are the basenames of the files.", "docstring_tokens": ["Return", "a", "dict", "summarizing", "the", "coverage", "data", "."], "sha": "fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb", "url": "https://github.com/tnkteja/myhelp/blob/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb/virtualEnvironment/lib/python2.7/site-packages/coverage/data.py#L248-L263", "partition": "test"}
{"repo": "drhagen/parsita", "path": "parsita/parsers.py", "func_name": "completely_parse_reader", "original_string": "def completely_parse_reader(parser: Parser[Input, Output], reader: Reader[Input]) -> Result[Output]:\n    \"\"\"Consume reader and return Success only on complete consumption.\n\n    This is a helper function for ``parse`` methods, which return ``Success``\n    when the input is completely consumed and ``Failure`` with an appropriate\n    message otherwise.\n\n    Args:\n        parser: The parser doing the consuming\n        reader: The input being consumed\n\n    Returns:\n        A parsing ``Result``\n    \"\"\"\n    result = (parser << eof).consume(reader)\n\n    if isinstance(result, Continue):\n        return Success(result.value)\n    else:\n        used = set()\n        unique_expected = []\n        for expected_lambda in result.expected:\n            expected = expected_lambda()\n            if expected not in used:\n                used.add(expected)\n                unique_expected.append(expected)\n\n        return Failure(result.farthest.expected_error(' or '.join(unique_expected)))", "language": "python", "code": "def completely_parse_reader(parser: Parser[Input, Output], reader: Reader[Input]) -> Result[Output]:\n    \"\"\"Consume reader and return Success only on complete consumption.\n\n    This is a helper function for ``parse`` methods, which return ``Success``\n    when the input is completely consumed and ``Failure`` with an appropriate\n    message otherwise.\n\n    Args:\n        parser: The parser doing the consuming\n        reader: The input being consumed\n\n    Returns:\n        A parsing ``Result``\n    \"\"\"\n    result = (parser << eof).consume(reader)\n\n    if isinstance(result, Continue):\n        return Success(result.value)\n    else:\n        used = set()\n        unique_expected = []\n        for expected_lambda in result.expected:\n            expected = expected_lambda()\n            if expected not in used:\n                used.add(expected)\n                unique_expected.append(expected)\n\n        return Failure(result.farthest.expected_error(' or '.join(unique_expected)))", "code_tokens": ["def", "completely_parse_reader", "(", "parser", ":", "Parser", "[", "Input", ",", "Output", "]", ",", "reader", ":", "Reader", "[", "Input", "]", ")", "->", "Result", "[", "Output", "]", ":", "result", "=", "(", "parser", "<<", "eof", ")", ".", "consume", "(", "reader", ")", "if", "isinstance", "(", "result", ",", "Continue", ")", ":", "return", "Success", "(", "result", ".", "value", ")", "else", ":", "used", "=", "set", "(", ")", "unique_expected", "=", "[", "]", "for", "expected_lambda", "in", "result", ".", "expected", ":", "expected", "=", "expected_lambda", "(", ")", "if", "expected", "not", "in", "used", ":", "used", ".", "add", "(", "expected", ")", "unique_expected", ".", "append", "(", "expected", ")", "return", "Failure", "(", "result", ".", "farthest", ".", "expected_error", "(", "' or '", ".", "join", "(", "unique_expected", ")", ")", ")"], "docstring": "Consume reader and return Success only on complete consumption.\n\n    This is a helper function for ``parse`` methods, which return ``Success``\n    when the input is completely consumed and ``Failure`` with an appropriate\n    message otherwise.\n\n    Args:\n        parser: The parser doing the consuming\n        reader: The input being consumed\n\n    Returns:\n        A parsing ``Result``", "docstring_tokens": ["Consume", "reader", "and", "return", "Success", "only", "on", "complete", "consumption", "."], "sha": "d97414a05541f48231381f607d1d2e6b50781d39", "url": "https://github.com/drhagen/parsita/blob/d97414a05541f48231381f607d1d2e6b50781d39/parsita/parsers.py#L155-L182", "partition": "test"}
{"repo": "open-mmlab/mmcv", "path": "mmcv/runner/checkpoint.py", "func_name": "load_state_dict", "original_string": "def load_state_dict(module, state_dict, strict=False, logger=None):\n    \"\"\"Load state_dict to a module.\n\n    This method is modified from :meth:`torch.nn.Module.load_state_dict`.\n    Default value for ``strict`` is set to ``False`` and the message for\n    param mismatch will be shown even if strict is False.\n\n    Args:\n        module (Module): Module that receives the state_dict.\n        state_dict (OrderedDict): Weights.\n        strict (bool): whether to strictly enforce that the keys\n            in :attr:`state_dict` match the keys returned by this module's\n            :meth:`~torch.nn.Module.state_dict` function. Default: ``False``.\n        logger (:obj:`logging.Logger`, optional): Logger to log the error\n            message. If not specified, print function will be used.\n    \"\"\"\n    unexpected_keys = []\n    own_state = module.state_dict()\n    for name, param in state_dict.items():\n        if name not in own_state:\n            unexpected_keys.append(name)\n            continue\n        if isinstance(param, torch.nn.Parameter):\n            # backwards compatibility for serialized parameters\n            param = param.data\n\n        try:\n            own_state[name].copy_(param)\n        except Exception:\n            raise RuntimeError('While copying the parameter named {}, '\n                               'whose dimensions in the model are {} and '\n                               'whose dimensions in the checkpoint are {}.'\n                               .format(name, own_state[name].size(),\n                                       param.size()))\n    missing_keys = set(own_state.keys()) - set(state_dict.keys())\n\n    err_msg = []\n    if unexpected_keys:\n        err_msg.append('unexpected key in source state_dict: {}\\n'.format(\n            ', '.join(unexpected_keys)))\n    if missing_keys:\n        err_msg.append('missing keys in source state_dict: {}\\n'.format(\n            ', '.join(missing_keys)))\n    err_msg = '\\n'.join(err_msg)\n    if err_msg:\n        if strict:\n            raise RuntimeError(err_msg)\n        elif logger is not None:\n            logger.warn(err_msg)\n        else:\n            print(err_msg)", "language": "python", "code": "def load_state_dict(module, state_dict, strict=False, logger=None):\n    \"\"\"Load state_dict to a module.\n\n    This method is modified from :meth:`torch.nn.Module.load_state_dict`.\n    Default value for ``strict`` is set to ``False`` and the message for\n    param mismatch will be shown even if strict is False.\n\n    Args:\n        module (Module): Module that receives the state_dict.\n        state_dict (OrderedDict): Weights.\n        strict (bool): whether to strictly enforce that the keys\n            in :attr:`state_dict` match the keys returned by this module's\n            :meth:`~torch.nn.Module.state_dict` function. Default: ``False``.\n        logger (:obj:`logging.Logger`, optional): Logger to log the error\n            message. If not specified, print function will be used.\n    \"\"\"\n    unexpected_keys = []\n    own_state = module.state_dict()\n    for name, param in state_dict.items():\n        if name not in own_state:\n            unexpected_keys.append(name)\n            continue\n        if isinstance(param, torch.nn.Parameter):\n            # backwards compatibility for serialized parameters\n            param = param.data\n\n        try:\n            own_state[name].copy_(param)\n        except Exception:\n            raise RuntimeError('While copying the parameter named {}, '\n                               'whose dimensions in the model are {} and '\n                               'whose dimensions in the checkpoint are {}.'\n                               .format(name, own_state[name].size(),\n                                       param.size()))\n    missing_keys = set(own_state.keys()) - set(state_dict.keys())\n\n    err_msg = []\n    if unexpected_keys:\n        err_msg.append('unexpected key in source state_dict: {}\\n'.format(\n            ', '.join(unexpected_keys)))\n    if missing_keys:\n        err_msg.append('missing keys in source state_dict: {}\\n'.format(\n            ', '.join(missing_keys)))\n    err_msg = '\\n'.join(err_msg)\n    if err_msg:\n        if strict:\n            raise RuntimeError(err_msg)\n        elif logger is not None:\n            logger.warn(err_msg)\n        else:\n            print(err_msg)", "code_tokens": ["def", "load_state_dict", "(", "module", ",", "state_dict", ",", "strict", "=", "False", ",", "logger", "=", "None", ")", ":", "unexpected_keys", "=", "[", "]", "own_state", "=", "module", ".", "state_dict", "(", ")", "for", "name", ",", "param", "in", "state_dict", ".", "items", "(", ")", ":", "if", "name", "not", "in", "own_state", ":", "unexpected_keys", ".", "append", "(", "name", ")", "continue", "if", "isinstance", "(", "param", ",", "torch", ".", "nn", ".", "Parameter", ")", ":", "# backwards compatibility for serialized parameters", "param", "=", "param", ".", "data", "try", ":", "own_state", "[", "name", "]", ".", "copy_", "(", "param", ")", "except", "Exception", ":", "raise", "RuntimeError", "(", "'While copying the parameter named {}, '", "'whose dimensions in the model are {} and '", "'whose dimensions in the checkpoint are {}.'", ".", "format", "(", "name", ",", "own_state", "[", "name", "]", ".", "size", "(", ")", ",", "param", ".", "size", "(", ")", ")", ")", "missing_keys", "=", "set", "(", "own_state", ".", "keys", "(", ")", ")", "-", "set", "(", "state_dict", ".", "keys", "(", ")", ")", "err_msg", "=", "[", "]", "if", "unexpected_keys", ":", "err_msg", ".", "append", "(", "'unexpected key in source state_dict: {}\\n'", ".", "format", "(", "', '", ".", "join", "(", "unexpected_keys", ")", ")", ")", "if", "missing_keys", ":", "err_msg", ".", "append", "(", "'missing keys in source state_dict: {}\\n'", ".", "format", "(", "', '", ".", "join", "(", "missing_keys", ")", ")", ")", "err_msg", "=", "'\\n'", ".", "join", "(", "err_msg", ")", "if", "err_msg", ":", "if", "strict", ":", "raise", "RuntimeError", "(", "err_msg", ")", "elif", "logger", "is", "not", "None", ":", "logger", ".", "warn", "(", "err_msg", ")", "else", ":", "print", "(", "err_msg", ")"], "docstring": "Load state_dict to a module.\n\n    This method is modified from :meth:`torch.nn.Module.load_state_dict`.\n    Default value for ``strict`` is set to ``False`` and the message for\n    param mismatch will be shown even if strict is False.\n\n    Args:\n        module (Module): Module that receives the state_dict.\n        state_dict (OrderedDict): Weights.\n        strict (bool): whether to strictly enforce that the keys\n            in :attr:`state_dict` match the keys returned by this module's\n            :meth:`~torch.nn.Module.state_dict` function. Default: ``False``.\n        logger (:obj:`logging.Logger`, optional): Logger to log the error\n            message. If not specified, print function will be used.", "docstring_tokens": ["Load", "state_dict", "to", "a", "module", "."], "sha": "0d77f61450aab4dde8b8585a577cc496acb95d7f", "url": "https://github.com/open-mmlab/mmcv/blob/0d77f61450aab4dde8b8585a577cc496acb95d7f/mmcv/runner/checkpoint.py#L31-L81", "partition": "test"}
{"repo": "Numergy/yoda", "path": "yoda/subcommand/status.py", "func_name": "Status.print_workspace", "original_string": "def print_workspace(self, name):\n        \"\"\"Print workspace status.\"\"\"\n        path_list = find_path(name, self.config)\n\n        if len(path_list) == 0:\n            self.logger.error(\"No matches for `%s`\" % name)\n            return False\n\n        for name, path in path_list.items():\n            self.print_status(name, path)", "language": "python", "code": "def print_workspace(self, name):\n        \"\"\"Print workspace status.\"\"\"\n        path_list = find_path(name, self.config)\n\n        if len(path_list) == 0:\n            self.logger.error(\"No matches for `%s`\" % name)\n            return False\n\n        for name, path in path_list.items():\n            self.print_status(name, path)", "code_tokens": ["def", "print_workspace", "(", "self", ",", "name", ")", ":", "path_list", "=", "find_path", "(", "name", ",", "self", ".", "config", ")", "if", "len", "(", "path_list", ")", "==", "0", ":", "self", ".", "logger", ".", "error", "(", "\"No matches for `%s`\"", "%", "name", ")", "return", "False", "for", "name", ",", "path", "in", "path_list", ".", "items", "(", ")", ":", "self", ".", "print_status", "(", "name", ",", "path", ")"], "docstring": "Print workspace status.", "docstring_tokens": ["Print", "workspace", "status", "."], "sha": "109f0e9441130488b0155f05883ef6531cf46ee9", "url": "https://github.com/Numergy/yoda/blob/109f0e9441130488b0155f05883ef6531cf46ee9/yoda/subcommand/status.py#L56-L65", "partition": "test"}
{"repo": "OpenKMIP/PyKMIP", "path": "kmip/pie/client.py", "func_name": "ProxyKmipClient._build_common_attributes", "original_string": "def _build_common_attributes(self, operation_policy_name=None):\n        '''\n         Build a list of common attributes that are shared across\n         symmetric as well as asymmetric objects\n        '''\n        common_attributes = []\n\n        if operation_policy_name:\n            common_attributes.append(\n                self.attribute_factory.create_attribute(\n                    enums.AttributeType.OPERATION_POLICY_NAME,\n                    operation_policy_name\n                )\n            )\n\n        return common_attributes", "language": "python", "code": "def _build_common_attributes(self, operation_policy_name=None):\n        '''\n         Build a list of common attributes that are shared across\n         symmetric as well as asymmetric objects\n        '''\n        common_attributes = []\n\n        if operation_policy_name:\n            common_attributes.append(\n                self.attribute_factory.create_attribute(\n                    enums.AttributeType.OPERATION_POLICY_NAME,\n                    operation_policy_name\n                )\n            )\n\n        return common_attributes", "code_tokens": ["def", "_build_common_attributes", "(", "self", ",", "operation_policy_name", "=", "None", ")", ":", "common_attributes", "=", "[", "]", "if", "operation_policy_name", ":", "common_attributes", ".", "append", "(", "self", ".", "attribute_factory", ".", "create_attribute", "(", "enums", ".", "AttributeType", ".", "OPERATION_POLICY_NAME", ",", "operation_policy_name", ")", ")", "return", "common_attributes"], "docstring": "Build a list of common attributes that are shared across\n         symmetric as well as asymmetric objects", "docstring_tokens": ["Build", "a", "list", "of", "common", "attributes", "that", "are", "shared", "across", "symmetric", "as", "well", "as", "asymmetric", "objects"], "sha": "b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e", "url": "https://github.com/OpenKMIP/PyKMIP/blob/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e/kmip/pie/client.py#L1581-L1596", "partition": "test"}
{"repo": "SmokinCaterpillar/pypet", "path": "pypet/brian2/network.py", "func_name": "NetworkRunner._execute_network_run", "original_string": "def _execute_network_run(self, traj, network, network_dict, component_list,\n                             analyser_list, pre_run=False):\n        \"\"\"Generic `execute_network_run` function, handles experimental runs as well as pre-runs.\n\n        See also :func:`~pypet.brian2.network.NetworkRunner.execute_network_run` and\n         :func:`~pypet.brian2.network.NetworkRunner.execute_network_pre_run`.\n\n        \"\"\"\n\n        # Initially extract the `subrun_list`\n        subrun_list = self._extract_subruns(traj, pre_run=pre_run)\n\n        # counter for subruns\n        subrun_number = 0\n\n        # Execute all subruns in order\n        while len(subrun_list) > 0:\n\n            # Get the next subrun\n            current_subrun = subrun_list.pop(0)\n\n            # 1. Call `add` of all normal components\n            for component in component_list:\n                component.add_to_network(traj, network, current_subrun, subrun_list,\n                                         network_dict)\n\n            # 2. Call `add` of all analyser components\n            for analyser in analyser_list:\n                analyser.add_to_network(traj, network, current_subrun, subrun_list,\n                                        network_dict)\n\n            # 3. Call `add` of the network runner itself\n            self.add_to_network(traj, network, current_subrun, subrun_list,\n                                network_dict)\n\n            # 4. Run the network\n            self._logger.info('STARTING subrun `%s` (#%d) lasting %s.' %\n                             (current_subrun.v_name, subrun_number, str(current_subrun.f_get())))\n            network.run(duration=current_subrun.f_get(), report=self._report,\n                              report_period=self._report_period)\n\n            # 5. Call `analyse` of all analyser components\n            for analyser in analyser_list:\n                analyser.analyse(traj, network, current_subrun, subrun_list,\n                                 network_dict)\n\n            # 6. Call `remove` of the network runner itself\n            self.remove_from_network(traj, network, current_subrun, subrun_list,\n                                     network_dict)\n\n            # 7. Call `remove` for all analyser components\n            for analyser in analyser_list:\n                analyser.remove_from_network(traj, network, current_subrun, subrun_list,\n                                             network_dict)\n\n            # 8. Call `remove` for all normal components\n            for component in component_list:\n                component.remove_from_network(traj, network, current_subrun, subrun_list,\n                                              network_dict)\n\n\n            subrun_number += 1", "language": "python", "code": "def _execute_network_run(self, traj, network, network_dict, component_list,\n                             analyser_list, pre_run=False):\n        \"\"\"Generic `execute_network_run` function, handles experimental runs as well as pre-runs.\n\n        See also :func:`~pypet.brian2.network.NetworkRunner.execute_network_run` and\n         :func:`~pypet.brian2.network.NetworkRunner.execute_network_pre_run`.\n\n        \"\"\"\n\n        # Initially extract the `subrun_list`\n        subrun_list = self._extract_subruns(traj, pre_run=pre_run)\n\n        # counter for subruns\n        subrun_number = 0\n\n        # Execute all subruns in order\n        while len(subrun_list) > 0:\n\n            # Get the next subrun\n            current_subrun = subrun_list.pop(0)\n\n            # 1. Call `add` of all normal components\n            for component in component_list:\n                component.add_to_network(traj, network, current_subrun, subrun_list,\n                                         network_dict)\n\n            # 2. Call `add` of all analyser components\n            for analyser in analyser_list:\n                analyser.add_to_network(traj, network, current_subrun, subrun_list,\n                                        network_dict)\n\n            # 3. Call `add` of the network runner itself\n            self.add_to_network(traj, network, current_subrun, subrun_list,\n                                network_dict)\n\n            # 4. Run the network\n            self._logger.info('STARTING subrun `%s` (#%d) lasting %s.' %\n                             (current_subrun.v_name, subrun_number, str(current_subrun.f_get())))\n            network.run(duration=current_subrun.f_get(), report=self._report,\n                              report_period=self._report_period)\n\n            # 5. Call `analyse` of all analyser components\n            for analyser in analyser_list:\n                analyser.analyse(traj, network, current_subrun, subrun_list,\n                                 network_dict)\n\n            # 6. Call `remove` of the network runner itself\n            self.remove_from_network(traj, network, current_subrun, subrun_list,\n                                     network_dict)\n\n            # 7. Call `remove` for all analyser components\n            for analyser in analyser_list:\n                analyser.remove_from_network(traj, network, current_subrun, subrun_list,\n                                             network_dict)\n\n            # 8. Call `remove` for all normal components\n            for component in component_list:\n                component.remove_from_network(traj, network, current_subrun, subrun_list,\n                                              network_dict)\n\n\n            subrun_number += 1", "code_tokens": ["def", "_execute_network_run", "(", "self", ",", "traj", ",", "network", ",", "network_dict", ",", "component_list", ",", "analyser_list", ",", "pre_run", "=", "False", ")", ":", "# Initially extract the `subrun_list`", "subrun_list", "=", "self", ".", "_extract_subruns", "(", "traj", ",", "pre_run", "=", "pre_run", ")", "# counter for subruns", "subrun_number", "=", "0", "# Execute all subruns in order", "while", "len", "(", "subrun_list", ")", ">", "0", ":", "# Get the next subrun", "current_subrun", "=", "subrun_list", ".", "pop", "(", "0", ")", "# 1. Call `add` of all normal components", "for", "component", "in", "component_list", ":", "component", ".", "add_to_network", "(", "traj", ",", "network", ",", "current_subrun", ",", "subrun_list", ",", "network_dict", ")", "# 2. Call `add` of all analyser components", "for", "analyser", "in", "analyser_list", ":", "analyser", ".", "add_to_network", "(", "traj", ",", "network", ",", "current_subrun", ",", "subrun_list", ",", "network_dict", ")", "# 3. Call `add` of the network runner itself", "self", ".", "add_to_network", "(", "traj", ",", "network", ",", "current_subrun", ",", "subrun_list", ",", "network_dict", ")", "# 4. Run the network", "self", ".", "_logger", ".", "info", "(", "'STARTING subrun `%s` (#%d) lasting %s.'", "%", "(", "current_subrun", ".", "v_name", ",", "subrun_number", ",", "str", "(", "current_subrun", ".", "f_get", "(", ")", ")", ")", ")", "network", ".", "run", "(", "duration", "=", "current_subrun", ".", "f_get", "(", ")", ",", "report", "=", "self", ".", "_report", ",", "report_period", "=", "self", ".", "_report_period", ")", "# 5. Call `analyse` of all analyser components", "for", "analyser", "in", "analyser_list", ":", "analyser", ".", "analyse", "(", "traj", ",", "network", ",", "current_subrun", ",", "subrun_list", ",", "network_dict", ")", "# 6. Call `remove` of the network runner itself", "self", ".", "remove_from_network", "(", "traj", ",", "network", ",", "current_subrun", ",", "subrun_list", ",", "network_dict", ")", "# 7. Call `remove` for all analyser components", "for", "analyser", "in", "analyser_list", ":", "analyser", ".", "remove_from_network", "(", "traj", ",", "network", ",", "current_subrun", ",", "subrun_list", ",", "network_dict", ")", "# 8. Call `remove` for all normal components", "for", "component", "in", "component_list", ":", "component", ".", "remove_from_network", "(", "traj", ",", "network", ",", "current_subrun", ",", "subrun_list", ",", "network_dict", ")", "subrun_number", "+=", "1"], "docstring": "Generic `execute_network_run` function, handles experimental runs as well as pre-runs.\n\n        See also :func:`~pypet.brian2.network.NetworkRunner.execute_network_run` and\n         :func:`~pypet.brian2.network.NetworkRunner.execute_network_pre_run`.", "docstring_tokens": ["Generic", "execute_network_run", "function", "handles", "experimental", "runs", "as", "well", "as", "pre", "-", "runs", "."], "sha": "97ad3e80d46dbdea02deeb98ea41f05a19565826", "url": "https://github.com/SmokinCaterpillar/pypet/blob/97ad3e80d46dbdea02deeb98ea41f05a19565826/pypet/brian2/network.py#L422-L483", "partition": "test"}
{"repo": "mseclab/PyJFuzz", "path": "pyjfuzz/core/pjf_executor.py", "func_name": "PJFExecutor.finish_read", "original_string": "def finish_read(self, timeout=2, stdin_content=\"\", stdin=False):\n        \"\"\"\n        Wait until we got output or until timeout is over\n        \"\"\"\n        process = Thread(target=self.get_output, args=(stdin_content, stdin))\n        process.start()\n        if timeout > 0:\n            process.join(timeout)\n        else:\n            process.join()\n        if process.is_alive():\n            self.close()\n            self.return_code = -signal.SIGHUP\n        else:\n            self.return_code = self.process.returncode", "language": "python", "code": "def finish_read(self, timeout=2, stdin_content=\"\", stdin=False):\n        \"\"\"\n        Wait until we got output or until timeout is over\n        \"\"\"\n        process = Thread(target=self.get_output, args=(stdin_content, stdin))\n        process.start()\n        if timeout > 0:\n            process.join(timeout)\n        else:\n            process.join()\n        if process.is_alive():\n            self.close()\n            self.return_code = -signal.SIGHUP\n        else:\n            self.return_code = self.process.returncode", "code_tokens": ["def", "finish_read", "(", "self", ",", "timeout", "=", "2", ",", "stdin_content", "=", "\"\"", ",", "stdin", "=", "False", ")", ":", "process", "=", "Thread", "(", "target", "=", "self", ".", "get_output", ",", "args", "=", "(", "stdin_content", ",", "stdin", ")", ")", "process", ".", "start", "(", ")", "if", "timeout", ">", "0", ":", "process", ".", "join", "(", "timeout", ")", "else", ":", "process", ".", "join", "(", ")", "if", "process", ".", "is_alive", "(", ")", ":", "self", ".", "close", "(", ")", "self", ".", "return_code", "=", "-", "signal", ".", "SIGHUP", "else", ":", "self", ".", "return_code", "=", "self", ".", "process", ".", "returncode"], "docstring": "Wait until we got output or until timeout is over", "docstring_tokens": ["Wait", "until", "we", "got", "output", "or", "until", "timeout", "is", "over"], "sha": "f777067076f62c9ab74ffea6e90fd54402b7a1b4", "url": "https://github.com/mseclab/PyJFuzz/blob/f777067076f62c9ab74ffea6e90fd54402b7a1b4/pyjfuzz/core/pjf_executor.py#L90-L104", "partition": "test"}
{"repo": "NarrativeScience/lsi", "path": "src/lsi/lsi.py", "func_name": "_get_path", "original_string": "def _get_path(cmd):\n    \"\"\"Queries bash to find the path to a commmand on the system.\"\"\"\n    if cmd in _PATHS:\n        return _PATHS[cmd]\n    out = subprocess.check_output('which {}'.format(cmd), shell=True)\n    _PATHS[cmd] = out.decode(\"utf-8\").strip()\n    return _PATHS[cmd]", "language": "python", "code": "def _get_path(cmd):\n    \"\"\"Queries bash to find the path to a commmand on the system.\"\"\"\n    if cmd in _PATHS:\n        return _PATHS[cmd]\n    out = subprocess.check_output('which {}'.format(cmd), shell=True)\n    _PATHS[cmd] = out.decode(\"utf-8\").strip()\n    return _PATHS[cmd]", "code_tokens": ["def", "_get_path", "(", "cmd", ")", ":", "if", "cmd", "in", "_PATHS", ":", "return", "_PATHS", "[", "cmd", "]", "out", "=", "subprocess", ".", "check_output", "(", "'which {}'", ".", "format", "(", "cmd", ")", ",", "shell", "=", "True", ")", "_PATHS", "[", "cmd", "]", "=", "out", ".", "decode", "(", "\"utf-8\"", ")", ".", "strip", "(", ")", "return", "_PATHS", "[", "cmd", "]"], "docstring": "Queries bash to find the path to a commmand on the system.", "docstring_tokens": ["Queries", "bash", "to", "find", "the", "path", "to", "a", "commmand", "on", "the", "system", "."], "sha": "7d901b03fdb1a34ef795e5412bfe9685d948e32d", "url": "https://github.com/NarrativeScience/lsi/blob/7d901b03fdb1a34ef795e5412bfe9685d948e32d/src/lsi/lsi.py#L353-L359", "partition": "test"}
{"repo": "yandex/yandex-tank", "path": "yandextank/plugins/ResourceCheck/plugin.py", "func_name": "Plugin.__check_disk", "original_string": "def __check_disk(self):\n        ''' raise exception on disk space exceeded '''\n        cmd = \"sh -c \\\"df --no-sync -m -P -l -x fuse -x tmpfs -x devtmpfs -x davfs -x nfs \"\n        cmd += self.core.artifacts_base_dir\n        cmd += \" | tail -n 1 | awk '{print \\$4}' \\\"\"\n        res = execute(cmd, True, 0.1, True)\n        logging.debug(\"Result: %s\", res)\n        if not len(res[1]):\n            self.log.debug(\"No disk usage info: %s\", res[2])\n            return\n        disk_free = res[1]\n        self.log.debug(\n            \"Disk free space: %s/%s\", disk_free.strip(), self.disk_limit)\n        if int(disk_free.strip()) < self.disk_limit:\n            raise RuntimeError(\n                \"Not enough local resources: disk space less than %sMB in %s: %sMB\"\n                % (\n                    self.disk_limit, self.core.artifacts_base_dir,\n                    int(disk_free.strip())))", "language": "python", "code": "def __check_disk(self):\n        ''' raise exception on disk space exceeded '''\n        cmd = \"sh -c \\\"df --no-sync -m -P -l -x fuse -x tmpfs -x devtmpfs -x davfs -x nfs \"\n        cmd += self.core.artifacts_base_dir\n        cmd += \" | tail -n 1 | awk '{print \\$4}' \\\"\"\n        res = execute(cmd, True, 0.1, True)\n        logging.debug(\"Result: %s\", res)\n        if not len(res[1]):\n            self.log.debug(\"No disk usage info: %s\", res[2])\n            return\n        disk_free = res[1]\n        self.log.debug(\n            \"Disk free space: %s/%s\", disk_free.strip(), self.disk_limit)\n        if int(disk_free.strip()) < self.disk_limit:\n            raise RuntimeError(\n                \"Not enough local resources: disk space less than %sMB in %s: %sMB\"\n                % (\n                    self.disk_limit, self.core.artifacts_base_dir,\n                    int(disk_free.strip())))", "code_tokens": ["def", "__check_disk", "(", "self", ")", ":", "cmd", "=", "\"sh -c \\\"df --no-sync -m -P -l -x fuse -x tmpfs -x devtmpfs -x davfs -x nfs \"", "cmd", "+=", "self", ".", "core", ".", "artifacts_base_dir", "cmd", "+=", "\" | tail -n 1 | awk '{print \\$4}' \\\"\"", "res", "=", "execute", "(", "cmd", ",", "True", ",", "0.1", ",", "True", ")", "logging", ".", "debug", "(", "\"Result: %s\"", ",", "res", ")", "if", "not", "len", "(", "res", "[", "1", "]", ")", ":", "self", ".", "log", ".", "debug", "(", "\"No disk usage info: %s\"", ",", "res", "[", "2", "]", ")", "return", "disk_free", "=", "res", "[", "1", "]", "self", ".", "log", ".", "debug", "(", "\"Disk free space: %s/%s\"", ",", "disk_free", ".", "strip", "(", ")", ",", "self", ".", "disk_limit", ")", "if", "int", "(", "disk_free", ".", "strip", "(", ")", ")", "<", "self", ".", "disk_limit", ":", "raise", "RuntimeError", "(", "\"Not enough local resources: disk space less than %sMB in %s: %sMB\"", "%", "(", "self", ".", "disk_limit", ",", "self", ".", "core", ".", "artifacts_base_dir", ",", "int", "(", "disk_free", ".", "strip", "(", ")", ")", ")", ")"], "docstring": "raise exception on disk space exceeded", "docstring_tokens": ["raise", "exception", "on", "disk", "space", "exceeded"], "sha": "d71d63b6ab5de8b8a5ea2b728b6ab9ac0b1ba71b", "url": "https://github.com/yandex/yandex-tank/blob/d71d63b6ab5de8b8a5ea2b728b6ab9ac0b1ba71b/yandextank/plugins/ResourceCheck/plugin.py#L52-L70", "partition": "test"}
{"repo": "ubyssey/dispatch", "path": "dispatch/modules/content/models.py", "func_name": "Image.get_medium_url", "original_string": "def get_medium_url(self):\n        \"\"\"Returns the medium size image URL.\"\"\"\n        if self.is_gif():\n            return self.get_absolute_url()\n        return '%s%s-%s.jpg' % (settings.MEDIA_URL, self.get_name(), 'medium')", "language": "python", "code": "def get_medium_url(self):\n        \"\"\"Returns the medium size image URL.\"\"\"\n        if self.is_gif():\n            return self.get_absolute_url()\n        return '%s%s-%s.jpg' % (settings.MEDIA_URL, self.get_name(), 'medium')", "code_tokens": ["def", "get_medium_url", "(", "self", ")", ":", "if", "self", ".", "is_gif", "(", ")", ":", "return", "self", ".", "get_absolute_url", "(", ")", "return", "'%s%s-%s.jpg'", "%", "(", "settings", ".", "MEDIA_URL", ",", "self", ".", "get_name", "(", ")", ",", "'medium'", ")"], "docstring": "Returns the medium size image URL.", "docstring_tokens": ["Returns", "the", "medium", "size", "image", "URL", "."], "sha": "8da6084fe61726f20e9cf675190480cfc45ee764", "url": "https://github.com/ubyssey/dispatch/blob/8da6084fe61726f20e9cf675190480cfc45ee764/dispatch/modules/content/models.py#L509-L513", "partition": "test"}
